[
  {
    "name": "Yisong Miao",
    "publication_urls": [
      "https://openreview.net/forum?id=awdLPni3HM",
      "https://openreview.net/forum?id=XUJ7JvUD2P"
    ],
    "list_of_pubs": [
      {
        "title": "Discursive Socratic Questioning: Evaluating the Faithfulness of Language Models' Understanding of Discourse Relations",
        "abstract": "While large language models have significantly enhanced the effectiveness of discourse relation classifications, it remains unclear whether their comprehension is faithful and reliable. We provide DiSQ, a new method for evaluating the faithfulness of understanding discourse based on question answering. We first employ in-context learning to annotate the reasoning for discourse comprehension, based on the connections among key events within the discourse. Following this, DiSQ interrogates the model with a sequence of questions to assess its grasp of core event relations, its resilience to counterfactual queries, as well as its consistency to its previous responses. then evaluate language models with different architectural designs using DiSQ, finding: (1) DiSQ presents a significant challenge for all models, with the top-performing GPT model attaining only 41% of the ideal performance in PDTB; (2) DiSQ is robust to domain shifts and paraphrase variations; (3) Open-source models generally lag behind their closed-source GPT counterparts, with notable exceptions being those enhanced with chat and code/math features; (4) Our analysis validates the effectiveness of explicitly signalled discourse connectives, the role of contextual information, and the benefits of using historical QA data."
      },
      {
        "title": "The ELCo Dataset: Bridging Emoji and Lexical Composition",
        "abstract": "Can emojis be composed to convey intricate meanings like English phrases? As a pioneering study, we present the Emoji-Lexical Composition (ELCo) dataset, a new resource that offers parallel annotations of emoji sequences corresponding to English phrases. Our dataset contains 1,655 instances, spanning 209 diverse concepts from tangible ones like \u201cright man\u201d (\u2714\ufe0f\ud83d\udc68) to abstract ones such as \u201cfull attention\u201d (\ud83e\uddd0\u270d\ufe0f, illustrating a metaphoric composition of a focusing face and writing hand). ELCo enables the analysis of the patterns shared between emoji and lexical composition. Through a corpus study, we discovered that simple strategies like direct representation and reduplication are sufficient for conveying certain concepts, but a richer, metaphorical strategy is essential for expressing more abstract ideas. We further introduce an evaluative task, Emoji-based Textual Entailment (EmoTE), to assess the proficiency of NLP models in comprehending emoji compositions. Our findings reveals the challenge of understanding emoji composition in a zero-shot setting for current models, including ChatGPT. Our analysis indicates that the intricacy of metaphorical compositions contributes to this challenge. Encouragingly, models show marked improvement when fine-tuned on the ELCo dataset, with larger models excelling in deciphering nuanced metaphorical compositions."
      }
    ],
    "summary": "The author has made significant contributions to the fields of natural language processing (NLP) and discourse analysis through two primary research avenues: evaluating language models' understanding of discourse relations and exploring the compositionality of emojis in relation to lexical semantics.\n\n1. **Evaluation of Discourse Understanding**: The author introduced the DiSQ (Discursive Socratic Questioning) method, which assesses the faithfulness of language models' comprehension of discourse relations. This method employs in-context learning to annotate reasoning based on event connections within discourse and interrogates models through a series of questions. The findings reveal that even top-performing models struggle with achieving high fidelity in understanding discourse, with only 41% of ideal performance noted. The research highlights the robustness of DiSQ across different domains and paraphrase variations, and it underscores the importance of discourse connectives and contextual information in enhancing model performance.\n\n2. **Emoji and Lexical Composition**: The author developed the Emoji-Lexical Composition (ELCo) dataset, which bridges the gap between emoji sequences and English phrases. This pioneering dataset allows for the analysis of how emojis can convey complex meanings, revealing that while simple strategies can represent certain concepts, more abstract ideas require metaphorical compositions. The introduction of the Emoji-based Textual Entailment (EmoTE) task further assesses NLP models' abilities to understand these emoji compositions. The research indicates that current models face challenges in zero-shot settings but can improve significantly when fine-tuned on the ELCo dataset, particularly in interpreting nuanced metaphorical meanings.\n\nOverall, the author's work advances the understanding of both discourse relations in language models and the intricate role of emojis in communication, providing valuable resources and methodologies for future research in NLP."
  },
  {
    "name": "Min-Yen Kan",
    "publication_urls": [
      "https://openreview.net/forum?id=awdLPni3HM",
      "https://openreview.net/forum?id=u9Qy5iW4ue",
      "https://openreview.net/forum?id=Aq5xKDcjTJ",
      "https://arxiv.org/abs/1011.0835",
      "https://arxiv.org/abs/2005.04364"
    ],
    "list_of_pubs": [
      {
        "title": "Discursive Socratic Questioning: Evaluating the Faithfulness of Language Models' Understanding of Discourse Relations",
        "abstract": "While large language models have significantly enhanced the effectiveness of discourse relation classifications, it remains unclear whether their comprehension is faithful and reliable. We provide DiSQ, a new method for evaluating the faithfulness of understanding discourse based on question answering. We first employ in-context learning to annotate the reasoning for discourse comprehension, based on the connections among key events within the discourse. Following this, DiSQ interrogates the model with a sequence of questions to assess its grasp of core event relations, its resilience to counterfactual queries, as well as its consistency to its previous responses. then evaluate language models with different architectural designs using DiSQ, finding: (1) DiSQ presents a significant challenge for all models, with the top-performing GPT model attaining only 41% of the ideal performance in PDTB; (2) DiSQ is robust to domain shifts and paraphrase variations; (3) Open-source models generally lag behind their closed-source GPT counterparts, with notable exceptions being those enhanced with chat and code/math features; (4) Our analysis validates the effectiveness of explicitly signalled discourse connectives, the role of contextual information, and the benefits of using historical QA data."
      },
      {
        "title": "Workshop on Reasoning and Planning for Large Language Models",
        "abstract": "This workshop explores the growing capabilities of large language models (LLMs), such as OpenAI's o1 model, in reasoning, planning, and decision-making, highlighting recent advances and challenges. We aim to examine how reinforcement learning methods, post-training optimization, and efficient inference techniques can further enhance LLMs' reasoning capabilities. Topics include training approach for enhancing reasoning and planning abilities, scaling inference for complex tasks, developing robust benchmarks, and extending LLMs to multi-modal and embodied environments. We will also discuss broader themes such as causal reasoning, collaborative multi-agent systems, uncertainty, and explainability to offer insights and guidance for the further development of reasoning and planning in LLMs."
      },
      {
        "title": "COrAL: Order-Agnostic Language Modeling for Efficient Iterative Refinement",
        "abstract": "Iterative refinement has emerged as an effective paradigm for enhancing the capabilities of large language models (LLMs) on complex tasks. However, existing approaches typically implement iterative refinement at the application or prompting level, relying on autoregressive (AR) modeling. The sequential token generation in AR models can lead to high inference latency. To overcome these challenges, we propose **C**ontext-Wise **Or**der-**A**gnostic **L**anguage Modeling (COrAL), which incorporates iterative refinement directly into the LLM architecture while maintaining computational efficiency. Our approach models multiple token dependencies within manageable context windows, enabling the model to perform iterative refinement internally during the generation process. Leveraging the order-agnostic nature of COrAL, we introduce sliding blockwise order-agnostic decoding, which performs multi-token forward prediction and backward reconstruction within context windows. This allows the model to iteratively refine its outputs in parallel in the sliding block, effectively capturing diverse dependencies without the high inference cost of sequential generation. Our findings reveal a quality--speed trade-off, elucidating how COrAL effectively augments the self-enhancement capabilities of conventional autoregressive models without necessitating additional architectural components or extensive pre-training. This work underscores the promise of order-agnostic modeling in advancing LLMs for more efficient and effective natural language processing."
      },
      {
        "title": "A PDTB-Styled End-to-End Discourse Parser",
        "abstract": "We have developed a full discourse parser in the Penn Discourse Treebank (PDTB) style. Our trained parser first identifies all discourse and non-discourse relations, locates and labels their arguments, and then classifies their relation types. When appropriate, the attribution spans to these relations are also determined. We present a comprehensive evaluation from both component-wise and error-cascading perspectives."
      },
      {
        "title": "It's Morphin' Time! Combating Linguistic Discrimination with Inflectional Perturbations",
        "abstract": "Training on only perfect Standard English corpora predisposes pre-trained neural networks to discriminate against minorities from non-standard linguistic backgrounds (e.g., African American Vernacular English, Colloquial Singapore English, etc.). We perturb the inflectional morphology of words to craft plausible and semantically similar adversarial examples that expose these biases in popular NLP models, e.g., BERT and Transformer, and show that adversarially fine-tuning them for a single epoch significantly improves robustness without sacrificing performance on clean data."
      }
    ],
    "summary": "The author has made significant contributions to the field of natural language processing (NLP) and large language models (LLMs) through a series of innovative methodologies and evaluations. Their research can be summarized as follows:\n\n1. **Evaluation of Discourse Understanding**: The author introduced the DiSQ method, which assesses the faithfulness of language models' understanding of discourse relations through a structured questioning approach. This work highlights the limitations of current models, revealing that even top-performing models struggle with discourse comprehension, and emphasizes the importance of discourse connectives and contextual information in enhancing model performance.\n\n2. **Advancements in Reasoning and Planning**: Through organizing a workshop focused on reasoning and planning for LLMs, the author has contributed to the discourse on improving LLM capabilities in complex reasoning tasks. This includes exploring reinforcement learning, post-training optimization, and the development of robust benchmarks, which are crucial for advancing the field.\n\n3. **Innovative Language Modeling Techniques**: The introduction of COrAL (Context-Wise Order-Agnostic Language Modeling) represents a significant advancement in iterative refinement techniques for LLMs. By enabling order-agnostic modeling, the author has addressed the inefficiencies of autoregressive models, allowing for parallel processing and improved inference speed while maintaining output quality.\n\n4. **Development of Discourse Parsing Tools**: The author has created a comprehensive end-to-end discourse parser based on the Penn Discourse Treebank (PDTB) framework. This parser not only identifies and classifies discourse relations but also evaluates its performance from multiple perspectives, contributing to the tools available for discourse analysis in NLP.\n\n5. **Addressing Linguistic Discrimination**: The research on combating linguistic discrimination through inflectional perturbations demonstrates the author's commitment to ethical considerations in NLP. By exposing biases in popular models and proposing adversarial fine-tuning techniques, the author has contributed to making NLP systems more robust and equitable for speakers of non-standard dialects.\n\nOverall, the author's work spans theoretical advancements, practical tools, and ethical considerations in NLP, positioning them as a key contributor to the ongoing development and evaluation of language models and their applications."
  },
  {
    "name": "Minzhi Li",
    "publication_urls": [
      "https://openreview.net/forum?id=SlsZZ25InC",
      "https://openreview.net/forum?id=CsoSWpR5xC",
      "https://openreview.net/forum?id=yuuyPlywuO",
      "https://openreview.net/forum?id=tL7hS11keH",
      "https://arxiv.org/abs/2204.02952"
    ],
    "list_of_pubs": [
      {
        "title": "A Survey of Frontiers in LLM Reasoning: Inference Scaling, Learning to Reason, and Agentic Systems",
        "abstract": "Reasoning is a fundamental cognitive process that enables logical inference, problem-solving, and decision-making. With the rapid advancement of large language models (LLMs), reasoning has emerged as a key capability that distinguishes advanced AI systems from conventional models that empower chatbots. In this survey, we categorize existing methods along two orthogonal dimensions: (1) Regimes, which define the stage at which reasoning is achieved (either at inference time or through dedicated training); and (2) Architectures, which determine the components involved in the reasoning process, distinguishing between standalone LLMs and agentic compound systems that incorporate external tools, and multiagent collaborations. Within each dimension, we analyze two key perspectives: (1) Input level, which focuses on techniques that construct high-quality prompts that the LLM condition on; and (2) Output level, which methods that refine multiple sampled candidates to enhance reasoning quality. This categorization provides a systematic understanding of the evolving landscape of LLM reasoning, highlighting emerging trends such as the shift from inference-scaling to learning-to-reason (e.g., DeepSeek-R1), and the transition to agentic workflows (e.g., OpenAI Deep Research, Manus Agent). Additionally, we cover a broad spectrum of learning algorithms, from supervised fine-tuning to reinforcement learning such as PPO and GRPO, and the training of reasoners and verifiers. We also examine key designs of agentic workflows, from established patterns like generator-evaluator and LLM debate to recent innovations. Finally, we identify emerging trends, such as domain-specific reasoning systems, and open challenges, such as evaluation and data quality. This survey aims to provide AI researchers and practitioners with a comprehensive foundation for advancing reasoning in LLMs, paving the way for more sophisticated and reliable AI systems."
      },
      {
        "title": "A Survey on Large Language Model-Based Social Agents in Game-Theoretic Scenarios",
        "abstract": "Game-theoretic scenarios have become pivotal in evaluating the social intelligence of Large Language Model (LLM)-based social agents. While numerous studies have explored these agents in such settings, there is a lack of a comprehensive survey summarizing the current progress. To address this gap, we systematically review existing research on LLM-based social agents within game-theoretic scenarios. Our survey organizes the findings into three core components: Game Framework, Social Agent, and Evaluation Protocol. The game framework encompasses diverse game scenarios, ranging from choice-focusing to communication-focusing games. The social agent part explores agents' preferences, beliefs, and reasoning abilities, as well as their interactions and synergistic effects on decision-making. The evaluation protocol covers both game-agnostic and game-specific metrics for assessing agent performance. Additionally, we analyze the performance of current social agents across various game scenarios. By reflecting on the current research and identifying future research directions, this survey provides insights to advance the development and evaluation of social agents in game-theoretic scenarios."
      },
      {
        "title": "Distilling an End-to-End Voice Assistant Without Instruction Training Data",
        "abstract": "Voice assistants, such as Siri and Google Assistant, typically model audio and text separately, resulting in lost speech information and increased complexity. Recent efforts to address this with end-to-end Speech Large Language Models (LLMs) trained with supervised finetuning (SFT) have led to models ``forgetting\" capabilities from text-only LLMs. Our work proposes an alternative paradigm for training Speech LLMs without instruction data, using the response of a text-only LLM to transcripts as self-supervision. Importantly, this process can be performed without annotated responses. We show that our Distilled Voice Assistant (DiVA) generalizes to Spoken Question Answering, Classification, and Translation. Furthermore, we show that DiVA better meets user preferences, achieving a 72\\% win rate compared with state-of-the-art models like Qwen 2 Audio, despite using $>$100x less training compute."
      },
      {
        "title": "CoAnnotating: Uncertainty-Guided Work Allocation between Human and Large Language Models for Data Annotation",
        "abstract": "Annotated data plays a critical role in Natural Language Processing (NLP) in training models and evaluating their performance. Given recent developments in Large Language Models (LLMs), models such as ChatGPT demonstrate zero-shot capability on many text-annotation tasks, comparable with or even exceeding human annotators. Such LLMs can serve as alternatives for manual annotation, due to lower costs and higher scalability. However, limited work has leveraged LLMs as complementary annotators, nor explored how annotation work is best allocated among humans and LLMs to achieve both quality and cost objectives. We propose CoAnnotating, a novel paradigm for Human-LLM co-annotation of unstructured texts at scale. Under this framework, we utilize uncertainty to estimate LLMs' annotation capability. Our empirical study shows CoAnnotating to be an effective means to allocate work from results on different datasets, with up to 21% performance improvement over random baseline. For code implementation, see https://github.com/SALT-NLP/CoAnnotating."
      },
      {
        "title": "Inducing Positive Perspectives with Text Reframing",
        "abstract": "Sentiment transfer is one popular example of a text style transfer task, where the goal is to reverse the sentiment polarity of a text. With a sentiment reversal comes also a reversal in meaning. We introduce a different but related task called positive reframing in which we neutralize a negative point of view and generate a more positive perspective for the author without contradicting the original meaning. Our insistence on meaning preservation makes positive reframing a challenging and semantically rich task. To facilitate rapid progress, we introduce a large-scale benchmark, Positive Psychology Frames, with 8,349 sentence pairs and 12,755 structured annotations to explain positive reframing in terms of six theoretically-motivated reframing strategies. Then we evaluate a set of state-of-the-art text style transfer models, and conclude by discussing key challenges and directions for future work."
      }
    ],
    "summary": "The author has made significant contributions to the field of artificial intelligence, particularly in the areas of large language models (LLMs), reasoning, social agents, voice assistants, data annotation, and sentiment analysis. Their research can be summarized as follows:\n\n1. **Advancements in LLM Reasoning**: The author has conducted a comprehensive survey on the reasoning capabilities of LLMs, categorizing methods based on inference regimes and architectural designs. This work highlights the evolution from inference-scaling to learning-to-reason approaches and the development of agentic systems that utilize external tools and multiagent collaborations. The survey identifies key trends, challenges, and future directions for enhancing reasoning in AI systems.\n\n2. **Game-Theoretic Social Agents**: The author has systematically reviewed the application of LLMs in game-theoretic scenarios, providing a structured analysis of game frameworks, social agent characteristics, and evaluation protocols. This work addresses the need for a comprehensive understanding of how LLM-based agents perform in social intelligence tasks, paving the way for improved design and assessment of these agents.\n\n3. **End-to-End Voice Assistants**: The author has proposed a novel training paradigm for voice assistants that eliminates the need for instruction training data. By leveraging self-supervision from text-only LLMs, their Distilled Voice Assistant (DiVA) demonstrates superior performance in various tasks while significantly reducing training compute requirements. This work contributes to the development of more efficient and effective voice interaction systems.\n\n4. **Human-LLM Co-Annotation**: The introduction of the CoAnnotating framework represents a significant advancement in data annotation practices. By utilizing uncertainty to guide the allocation of annotation tasks between humans and LLMs, the author has shown that this approach can enhance annotation quality and efficiency, offering a scalable solution for NLP tasks.\n\n5. **Positive Text Reframing**: The author has explored the concept of positive reframing in sentiment analysis, introducing a new benchmark for evaluating models on this task. By focusing on meaning preservation while altering sentiment, this research addresses a complex challenge in text style transfer and provides a foundation for future work in sentiment manipulation.\n\nOverall, the author's research contributions span a diverse range of topics within AI, emphasizing the integration of LLMs in reasoning, social interaction, and practical applications, while also addressing methodological advancements in data annotation and sentiment analysis."
  },
  {
    "name": "Neel Nanda",
    "publication_urls": [
      "https://openreview.net/forum?id=iSHcmOjrvY",
      "https://openreview.net/forum?id=VUhRdZp8ke",
      "https://openreview.net/forum?id=mBK25ywGVZ",
      "https://openreview.net/forum?id=98tGjkTlC4",
      "https://openreview.net/forum?id=n9oeNUgm5Q"
    ],
    "list_of_pubs": [
      {
        "title": "Model Organisms for Emergent Misalignment",
        "abstract": "Recent work discovered Emergent Misalignment (EM): fine-tuning large language models on narrowly harmful datasets can lead them to become broadly misaligned. A survey of experts prior to publication revealed that this was highly unexpected, demonstrating critical gaps in our understanding of model alignment. In this work, we advance understanding of this phenomena and provide tools for future research. Using new narrowly misaligned datasets, we create improved model organisms that achieve 99\\% coherence (vs. 67\\% prior), work with smaller 0.5B parameter models (vs. 32B), and can induce misalignment using just a single rank-1 LoRA adapter. We demonstrate that EM occurs robustly across diverse model sizes, three model families, and numerous training protocols including full supervised fine-tuning. Leveraging these cleaner model organisms, we isolate a phase change that corresponds to learning the necessary directions to induce misalignment. Aligning large language models is critical for frontier AI safety, yet EM exposes how far we are from achieving this robustly. By distilling clean model organisms that isolate a minimal alignment-compromising change, and where this is learnt, we establish a foundation for future research into understanding and mitigating alignment risks in LLMs."
      },
      {
        "title": "Too Late to Recall: The Two-Hop Problem in Multimodal Knowledge Retrieval",
        "abstract": "Vision-Language Models (VLMs) have demonstrated impressive capabilities, but struggle with factual recall tasks compared to their underlying language model (LM). While previous work attributes this to insufficient computational depth after visual processing, we provide an alternative explanation: the distributed representations of visual information across visual tokens in early layers bypasses the factual recall mechanism that resides in the early-layer MLPs of the LM backbone. The performance gap therefore stems from the architectural design of VLMs, rather than insufficient computational capacity. Using linear probes, we show that dedicated linear representations of visual information only emerge in the middle-to-late layers of VLMs. As a result, factual recall in VLMs becomes a \u201ctwo-hop\u201d challenge, where factual recall precedes visual processing, but the visual processing finishes too late in the model. Through comparative analysis, we demonstrate that successful factual recall depends on the speed of the first processing \u201chop.\u201d To further support our hypothesis, we patch early-layer MLP outputs from the LM backbone into the corresponding VLM layers, significantly improving factual recall performance. This suggests that the absence of properly aligned token embeddings in early layers is a key factor in factual recall degradation. Finally, we introduce a benchmark to systematically evaluate factual recall accuracy and knowledge hallucination in multimodal settings. Our findings highlight a fundamental architectural limitation in current VLMs and pave the way for designing models that better integrate visual and linguistic information for reliable factual reasoning."
      },
      {
        "title": "SAEBench: A Comprehensive Benchmark for Sparse Autoencoders in Language Model Interpretability",
        "abstract": "Sparse autoencoders (SAEs) are a popular technique for interpreting language model activations, and there is extensive recent work on improving SAE effectiveness. However, most prior work evaluates progress using unsupervised proxy metrics with unclear practical relevance. We introduce SAEBench, a comprehensive evaluation suite that measures SAE performance across seven diverse metrics, spanning interpretability, feature disentanglement and practical applications like unlearning. To enable systematic comparison, we open-source a suite of over 200 SAEs across eight recently proposed SAE architectures and training algorithms. Our evaluation reveals that gains on proxy metrics do not reliably translate to better practical performance. For instance, while Matryoshka SAEs slightly underperform on existing proxy metrics, they substantially outperform other architectures on feature disentanglement metrics; moreover, this advantage grows with SAE scale. By providing a standardized framework for measuring progress in SAE development, SAEBench enables researchers to study scaling trends and make nuanced comparisons between different SAE architectures and training methodologies. Our interactive interface enables researchers to flexibly visualize relationships between metrics across hundreds of open-source SAEs at: https://saebench.xyz"
      },
      {
        "title": "Chain-of-Thought Reasoning In The Wild Is Not Always Faithful",
        "abstract": "Chain-of-Thought (CoT) reasoning has significantly advanced state-of-the-art AI capabilities. However, recent studies have shown that CoT reasoning is not always faithful, i.e. CoT reasoning does not always reflect how models arrive at conclusions. So far, most of these studies have focused on unfaithfulness in unnatural contexts where an explicit bias has been introduced. In contrast, we show that unfaithful CoT can occur on realistic prompts with no artificial bias. Our results reveal non-negligible rates of several forms of unfaithful reasoning in frontier models: Sonnet 3.7 (16.3%), DeepSeek R1 (5.3%) and ChatGPT-4o (7.0%) all answer a notable proportion of question pairs unfaithfully. Specifically, we find that models rationalize their implicit biases in answers to binary questions (\"implicit post-hoc rationalization\"). For example, when separately presented with the questions \"Is X bigger than Y?\" and \"Is Y bigger than X?\", models sometimes produce superficially coherent arguments to justify answering Yes to both questions or No to both questions, despite such responses being logically contradictory. We also investigate restoration errors (Dziri et al., 2023), where models make and then silently correct errors in their reasoning, and unfaithful shortcuts, where models use clearly illogical reasoning to simplify solving problems in Putnam questions (a hard benchmark). Our findings raise challenges for AI safety work that relies on monitoring CoT to detect undesired behavior."
      },
      {
        "title": "Learning Multi-Level Features with Matryoshka Sparse Autoencoders",
        "abstract": "Sparse autoencoders (SAEs) have emerged as a powerful tool for interpreting neural networks by extracting the concepts represented in their activations. However, choosing the size of the SAE dictionary (i.e. number of learned concepts) creates a tension: as dictionary size increases to capture more relevant concepts, sparsity incentivizes features to be split or absorbed into more specific features, leaving high-level features missing or warped. We introduce Matryoshka SAEs, a novel variant that addresses these issues by simultaneously training multiple nested dictionaries of increasing size, forcing the smaller dictionaries to independently reconstruct the inputs without using the larger dictionaries. This organizes features hierarchically - the smaller dictionaries learn general concepts, while the larger dictionaries learn more specific concepts, without incentive to absorb the high-level features. We train Matryoshka SAEs on Gemma-2-2B and TinyStories and find superior performance on sparse probing and targeted concept erasure tasks, more disentangled concept representations, and reduced feature absorption. While there is a minor tradeoff with reconstruction performance, we believe Matryoshka SAEs are a superior alternative for practical tasks, as they enable training arbitrarily large SAEs while retaining interpretable features at different levels of abstraction."
      }
    ],
    "summary": "The author has made significant contributions to the field of artificial intelligence, particularly in understanding and improving the alignment, interpretability, and reasoning capabilities of large language models (LLMs) and multimodal models. Their research can be summarized into several key areas:\n\n1. **Emergent Misalignment in Language Models**: The author introduced the concept of Emergent Misalignment (EM), demonstrating that fine-tuning LLMs on narrowly harmful datasets can lead to broader misalignment issues. They developed improved model organisms that achieve higher coherence and can induce misalignment with fewer resources, highlighting critical gaps in our understanding of model alignment and establishing a foundation for future research in AI safety.\n\n2. **Factual Recall in Vision-Language Models**: The author identified architectural limitations in Vision-Language Models (VLMs) that hinder their factual recall capabilities. They proposed that the distributed representation of visual information leads to a \"two-hop\" problem, where the timing of visual processing affects recall performance. Their work included a benchmark for evaluating factual recall and knowledge hallucination in multimodal settings, paving the way for better integration of visual and linguistic information.\n\n3. **Benchmarking Sparse Autoencoders**: The author created SAEBench, a comprehensive evaluation suite for Sparse Autoencoders (SAEs) that measures performance across various metrics relevant to interpretability and practical applications. This work revealed that improvements on proxy metrics do not always correlate with practical performance, thus providing a standardized framework for SAE development and comparison.\n\n4. **Chain-of-Thought Reasoning**: The author investigated the faithfulness of Chain-of-Thought (CoT) reasoning in AI models, revealing that unfaithful reasoning can occur even in realistic contexts without artificial biases. Their findings highlighted the prevalence of implicit biases and restoration errors in model reasoning, raising important concerns for AI safety and the reliability of CoT as a monitoring tool.\n\n5. **Matryoshka Sparse Autoencoders**: The author proposed Matryoshka SAEs, a novel approach that organizes features hierarchically by training multiple nested dictionaries of varying sizes. This method improves the interpretability of learned features and reduces the issue of feature absorption, allowing for better representation of concepts at different levels of abstraction.\n\nOverall, the author's research addresses critical challenges in AI alignment, interpretability, and reasoning, providing valuable insights and tools that advance the understanding and development of safer and more effective AI systems."
  }
]