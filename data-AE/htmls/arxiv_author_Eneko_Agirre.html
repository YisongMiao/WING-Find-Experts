<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;50 of 72 results for author: <span class="mathjax">Eneko Agirre</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/"  aria-role="search">
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Eneko Agirre">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Eneko+Agirre&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Eneko Agirre">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?query=Eneko+Agirre&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?query=Eneko+Agirre&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?query=Eneko+Agirre&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.09691">arXiv:2506.09691</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.09691">pdf</a>, <a href="https://arxiv.org/ps/2506.09691">ps</a>, <a href="https://arxiv.org/format/2506.09691">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adding simple structure at inference improves Vision-Language Compositionality
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Miranda%2C+I">Imanol Miranda</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Salaberria%2C+A">Ander Salaberria</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Azkune%2C+G">Gorka Azkune</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.09691v1-abstract-short" style="display: inline;">
        Dual encoder Vision-Language Models (VLM) such as CLIP are widely used for image-text retrieval tasks. However, those models struggle with compositionality, showing a bag-of-words-like behavior that limits their retrieval performance. Many different training approaches have been proposed to improve the vision-language compositionality capabilities of those models. In comparison, inference-time tec&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.09691v1-abstract-full').style.display = 'inline'; document.getElementById('2506.09691v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.09691v1-abstract-full" style="display: none;">
        Dual encoder Vision-Language Models (VLM) such as CLIP are widely used for image-text retrieval tasks. However, those models struggle with compositionality, showing a bag-of-words-like behavior that limits their retrieval performance. Many different training approaches have been proposed to improve the vision-language compositionality capabilities of those models. In comparison, inference-time techniques have received little attention. In this paper, we propose to add simple structure at inference, where, given an image and a caption: i) we divide the image into different smaller crops, ii) we extract text segments, capturing objects, attributes and relations, iii) using a VLM, we find the image crops that better align with text segments obtaining matches, and iv) we compute the final image-text similarity aggregating the individual similarities of the matches. Based on various popular dual encoder VLMs, we evaluate our approach in controlled and natural datasets for VL compositionality. We find that our approach consistently improves the performance of evaluated VLMs without any training, which shows the potential of inference-time techniques. The results are especially good for attribute-object binding as shown in the controlled dataset. As a result of an extensive analysis: i) we show that processing image crops is actually essential for the observed gains in performance, and ii) we identify specific areas to further improve inference-time approaches.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.09691v1-abstract-full').style.display = 'none'; document.getElementById('2506.09691v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.07597">arXiv:2506.07597</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.07597">pdf</a>, <a href="https://arxiv.org/ps/2506.07597">ps</a>, <a href="https://arxiv.org/format/2506.07597">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Instructing Large Language Models for Low-Resource Languages: A Systematic Study for Basque
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sainz%2C+O">Oscar Sainz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Perez%2C+N">Naiara Perez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Etxaniz%2C+J">Julen Etxaniz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=de+Landa%2C+J+F">Joseba Fernandez de Landa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aldabe%2C+I">Itziar Aldabe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Garc%C3%ADa-Ferrero%2C+I">Iker García-Ferrero</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zabala%2C+A">Aimar Zabala</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Azurmendi%2C+E">Ekhi Azurmendi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rigau%2C+G">German Rigau</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Artetxe%2C+M">Mikel Artetxe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Soroa%2C+A">Aitor Soroa</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.07597v1-abstract-short" style="display: inline;">
        Instructing language models with user intent requires large instruction datasets, which are only available for a limited set of languages. In this paper, we explore alternatives to conventional instruction adaptation pipelines in low-resource scenarios. We assume a realistic scenario for low-resource languages, where only the following are available: corpora in the target language, existing open-w&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.07597v1-abstract-full').style.display = 'inline'; document.getElementById('2506.07597v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.07597v1-abstract-full" style="display: none;">
        Instructing language models with user intent requires large instruction datasets, which are only available for a limited set of languages. In this paper, we explore alternatives to conventional instruction adaptation pipelines in low-resource scenarios. We assume a realistic scenario for low-resource languages, where only the following are available: corpora in the target language, existing open-weight multilingual base and instructed backbone LLMs, and synthetically generated instructions sampled from the instructed backbone. We present a comprehensive set of experiments for Basque that systematically study different combinations of these components evaluated on benchmarks and human preferences from 1,680 participants. Our conclusions show that target language corpora are essential, with synthetic instructions yielding robust models, and, most importantly, that using as backbone an instruction-tuned model outperforms using a base non-instructed model, and improved results when scaling up. Using Llama 3.1 instruct 70B as backbone our model comes near frontier models of much larger sizes for Basque, without using any Basque data apart from the 1.2B word corpora. We release code, models, instruction datasets, and human preferences to support full reproducibility in future research on low-resource language adaptation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.07597v1-abstract-full').style.display = 'none'; document.getElementById('2506.07597v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Under review</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.00649">arXiv:2506.00649</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.00649">pdf</a>, <a href="https://arxiv.org/ps/2506.00649">ps</a>, <a href="https://arxiv.org/format/2506.00649">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        GuideX: Guided Synthetic Data Generation for Zero-Shot Information Extraction
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=De+La+Fuente%2C+N">Neil De La Fuente</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sainz%2C+O">Oscar Sainz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Garc%C3%ADa-Ferrero%2C+I">Iker García-Ferrero</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.00649v1-abstract-short" style="display: inline;">
        Information Extraction (IE) systems are traditionally domain-specific, requiring costly adaptation that involves expert schema design, data annotation, and model training. While Large Language Models have shown promise in zero-shot IE, performance degrades significantly in unseen domains where label definitions differ. This paper introduces GUIDEX, a novel method that automatically defines domain-&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.00649v1-abstract-full').style.display = 'inline'; document.getElementById('2506.00649v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.00649v1-abstract-full" style="display: none;">
        Information Extraction (IE) systems are traditionally domain-specific, requiring costly adaptation that involves expert schema design, data annotation, and model training. While Large Language Models have shown promise in zero-shot IE, performance degrades significantly in unseen domains where label definitions differ. This paper introduces GUIDEX, a novel method that automatically defines domain-specific schemas, infers guidelines, and generates synthetically labeled instances, allowing for better out-of-domain generalization. Fine-tuning Llama 3.1 with GUIDEX sets a new state-of-the-art across seven zeroshot Named Entity Recognition benchmarks. Models trained with GUIDEX gain up to 7 F1 points over previous methods without humanlabeled data, and nearly 2 F1 points higher when combined with it. Models trained on GUIDEX demonstrate enhanced comprehension of complex, domain-specific annotation schemas. Code, models, and synthetic datasets are available at neilus03.github.io/guidex.com
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.00649v1-abstract-full').style.display = 'none'; document.getElementById('2506.00649v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 May, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ACL Findings 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.00288">arXiv:2506.00288</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.00288">pdf</a>, <a href="https://arxiv.org/ps/2506.00288">ps</a>, <a href="https://arxiv.org/format/2506.00288">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Emergent Abilities of Large Language Models under Continued Pretraining for Language Adaptation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Elhady%2C+A">Ahmed Elhady</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Artetxe%2C+M">Mikel Artetxe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.00288v2-abstract-short" style="display: inline;">
        Continued pretraining (CPT) is a popular approach to adapt existing large language models (LLMs) to new languages. When doing so, it is common practice to include a portion of English data in the mixture, but its role has not been carefully studied to date. In this work, we show that including English does not impact validation perplexity, yet it is critical for the emergence of downstream capabil&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.00288v2-abstract-full').style.display = 'inline'; document.getElementById('2506.00288v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.00288v2-abstract-full" style="display: none;">
        Continued pretraining (CPT) is a popular approach to adapt existing large language models (LLMs) to new languages. When doing so, it is common practice to include a portion of English data in the mixture, but its role has not been carefully studied to date. In this work, we show that including English does not impact validation perplexity, yet it is critical for the emergence of downstream capabilities in the target language. We introduce a language-agnostic benchmark for in-context learning (ICL), which reveals catastrophic forgetting early on CPT when English is not included. This in turn damages the ability of the model to generalize to downstream prompts in the target language as measured by perplexity, even if it does not manifest in terms of accuracy until later in training, and can be tied to a big shift in the model parameters. Based on these insights, we introduce curriculum learning and exponential moving average (EMA) of weights as effective alternatives to mitigate the need for English. All in all, our work sheds light into the dynamics by which emergent abilities arise when doing CPT for language adaptation, and can serve as a foundation to design more effective methods in the future.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.00288v2-abstract-full').style.display = 'none'; document.getElementById('2506.00288v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 June, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 30 May, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To appear in ACL 2025 Main</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2502.18316">arXiv:2502.18316</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2502.18316">pdf</a>, <a href="https://arxiv.org/format/2502.18316">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        WiCkeD: A Simple Method to Make Multiple Choice Benchmarks More Challenging
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Elhady%2C+A">Ahmed Elhady</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Artetxe%2C+M">Mikel Artetxe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2502.18316v1-abstract-short" style="display: inline;">
        We introduce WiCkeD, a simple method to increase the complexity of existing multiple-choice benchmarks by randomly replacing a choice with &#34;None of the above&#34;, a method often used in educational tests. We show that WiCkeD can be automatically applied to any existing benchmark, making it more challenging. We apply WiCkeD to 6 popular benchmarks and use it to evaluate 18 open-weight LLMs. The perfor&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.18316v1-abstract-full').style.display = 'inline'; document.getElementById('2502.18316v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2502.18316v1-abstract-full" style="display: none;">
        We introduce WiCkeD, a simple method to increase the complexity of existing multiple-choice benchmarks by randomly replacing a choice with &#34;None of the above&#34;, a method often used in educational tests. We show that WiCkeD can be automatically applied to any existing benchmark, making it more challenging. We apply WiCkeD to 6 popular benchmarks and use it to evaluate 18 open-weight LLMs. The performance of the models drops 12.1 points on average with respect to the original versions of the datasets. When using chain-of-thought on 3 MMLU datasets, the performance drop for the WiCkeD variant is similar to the one observed when using the LLMs directly, showing that WiCkeD is also challenging for models with enhanced reasoning abilities. WiCkeD also uncovers that some models are more sensitive to the extra reasoning required, providing additional information with respect to the original benchmarks. We relase our code and data at https://github.com/ahmedselhady/wicked-benchmarks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.18316v1-abstract-full').style.display = 'none'; document.getElementById('2502.18316v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 February, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.21530">arXiv:2407.21530</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.21530">pdf</a>, <a href="https://arxiv.org/format/2407.21530">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Data Contamination Report from the 2024 CONDA Shared Task
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sainz%2C+O">Oscar Sainz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Garc%C3%ADa-Ferrero%2C+I">Iker García-Ferrero</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jacovi%2C+A">Alon Jacovi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Campos%2C+J+A">Jon Ander Campos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elazar%2C+Y">Yanai Elazar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goldberg%2C+Y">Yoav Goldberg</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+W">Wei-Lin Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chim%2C+J">Jenny Chim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Choshen%2C+L">Leshem Choshen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=D%27Amico-Wong%2C+L">Luca D&#39;Amico-Wong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dell%2C+M">Melissa Dell</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fan%2C+R">Run-Ze Fan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Golchin%2C+S">Shahriar Golchin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yucheng Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+P">Pengfei Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pahwa%2C+B">Bhavish Pahwa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Prabhu%2C+A">Ameya Prabhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sharma%2C+S">Suryansh Sharma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Silcock%2C+E">Emily Silcock</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Solonko%2C+K">Kateryna Solonko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Stap%2C+D">David Stap</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Surdeanu%2C+M">Mihai Surdeanu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tseng%2C+Y">Yu-Min Tseng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Udandarao%2C+V">Vishaal Udandarao</a>
      , et al. (3 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.21530v2-abstract-short" style="display: inline;">
        The 1st Workshop on Data Contamination (CONDA 2024) focuses on all relevant aspects of data contamination in natural language processing, where data contamination is understood as situations where evaluation data is included in pre-training corpora used to train large scale models, compromising evaluation results. The workshop fostered a shared task to collect evidence on data contamination in cur&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.21530v2-abstract-full').style.display = 'inline'; document.getElementById('2407.21530v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.21530v2-abstract-full" style="display: none;">
        The 1st Workshop on Data Contamination (CONDA 2024) focuses on all relevant aspects of data contamination in natural language processing, where data contamination is understood as situations where evaluation data is included in pre-training corpora used to train large scale models, compromising evaluation results. The workshop fostered a shared task to collect evidence on data contamination in current available datasets and models. The goal of the shared task and associated database is to assist the community in understanding the extent of the problem and to assist researchers in avoiding reporting evaluation results on known contaminated resources. The shared task provides a structured, centralized public database for the collection of contamination evidence, open to contributions from the community via GitHub pool requests. This first compilation paper is based on 566 reported entries over 91 contaminated sources from a total of 23 contributors. The details of the individual contamination events are available in the platform. The platform continues to be online, open to contributions from the community.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.21530v2-abstract-full').style.display = 'none'; document.getElementById('2407.21530v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 August, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 31 July, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">https://huggingface.co/spaces/CONDA-Workshop/Data-Contamination-Database</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.09952">arXiv:2406.09952</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.09952">pdf</a>, <a href="https://arxiv.org/format/2406.09952">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        BiVLC: Extending Vision-Language Compositionality Evaluation with Text-to-Image Retrieval
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Miranda%2C+I">Imanol Miranda</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Salaberria%2C+A">Ander Salaberria</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Azkune%2C+G">Gorka Azkune</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.09952v2-abstract-short" style="display: inline;">
        Existing Vision-Language Compositionality (VLC) benchmarks like SugarCrepe are formulated as image-to-text retrieval problems, where, given an image, the models need to select between the correct textual description and a synthetic hard negative text. In this work, we present the Bidirectional Vision-Language Compositionality (BiVLC) dataset. The novelty of BiVLC is to add a synthetic hard negativ&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09952v2-abstract-full').style.display = 'inline'; document.getElementById('2406.09952v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.09952v2-abstract-full" style="display: none;">
        Existing Vision-Language Compositionality (VLC) benchmarks like SugarCrepe are formulated as image-to-text retrieval problems, where, given an image, the models need to select between the correct textual description and a synthetic hard negative text. In this work, we present the Bidirectional Vision-Language Compositionality (BiVLC) dataset. The novelty of BiVLC is to add a synthetic hard negative image generated from the synthetic text, resulting in two image-to-text retrieval examples (one for each image) and, more importantly, two text-to-image retrieval examples (one for each text). Human annotators filter out ill-formed examples ensuring the validity of the benchmark. The experiments on BiVLC uncover a weakness of current multimodal models, as they perform poorly in the text-to-image direction. In fact, when considering both retrieval directions, the conclusions obtained in previous works change significantly. In addition to the benchmark, we show that a contrastive model trained using synthetic images and texts significantly improves over the base model in SugarCrepe and in BiVLC for both retrieval directions. The gap to human performance in BiVLC confirms that Vision-Language Compositionality is still a challenging problem. BiVLC and code are available at https://imirandam.github.io/BiVLC_project_page.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.09952v2-abstract-full').style.display = 'none'; document.getElementById('2406.09952v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 November, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 14 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to NeurIPS 24 Datasets and Benchmarks Track; Project page at: https://imirandam.github.io/BiVLC_project_page/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.06392">arXiv:2404.06392</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.06392">pdf</a>, <a href="https://arxiv.org/format/2404.06392">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Event Extraction in Basque: Typologically motivated Cross-Lingual Transfer-Learning Analysis
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zubillaga%2C+M">Mikel Zubillaga</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sainz%2C+O">Oscar Sainz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Estarrona%2C+A">Ainara Estarrona</a>, 
      
      <a href="/search/?searchtype=author&amp;query=de+Lacalle%2C+O+L">Oier Lopez de Lacalle</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.06392v1-abstract-short" style="display: inline;">
        Cross-lingual transfer-learning is widely used in Event Extraction for low-resource languages and involves a Multilingual Language Model that is trained in a source language and applied to the target language. This paper studies whether the typological similarity between source and target languages impacts the performance of cross-lingual transfer, an under-explored topic. We first focus on Basque&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.06392v1-abstract-full').style.display = 'inline'; document.getElementById('2404.06392v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.06392v1-abstract-full" style="display: none;">
        Cross-lingual transfer-learning is widely used in Event Extraction for low-resource languages and involves a Multilingual Language Model that is trained in a source language and applied to the target language. This paper studies whether the typological similarity between source and target languages impacts the performance of cross-lingual transfer, an under-explored topic. We first focus on Basque as the target language, which is an ideal target language because it is typologically different from surrounding languages. Our experiments on three Event Extraction tasks show that the shared linguistic characteristic between source and target languages does have an impact on transfer quality. Further analysis of 72 language pairs reveals that for tasks that involve token classification such as entity and event trigger identification, common writing script and morphological features produce higher quality cross-lingual transfer. In contrast, for tasks involving structural prediction like argument extraction, common word order is the most relevant feature. In addition, we show that when increasing the training size, not all the languages scale in the same way in the cross-lingual setting. To perform the experiments we introduce EusIE, an event extraction dataset for Basque, which follows the Multilingual Event Extraction dataset (MEE). The dataset and code are publicly available.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.06392v1-abstract-full').style.display = 'none'; document.getElementById('2404.06392v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at LREC-Coling 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.20266">arXiv:2403.20266</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.20266">pdf</a>, <a href="https://arxiv.org/format/2403.20266">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Latxa: An Open Language Model and Evaluation Suite for Basque
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Etxaniz%2C+J">Julen Etxaniz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sainz%2C+O">Oscar Sainz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Perez%2C+N">Naiara Perez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aldabe%2C+I">Itziar Aldabe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rigau%2C+G">German Rigau</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ormazabal%2C+A">Aitor Ormazabal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Artetxe%2C+M">Mikel Artetxe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Soroa%2C+A">Aitor Soroa</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.20266v2-abstract-short" style="display: inline;">
        We introduce Latxa, a family of large language models for Basque ranging from 7 to 70 billion parameters. Latxa is based on Llama 2, which we continue pretraining on a new Basque corpus comprising 4.3M documents and 4.2B tokens. Addressing the scarcity of high-quality benchmarks for Basque, we further introduce 4 multiple choice evaluation datasets: EusProficiency, comprising 5,169 questions from&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.20266v2-abstract-full').style.display = 'inline'; document.getElementById('2403.20266v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.20266v2-abstract-full" style="display: none;">
        We introduce Latxa, a family of large language models for Basque ranging from 7 to 70 billion parameters. Latxa is based on Llama 2, which we continue pretraining on a new Basque corpus comprising 4.3M documents and 4.2B tokens. Addressing the scarcity of high-quality benchmarks for Basque, we further introduce 4 multiple choice evaluation datasets: EusProficiency, comprising 5,169 questions from official language proficiency exams; EusReading, comprising 352 reading comprehension questions; EusTrivia, comprising 1,715 trivia questions from 5 knowledge areas; and EusExams, comprising 16,774 questions from public examinations. In our extensive evaluation, Latxa outperforms all previous open models we compare to by a large margin. In addition, it is competitive with GPT-4 Turbo in language proficiency and understanding, despite lagging behind in reading comprehension and knowledge-intensive tasks. Both the Latxa family of models, as well as our new pretraining corpora and evaluation datasets, are publicly available under open licenses. Our suite enables reproducible research on methods to build LLMs for low-resource languages.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.20266v2-abstract-full').style.display = 'none'; document.getElementById('2403.20266v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 29 March, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ACL 2024</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Proceedings of the 62nd Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers), pages 14952--14972. 2024
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.13666">arXiv:2403.13666</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.13666">pdf</a>, <a href="https://arxiv.org/format/2403.13666">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1016/j.neunet.2023.11.031">10.1016/j.neunet.2023.11.031 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Grounding Spatial Relations in Text-Only Language Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Azkune%2C+G">Gorka Azkune</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Salaberria%2C+A">Ander Salaberria</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.13666v1-abstract-short" style="display: inline;">
        This paper shows that text-only Language Models (LM) can learn to ground spatial relations like &#34;left of&#34; or &#34;below&#34; if they are provided with explicit location information of objects and they are properly trained to leverage those locations. We perform experiments on a verbalized version of the Visual Spatial Reasoning (VSR) dataset, where images are coupled with textual statements which contain&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.13666v1-abstract-full').style.display = 'inline'; document.getElementById('2403.13666v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.13666v1-abstract-full" style="display: none;">
        This paper shows that text-only Language Models (LM) can learn to ground spatial relations like &#34;left of&#34; or &#34;below&#34; if they are provided with explicit location information of objects and they are properly trained to leverage those locations. We perform experiments on a verbalized version of the Visual Spatial Reasoning (VSR) dataset, where images are coupled with textual statements which contain real or fake spatial relations between two objects of the image. We verbalize the images using an off-the-shelf object detector, adding location tokens to every object label to represent their bounding boxes in textual form. Given the small size of VSR, we do not observe any improvement when using locations, but pretraining the LM over a synthetic dataset automatically derived by us improves results significantly when using location tokens. We thus show that locations allow LMs to ground spatial relations, with our text-only LMs outperforming Vision-and-Language Models and setting the new state-of-the-art for the VSR dataset. Our analysis show that our text-only LMs can generalize beyond the relations seen in the synthetic dataset to some extent, learning also more useful information than that encoded in the spatial rules we used to create the synthetic dataset itself.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.13666v1-abstract-full').style.display = 'none'; document.getElementById('2403.13666v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted in Neural Networks</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.00587">arXiv:2403.00587</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.00587">pdf</a>, <a href="https://arxiv.org/format/2403.00587">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Improving Explicit Spatial Relationships in Text-to-Image Generation through an Automatically Derived Dataset
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Salaberria%2C+A">Ander Salaberria</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Azkune%2C+G">Gorka Azkune</a>, 
      
      <a href="/search/?searchtype=author&amp;query=de+Lacalle%2C+O+L">Oier Lopez de Lacalle</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Soroa%2C+A">Aitor Soroa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Keller%2C+F">Frank Keller</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.00587v1-abstract-short" style="display: inline;">
        Existing work has observed that current text-to-image systems do not accurately reflect explicit spatial relations between objects such as &#39;left of&#39; or &#39;below&#39;. We hypothesize that this is because explicit spatial relations rarely appear in the image captions used to train these models. We propose an automatic method that, given existing images, generates synthetic captions that contain 14 explici&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.00587v1-abstract-full').style.display = 'inline'; document.getElementById('2403.00587v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.00587v1-abstract-full" style="display: none;">
        Existing work has observed that current text-to-image systems do not accurately reflect explicit spatial relations between objects such as &#39;left of&#39; or &#39;below&#39;. We hypothesize that this is because explicit spatial relations rarely appear in the image captions used to train these models. We propose an automatic method that, given existing images, generates synthetic captions that contain 14 explicit spatial relations. We introduce the Spatial Relation for Generation (SR4G) dataset, which contains 9.9 millions image-caption pairs for training, and more than 60 thousand captions for evaluation. In order to test generalization we also provide an &#39;unseen&#39; split, where the set of objects in the train and test captions are disjoint. SR4G is the first dataset that can be used to spatially fine-tune text-to-image systems. We show that fine-tuning two different Stable Diffusion models (denoted as SD$_{SR4G}$) yields up to 9 points improvements in the VISOR metric. The improvement holds in the &#39;unseen&#39; split, showing that SD$_{SR4G}$ is able to generalize to unseen objects. SD$_{SR4G}$ improves the state-of-the-art with fewer parameters, and avoids complex architectures. Our analysis shows that improvement is consistent for all relations. The dataset and the code will be publicly available.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.00587v1-abstract-full').style.display = 'none'; document.getElementById('2403.00587v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages and 5 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2311.09808">arXiv:2311.09808</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2311.09808">pdf</a>, <a href="https://arxiv.org/format/2311.09808">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PixT3: Pixel-based Table-To-Text Generation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Alonso%2C+I">Iñigo Alonso</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lapata%2C+M">Mirella Lapata</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2311.09808v3-abstract-short" style="display: inline;">
        Table-to-text generation involves generating appropriate textual descriptions given structured tabular data. It has attracted increasing attention in recent years thanks to the popularity of neural network models and the availability of large-scale datasets. A common feature across existing methods is their treatment of the input as a string, i.e., by employing linearization techniques that do not&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2311.09808v3-abstract-full').style.display = 'inline'; document.getElementById('2311.09808v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2311.09808v3-abstract-full" style="display: none;">
        Table-to-text generation involves generating appropriate textual descriptions given structured tabular data. It has attracted increasing attention in recent years thanks to the popularity of neural network models and the availability of large-scale datasets. A common feature across existing methods is their treatment of the input as a string, i.e., by employing linearization techniques that do not always preserve information in the table, are verbose, and lack space efficiency. We propose to rethink data-to-text generation as a visual recognition task, removing the need for rendering the input in a string format. We present PixT3, a multimodal table-to-text model that overcomes the challenges of linearization and input size limitations encountered by existing models. PixT3 is trained with a new self-supervised learning objective to reinforce table structure awareness and is applicable to open-ended and controlled generation settings. Experiments on the ToTTo and Logic2Text benchmarks show that PixT3 is competitive and, in some settings, superior to generators that operate solely on text.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2311.09808v3-abstract-full').style.display = 'none'; document.getElementById('2311.09808v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 June, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 November, 2023;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2023.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2310.18018">arXiv:2310.18018</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2310.18018">pdf</a>, <a href="https://arxiv.org/format/2310.18018">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        NLP Evaluation in trouble: On the Need to Measure LLM Data Contamination for each Benchmark
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sainz%2C+O">Oscar Sainz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Campos%2C+J+A">Jon Ander Campos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Garc%C3%ADa-Ferrero%2C+I">Iker García-Ferrero</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Etxaniz%2C+J">Julen Etxaniz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=de+Lacalle%2C+O+L">Oier Lopez de Lacalle</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2310.18018v1-abstract-short" style="display: inline;">
        In this position paper, we argue that the classical evaluation on Natural Language Processing (NLP) tasks using annotated benchmarks is in trouble. The worst kind of data contamination happens when a Large Language Model (LLM) is trained on the test split of a benchmark, and then evaluated in the same benchmark. The extent of the problem is unknown, as it is not straightforward to measure. Contami&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2310.18018v1-abstract-full').style.display = 'inline'; document.getElementById('2310.18018v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2310.18018v1-abstract-full" style="display: none;">
        In this position paper, we argue that the classical evaluation on Natural Language Processing (NLP) tasks using annotated benchmarks is in trouble. The worst kind of data contamination happens when a Large Language Model (LLM) is trained on the test split of a benchmark, and then evaluated in the same benchmark. The extent of the problem is unknown, as it is not straightforward to measure. Contamination causes an overestimation of the performance of a contaminated model in a target benchmark and associated task with respect to their non-contaminated counterparts. The consequences can be very harmful, with wrong scientific conclusions being published while other correct ones are discarded. This position paper defines different levels of data contamination and argues for a community effort, including the development of automatic and semi-automatic measures to detect when data from a benchmark was exposed to a model, and suggestions for flagging papers with conclusions that are compromised by data contamination.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2310.18018v1-abstract-full').style.display = 'none'; document.getElementById('2310.18018v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2023; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2023.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at EMNLP2024-Findings</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2310.17279">arXiv:2310.17279</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2310.17279">pdf</a>, <a href="https://arxiv.org/format/2310.17279">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1016/j.eswa.2023.121869">10.1016/j.eswa.2023.121869 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Automatic Logical Forms improve fidelity in Table-to-Text generation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Alonso%2C+I">Iñigo Alonso</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2310.17279v2-abstract-short" style="display: inline;">
        Table-to-text systems generate natural language statements from structured data like tables. While end-to-end techniques suffer from low factual correctness (fidelity), a previous study reported gains when using manual logical forms (LF) that represent the selected content and the semantics of the target text. Given the manual step, it was not clear whether automatic LFs would be effective, or whe&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2310.17279v2-abstract-full').style.display = 'inline'; document.getElementById('2310.17279v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2310.17279v2-abstract-full" style="display: none;">
        Table-to-text systems generate natural language statements from structured data like tables. While end-to-end techniques suffer from low factual correctness (fidelity), a previous study reported gains when using manual logical forms (LF) that represent the selected content and the semantics of the target text. Given the manual step, it was not clear whether automatic LFs would be effective, or whether the improvement came from content selection alone. We present TlT which, given a table and a selection of the content, first produces LFs and then the textual statement. We show for the first time that automatic LFs improve quality, with an increase in fidelity of 30 points over a comparable system not using LFs. Our experiments allow to quantify the remaining challenges for high factual correctness, with automatic selection of content coming first, followed by better Logic-to-Text generation and, to a lesser extent, better Table-to-Logic parsing.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2310.17279v2-abstract-full').style.display = 'none'; document.getElementById('2310.17279v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 January, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 26 October, 2023;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2023.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Expert Systems with Applications, Volume 238, Part D, 15 March 2024, 121869
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2310.09350">arXiv:2310.09350</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2310.09350">pdf</a>, <a href="https://arxiv.org/format/2310.09350">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Unsupervised Domain Adaption for Neural Information Retrieval
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dominguez%2C+C">Carlos Dominguez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Campos%2C+J+A">Jon Ander Campos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Azkune%2C+G">Gorka Azkune</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2310.09350v1-abstract-short" style="display: inline;">
        Neural information retrieval requires costly annotated data for each target domain to be competitive. Synthetic annotation by query generation using Large Language Models or rule-based string manipulation has been proposed as an alternative, but their relative merits have not been analysed. In this paper, we compare both methods head-to-head using the same neural IR architecture. We focus on the B&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2310.09350v1-abstract-full').style.display = 'inline'; document.getElementById('2310.09350v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2310.09350v1-abstract-full" style="display: none;">
        Neural information retrieval requires costly annotated data for each target domain to be competitive. Synthetic annotation by query generation using Large Language Models or rule-based string manipulation has been proposed as an alternative, but their relative merits have not been analysed. In this paper, we compare both methods head-to-head using the same neural IR architecture. We focus on the BEIR benchmark, which includes test datasets from several domains with no training data, and explore two scenarios: zero-shot, where the supervised system is trained in a large out-of-domain dataset (MS-MARCO); and unsupervised domain adaptation, where, in addition to MS-MARCO, the system is fine-tuned in synthetic data from the target domain. Our results indicate that Large Language Models outperform rule-based methods in all scenarios by a large margin, and, more importantly, that unsupervised domain adaptation is effective compared to applying a supervised IR system in a zero-shot fashion. In addition we explore several sizes of open Large Language Models to generate synthetic data and find that a medium-sized model suffices. Code and models are publicly available for reproducibility.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2310.09350v1-abstract-full').style.display = 'none'; document.getElementById('2310.09350v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 October, 2023; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2023.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2310.03668">arXiv:2310.03668</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2310.03668">pdf</a>, <a href="https://arxiv.org/format/2310.03668">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        GoLLIE: Annotation Guidelines improve Zero-Shot Information-Extraction
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sainz%2C+O">Oscar Sainz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Garc%C3%ADa-Ferrero%2C+I">Iker García-Ferrero</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agerri%2C+R">Rodrigo Agerri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=de+Lacalle%2C+O+L">Oier Lopez de Lacalle</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rigau%2C+G">German Rigau</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2310.03668v5-abstract-short" style="display: inline;">
        Large Language Models (LLMs) combined with instruction tuning have made significant progress when generalizing to unseen tasks. However, they have been less successful in Information Extraction (IE), lagging behind task-specific models. Typically, IE tasks are characterized by complex annotation guidelines that describe the task and give examples to humans. Previous attempts to leverage such infor&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2310.03668v5-abstract-full').style.display = 'inline'; document.getElementById('2310.03668v5-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2310.03668v5-abstract-full" style="display: none;">
        Large Language Models (LLMs) combined with instruction tuning have made significant progress when generalizing to unseen tasks. However, they have been less successful in Information Extraction (IE), lagging behind task-specific models. Typically, IE tasks are characterized by complex annotation guidelines that describe the task and give examples to humans. Previous attempts to leverage such information have failed, even with the largest models, as they are not able to follow the guidelines out of the box. In this paper, we propose GoLLIE (Guideline-following Large Language Model for IE), a model able to improve zero-shot results on unseen IE tasks by virtue of being fine-tuned to comply with annotation guidelines. Comprehensive evaluation empirically demonstrates that GoLLIE is able to generalize to and follow unseen guidelines, outperforming previous attempts at zero-shot information extraction. The ablation study shows that detailed guidelines are key for good results.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2310.03668v5-abstract-full').style.display = 'none'; document.getElementById('2310.03668v5-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 March, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 October, 2023;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2023.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">The Twelfth International Conference on Learning Representations - ICLR 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2305.16876">arXiv:2305.16876</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2305.16876">pdf</a>, <a href="https://arxiv.org/format/2305.16876">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CombLM: Adapting Black-Box Language Models through Small Fine-Tuned Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ormazabal%2C+A">Aitor Ormazabal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Artetxe%2C+M">Mikel Artetxe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2305.16876v1-abstract-short" style="display: inline;">
        Methods for adapting language models (LMs) to new tasks and domains have traditionally assumed white-box access to the model, and work by modifying its parameters. However, this is incompatible with a recent trend in the field, where the highest quality models are only available as black-boxes through inference APIs. Even when the model weights are available, the computational cost of fine-tuning&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2305.16876v1-abstract-full').style.display = 'inline'; document.getElementById('2305.16876v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2305.16876v1-abstract-full" style="display: none;">
        Methods for adapting language models (LMs) to new tasks and domains have traditionally assumed white-box access to the model, and work by modifying its parameters. However, this is incompatible with a recent trend in the field, where the highest quality models are only available as black-boxes through inference APIs. Even when the model weights are available, the computational cost of fine-tuning large LMs can be prohibitive for most practitioners. In this work, we present a lightweight method for adapting large LMs to new domains and tasks, assuming no access to their weights or intermediate activations. Our approach fine-tunes a small white-box LM and combines it with the large black-box LM at the probability level through a small network, learned on a small validation set. We validate our approach by adapting a large LM (OPT-30B) to several domains and a downstream task (machine translation), observing improved performance in all cases, of up to 9%, while using a domain expert 23x smaller.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2305.16876v1-abstract-full').style.display = 'none'; document.getElementById('2305.16876v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 May, 2023; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2023.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This previously appeared as arXiv:2205.12213v2, which was submitted as new by mistake</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2302.03353">arXiv:2302.03353</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2302.03353">pdf</a>, <a href="https://arxiv.org/format/2302.03353">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        What do Language Models know about word senses? Zero-Shot WSD with Language Models and Domain Inventories
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sainz%2C+O">Oscar Sainz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=de+Lacalle%2C+O+L">Oier Lopez de Lacalle</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rigau%2C+G">German Rigau</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2302.03353v1-abstract-short" style="display: inline;">
        Language Models are the core for almost any Natural Language Processing system nowadays. One of their particularities is their contextualized representations, a game changer feature when a disambiguation between word senses is necessary. In this paper we aim to explore to what extent language models are capable of discerning among senses at inference time. We performed this analysis by prompting c&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2302.03353v1-abstract-full').style.display = 'inline'; document.getElementById('2302.03353v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2302.03353v1-abstract-full" style="display: none;">
        Language Models are the core for almost any Natural Language Processing system nowadays. One of their particularities is their contextualized representations, a game changer feature when a disambiguation between word senses is necessary. In this paper we aim to explore to what extent language models are capable of discerning among senses at inference time. We performed this analysis by prompting commonly used Languages Models such as BERT or RoBERTa to perform the task of Word Sense Disambiguation (WSD). We leverage the relation between word senses and domains, and cast WSD as a textual entailment problem, where the different hypothesis refer to the domains of the word senses. Our results show that this approach is indeed effective, close to supervised systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2302.03353v1-abstract-full').style.display = 'none'; document.getElementById('2302.03353v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 February, 2023; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2023.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Presented at GWC2023</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2212.08390">arXiv:2212.08390</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2212.08390">pdf</a>, <a href="https://arxiv.org/ps/2212.08390">ps</a>, <a href="https://arxiv.org/format/2212.08390">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Lessons learned from the evaluation of Spanish Language Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Agerri%2C+R">Rodrigo Agerri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2212.08390v2-abstract-short" style="display: inline;">
        Given the impact of language models on the field of Natural Language Processing, a number of Spanish encoder-only masked language models (aka BERTs) have been trained and released. These models were developed either within large projects using very large private corpora or by means of smaller scale academic efforts leveraging freely available data. In this paper we present a comprehensive head-to-&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2212.08390v2-abstract-full').style.display = 'inline'; document.getElementById('2212.08390v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2212.08390v2-abstract-full" style="display: none;">
        Given the impact of language models on the field of Natural Language Processing, a number of Spanish encoder-only masked language models (aka BERTs) have been trained and released. These models were developed either within large projects using very large private corpora or by means of smaller scale academic efforts leveraging freely available data. In this paper we present a comprehensive head-to-head comparison of language models for Spanish with the following results: (i) Previously ignored multilingual models from large companies fare better than monolingual models, substantially changing the evaluation landscape of language models in Spanish; (ii) Results across the monolingual models are not conclusive, with supposedly smaller and inferior models performing competitively. Based on these empirical results, we argue for the need of more research to understand the factors underlying them. In this sense, the effect of corpus size, quality and pre-training techniques need to be further investigated to be able to obtain Spanish monolingual models significantly better than the multilingual ones released by large private companies, specially in the face of rapid ongoing progress in the field. The recent activity in the development of language technology for Spanish is to be welcomed, but our results show that building language models remains an open, resource-heavy problem which requires to marry resources (monetary and/or computational) with the best research expertise and practice.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2212.08390v2-abstract-full').style.display = 'none'; document.getElementById('2212.08390v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 September, 2023; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 December, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">11 pages, three tables</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Procesamiento del Lenguaje Natural (70), pp 157-170, 2023
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2205.12213">arXiv:2205.12213</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2205.12213">pdf</a>, <a href="https://arxiv.org/format/2205.12213">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Principled Paraphrase Generation with Parallel Corpora
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ormazabal%2C+A">Aitor Ormazabal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Artetxe%2C+M">Mikel Artetxe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Soroa%2C+A">Aitor Soroa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Labaka%2C+G">Gorka Labaka</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2205.12213v3-abstract-short" style="display: inline;">
        Round-trip Machine Translation (MT) is a popular choice for paraphrase generation, which leverages readily available parallel corpora for supervision. In this paper, we formalize the implicit similarity function induced by this approach, and show that it is susceptible to non-paraphrase pairs sharing a single ambiguous translation. Based on these insights, we design an alternative similarity metri&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.12213v3-abstract-full').style.display = 'inline'; document.getElementById('2205.12213v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2205.12213v3-abstract-full" style="display: none;">
        Round-trip Machine Translation (MT) is a popular choice for paraphrase generation, which leverages readily available parallel corpora for supervision. In this paper, we formalize the implicit similarity function induced by this approach, and show that it is susceptible to non-paraphrase pairs sharing a single ambiguous translation. Based on these insights, we design an alternative similarity metric that mitigates this issue by requiring the entire translation distribution to match, and implement a relaxation of it through the Information Bottleneck method. Our approach incorporates an adversarial term into MT training in order to learn representations that encode as much information about the reference translation as possible, while keeping as little information about the input as possible. Paraphrases can be generated by decoding back to the source from this representation, without having to generate pivot translations. In addition to being more principled and efficient than round-trip MT, our approach offers an adjustable parameter to control the fidelity-diversity trade-off, and obtains better results in our experiments.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.12213v3-abstract-full').style.display = 'none'; document.getElementById('2205.12213v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 May, 2023; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 24 May, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2205.12206">arXiv:2205.12206</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2205.12206">pdf</a>, <a href="https://arxiv.org/format/2205.12206">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PoeLM: A Meter- and Rhyme-Controllable Language Model for Unsupervised Poetry Generation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ormazabal%2C+A">Aitor Ormazabal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Artetxe%2C+M">Mikel Artetxe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirrezabal%2C+M">Manex Agirrezabal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Soroa%2C+A">Aitor Soroa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2205.12206v2-abstract-short" style="display: inline;">
        Formal verse poetry imposes strict constraints on the meter and rhyme scheme of poems. Most prior work on generating this type of poetry uses existing poems for supervision, which are difficult to obtain for most languages and poetic forms. In this work, we propose an unsupervised approach to generate poems following any given meter and rhyme scheme, without requiring any poetic text for training.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.12206v2-abstract-full').style.display = 'inline'; document.getElementById('2205.12206v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2205.12206v2-abstract-full" style="display: none;">
        Formal verse poetry imposes strict constraints on the meter and rhyme scheme of poems. Most prior work on generating this type of poetry uses existing poems for supervision, which are difficult to obtain for most languages and poetic forms. In this work, we propose an unsupervised approach to generate poems following any given meter and rhyme scheme, without requiring any poetic text for training. Our method works by splitting a regular, non-poetic corpus into phrases, prepending control codes that describe the length and end rhyme of each phrase, and training a transformer language model in the augmented corpus. During inference, we build control codes for the desired meter and rhyme scheme, and condition our language model on them to generate formal verse poetry. Experiments in Spanish and Basque show that our approach is able to generate valid poems, which are often comparable in quality to those written by humans.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.12206v2-abstract-full').style.display = 'none'; document.getElementById('2205.12206v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 October, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 24 May, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">EMNLP Findings 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2205.01376">arXiv:2205.01376</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2205.01376">pdf</a>, <a href="https://arxiv.org/format/2205.01376">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Textual Entailment for Event Argument Extraction: Zero- and Few-Shot with Multi-Source Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sainz%2C+O">Oscar Sainz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gonzalez-Dios%2C+I">Itziar Gonzalez-Dios</a>, 
      
      <a href="/search/?searchtype=author&amp;query=de+Lacalle%2C+O+L">Oier Lopez de Lacalle</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Min%2C+B">Bonan Min</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2205.01376v1-abstract-short" style="display: inline;">
        Recent work has shown that NLP tasks such as Relation Extraction (RE) can be recasted as Textual Entailment tasks using verbalizations, with strong performance in zero-shot and few-shot settings thanks to pre-trained entailment models. The fact that relations in current RE datasets are easily verbalized casts doubts on whether entailment would be effective in more complex tasks. In this work we sh&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.01376v1-abstract-full').style.display = 'inline'; document.getElementById('2205.01376v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2205.01376v1-abstract-full" style="display: none;">
        Recent work has shown that NLP tasks such as Relation Extraction (RE) can be recasted as Textual Entailment tasks using verbalizations, with strong performance in zero-shot and few-shot settings thanks to pre-trained entailment models. The fact that relations in current RE datasets are easily verbalized casts doubts on whether entailment would be effective in more complex tasks. In this work we show that entailment is also effective in Event Argument Extraction (EAE), reducing the need of manual annotation to 50% and 20% in ACE and WikiEvents respectively, while achieving the same performance as with full training. More importantly, we show that recasting EAE as entailment alleviates the dependency on schemas, which has been a road-block for transferring annotations between domains. Thanks to the entailment, the multi-source transfer between ACE and WikiEvents further reduces annotation down to 10% and 5% (respectively) of the full training without transfer. Our analysis shows that the key to good results is the use of several entailment datasets to pre-train the entailment model. Similar to previous approaches, our method requires a small amount of effort for manual verbalization: only less than 15 minutes per event argument type is needed, and comparable results can be achieved with users with different level of expertise.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.01376v1-abstract-full').style.display = 'none'; document.getElementById('2205.01376v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 May, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted as Findings of NAACL2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.13602">arXiv:2203.13602</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.13602">pdf</a>, <a href="https://arxiv.org/format/2203.13602">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ZS4IE: A toolkit for Zero-Shot Information Extraction with simple Verbalizations
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sainz%2C+O">Oscar Sainz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qiu%2C+H">Haoling Qiu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=de+Lacalle%2C+O+L">Oier Lopez de Lacalle</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Min%2C+B">Bonan Min</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.13602v3-abstract-short" style="display: inline;">
        The current workflow for Information Extraction (IE) analysts involves the definition of the entities/relations of interest and a training corpus with annotated examples. In this demonstration we introduce a new workflow where the analyst directly verbalizes the entities/relations, which are then used by a Textual Entailment model to perform zero-shot IE. We present the design and implementation o&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.13602v3-abstract-full').style.display = 'inline'; document.getElementById('2203.13602v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.13602v3-abstract-full" style="display: none;">
        The current workflow for Information Extraction (IE) analysts involves the definition of the entities/relations of interest and a training corpus with annotated examples. In this demonstration we introduce a new workflow where the analyst directly verbalizes the entities/relations, which are then used by a Textual Entailment model to perform zero-shot IE. We present the design and implementation of a toolkit with a user interface, as well as experiments on four IE tasks that show that the system achieves very good performance at zero-shot learning using only 5--15 minutes per type of a user&#39;s effort. Our demonstration system is open-sourced at https://github.com/BBN-E/ZS4IE . A demonstration video is available at https://vimeo.com/676138340 .
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.13602v3-abstract-full').style.display = 'none'; document.getElementById('2203.13602v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 May, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 25 March, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at NAACL2022 Demo track</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.01243">arXiv:2111.01243</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.01243">pdf</a>, <a href="https://arxiv.org/format/2111.01243">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Recent Advances in Natural Language Processing via Large Pre-Trained Language Models: A Survey
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Min%2C+B">Bonan Min</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ross%2C+H">Hayley Ross</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sulem%2C+E">Elior Sulem</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Veyseh%2C+A+P+B">Amir Pouran Ben Veyseh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nguyen%2C+T+H">Thien Huu Nguyen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sainz%2C+O">Oscar Sainz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Heinz%2C+I">Ilana Heinz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Roth%2C+D">Dan Roth</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.01243v1-abstract-short" style="display: inline;">
        Large, pre-trained transformer-based language models such as BERT have drastically changed the Natural Language Processing (NLP) field. We present a survey of recent work that uses these large language models to solve NLP tasks via pre-training then fine-tuning, prompting, or text generation approaches. We also present approaches that use pre-trained language models to generate data for training a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.01243v1-abstract-full').style.display = 'inline'; document.getElementById('2111.01243v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.01243v1-abstract-full" style="display: none;">
        Large, pre-trained transformer-based language models such as BERT have drastically changed the Natural Language Processing (NLP) field. We present a survey of recent work that uses these large language models to solve NLP tasks via pre-training then fine-tuning, prompting, or text generation approaches. We also present approaches that use pre-trained language models to generate data for training augmentation or other purposes. We conclude with discussions on limitations and suggested directions for future research.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.01243v1-abstract-full').style.display = 'none'; document.getElementById('2111.01243v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 November, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2109.08029">arXiv:2109.08029</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2109.08029">pdf</a>, <a href="https://arxiv.org/format/2109.08029">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1016/j.eswa.2022.118669">10.1016/j.eswa.2022.118669 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Image Captioning for Effective Use of Language Models in Knowledge-Based Visual Question Answering
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Salaberria%2C+A">Ander Salaberria</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Azkune%2C+G">Gorka Azkune</a>, 
      
      <a href="/search/?searchtype=author&amp;query=de+Lacalle%2C+O+L">Oier Lopez de Lacalle</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Soroa%2C+A">Aitor Soroa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2109.08029v3-abstract-short" style="display: inline;">
        Integrating outside knowledge for reasoning in visio-linguistic tasks such as visual question answering (VQA) is an open problem. Given that pretrained language models have been shown to include world knowledge, we propose to use a unimodal (text-only) train and inference procedure based on automatic off-the-shelf captioning of images and pretrained language models. Our results on a visual questio&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.08029v3-abstract-full').style.display = 'inline'; document.getElementById('2109.08029v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2109.08029v3-abstract-full" style="display: none;">
        Integrating outside knowledge for reasoning in visio-linguistic tasks such as visual question answering (VQA) is an open problem. Given that pretrained language models have been shown to include world knowledge, we propose to use a unimodal (text-only) train and inference procedure based on automatic off-the-shelf captioning of images and pretrained language models. Our results on a visual question answering task which requires external knowledge (OK-VQA) show that our text-only model outperforms pretrained multimodal (image-text) models of comparable number of parameters. In contrast, our model is less effective in a standard VQA task (VQA 2.0) confirming that our text-only method is specially effective for tasks requiring external knowledge. In addition, we show that increasing the language model&#39;s size improves notably its performance, yielding results comparable to the state-of-the-art with our largest model, significantly outperforming current multimodal systems, even though augmented with external knowledge. Our qualitative analysis on OK-VQA reveals that automatic captions often fail to capture relevant information in the images, which seems to be balanced by the better inference ability of the text-only language models. Our work opens up possibilities to further improve inference in visio-linguistic tasks
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.08029v3-abstract-full').style.display = 'none'; document.getElementById('2109.08029v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 March, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 September, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Under review. 25 pages with 4 figures</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Expert Systems with Applications, Volume 212, 2023, 118669
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2109.03659">arXiv:2109.03659</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2109.03659">pdf</a>, <a href="https://arxiv.org/format/2109.03659">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Label Verbalization and Entailment for Effective Zero- and Few-Shot Relation Extraction
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sainz%2C+O">Oscar Sainz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=de+Lacalle%2C+O+L">Oier Lopez de Lacalle</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Labaka%2C+G">Gorka Labaka</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Barrena%2C+A">Ander Barrena</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2109.03659v1-abstract-short" style="display: inline;">
        Relation extraction systems require large amounts of labeled examples which are costly to annotate. In this work we reformulate relation extraction as an entailment task, with simple, hand-made, verbalizations of relations produced in less than 15 min per relation. The system relies on a pretrained textual entailment engine which is run as-is (no training examples, zero-shot) or further fine-tuned&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.03659v1-abstract-full').style.display = 'inline'; document.getElementById('2109.03659v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2109.03659v1-abstract-full" style="display: none;">
        Relation extraction systems require large amounts of labeled examples which are costly to annotate. In this work we reformulate relation extraction as an entailment task, with simple, hand-made, verbalizations of relations produced in less than 15 min per relation. The system relies on a pretrained textual entailment engine which is run as-is (no training examples, zero-shot) or further fine-tuned on labeled examples (few-shot or fully trained). In our experiments on TACRED we attain 63% F1 zero-shot, 69% with 16 examples per relation (17% points better than the best supervised system on the same conditions), and only 4 points short to the state-of-the-art (which uses 20 times more training data). We also show that the performance can be improved significantly with larger entailment models, up to 12 points in zero-shot, allowing to report the best results to date on TACRED when fully trained. The analysis shows that our few-shot systems are specially effective when discriminating between relations, and that the performance difference in low data regimes comes mainly from identifying no-relation cases.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.03659v1-abstract-full').style.display = 'none'; document.getElementById('2109.03659v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at EMNLP2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.10419">arXiv:2105.10419</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.10419">pdf</a>, <a href="https://arxiv.org/format/2105.10419">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.18653/v1/2020.acl-srw.34">10.18653/v1/2020.acl-srw.34 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Unsupervised Multilingual Sentence Embeddings for Parallel Corpus Mining
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kvapil%C4%B1kova%2C+I">Ivana Kvapilıkova</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Artetxe%2C+M">Mikel Artetxe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Labaka%2C+G">Gorka Labaka</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bojar%2C+O">Ondřej Bojar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.10419v1-abstract-short" style="display: inline;">
        Existing models of multilingual sentence embeddings require large parallel data resources which are not available for low-resource languages. We propose a novel unsupervised method to derive multilingual sentence embeddings relying only on monolingual data. We first produce a synthetic parallel corpus using unsupervised machine translation, and use it to fine-tune a pretrained cross-lingual masked&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.10419v1-abstract-full').style.display = 'inline'; document.getElementById('2105.10419v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.10419v1-abstract-full" style="display: none;">
        Existing models of multilingual sentence embeddings require large parallel data resources which are not available for low-resource languages. We propose a novel unsupervised method to derive multilingual sentence embeddings relying only on monolingual data. We first produce a synthetic parallel corpus using unsupervised machine translation, and use it to fine-tune a pretrained cross-lingual masked language model (XLM) to derive the multilingual sentence representations. The quality of the representations is evaluated on two parallel corpus mining tasks with improvements of up to 22 F1 points over vanilla XLM. In addition, we observe that a single synthetic bilingual corpus is able to improve results for other language pairs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.10419v1-abstract-full').style.display = 'none'; document.getElementById('2105.10419v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ACL SRW 2020</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics - Student Research Workshop, pages 255-262, Association for Computational Linguistics, 2020
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.00997">arXiv:2102.00997</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.00997">pdf</a>, <a href="https://arxiv.org/format/2102.00997">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1016/j.patcog.2021.107847">10.1016/j.patcog.2021.107847 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Inferring spatial relations from textual descriptions of images
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Elu%2C+A">Aitzol Elu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Azkune%2C+G">Gorka Azkune</a>, 
      
      <a href="/search/?searchtype=author&amp;query=de+Lacalle%2C+O+L">Oier Lopez de Lacalle</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Arganda-Carreras%2C+I">Ignacio Arganda-Carreras</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Soroa%2C+A">Aitor Soroa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.00997v1-abstract-short" style="display: inline;">
        Generating an image from its textual description requires both a certain level of language understanding and common sense knowledge about the spatial relations of the physical entities being described. In this work, we focus on inferring the spatial relation between entities, a key step in the process of composing scenes based on text. More specifically, given a caption containing a mention to a s&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.00997v1-abstract-full').style.display = 'inline'; document.getElementById('2102.00997v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.00997v1-abstract-full" style="display: none;">
        Generating an image from its textual description requires both a certain level of language understanding and common sense knowledge about the spatial relations of the physical entities being described. In this work, we focus on inferring the spatial relation between entities, a key step in the process of composing scenes based on text. More specifically, given a caption containing a mention to a subject and the location and size of the bounding box of that subject, our goal is to predict the location and size of an object mentioned in the caption. Previous work did not use the caption text information, but a manually provided relation holding between the subject and the object. In fact, the used evaluation datasets contain manually annotated ontological triplets but no captions, making the exercise unrealistic: a manual step was required; and systems did not leverage the richer information in captions. Here we present a system that uses the full caption, and Relations in Captions (REC-COCO), a dataset derived from MS-COCO which allows to evaluate spatial relation inference from captions directly. Our experiments show that: (1) it is possible to infer the size and location of an object with respect to a given subject directly from the caption; (2) the use of full text allows to place the object better than using a manually annotated relation. Our work paves the way for systems that, given a caption, decide which entities need to be depicted and their respective location and sizes, in order to then generate the final image.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.00997v1-abstract-full').style.display = 'none'; document.getElementById('2102.00997v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted in Pattern Recognition</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Pattern Recognition, Volume 113, 2021, 107847
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2012.15715">arXiv:2012.15715</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2012.15715">pdf</a>, <a href="https://arxiv.org/format/2012.15715">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.18653/v1/2021.acl-long.506">10.18653/v1/2021.acl-long.506 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Beyond Offline Mapping: Learning Cross Lingual Word Embeddings through Context Anchoring
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ormazabal%2C+A">Aitor Ormazabal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Artetxe%2C+M">Mikel Artetxe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Soroa%2C+A">Aitor Soroa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Labaka%2C+G">Gorka Labaka</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2012.15715v2-abstract-short" style="display: inline;">
        Recent research on cross-lingual word embeddings has been dominated by unsupervised mapping approaches that align monolingual embeddings. Such methods critically rely on those embeddings having a similar structure, but it was recently shown that the separate training in different languages causes departures from this assumption. In this paper, we propose an alternative approach that does not have&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.15715v2-abstract-full').style.display = 'inline'; document.getElementById('2012.15715v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2012.15715v2-abstract-full" style="display: none;">
        Recent research on cross-lingual word embeddings has been dominated by unsupervised mapping approaches that align monolingual embeddings. Such methods critically rely on those embeddings having a similar structure, but it was recently shown that the separate training in different languages causes departures from this assumption. In this paper, we propose an alternative approach that does not have this limitation, while requiring a weak seed dictionary (e.g., a list of identical words) as the only form of supervision. Rather than aligning two fixed embedding spaces, our method works by fixing the target language embeddings, and learning a new set of embeddings for the source language that are aligned with them. To that end, we use an extension of skip-gram that leverages translated context words as anchor points, and incorporates self-learning and iterative restarts to reduce the dependency on the initial dictionary. Our approach outperforms conventional mapping methods on bilingual lexicon induction, and obtains competitive results in the downstream XNLI task.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.15715v2-abstract-full').style.display = 'none'; document.getElementById('2012.15715v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 August, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 31 December, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ACL 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2011.00615">arXiv:2011.00615</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2011.00615">pdf</a>, <a href="https://arxiv.org/format/2011.00615">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Improving Conversational Question Answering Systems after Deployment using Feedback-Weighted Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Campos%2C+J+A">Jon Ander Campos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Otegi%2C+A">Arantxa Otegi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Soroa%2C+A">Aitor Soroa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Azkune%2C+G">Gorka Azkune</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2011.00615v1-abstract-short" style="display: inline;">
        The interaction of conversational systems with users poses an exciting opportunity for improving them after deployment, but little evidence has been provided of its feasibility. In most applications, users are not able to provide the correct answer to the system, but they are able to provide binary (correct, incorrect) feedback. In this paper we propose feedback-weighted learning based on importan&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.00615v1-abstract-full').style.display = 'inline'; document.getElementById('2011.00615v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2011.00615v1-abstract-full" style="display: none;">
        The interaction of conversational systems with users poses an exciting opportunity for improving them after deployment, but little evidence has been provided of its feasibility. In most applications, users are not able to provide the correct answer to the system, but they are able to provide binary (correct, incorrect) feedback. In this paper we propose feedback-weighted learning based on importance sampling to improve upon an initial supervised system using binary user feedback. We perform simulated experiments on document classification (for development) and Conversational Question Answering datasets like QuAC and DoQA, where binary user feedback is derived from gold annotations. The results show that our method is able to improve over the initial supervised system, getting close to a fully-supervised system that has access to the same labeled examples in in-domain experiments (QuAC), and even matching in out-of-domain experiments (DoQA). Our work opens the prospect to exploit interactions with real users and improve conversational systems after deployment.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.00615v1-abstract-full').style.display = 'none'; document.getElementById('2011.00615v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 November, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at COLING 2020. 11 pages, 5 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2010.02140">arXiv:2010.02140</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2010.02140">pdf</a>, <a href="https://arxiv.org/format/2010.02140">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Spot The Bot: A Robust and Efficient Framework for the Evaluation of Conversational Dialogue Systems
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Deriu%2C+J">Jan Deriu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tuggener%2C+D">Don Tuggener</a>, 
      
      <a href="/search/?searchtype=author&amp;query=von+D%C3%A4niken%2C+P">Pius von Däniken</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Campos%2C+J+A">Jon Ander Campos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rodrigo%2C+A">Alvaro Rodrigo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Belkacem%2C+T">Thiziri Belkacem</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Soroa%2C+A">Aitor Soroa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cieliebak%2C+M">Mark Cieliebak</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2010.02140v1-abstract-short" style="display: inline;">
        The lack of time-efficient and reliable evaluation methods hamper the development of conversational dialogue systems (chatbots). Evaluations requiring humans to converse with chatbots are time and cost-intensive, put high cognitive demands on the human judges, and yield low-quality results. In this work, we introduce \emph{Spot The Bot}, a cost-efficient and robust evaluation framework that replac&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.02140v1-abstract-full').style.display = 'inline'; document.getElementById('2010.02140v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2010.02140v1-abstract-full" style="display: none;">
        The lack of time-efficient and reliable evaluation methods hamper the development of conversational dialogue systems (chatbots). Evaluations requiring humans to converse with chatbots are time and cost-intensive, put high cognitive demands on the human judges, and yield low-quality results. In this work, we introduce \emph{Spot The Bot}, a cost-efficient and robust evaluation framework that replaces human-bot conversations with conversations between bots. Human judges then only annotate for each entity in a conversation whether they think it is human or not (assuming there are humans participants in these conversations). These annotations then allow us to rank chatbots regarding their ability to mimic the conversational behavior of humans. Since we expect that all bots are eventually recognized as such, we incorporate a metric that measures which chatbot can uphold human-like behavior the longest, i.e., \emph{Survival Analysis}. This metric has the ability to correlate a bot&#39;s performance to certain of its characteristics (e.g., \ fluency or sensibleness), yielding interpretable results. The comparably low cost of our framework allows for frequent evaluations of chatbots during their evaluation cycle. We empirically validate our claims by applying \emph{Spot The Bot} to three domains, evaluating several state-of-the-art chatbots, and drawing comparisons to related work. The framework is released as a ready-to-use tool.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.02140v1-abstract-full').style.display = 'none'; document.getElementById('2010.02140v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 October, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2005.01328">arXiv:2005.01328</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2005.01328">pdf</a>, <a href="https://arxiv.org/format/2005.01328">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DoQA -- Accessing Domain-Specific FAQs via Conversational QA
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Campos%2C+J+A">Jon Ander Campos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Otegi%2C+A">Arantxa Otegi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Soroa%2C+A">Aitor Soroa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deriu%2C+J">Jan Deriu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cieliebak%2C+M">Mark Cieliebak</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2005.01328v2-abstract-short" style="display: inline;">
        The goal of this work is to build conversational Question Answering (QA) interfaces for the large body of domain-specific information available in FAQ sites. We present DoQA, a dataset with 2,437 dialogues and 10,917 QA pairs. The dialogues are collected from three Stack Exchange sites using the Wizard of Oz method with crowdsourcing. Compared to previous work, DoQA comprises well-defined informat&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.01328v2-abstract-full').style.display = 'inline'; document.getElementById('2005.01328v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2005.01328v2-abstract-full" style="display: none;">
        The goal of this work is to build conversational Question Answering (QA) interfaces for the large body of domain-specific information available in FAQ sites. We present DoQA, a dataset with 2,437 dialogues and 10,917 QA pairs. The dialogues are collected from three Stack Exchange sites using the Wizard of Oz method with crowdsourcing. Compared to previous work, DoQA comprises well-defined information needs, leading to more coherent and natural conversations with less factoid questions and is multi-domain. In addition, we introduce a more realistic information retrieval(IR) scenario where the system needs to find the answer in any of the FAQ documents. The results of an existing, strong, system show that, thanks to transfer learning from a Wikipedia QA dataset and fine tuning on a single FAQ domain, it is possible to build high quality conversational QA systems for FAQs without in-domain training data. The good results carry over into the more challenging IR scenario. In both cases, there is still ample room for improvement, as indicated by the higher human upperbound.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.01328v2-abstract-full').style.display = 'none'; document.getElementById('2005.01328v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 May, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 4 May, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at ACL 2020. 13 pages 4 figures</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2004.14958">arXiv:2004.14958</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2004.14958">pdf</a>, <a href="https://arxiv.org/format/2004.14958">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.18653/v1/2020.acl-main.658">10.18653/v1/2020.acl-main.658 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Call for More Rigor in Unsupervised Cross-lingual Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Artetxe%2C+M">Mikel Artetxe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ruder%2C+S">Sebastian Ruder</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yogatama%2C+D">Dani Yogatama</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Labaka%2C+G">Gorka Labaka</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2004.14958v1-abstract-short" style="display: inline;">
        We review motivations, definition, approaches, and methodology for unsupervised cross-lingual learning and call for a more rigorous position in each of them. An existing rationale for such research is based on the lack of parallel data for many of the world&#39;s languages. However, we argue that a scenario without any parallel data and abundant monolingual data is unrealistic in practice. We also dis&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.14958v1-abstract-full').style.display = 'inline'; document.getElementById('2004.14958v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2004.14958v1-abstract-full" style="display: none;">
        We review motivations, definition, approaches, and methodology for unsupervised cross-lingual learning and call for a more rigorous position in each of them. An existing rationale for such research is based on the lack of parallel data for many of the world&#39;s languages. However, we argue that a scenario without any parallel data and abundant monolingual data is unrealistic in practice. We also discuss different training signals that have been used in previous work, which depart from the pure unsupervised setting. We then describe common methodological issues in tuning and evaluation of unsupervised cross-lingual models and present best practices. Finally, we provide a unified outlook for different types of research in this area (i.e., cross-lingual word embeddings, deep multilingual pretraining, and unsupervised machine translation) and argue for comparable evaluation of these models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.14958v1-abstract-full').style.display = 'none'; document.getElementById('2004.14958v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 April, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ACL 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2004.07633">arXiv:2004.07633</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2004.07633">pdf</a>, <a href="https://arxiv.org/format/2004.07633">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Methodology for Creating Question Answering Corpora Using Inverse Data Annotation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Deriu%2C+J">Jan Deriu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mlynchyk%2C+K">Katsiaryna Mlynchyk</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schl%C3%A4pfer%2C+P">Philippe Schläpfer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rodrigo%2C+A">Alvaro Rodrigo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=von+Gr%C3%BCnigen%2C+D">Dirk von Grünigen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kaiser%2C+N">Nicolas Kaiser</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Stockinger%2C+K">Kurt Stockinger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cieliebak%2C+M">Mark Cieliebak</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2004.07633v2-abstract-short" style="display: inline;">
        In this paper, we introduce a novel methodology to efficiently construct a corpus for question answering over structured data. For this, we introduce an intermediate representation that is based on the logical query plan in a database called Operation Trees (OT). This representation allows us to invert the annotation process without losing flexibility in the types of queries that we generate. Furt&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.07633v2-abstract-full').style.display = 'inline'; document.getElementById('2004.07633v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2004.07633v2-abstract-full" style="display: none;">
        In this paper, we introduce a novel methodology to efficiently construct a corpus for question answering over structured data. For this, we introduce an intermediate representation that is based on the logical query plan in a database called Operation Trees (OT). This representation allows us to invert the annotation process without losing flexibility in the types of queries that we generate. Furthermore, it allows for fine-grained alignment of query tokens to OT operations. In our method, we randomly generate OTs from a context-free grammar. Afterwards, annotators have to write the appropriate natural language question that is represented by the OT. Finally, the annotators assign the tokens to the OT operations. We apply the method to create a new corpus OTTA (Operation Trees and Token Assignment), a large semantic parsing corpus for evaluating natural language interfaces to databases. We compare OTTA to Spider and LC-QuaD 2.0 and show that our methodology more than triples the annotation speed while maintaining the complexity of the queries. Finally, we train a state-of-the-art semantic parsing model on our data and show that our corpus is a challenging dataset and that the token alignment can be leveraged to increase the performance significantly.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.07633v2-abstract-full').style.display = 'none'; document.getElementById('2004.07633v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 June, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 April, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2020.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Proceedings of the 58th Annual Meeting of the Association for Computational Linguistics. 2020
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2004.04721">arXiv:2004.04721</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2004.04721">pdf</a>, <a href="https://arxiv.org/format/2004.04721">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.18653/v1/2020.emnlp-main.618">10.18653/v1/2020.emnlp-main.618 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Translation Artifacts in Cross-lingual Transfer Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Artetxe%2C+M">Mikel Artetxe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Labaka%2C+G">Gorka Labaka</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2004.04721v4-abstract-short" style="display: inline;">
        Both human and machine translation play a central role in cross-lingual transfer learning: many multilingual datasets have been created through professional translation services, and using machine translation to translate either the test set or the training set is a widely used transfer technique. In this paper, we show that such translation process can introduce subtle artifacts that have a notab&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.04721v4-abstract-full').style.display = 'inline'; document.getElementById('2004.04721v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2004.04721v4-abstract-full" style="display: none;">
        Both human and machine translation play a central role in cross-lingual transfer learning: many multilingual datasets have been created through professional translation services, and using machine translation to translate either the test set or the training set is a widely used transfer technique. In this paper, we show that such translation process can introduce subtle artifacts that have a notable impact in existing cross-lingual models. For instance, in natural language inference, translating the premise and the hypothesis independently can reduce the lexical overlap between them, which current models are highly sensitive to. We show that some previous findings in cross-lingual transfer learning need to be reconsidered in the light of this phenomenon. Based on the gained insights, we also improve the state-of-the-art in XNLI for the translate-test and zero-shot approaches by 4.3 and 2.8 points, respectively.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.04721v4-abstract-full').style.display = 'none'; document.getElementById('2004.04721v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 December, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 9 April, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">EMNLP 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2004.01894">arXiv:2004.01894</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2004.01894">pdf</a>, <a href="https://arxiv.org/format/2004.01894">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Evaluating Multimodal Representations on Visual Semantic Textual Similarity
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=de+Lacalle%2C+O+L">Oier Lopez de Lacalle</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Salaberria%2C+A">Ander Salaberria</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Soroa%2C+A">Aitor Soroa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Azkune%2C+G">Gorka Azkune</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2004.01894v1-abstract-short" style="display: inline;">
        The combination of visual and textual representations has produced excellent results in tasks such as image captioning and visual question answering, but the inference capabilities of multimodal representations are largely untested. In the case of textual representations, inference tasks such as Textual Entailment and Semantic Textual Similarity have been often used to benchmark the quality of tex&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.01894v1-abstract-full').style.display = 'inline'; document.getElementById('2004.01894v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2004.01894v1-abstract-full" style="display: none;">
        The combination of visual and textual representations has produced excellent results in tasks such as image captioning and visual question answering, but the inference capabilities of multimodal representations are largely untested. In the case of textual representations, inference tasks such as Textual Entailment and Semantic Textual Similarity have been often used to benchmark the quality of textual representations. The long term goal of our research is to devise multimodal representation techniques that improve current inference capabilities. We thus present a novel task, Visual Semantic Textual Similarity (vSTS), where such inference ability can be tested directly. Given two items comprised each by an image and its accompanying caption, vSTS systems need to assess the degree to which the captions in context are semantically equivalent to each other. Our experiments using simple multimodal representations show that the addition of image representations produces better inference, compared to text-only representations. The improvement is observed both when directly computing the similarity between the representations of the two items, and when learning a siamese network based on vSTS training data. Our work shows, for the first time, the successful contribution of visual information to textual inference, with ample room for benchmarking more complex multimodal representation options.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.01894v1-abstract-full').style.display = 'none'; document.getElementById('2004.01894v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 April, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted in ECAI-2020, 8 pages, 6 tables, 6 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2004.00033">arXiv:2004.00033</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2004.00033">pdf</a>, <a href="https://arxiv.org/ps/2004.00033">ps</a>, <a href="https://arxiv.org/format/2004.00033">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Give your Text Representation Models some Love: the Case for Basque
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Agerri%2C+R">Rodrigo Agerri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vicente%2C+I+S">Iñaki San Vicente</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Campos%2C+J+A">Jon Ander Campos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Barrena%2C+A">Ander Barrena</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Saralegi%2C+X">Xabier Saralegi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Soroa%2C+A">Aitor Soroa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2004.00033v2-abstract-short" style="display: inline;">
        Word embeddings and pre-trained language models allow to build rich representations of text and have enabled improvements across most NLP tasks. Unfortunately they are very expensive to train, and many small companies and research groups tend to use models that have been pre-trained and made available by third parties, rather than building their own. This is suboptimal as, for many languages, the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.00033v2-abstract-full').style.display = 'inline'; document.getElementById('2004.00033v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2004.00033v2-abstract-full" style="display: none;">
        Word embeddings and pre-trained language models allow to build rich representations of text and have enabled improvements across most NLP tasks. Unfortunately they are very expensive to train, and many small companies and research groups tend to use models that have been pre-trained and made available by third parties, rather than building their own. This is suboptimal as, for many languages, the models have been trained on smaller (or lower quality) corpora. In addition, monolingual pre-trained models for non-English languages are not always available. At best, models for those languages are included in multilingual versions, where each language shares the quota of substrings and parameters with the rest of the languages. This is particularly true for smaller languages such as Basque. In this paper we show that a number of monolingual models (FastText word embeddings, FLAIR and BERT language models) trained with larger Basque corpora produce much better results than publicly available versions in downstream NLP tasks, including topic classification, sentiment classification, PoS tagging and NER. This work sets a new state-of-the-art in those tasks for Basque. All benchmarks and models used in this work are publicly available.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.00033v2-abstract-full').style.display = 'none'; document.getElementById('2004.00033v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 April, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 31 March, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at LREC 2020; 8 pages, 7 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2002.12867">arXiv:2002.12867</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2002.12867">pdf</a>, <a href="https://arxiv.org/format/2002.12867">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1016/j.knosys.2020.106401">10.1016/j.knosys.2020.106401 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Do all Roads Lead to Rome? Understanding the Role of Initialization in Iterative Back-Translation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Artetxe%2C+M">Mikel Artetxe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Labaka%2C+G">Gorka Labaka</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Casas%2C+N">Noe Casas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2002.12867v1-abstract-short" style="display: inline;">
        Back-translation provides a simple yet effective approach to exploit monolingual corpora in Neural Machine Translation (NMT). Its iterative variant, where two opposite NMT models are jointly trained by alternately using a synthetic parallel corpus generated by the reverse model, plays a central role in unsupervised machine translation. In order to start producing sound translations and provide a m&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2002.12867v1-abstract-full').style.display = 'inline'; document.getElementById('2002.12867v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2002.12867v1-abstract-full" style="display: none;">
        Back-translation provides a simple yet effective approach to exploit monolingual corpora in Neural Machine Translation (NMT). Its iterative variant, where two opposite NMT models are jointly trained by alternately using a synthetic parallel corpus generated by the reverse model, plays a central role in unsupervised machine translation. In order to start producing sound translations and provide a meaningful training signal to each other, existing approaches rely on either a separate machine translation system to warm up the iterative procedure, or some form of pre-training to initialize the weights of the model. In this paper, we analyze the role that such initialization plays in iterative back-translation. Is the behavior of the final system heavily dependent on it? Or does iterative back-translation converge to a similar solution given any reasonable initialization? Through a series of empirical experiments over a diverse set of warmup systems, we show that, although the quality of the initial system does affect final performance, its effect is relatively small, as iterative back-translation has a strong tendency to convergence to a similar solution. As such, the margin of improvement left for the initialization method is narrow, suggesting that future research should focus more on improving the iterative mechanism itself.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2002.12867v1-abstract-full').style.display = 'none'; document.getElementById('2002.12867v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 February, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1907.10761">arXiv:1907.10761</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1907.10761">pdf</a>, <a href="https://arxiv.org/format/1907.10761">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.18653/v1/P19-1494">10.18653/v1/P19-1494 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Bilingual Lexicon Induction through Unsupervised Machine Translation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Artetxe%2C+M">Mikel Artetxe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Labaka%2C+G">Gorka Labaka</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1907.10761v1-abstract-short" style="display: inline;">
        A recent research line has obtained strong results on bilingual lexicon induction by aligning independently trained word embeddings in two languages and using the resulting cross-lingual embeddings to induce word translation pairs through nearest neighbor or related retrieval methods. In this paper, we propose an alternative approach to this problem that builds on the recent work on unsupervised m&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1907.10761v1-abstract-full').style.display = 'inline'; document.getElementById('1907.10761v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1907.10761v1-abstract-full" style="display: none;">
        A recent research line has obtained strong results on bilingual lexicon induction by aligning independently trained word embeddings in two languages and using the resulting cross-lingual embeddings to induce word translation pairs through nearest neighbor or related retrieval methods. In this paper, we propose an alternative approach to this problem that builds on the recent work on unsupervised machine translation. This way, instead of directly inducing a bilingual lexicon from cross-lingual embeddings, we use them to build a phrase-table, combine it with a language model, and use the resulting machine translation system to generate a synthetic parallel corpus, from which we extract the bilingual lexicon using statistical word alignment techniques. As such, our method can work with any word embedding and cross-lingual mapping technique, and it does not require any additional resource besides the monolingual corpus used to train the embeddings. When evaluated on the exact same cross-lingual embeddings, our proposed method obtains an average improvement of 6 accuracy points over nearest neighbor and 4 points over CSLS retrieval, establishing a new state-of-the-art in the standard MUSE dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1907.10761v1-abstract-full').style.display = 'none'; document.getElementById('1907.10761v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 July, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ACL 2019</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1906.05407">arXiv:1906.05407</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1906.05407">pdf</a>, <a href="https://arxiv.org/format/1906.05407">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.18653/v1/P19-1492">10.18653/v1/P19-1492 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Analyzing the Limitations of Cross-lingual Word Embedding Mappings
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ormazabal%2C+A">Aitor Ormazabal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Artetxe%2C+M">Mikel Artetxe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Labaka%2C+G">Gorka Labaka</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Soroa%2C+A">Aitor Soroa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1906.05407v1-abstract-short" style="display: inline;">
        Recent research in cross-lingual word embeddings has almost exclusively focused on offline methods, which independently train word embeddings in different languages and map them to a shared space through linear transformations. While several authors have questioned the underlying isomorphism assumption, which states that word embeddings in different languages have approximately the same structure,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1906.05407v1-abstract-full').style.display = 'inline'; document.getElementById('1906.05407v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1906.05407v1-abstract-full" style="display: none;">
        Recent research in cross-lingual word embeddings has almost exclusively focused on offline methods, which independently train word embeddings in different languages and map them to a shared space through linear transformations. While several authors have questioned the underlying isomorphism assumption, which states that word embeddings in different languages have approximately the same structure, it is not clear whether this is an inherent limitation of mapping approaches or a more general issue when learning cross-lingual embeddings. So as to answer this question, we experiment with parallel corpora, which allows us to compare offline mapping to an extension of skip-gram that jointly learns both embedding spaces. We observe that, under these ideal conditions, joint learning yields to more isomorphic embeddings, is less sensitive to hubness, and obtains stronger results in bilingual lexicon induction. We thus conclude that current mapping methods do have strong limitations, calling for further research to jointly learn cross-lingual embeddings with a weaker cross-lingual signal.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1906.05407v1-abstract-full').style.display = 'none'; document.getElementById('1906.05407v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 June, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ACL 2019</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1906.03608">arXiv:1906.03608</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1906.03608">pdf</a>, <a href="https://arxiv.org/format/1906.03608">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Probing for Semantic Classes: Diagnosing the Meaning Content of Word Embeddings
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yaghoobzadeh%2C+Y">Yadollah Yaghoobzadeh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kann%2C+K">Katharina Kann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hazen%2C+T+J">Timothy J. Hazen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sch%C3%BCtze%2C+H">Hinrich Schütze</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1906.03608v1-abstract-short" style="display: inline;">
        Word embeddings typically represent different meanings of a word in a single conflated vector. Empirical analysis of embeddings of ambiguous words is currently limited by the small size of manually annotated resources and by the fact that word senses are treated as unrelated individual concepts. We present a large dataset based on manual Wikipedia annotations and word senses, where word senses fro&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1906.03608v1-abstract-full').style.display = 'inline'; document.getElementById('1906.03608v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1906.03608v1-abstract-full" style="display: none;">
        Word embeddings typically represent different meanings of a word in a single conflated vector. Empirical analysis of embeddings of ambiguous words is currently limited by the small size of manually annotated resources and by the fact that word senses are treated as unrelated individual concepts. We present a large dataset based on manual Wikipedia annotations and word senses, where word senses from different words are related by semantic classes. This is the basis for novel diagnostic tests for an embedding&#39;s content: we probe word embeddings for semantic classes and analyze the embedding space by classifying embeddings into semantic classes. Our main findings are: (i) Information about a sense is generally represented well in a single-vector embedding - if the sense is frequent. (ii) A classifier can accurately predict whether a word is single-sense or multi-sense, based only on its embedding. (iii) Although rare senses are not well represented in single-vector embeddings, this does not have negative impact on an NLP application whose performance depends on frequent senses.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1906.03608v1-abstract-full').style.display = 'none'; document.getElementById('1906.03608v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 June, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">14 pages, Accepted at ACL 2019</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1905.04071">arXiv:1905.04071</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1905.04071">pdf</a>, <a href="https://arxiv.org/format/1905.04071">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1007/s10462-020-09866-x">10.1007/s10462-020-09866-x <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Survey on Evaluation Methods for Dialogue Systems
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Deriu%2C+J">Jan Deriu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rodrigo%2C+A">Alvaro Rodrigo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Otegi%2C+A">Arantxa Otegi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Echegoyen%2C+G">Guillermo Echegoyen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rosset%2C+S">Sophie Rosset</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cieliebak%2C+M">Mark Cieliebak</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1905.04071v2-abstract-short" style="display: inline;">
        In this paper we survey the methods and concepts developed for the evaluation of dialogue systems. Evaluation is a crucial part during the development process. Often, dialogue systems are evaluated by means of human evaluations and questionnaires. However, this tends to be very cost and time intensive. Thus, much work has been put into finding methods, which allow to reduce the involvement of huma&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1905.04071v2-abstract-full').style.display = 'inline'; document.getElementById('1905.04071v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1905.04071v2-abstract-full" style="display: none;">
        In this paper we survey the methods and concepts developed for the evaluation of dialogue systems. Evaluation is a crucial part during the development process. Often, dialogue systems are evaluated by means of human evaluations and questionnaires. However, this tends to be very cost and time intensive. Thus, much work has been put into finding methods, which allow to reduce the involvement of human labour. In this survey, we present the main concepts and methods. For this, we differentiate between the various classes of dialogue systems (task-oriented dialogue systems, conversational dialogue systems, and question-answering dialogue systems). We cover each class by introducing the main technologies developed for the dialogue systems and then by presenting the evaluation methods regarding this class.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1905.04071v2-abstract-full').style.display = 'none'; document.getElementById('1905.04071v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 June, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 10 May, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2019.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Artificial Intelligence Review, June 2020
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1902.01313">arXiv:1902.01313</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1902.01313">pdf</a>, <a href="https://arxiv.org/format/1902.01313">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.18653/v1/P19-1019">10.18653/v1/P19-1019 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An Effective Approach to Unsupervised Machine Translation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Artetxe%2C+M">Mikel Artetxe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Labaka%2C+G">Gorka Labaka</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1902.01313v2-abstract-short" style="display: inline;">
        While machine translation has traditionally relied on large amounts of parallel corpora, a recent research line has managed to train both Neural Machine Translation (NMT) and Statistical Machine Translation (SMT) systems using monolingual corpora only. In this paper, we identify and address several deficiencies of existing unsupervised SMT approaches by exploiting subword information, developing a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1902.01313v2-abstract-full').style.display = 'inline'; document.getElementById('1902.01313v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1902.01313v2-abstract-full" style="display: none;">
        While machine translation has traditionally relied on large amounts of parallel corpora, a recent research line has managed to train both Neural Machine Translation (NMT) and Statistical Machine Translation (SMT) systems using monolingual corpora only. In this paper, we identify and address several deficiencies of existing unsupervised SMT approaches by exploiting subword information, developing a theoretically well founded unsupervised tuning method, and incorporating a joint refinement procedure. Moreover, we use our improved SMT system to initialize a dual NMT model, which is further fine-tuned through on-the-fly back-translation. Together, we obtain large improvements over the previous state-of-the-art in unsupervised machine translation. For instance, we get 22.5 BLEU points in English-to-German WMT 2014, 5.5 points more than the previous best unsupervised system, and 0.5 points more than the (supervised) shared task winner back in 2014.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1902.01313v2-abstract-full').style.display = 'none'; document.getElementById('1902.01313v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 July, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 4 February, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ACL 2019</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1809.03695">arXiv:1809.03695</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1809.03695">pdf</a>, <a href="https://arxiv.org/format/1809.03695">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Evaluating Multimodal Representations on Sentence Similarity: vSTS, Visual Semantic Textual Similarity Dataset
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=de+Lacalle%2C+O+L">Oier Lopez de Lacalle</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Soroa%2C+A">Aitor Soroa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1809.03695v1-abstract-short" style="display: inline;">
        In this paper we introduce vSTS, a new dataset for measuring textual similarity of sentences using multimodal information. The dataset is comprised by images along with its respectively textual captions. We describe the dataset both quantitatively and qualitatively, and claim that it is a valid gold standard for measuring automatic multimodal textual similarity systems. We also describe the initia&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.03695v1-abstract-full').style.display = 'inline'; document.getElementById('1809.03695v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1809.03695v1-abstract-full" style="display: none;">
        In this paper we introduce vSTS, a new dataset for measuring textual similarity of sentences using multimodal information. The dataset is comprised by images along with its respectively textual captions. We describe the dataset both quantitatively and qualitatively, and claim that it is a valid gold standard for measuring automatic multimodal textual similarity systems. We also describe the initial experiments combining the multimodal information.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.03695v1-abstract-full').style.display = 'none'; document.getElementById('1809.03695v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 September, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2018.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        ICCV17: second workshop on Closing the Loop Between Vision and Language. Venice, Italy. 2017
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1809.02094">arXiv:1809.02094</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1809.02094">pdf</a>, <a href="https://arxiv.org/format/1809.02094">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.18653/v1/K18-1028">10.18653/v1/K18-1028 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Uncovering divergent linguistic information in word embeddings with lessons for intrinsic and extrinsic evaluation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Artetxe%2C+M">Mikel Artetxe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Labaka%2C+G">Gorka Labaka</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lopez-Gazpio%2C+I">Iñigo Lopez-Gazpio</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1809.02094v1-abstract-short" style="display: inline;">
        Following the recent success of word embeddings, it has been argued that there is no such thing as an ideal representation for words, as different models tend to capture divergent and often mutually incompatible aspects like semantics/syntax and similarity/relatedness. In this paper, we show that each embedding model captures more information than directly apparent. A linear transformation that ad&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.02094v1-abstract-full').style.display = 'inline'; document.getElementById('1809.02094v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1809.02094v1-abstract-full" style="display: none;">
        Following the recent success of word embeddings, it has been argued that there is no such thing as an ideal representation for words, as different models tend to capture divergent and often mutually incompatible aspects like semantics/syntax and similarity/relatedness. In this paper, we show that each embedding model captures more information than directly apparent. A linear transformation that adjusts the similarity order of the model without any external resource can tailor it to achieve better results in those aspects, providing a new perspective on how embeddings encode divergent linguistic information. In addition, we explore the relation between intrinsic and extrinsic evaluation, as the effect of our transformations in downstream tasks is higher for unsupervised systems than for supervised ones.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.02094v1-abstract-full').style.display = 'none'; document.getElementById('1809.02094v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 September, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">CoNLL 2018</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1809.01272">arXiv:1809.01272</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1809.01272">pdf</a>, <a href="https://arxiv.org/format/1809.01272">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.18653/v1/D18-1399">10.18653/v1/D18-1399 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Unsupervised Statistical Machine Translation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Artetxe%2C+M">Mikel Artetxe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Labaka%2C+G">Gorka Labaka</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1809.01272v1-abstract-short" style="display: inline;">
        While modern machine translation has relied on large parallel corpora, a recent line of work has managed to train Neural Machine Translation (NMT) systems from monolingual corpora only (Artetxe et al., 2018c; Lample et al., 2018). Despite the potential of this approach for low-resource settings, existing systems are far behind their supervised counterparts, limiting their practical interest. In th&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.01272v1-abstract-full').style.display = 'inline'; document.getElementById('1809.01272v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1809.01272v1-abstract-full" style="display: none;">
        While modern machine translation has relied on large parallel corpora, a recent line of work has managed to train Neural Machine Translation (NMT) systems from monolingual corpora only (Artetxe et al., 2018c; Lample et al., 2018). Despite the potential of this approach for low-resource settings, existing systems are far behind their supervised counterparts, limiting their practical interest. In this paper, we propose an alternative approach based on phrase-based Statistical Machine Translation (SMT) that significantly closes the gap with supervised systems. Our method profits from the modular architecture of SMT: we first induce a phrase table from monolingual corpora through cross-lingual embedding mappings, combine it with an n-gram language model, and fine-tune hyperparameters through an unsupervised MERT variant. In addition, iterative backtranslation improves results further, yielding, for instance, 14.08 and 26.22 BLEU points in WMT 2014 English-German and English-French, respectively, an improvement of more than 7-10 BLEU points over previous unsupervised systems, and closing the gap with supervised SMT (Moses trained on Europarl) down to 2-5 BLEU points. Our implementation is available at https://github.com/artetxem/monoses
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.01272v1-abstract-full').style.display = 'none'; document.getElementById('1809.01272v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 September, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">EMNLP 2018</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1805.06297">arXiv:1805.06297</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1805.06297">pdf</a>, <a href="https://arxiv.org/format/1805.06297">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.18653/v1/P18-1073">10.18653/v1/P18-1073 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A robust self-learning method for fully unsupervised cross-lingual mappings of word embeddings
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Artetxe%2C+M">Mikel Artetxe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Labaka%2C+G">Gorka Labaka</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1805.06297v2-abstract-short" style="display: inline;">
        Recent work has managed to learn cross-lingual word embeddings without parallel data by mapping monolingual embeddings to a shared space through adversarial training. However, their evaluation has focused on favorable conditions, using comparable corpora or closely-related languages, and we show that they often fail in more realistic scenarios. This work proposes an alternative approach based on a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1805.06297v2-abstract-full').style.display = 'inline'; document.getElementById('1805.06297v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1805.06297v2-abstract-full" style="display: none;">
        Recent work has managed to learn cross-lingual word embeddings without parallel data by mapping monolingual embeddings to a shared space through adversarial training. However, their evaluation has focused on favorable conditions, using comparable corpora or closely-related languages, and we show that they often fail in more realistic scenarios. This work proposes an alternative approach based on a fully unsupervised initialization that explicitly exploits the structural similarity of the embeddings, and a robust self-learning algorithm that iteratively improves this solution. Our method succeeds in all tested scenarios and obtains the best published results in standard datasets, even surpassing previous supervised systems. Our implementation is released as an open source project at https://github.com/artetxem/vecmap
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1805.06297v2-abstract-full').style.display = 'none'; document.getElementById('1805.06297v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 May, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 May, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ACL 2018</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1805.04277">arXiv:1805.04277</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1805.04277">pdf</a>, <a href="https://arxiv.org/ps/1805.04277">ps</a>, <a href="https://arxiv.org/format/1805.04277">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The risk of sub-optimal use of Open Source NLP Software: UKB is inadvertently state-of-the-art in knowledge-based WSD
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>, 
      
      <a href="/search/?searchtype=author&amp;query=de+Lacalle%2C+O+L">Oier López de Lacalle</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Soroa%2C+A">Aitor Soroa</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1805.04277v1-abstract-short" style="display: inline;">
        UKB is an open source collection of programs for performing, among other tasks, knowledge-based Word Sense Disambiguation (WSD). Since it was released in 2009 it has been often used out-of-the-box in sub-optimal settings. We show that nine years later it is the state-of-the-art on knowledge-based WSD. This case shows the pitfalls of releasing open source NLP software without optimal default settin&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1805.04277v1-abstract-full').style.display = 'inline'; document.getElementById('1805.04277v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1805.04277v1-abstract-full" style="display: none;">
        UKB is an open source collection of programs for performing, among other tasks, knowledge-based Word Sense Disambiguation (WSD). Since it was released in 2009 it has been often used out-of-the-box in sub-optimal settings. We show that nine years later it is the state-of-the-art on knowledge-based WSD. This case shows the pitfalls of releasing open source NLP software without optimal default settings and precise instructions for reproducibility.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1805.04277v1-abstract-full').style.display = 'none'; document.getElementById('1805.04277v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 May, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1804.08316">arXiv:1804.08316</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1804.08316">pdf</a>, <a href="https://arxiv.org/ps/1804.08316">ps</a>, <a href="https://arxiv.org/format/1804.08316">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1016/j.knosys.2018.03.017">10.1016/j.knosys.2018.03.017 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Bilingual Embeddings with Random Walks over Multilingual Wordnets
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Goikoetxea%2C+J">J. Goikoetxea</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Soroa%2C+A">A. Soroa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">E. Agirre</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1804.08316v1-abstract-short" style="display: inline;">
        Bilingual word embeddings represent words of two languages in the same space, and allow to transfer knowledge from one language to the other without machine translation. The main approach is to train monolingual embeddings first and then map them using bilingual dictionaries. In this work, we present a novel method to learn bilingual embeddings based on multilingual knowledge bases (KB) such as Wo&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.08316v1-abstract-full').style.display = 'inline'; document.getElementById('1804.08316v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1804.08316v1-abstract-full" style="display: none;">
        Bilingual word embeddings represent words of two languages in the same space, and allow to transfer knowledge from one language to the other without machine translation. The main approach is to train monolingual embeddings first and then map them using bilingual dictionaries. In this work, we present a novel method to learn bilingual embeddings based on multilingual knowledge bases (KB) such as WordNet. Our method extracts bilingual information from multilingual wordnets via random walks and learns a joint embedding space in one go. We further reinforce cross-lingual equivalence adding bilingual con- straints in the loss function of the popular skipgram model. Our experiments involve twelve cross-lingual word similarity and relatedness datasets in six lan- guage pairs covering four languages, and show that: 1) random walks over mul- tilingual wordnets improve results over just using dictionaries; 2) multilingual wordnets on their own improve over text-based systems in similarity datasets; 3) the good results are consistent for large wordnets (e.g. English, Spanish), smaller wordnets (e.g. Basque) or loosely aligned wordnets (e.g. Italian); 4) the combination of wordnets and text yields the best results, above mapping-based approaches. Our method can be applied to richer KBs like DBpedia or Babel- Net, and can be easily extended to multilingual embeddings. All software and resources are open source.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1804.08316v1-abstract-full').style.display = 'none'; document.getElementById('1804.08316v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 April, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Preprint version, Knowledge-Based Systems (ISSN: 0950-7051). (2018)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1710.11041">arXiv:1710.11041</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1710.11041">pdf</a>, <a href="https://arxiv.org/format/1710.11041">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Unsupervised Neural Machine Translation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Artetxe%2C+M">Mikel Artetxe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Labaka%2C+G">Gorka Labaka</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agirre%2C+E">Eneko Agirre</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cho%2C+K">Kyunghyun Cho</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1710.11041v2-abstract-short" style="display: inline;">
        In spite of the recent success of neural machine translation (NMT) in standard benchmarks, the lack of large parallel corpora poses a major practical problem for many language pairs. There have been several proposals to alleviate this issue with, for instance, triangulation and semi-supervised learning techniques, but they still require a strong cross-lingual signal. In this work, we completely re&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1710.11041v2-abstract-full').style.display = 'inline'; document.getElementById('1710.11041v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1710.11041v2-abstract-full" style="display: none;">
        In spite of the recent success of neural machine translation (NMT) in standard benchmarks, the lack of large parallel corpora poses a major practical problem for many language pairs. There have been several proposals to alleviate this issue with, for instance, triangulation and semi-supervised learning techniques, but they still require a strong cross-lingual signal. In this work, we completely remove the need of parallel data and propose a novel method to train an NMT system in a completely unsupervised manner, relying on nothing but monolingual corpora. Our model builds upon the recent work on unsupervised embedding mappings, and consists of a slightly modified attentional encoder-decoder model that can be trained on monolingual corpora alone using a combination of denoising and backtranslation. Despite the simplicity of the approach, our system obtains 15.56 and 10.21 BLEU points in WMT 2014 French-to-English and German-to-English translation. The model can also profit from small parallel corpora, and attains 21.81 and 15.24 points when combined with 100,000 parallel sentences, respectively. Our implementation is released as an open source project.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1710.11041v2-abstract-full').style.display = 'none'; document.getElementById('1710.11041v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 February, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 30 October, 2017;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2017.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published as a conference paper at ICLR 2018</span>
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?query=Eneko+Agirre&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?query=Eneko+Agirre&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?query=Eneko+Agirre&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
      
    </ul>
  </nav>
  

  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>