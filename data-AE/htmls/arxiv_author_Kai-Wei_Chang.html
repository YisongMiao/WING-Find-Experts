<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;50 of 297 results for author: <span class="mathjax">Kai-Wei Chang</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/"  aria-role="search">
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Kai-Wei Chang">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Kai-Wei+Chang&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Kai-Wei Chang">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?query=Kai-Wei+Chang&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?query=Kai-Wei+Chang&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?query=Kai-Wei+Chang&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
        <li>
          <a href="/search/?query=Kai-Wei+Chang&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=100"
            class="pagination-link "
            aria-label="Page 3"
            aria-current="page">3
          </a>
        </li>
        
        <li>
          <a href="/search/?query=Kai-Wei+Chang&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=150"
            class="pagination-link "
            aria-label="Page 4"
            aria-current="page">4
          </a>
        </li>
        
        <li>
          <a href="/search/?query=Kai-Wei+Chang&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=200"
            class="pagination-link "
            aria-label="Page 5"
            aria-current="page">5
          </a>
        </li>
        
        <li>
          <a href="/search/?query=Kai-Wei+Chang&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=250"
            class="pagination-link "
            aria-label="Page 6"
            aria-current="page">6
          </a>
        </li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2509.02682">arXiv:2509.02682</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2509.02682">pdf</a>, <a href="https://arxiv.org/ps/2509.02682">ps</a>, <a href="https://arxiv.org/format/2509.02682">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Mesoscale and Nanoscale Physics">cond-mat.mes-hall</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Ultrafast anisotropic exciton transport in phosphorene
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Thompson%2C+J+J+P">Joshua J. P. Thompson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Monserrat%2C+B">Bartomeu Monserrat</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2509.02682v1-abstract-short" style="display: inline;">
        Phosphorene is a two-dimensional (2D) material exhibiting strong in-plane structural anisotropy. In this work, we investigate the influence of structural anisotropy on the optics, dynamics, and transport of excitons in phosphorene by combining microscopic many-body theory with first principles calculations. Our framework offers a complete and material specific description of the excitonic properti&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.02682v1-abstract-full').style.display = 'inline'; document.getElementById('2509.02682v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2509.02682v1-abstract-full" style="display: none;">
        Phosphorene is a two-dimensional (2D) material exhibiting strong in-plane structural anisotropy. In this work, we investigate the influence of structural anisotropy on the optics, dynamics, and transport of excitons in phosphorene by combining microscopic many-body theory with first principles calculations. Our framework offers a complete and material specific description of the excitonic properties of phosphorene, including exciton states and exciton-phonon interactions, which allow us to quantitatively evaluate the optical absorption spectra, exciton relaxation, and exciton transport, revealing direction-dependent characteristics. Interestingly, we identify the critical role of long-range exchange interactions, which significantly enhance the anisotropy of exciton diffusion, particularly at low temperatures. Our work provides fundamental insights into exciton dynamics in an intrinsically anisotropic 2D material, offering guiding principles for the design of next-generation optoelectronic devices.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.02682v1-abstract-full').style.display = 'none'; document.getElementById('2509.02682v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 September, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.20503">arXiv:2507.20503</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.20503">pdf</a>, <a href="https://arxiv.org/ps/2507.20503">ps</a>, <a href="https://arxiv.org/format/2507.20503">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Customize Multi-modal RAI Guardrails with Precedent-based predictions
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+C">Cheng-Fu Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tran%2C+T">Thanh Tran</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Christodoulopoulos%2C+C">Christos Christodoulopoulos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ruan%2C+W">Weitong Ruan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gupta%2C+R">Rahul Gupta</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.20503v1-abstract-short" style="display: inline;">
        A multi-modal guardrail must effectively filter image content based on user-defined policies, identifying material that may be hateful, reinforce harmful stereotypes, contain explicit material, or spread misinformation. Deploying such guardrails in real-world applications, however, poses significant challenges. Users often require varied and highly customizable policies and typically cannot provid&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.20503v1-abstract-full').style.display = 'inline'; document.getElementById('2507.20503v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.20503v1-abstract-full" style="display: none;">
        A multi-modal guardrail must effectively filter image content based on user-defined policies, identifying material that may be hateful, reinforce harmful stereotypes, contain explicit material, or spread misinformation. Deploying such guardrails in real-world applications, however, poses significant challenges. Users often require varied and highly customizable policies and typically cannot provide abundant examples for each custom policy. Consequently, an ideal guardrail should be scalable to the multiple policies and adaptable to evolving user standards with minimal retraining. Existing fine-tuning methods typically condition predictions on pre-defined policies, restricting their generalizability to new policies or necessitating extensive retraining to adapt. Conversely, training-free methods struggle with limited context lengths, making it difficult to incorporate all the policies comprehensively. To overcome these limitations, we propose to condition model&#39;s judgment on &#34;precedents&#34;, which are the reasoning processes of prior data points similar to the given input. By leveraging precedents instead of fixed policies, our approach greatly enhances the flexibility and adaptability of the guardrail. In this paper, we introduce a critique-revise mechanism for collecting high-quality precedents and two strategies that utilize precedents for robust prediction. Experimental results demonstrate that our approach outperforms previous methods across both few-shot and full-dataset scenarios and exhibits superior generalization to novel policies.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.20503v1-abstract-full').style.display = 'none'; document.getElementById('2507.20503v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 July, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to COLM 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.02768">arXiv:2507.02768</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.02768">pdf</a>, <a href="https://arxiv.org/ps/2507.02768">ps</a>, <a href="https://arxiv.org/format/2507.02768">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DeSTA2.5-Audio: Toward General-Purpose Large Audio Language Model with Self-Generated Cross-Modal Alignment
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+K">Ke-Han Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Z">Zhehuai Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fu%2C+S">Szu-Wei Fu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+C+H">Chao-Han Huck Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+S">Sung-Feng Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+C">Chih-Kai Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+C">Chee-En Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+C">Chun-Wei Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+W">Wei-Chih Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+C">Chien-yu Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+Y">Yi-Cheng Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+Y">Yu-Xiang Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fu%2C+C">Chi-An Fu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kuan%2C+C">Chun-Yi Kuan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ren%2C+W">Wenze Ren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xuanjun Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+W">Wei-Ping Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+E">En-Pei Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+T">Tzu-Quan Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+Y">Yuan-Kuei Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+K">Kuan-Po Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+H">Hsiao-Ying Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chou%2C+H">Huang-Cheng Chou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chiang%2C+C">Cheng-Han Chiang</a>
      , et al. (3 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.02768v1-abstract-short" style="display: inline;">
        We introduce DeSTA2.5-Audio, a general-purpose Large Audio Language Model (LALM) designed for robust auditory perception and instruction-following, without requiring task-specific audio instruction-tuning. Recent LALMs typically augment Large Language Models (LLMs) with auditory capabilities by training on large-scale, manually curated or LLM-synthesized audio-instruction datasets. However, these&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.02768v1-abstract-full').style.display = 'inline'; document.getElementById('2507.02768v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.02768v1-abstract-full" style="display: none;">
        We introduce DeSTA2.5-Audio, a general-purpose Large Audio Language Model (LALM) designed for robust auditory perception and instruction-following, without requiring task-specific audio instruction-tuning. Recent LALMs typically augment Large Language Models (LLMs) with auditory capabilities by training on large-scale, manually curated or LLM-synthesized audio-instruction datasets. However, these approaches have often suffered from the catastrophic forgetting of the LLM&#39;s original language abilities. To address this, we revisit the data construction pipeline and propose DeSTA, a self-generated cross-modal alignment strategy in which the backbone LLM generates its own training targets. This approach preserves the LLM&#39;s native language proficiency while establishing effective audio-text alignment, thereby enabling zero-shot generalization without task-specific tuning. Using DeSTA, we construct DeSTA-AQA5M, a large-scale, task-agnostic dataset containing 5 million training samples derived from 7,000 hours of audio spanning 50 diverse datasets, including speech, environmental sounds, and music. DeSTA2.5-Audio achieves state-of-the-art or competitive performance across a wide range of audio-language benchmarks, including Dynamic-SUPERB, MMAU, SAKURA, Speech-IFEval, and VoiceBench. Comprehensive comparative studies demonstrate that our self-generated strategy outperforms widely adopted data construction and training strategies in both auditory perception and instruction-following capabilities. Our findings underscore the importance of carefully designed data construction in LALM development and offer practical insights for building robust, general-purpose LALMs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.02768v1-abstract-full').style.display = 'none'; document.getElementById('2507.02768v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 July, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Model and code available at: https://github.com/kehanlu/DeSTA2.5-Audio</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.17514">arXiv:2506.17514</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.17514">pdf</a>, <a href="https://arxiv.org/ps/2506.17514">ps</a>, <a href="https://arxiv.org/format/2506.17514">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Kaleidoscopic Teaming in Multi Agent Simulations
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mehrabi%2C+N">Ninareh Mehrabi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kumarage%2C+T">Tharindu Kumarage</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Galstyan%2C+A">Aram Galstyan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gupta%2C+R">Rahul Gupta</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.17514v1-abstract-short" style="display: inline;">
        Warning: This paper contains content that may be inappropriate or offensive.
  AI agents have gained significant recent attention due to their autonomous tool usage capabilities and their integration in various real-world applications. This autonomy poses novel challenges for the safety of such systems, both in single- and multi-agent scenarios. We argue that existing red teaming or safety evaluat&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.17514v1-abstract-full').style.display = 'inline'; document.getElementById('2506.17514v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.17514v1-abstract-full" style="display: none;">
        Warning: This paper contains content that may be inappropriate or offensive.
  AI agents have gained significant recent attention due to their autonomous tool usage capabilities and their integration in various real-world applications. This autonomy poses novel challenges for the safety of such systems, both in single- and multi-agent scenarios. We argue that existing red teaming or safety evaluation frameworks fall short in evaluating safety risks in complex behaviors, thought processes and actions taken by agents. Moreover, they fail to consider risks in multi-agent setups where various vulnerabilities can be exposed when agents engage in complex behaviors and interactions with each other. To address this shortcoming, we introduce the term kaleidoscopic teaming which seeks to capture complex and wide range of vulnerabilities that can happen in agents both in single-agent and multi-agent scenarios. We also present a new kaleidoscopic teaming framework that generates a diverse array of scenarios modeling real-world human societies. Our framework evaluates safety of agents in both single-agent and multi-agent setups. In single-agent setup, an agent is given a scenario that it needs to complete using the tools it has access to. In multi-agent setup, multiple agents either compete against or cooperate together to complete a task in the scenario through which we capture existing safety vulnerabilities in agents. We introduce new in-context optimization techniques that can be used in our kaleidoscopic teaming framework to generate better scenarios for safety analysis. Lastly, we present appropriate metrics that can be used along with our framework to measure safety of agents. Utilizing our kaleidoscopic teaming framework, we identify vulnerabilities in various models with respect to their safety in agentic use-cases.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.17514v1-abstract-full').style.display = 'none'; document.getElementById('2506.17514v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.15677">arXiv:2506.15677</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.15677">pdf</a>, <a href="https://arxiv.org/ps/2506.15677">ps</a>, <a href="https://arxiv.org/format/2506.15677">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Embodied Web Agents: Bridging Physical-Digital Realms for Integrated Agent Intelligence
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hong%2C+Y">Yining Hong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+R">Rui Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+B">Bingxuan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yao%2C+X">Xingcheng Yao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+M">Maxine Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chien%2C+A">Alexander Chien</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+D">Da Yin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+Y+N">Ying Nian Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Z+J">Zhecan James Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.15677v3-abstract-short" style="display: inline;">
        AI agents today are mostly siloed - they either retrieve and reason over vast amount of digital information and knowledge obtained online; or interact with the physical world through embodied perception, planning and action - but rarely both. This separation limits their ability to solve tasks that require integrated physical and digital intelligence, such as cooking from online recipes, navigatin&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.15677v3-abstract-full').style.display = 'inline'; document.getElementById('2506.15677v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.15677v3-abstract-full" style="display: none;">
        AI agents today are mostly siloed - they either retrieve and reason over vast amount of digital information and knowledge obtained online; or interact with the physical world through embodied perception, planning and action - but rarely both. This separation limits their ability to solve tasks that require integrated physical and digital intelligence, such as cooking from online recipes, navigating with dynamic map data, or interpreting real-world landmarks using web knowledge. We introduce Embodied Web Agents, a novel paradigm for AI agents that fluidly bridge embodiment and web-scale reasoning. To operationalize this concept, we first develop the Embodied Web Agents task environments, a unified simulation platform that tightly integrates realistic 3D indoor and outdoor environments with functional web interfaces. Building upon this platform, we construct and release the Embodied Web Agents Benchmark, which encompasses a diverse suite of tasks including cooking, navigation, shopping, tourism, and geolocation - all requiring coordinated reasoning across physical and digital realms for systematic assessment of cross-domain intelligence. Experimental results reveal significant performance gaps between state-of-the-art AI systems and human capabilities, establishing both challenges and opportunities at the intersection of embodied cognition and web-scale knowledge access. All datasets, codes and websites are publicly available at our project page https://embodied-web-agent.github.io/.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.15677v3-abstract-full').style.display = 'none'; document.getElementById('2506.15677v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 July, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 June, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.12103">arXiv:2506.12103</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.12103">pdf</a>, <a href="https://arxiv.org/format/2506.12103">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Amazon Nova Family of Models: Technical Report and Model Card
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=AGI%2C+A">Amazon AGI</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Langford%2C+A">Aaron Langford</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+A">Aayush Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gupta%2C+A">Abhanshu Gupta</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bhatter%2C+A">Abhimanyu Bhatter</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goyal%2C+A">Abhinav Goyal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mathur%2C+A">Abhinav Mathur</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mohanty%2C+A">Abhinav Mohanty</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kumar%2C+A">Abhishek Kumar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sethi%2C+A">Abhishek Sethi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Komma%2C+A">Abi Komma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pena%2C+A">Abner Pena</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jain%2C+A">Achin Jain</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kunysz%2C+A">Adam Kunysz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Opyrchal%2C+A">Adam Opyrchal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Singh%2C+A">Adarsh Singh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rawal%2C+A">Aditya Rawal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Prasad%2C+A+A+B">Adok Achar Budihal Prasad</a>, 
      
      <a href="/search/?searchtype=author&amp;query=de+Gispert%2C+A">Adri√† de Gispert</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kumar%2C+A">Agnika Kumar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aryamane%2C+A">Aishwarya Aryamane</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nair%2C+A">Ajay Nair</a>, 
      
      <a href="/search/?searchtype=author&amp;query=M%2C+A">Akilan M</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Iyengar%2C+A">Akshaya Iyengar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shanbhogue%2C+A+V+K">Akshaya Vishnu Kudlu Shanbhogue</a>
      , et al. (761 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.12103v1-abstract-short" style="display: inline;">
        We present Amazon Nova, a new generation of state-of-the-art foundation models that deliver frontier intelligence and industry-leading price performance. Amazon Nova Pro is a highly-capable multimodal model with the best combination of accuracy, speed, and cost for a wide range of tasks. Amazon Nova Lite is a low-cost multimodal model that is lightning fast for processing images, video, documents&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.12103v1-abstract-full').style.display = 'inline'; document.getElementById('2506.12103v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.12103v1-abstract-full" style="display: none;">
        We present Amazon Nova, a new generation of state-of-the-art foundation models that deliver frontier intelligence and industry-leading price performance. Amazon Nova Pro is a highly-capable multimodal model with the best combination of accuracy, speed, and cost for a wide range of tasks. Amazon Nova Lite is a low-cost multimodal model that is lightning fast for processing images, video, documents and text. Amazon Nova Micro is a text-only model that delivers our lowest-latency responses at very low cost. Amazon Nova Canvas is an image generation model that creates professional grade images with rich customization controls. Amazon Nova Reel is a video generation model offering high-quality outputs, customization, and motion control. Our models were built responsibly and with a commitment to customer trust, security, and reliability. We report benchmarking results for core capabilities, agentic performance, long context, functional adaptation, runtime performance, and human evaluation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.12103v1-abstract-full').style.display = 'none'; document.getElementById('2506.12103v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 March, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">48 pages, 10 figures</span>
    </p>
    

    
      <p class="comments is-size-7">
        
          <span class="has-text-black-bis has-text-weight-semibold">Report number:</span>
          20250317
        

        

        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.08164">arXiv:2506.08164</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.08164">pdf</a>, <a href="https://arxiv.org/ps/2506.08164">ps</a>, <a href="https://arxiv.org/format/2506.08164">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        BLUR: A Bi-Level Optimization Approach for LLM Unlearning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Reisizadeh%2C+H">Hadi Reisizadeh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jia%2C+J">Jinghan Jia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bu%2C+Z">Zhiqi Bu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vinzamuri%2C+B">Bhanukiran Vinzamuri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ramakrishna%2C+A">Anil Ramakrishna</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cevher%2C+V">Volkan Cevher</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+S">Sijia Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hong%2C+M">Mingyi Hong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.08164v1-abstract-short" style="display: inline;">
        Enabling large language models (LLMs) to unlearn knowledge and capabilities acquired during training has proven vital for ensuring compliance with data regulations and promoting ethical practices in generative AI. Although there are growing interests in developing various unlearning algorithms, it remains unclear how to best formulate the unlearning problem. The most popular formulation uses a wei&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.08164v1-abstract-full').style.display = 'inline'; document.getElementById('2506.08164v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.08164v1-abstract-full" style="display: none;">
        Enabling large language models (LLMs) to unlearn knowledge and capabilities acquired during training has proven vital for ensuring compliance with data regulations and promoting ethical practices in generative AI. Although there are growing interests in developing various unlearning algorithms, it remains unclear how to best formulate the unlearning problem. The most popular formulation uses a weighted sum of forget and retain loss, but it often leads to performance degradation due to the inherent trade-off between forget and retain losses. In this work, we argue that it is important to model the hierarchical structure of the unlearning problem, where the forget problem (which \textit{unlearns} certain knowledge and/or capabilities) takes priority over the retain problem (which preserves model utility). This hierarchical structure naturally leads to a bi-level optimization formulation where the lower-level objective focuses on minimizing the forget loss, while the upper-level objective aims to maintain the model&#39;s utility. Based on this new formulation, we propose a novel algorithm, termed Bi-Level UnleaRning (\texttt{BLUR}), which not only possesses strong theoretical guarantees but more importantly, delivers superior performance. In particular, our extensive experiments demonstrate that \texttt{BLUR} consistently outperforms all the state-of-the-art algorithms across various unlearning tasks, models, and metrics. Codes are available at https://github.com/OptimAI-Lab/BLURLLMUnlearning.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.08164v1-abstract-full').style.display = 'none'; document.getElementById('2506.08164v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.05128">arXiv:2506.05128</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.05128">pdf</a>, <a href="https://arxiv.org/ps/2506.05128">ps</a>, <a href="https://arxiv.org/format/2506.05128">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DiCoRe: Enhancing Zero-shot Event Detection via Divergent-Convergent LLM Reasoning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Parekh%2C+T">Tanmay Parekh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mehta%2C+K">Kartik Mehta</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mehrabi%2C+N">Ninareh Mehrabi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+N">Nanyun Peng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.05128v1-abstract-short" style="display: inline;">
        Zero-shot Event Detection (ED), the task of identifying event mentions in natural language text without any training data, is critical for document understanding in specialized domains. Understanding the complex event ontology, extracting domain-specific triggers from the passage, and structuring them appropriately overloads and limits the utility of Large Language Models (LLMs) for zero-shot ED.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.05128v1-abstract-full').style.display = 'inline'; document.getElementById('2506.05128v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.05128v1-abstract-full" style="display: none;">
        Zero-shot Event Detection (ED), the task of identifying event mentions in natural language text without any training data, is critical for document understanding in specialized domains. Understanding the complex event ontology, extracting domain-specific triggers from the passage, and structuring them appropriately overloads and limits the utility of Large Language Models (LLMs) for zero-shot ED. To this end, we propose DiCoRe, a divergent-convergent reasoning framework that decouples the task of ED using Dreamer and Grounder. Dreamer encourages divergent reasoning through open-ended event discovery, which helps to boost event coverage. Conversely, Grounder introduces convergent reasoning to align the free-form predictions with the task-specific instructions using finite-state machine guided constrained decoding. Additionally, an LLM-Judge verifies the final outputs to ensure high precision. Through extensive experiments on six datasets across five domains and nine LLMs, we demonstrate how DiCoRe consistently outperforms prior zero-shot, transfer-learning, and reasoning baselines, achieving 4-7% average F1 gains over the best baseline -- establishing DiCoRe as a strong zero-shot ED framework.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.05128v1-abstract-full').style.display = 'none'; document.getElementById('2506.05128v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted at ACL ARR May 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.04178">arXiv:2506.04178</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.04178">pdf</a>, <a href="https://arxiv.org/ps/2506.04178">ps</a>, <a href="https://arxiv.org/format/2506.04178">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        OpenThoughts: Data Recipes for Reasoning Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Guha%2C+E">Etash Guha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Marten%2C+R">Ryan Marten</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Keh%2C+S">Sedrick Keh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Raoof%2C+N">Negin Raoof</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Smyrnis%2C+G">Georgios Smyrnis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bansal%2C+H">Hritik Bansal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nezhurina%2C+M">Marianna Nezhurina</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mercat%2C+J">Jean Mercat</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vu%2C+T">Trung Vu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sprague%2C+Z">Zayne Sprague</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Suvarna%2C+A">Ashima Suvarna</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feuer%2C+B">Benjamin Feuer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+L">Liangyu Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Khan%2C+Z">Zaid Khan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Frankel%2C+E">Eric Frankel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Grover%2C+S">Sachin Grover</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Choi%2C+C">Caroline Choi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Muennighoff%2C+N">Niklas Muennighoff</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Su%2C+S">Shiye Su</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+W">Wanjia Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+J">John Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pimpalgaonkar%2C+S">Shreyas Pimpalgaonkar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sharma%2C+K">Kartik Sharma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ji%2C+C+C">Charlie Cheng-Jie Ji</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+Y">Yichuan Deng</a>
      , et al. (25 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.04178v2-abstract-short" style="display: inline;">
        Reasoning models have made rapid progress on many benchmarks involving math, code, and science. Yet, there are still many open questions about the best training recipes for reasoning since state-of-the-art models often rely on proprietary datasets with little to no public information available. To address this, the goal of the OpenThoughts project is to create open-source datasets for training rea&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.04178v2-abstract-full').style.display = 'inline'; document.getElementById('2506.04178v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.04178v2-abstract-full" style="display: none;">
        Reasoning models have made rapid progress on many benchmarks involving math, code, and science. Yet, there are still many open questions about the best training recipes for reasoning since state-of-the-art models often rely on proprietary datasets with little to no public information available. To address this, the goal of the OpenThoughts project is to create open-source datasets for training reasoning models. After initial explorations, our OpenThoughts2-1M dataset led to OpenThinker2-32B, the first model trained on public reasoning data to match DeepSeek-R1-Distill-32B on standard reasoning benchmarks such as AIME and LiveCodeBench. We then improve our dataset further by systematically investigating each step of our data generation pipeline with 1,000+ controlled experiments, which led to OpenThoughts3. Scaling the pipeline to 1.2M examples and using QwQ-32B as teacher yields our OpenThoughts3-7B model, which achieves state-of-the-art results: 53% on AIME 2025, 51% on LiveCodeBench 06/24-01/25, and 54% on GPQA Diamond - improvements of 15.3, 17.2, and 20.5 percentage points compared to the DeepSeek-R1-Distill-Qwen-7B. All of our datasets and models are available on https://openthoughts.ai.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.04178v2-abstract-full').style.display = 'none'; document.getElementById('2506.04178v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 June, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 4 June, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">https://www.openthoughts.ai/blog/ot3. arXiv admin note: text overlap with arXiv:2505.23754 by other authors</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.00876">arXiv:2506.00876</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.00876">pdf</a>, <a href="https://arxiv.org/ps/2506.00876">ps</a>, <a href="https://arxiv.org/format/2506.00876">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Not Every Token Needs Forgetting: Selective Unlearning to Limit Change in Utility in Large Language Model Unlearning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wan%2C+Y">Yixin Wan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ramakrishna%2C+A">Anil Ramakrishna</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cevher%2C+V">Volkan Cevher</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gupta%2C+R">Rahul Gupta</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.00876v1-abstract-short" style="display: inline;">
        Large Language Model (LLM) unlearning has recently gained significant attention, driven by the need to remove unwanted information, such as private, sensitive, or copyrighted content, from LLMs. However, conventional unlearning approaches indiscriminately update model parameters to forget all tokens in a target document, including common tokens (e.g., pronouns, prepositions, general nouns) that ca&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.00876v1-abstract-full').style.display = 'inline'; document.getElementById('2506.00876v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.00876v1-abstract-full" style="display: none;">
        Large Language Model (LLM) unlearning has recently gained significant attention, driven by the need to remove unwanted information, such as private, sensitive, or copyrighted content, from LLMs. However, conventional unlearning approaches indiscriminately update model parameters to forget all tokens in a target document, including common tokens (e.g., pronouns, prepositions, general nouns) that carry general knowledge. In this paper, we highlight that not every token needs forgetting. We propose Selective Unlearning (SU), which identifies a critical subset of tokens within the forgetting set that is relevant to the unwanted information, and unlearns only those tokens. Experiments on two benchmarks and six baseline unlearning algorithms demonstrate that SU not only achieves effective unlearning on the targeted forget data, but also significantly preserves the model&#39;s utility in the retaining set.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.00876v1-abstract-full').style.display = 'none'; document.getElementById('2506.00876v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.00241">arXiv:2506.00241</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.00241">pdf</a>, <a href="https://arxiv.org/format/2506.00241">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Designing AI Tools for Clinical Care Teams to Support Serious Illness Conversations with Older Adults in the Emergency Department
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+M">Menglin Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yong%2C+Z">Zhuorui Yong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guan%2C+R">Ruijia Guan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Haimovich%2C+A">Adrian Haimovich</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ouchi%2C+K">Kei Ouchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bickmore%2C+T">Timothy Bickmore</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yao%2C+B">Bingsheng Yao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+D">Dakuo Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Desai%2C+S">Smit Desai</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.00241v1-abstract-short" style="display: inline;">
        Serious illness conversations (SICs), discussions between clinical care teams and patients with serious, life-limiting illnesses about their values, goals, and care preferences, are critical for patient-centered care. Without these conversations, patients often receive aggressive interventions that may not align with their goals. Clinical care teams face significant barriers when conducting seriou&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.00241v1-abstract-full').style.display = 'inline'; document.getElementById('2506.00241v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.00241v1-abstract-full" style="display: none;">
        Serious illness conversations (SICs), discussions between clinical care teams and patients with serious, life-limiting illnesses about their values, goals, and care preferences, are critical for patient-centered care. Without these conversations, patients often receive aggressive interventions that may not align with their goals. Clinical care teams face significant barriers when conducting serious illness conversations with older adult patients in Emergency Department (ED) settings, where most older adult patients lack documented treatment goals. To understand current practices and identify AI support opportunities, we conducted interviews with two domain experts and nine ED clinical care team members. Through thematic analysis, we characterized a four-phase serious illness conversation workflow (identification, preparation, conduction, documentation) and identified key needs and challenges at each stage. Clinical care teams struggle with fragmented EHR data access, time constraints, emotional preparation demands, and documentation burdens. While participants expressed interest in AI tools for information synthesis, conversational support, and automated documentation, they emphasized preserving human connection and clinical autonomy. We present design guidelines for AI tools supporting SIC workflows that fit within existing clinical practices. This work contributes empirical understanding of ED-based serious illness conversations and provides design considerations for AI in high-stakes clinical environments.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.00241v1-abstract-full').style.display = 'none'; document.getElementById('2506.00241v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 May, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2505.22657">arXiv:2505.22657</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2505.22657">pdf</a>, <a href="https://arxiv.org/format/2505.22657">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        3DLLM-Mem: Long-Term Spatial-Temporal Memory for Embodied 3D Large Language Model
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+W">Wenbo Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hong%2C+Y">Yining Hong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yanjun Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+L">Leison Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wei%2C+Z">Zibu Wei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yao%2C+X">Xingcheng Yao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+N">Nanyun Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bitton%2C+Y">Yonatan Bitton</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Szpektor%2C+I">Idan Szpektor</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2505.22657v1-abstract-short" style="display: inline;">
        Humans excel at performing complex tasks by leveraging long-term memory across temporal and spatial experiences. In contrast, current Large Language Models (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D environments. We posit that part of this limitation is due to the lack of proper 3D spatial-temporal memory modeling in LLMs. To address this, we first introduce 3DMem-Bench,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.22657v1-abstract-full').style.display = 'inline'; document.getElementById('2505.22657v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2505.22657v1-abstract-full" style="display: none;">
        Humans excel at performing complex tasks by leveraging long-term memory across temporal and spatial experiences. In contrast, current Large Language Models (LLMs) struggle to effectively plan and act in dynamic, multi-room 3D environments. We posit that part of this limitation is due to the lack of proper 3D spatial-temporal memory modeling in LLMs. To address this, we first introduce 3DMem-Bench, a comprehensive benchmark comprising over 26,000 trajectories and 2,892 embodied tasks, question-answering and captioning, designed to evaluate an agent&#39;s ability to reason over long-term memory in 3D environments. Second, we propose 3DLLM-Mem, a novel dynamic memory management and fusion model for embodied spatial-temporal reasoning and actions in LLMs. Our model uses working memory tokens, which represents current observations, as queries to selectively attend to and fuse the most useful spatial and temporal features from episodic memory, which stores past observations and interactions. Our approach allows the agent to focus on task-relevant information while maintaining memory efficiency in complex, long-horizon environments. Experimental results demonstrate that 3DLLM-Mem achieves state-of-the-art performance across various tasks, outperforming the strongest baselines by 16.5% in success rate on 3DMem-Bench&#39;s most challenging in-the-wild embodied tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.22657v1-abstract-full').style.display = 'none'; document.getElementById('2505.22657v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 May, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">demos at: https://3dllm-mem.github.io</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2505.21784">arXiv:2505.21784</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2505.21784">pdf</a>, <a href="https://arxiv.org/ps/2505.21784">ps</a>, <a href="https://arxiv.org/format/2505.21784">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards Safety Reasoning in LLMs: AI-agentic Deliberation for Policy-embedded CoT Data Creation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kumarage%2C+T">Tharindu Kumarage</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mehrabi%2C+N">Ninareh Mehrabi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ramakrishna%2C+A">Anil Ramakrishna</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+X">Xinyan Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zemel%2C+R">Richard Zemel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Galstyan%2C+A">Aram Galstyan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gupta%2C+R">Rahul Gupta</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peris%2C+C">Charith Peris</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2505.21784v1-abstract-short" style="display: inline;">
        Safety reasoning is a recent paradigm where LLMs reason over safety policies before generating responses, thereby mitigating limitations in existing safety measures such as over-refusal and jailbreak vulnerabilities. However, implementing this paradigm is challenging due to the resource-intensive process of creating high-quality policy-embedded chain-of-thought (CoT) datasets while ensuring reason&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.21784v1-abstract-full').style.display = 'inline'; document.getElementById('2505.21784v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2505.21784v1-abstract-full" style="display: none;">
        Safety reasoning is a recent paradigm where LLMs reason over safety policies before generating responses, thereby mitigating limitations in existing safety measures such as over-refusal and jailbreak vulnerabilities. However, implementing this paradigm is challenging due to the resource-intensive process of creating high-quality policy-embedded chain-of-thought (CoT) datasets while ensuring reasoning remains accurate and free from hallucinations or policy conflicts. To tackle this, we propose AIDSAFE: Agentic Iterative Deliberation for Safety Reasoning, a novel data generation recipe that leverages multi-agent deliberation to iteratively expand reasoning on safety policies. A data refiner stage in AIDSAFE ensures high-quality outputs by eliminating repetitive, redundant, and deceptive thoughts. AIDSAFE-generated CoTs provide a strong foundation for supervised fine-tuning (SFT)-based safety training. Additionally, to address the need of preference data in alignment stages, such as DPO training, we introduce a supplemental recipe that uses belief augmentation to create distinct selected and rejected CoT samples. Our evaluations demonstrate that AIDSAFE-generated CoTs achieve superior policy adherence and reasoning quality. Consequently, we show that fine-tuning open-source LLMs on these CoTs can significantly improve safety generalization and jailbreak robustness while maintaining acceptable utility and over-refusal accuracy. AIDSAFE-generated CoT datasets can be found here: https://huggingface.co/datasets/AmazonScience/AIDSAFE
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.21784v1-abstract-full').style.display = 'none'; document.getElementById('2505.21784v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 May, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to ACL 2025 (Findings)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2505.20759">arXiv:2505.20759</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2505.20759">pdf</a>, <a href="https://arxiv.org/ps/2505.20759">ps</a>, <a href="https://arxiv.org/format/2505.20759">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PARTONOMY: Large Multimodal Models with Part-Level Visual Understanding
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Blume%2C+A">Ansel Blume</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+J">Jeonghwan Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ha%2C+H">Hyeonjeong Ha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chatikyan%2C+E">Elen Chatikyan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jin%2C+X">Xiaomeng Jin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nguyen%2C+K+D">Khanh Duy Nguyen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+N">Nanyun Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hoiem%2C+D">Derek Hoiem</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ji%2C+H">Heng Ji</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2505.20759v2-abstract-short" style="display: inline;">
        Real-world objects are composed of distinctive, object-specific parts. Identifying these parts is key to performing fine-grained, compositional reasoning-yet, large multimodal models (LMMs) struggle to perform this seemingly straightforward task. In this work, we introduce PARTONOMY, an LMM benchmark designed for pixel-level part grounding. We construct PARTONOMY from existing part datasets and ou&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.20759v2-abstract-full').style.display = 'inline'; document.getElementById('2505.20759v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2505.20759v2-abstract-full" style="display: none;">
        Real-world objects are composed of distinctive, object-specific parts. Identifying these parts is key to performing fine-grained, compositional reasoning-yet, large multimodal models (LMMs) struggle to perform this seemingly straightforward task. In this work, we introduce PARTONOMY, an LMM benchmark designed for pixel-level part grounding. We construct PARTONOMY from existing part datasets and our own rigorously annotated set of images, encompassing 862 part labels and 534 object labels for evaluation. Unlike existing datasets that simply ask models to identify generic parts, PARTONOMY uses specialized concepts (e.g., agricultural airplane), and challenges models to compare objects&#39; parts, consider part-whole relationships, and justify textual predictions with visual segmentations. Our experiments demonstrate significant limitations in state-of-the-art LMMs (e.g., LISA-13B achieves only 5.9% gIoU), highlighting a critical gap in their part grounding abilities. We note that existing segmentation-enabled LMMs (segmenting LMMs) have two key architectural shortcomings: they use special [SEG] tokens not seen during pretraining which induce distribution shift, and they discard predicted segmentations instead of using past predictions to guide future ones. To address these deficiencies, we train several part-centric LMMs and propose PLUM, a novel segmenting LMM that uses span tagging instead of segmentation tokens and that conditions on prior predictions in a feedback loop. We find that pretrained PLUM outperforms existing segmenting LMMs on reasoning segmentation, VQA, and visual hallucination benchmarks. In addition, PLUM finetuned on our proposed Explanatory Part Segmentation task is competitive with segmenting LMMs trained on significantly more segmentation data. Our work opens up new avenues towards enabling fine-grained, grounded visual understanding in LMMs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.20759v2-abstract-full').style.display = 'none'; document.getElementById('2505.20759v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 June, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 27 May, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">18 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2505.20291">arXiv:2505.20291</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2505.20291">pdf</a>, <a href="https://arxiv.org/format/2505.20291">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Visualized Text-to-Image Retrieval
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+D">Di Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wan%2C+Y">Yixin Wan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2505.20291v1-abstract-short" style="display: inline;">
        We propose Visualize-then-Retrieve (VisRet), a new paradigm for Text-to-Image (T2I) retrieval that mitigates the limitations of cross-modal similarity alignment of existing multi-modal embeddings. VisRet first projects textual queries into the image modality via T2I generation. Then, it performs retrieval within the image modality to bypass the weaknesses of cross-modal retrievers in recognizing s&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.20291v1-abstract-full').style.display = 'inline'; document.getElementById('2505.20291v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2505.20291v1-abstract-full" style="display: none;">
        We propose Visualize-then-Retrieve (VisRet), a new paradigm for Text-to-Image (T2I) retrieval that mitigates the limitations of cross-modal similarity alignment of existing multi-modal embeddings. VisRet first projects textual queries into the image modality via T2I generation. Then, it performs retrieval within the image modality to bypass the weaknesses of cross-modal retrievers in recognizing subtle visual-spatial features. Experiments on three knowledge-intensive T2I retrieval benchmarks, including a newly introduced multi-entity benchmark, demonstrate that VisRet consistently improves T2I retrieval by 24.5% to 32.7% NDCG@10 across different embedding models. VisRet also significantly benefits downstream visual question answering accuracy when used in retrieval-augmented generation pipelines. The method is plug-and-play and compatible with off-the-shelf retrievers, making it an effective module for knowledge-intensive multi-modal systems. Our code and the new benchmark are publicly available at https://github.com/xiaowu0162/Visualize-then-Retrieve.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.20291v1-abstract-full').style.display = 'none'; document.getElementById('2505.20291v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 May, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Work in Progress</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2505.17496">arXiv:2505.17496</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2505.17496">pdf</a>, <a href="https://arxiv.org/ps/2505.17496">ps</a>, <a href="https://arxiv.org/format/2505.17496">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Analyzing Mitigation Strategies for Catastrophic Forgetting in End-to-End Training of Spoken Language Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hsiao%2C+C">Chi-Yuan Hsiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+K">Ke-Han Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+C">Chih-Kai Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+W">Wei-Chih Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+H">Hung-yi Lee</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2505.17496v1-abstract-short" style="display: inline;">
        End-to-end training of Spoken Language Models (SLMs) commonly involves adapting pre-trained text-based Large Language Models (LLMs) to the speech modality through multi-stage training on diverse tasks such as ASR, TTS and spoken question answering (SQA). Although this multi-stage continual learning equips LLMs with both speech understanding and generation capabilities, the substantial differences&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.17496v1-abstract-full').style.display = 'inline'; document.getElementById('2505.17496v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2505.17496v1-abstract-full" style="display: none;">
        End-to-end training of Spoken Language Models (SLMs) commonly involves adapting pre-trained text-based Large Language Models (LLMs) to the speech modality through multi-stage training on diverse tasks such as ASR, TTS and spoken question answering (SQA). Although this multi-stage continual learning equips LLMs with both speech understanding and generation capabilities, the substantial differences in task and data distributions across stages can lead to catastrophic forgetting, where previously acquired knowledge is lost. This paper investigates catastrophic forgetting and evaluates three mitigation strategies-model merging, discounting the LoRA scaling factor, and experience replay to balance knowledge retention with new learning. Results show that experience replay is the most effective, with further gains achieved by combining it with other methods. These findings provide insights for developing more robust and efficient SLM training pipelines.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.17496v1-abstract-full').style.display = 'none'; document.getElementById('2505.17496v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 May, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to Interspeech 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2505.16839">arXiv:2505.16839</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2505.16839">pdf</a>, <a href="https://arxiv.org/ps/2505.16839">ps</a>, <a href="https://arxiv.org/format/2505.16839">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LaViDa: A Large Diffusion Language Model for Multimodal Understanding
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+S">Shufan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kallidromitis%2C+K">Konstantinos Kallidromitis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bansal%2C+H">Hritik Bansal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gokul%2C+A">Akash Gokul</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kato%2C+Y">Yusuke Kato</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kozuka%2C+K">Kazuki Kozuka</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kuen%2C+J">Jason Kuen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+Z">Zhe Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Grover%2C+A">Aditya Grover</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2505.16839v3-abstract-short" style="display: inline;">
        Modern Vision-Language Models (VLMs) can solve a wide range of tasks requiring visual reasoning. In real-world scenarios, desirable properties for VLMs include fast inference and controllable generation (e.g., constraining outputs to adhere to a desired format). However, existing autoregressive (AR) VLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs) offer a promising altern&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.16839v3-abstract-full').style.display = 'inline'; document.getElementById('2505.16839v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2505.16839v3-abstract-full" style="display: none;">
        Modern Vision-Language Models (VLMs) can solve a wide range of tasks requiring visual reasoning. In real-world scenarios, desirable properties for VLMs include fast inference and controllable generation (e.g., constraining outputs to adhere to a desired format). However, existing autoregressive (AR) VLMs like LLaVA struggle in these aspects. Discrete diffusion models (DMs) offer a promising alternative, enabling parallel decoding for faster inference and bidirectional context for controllable generation through text-infilling. While effective in language-only settings, DMs&#39; potential for multimodal tasks is underexplored. We introduce LaViDa, a family of VLMs built on DMs. We build LaViDa by equipping DMs with a vision encoder and jointly fine-tune the combined parts for multimodal instruction following. To address challenges encountered, LaViDa incorporates novel techniques such as complementary masking for effective training, prefix KV cache for efficient inference, and timestep shifting for high-quality sampling. Experiments show that LaViDa achieves competitive or superior performance to AR VLMs on multi-modal benchmarks such as MMMU, while offering unique advantages of DMs, including flexible speed-quality tradeoff, controllability, and bidirectional reasoning. On COCO captioning, LaViDa surpasses Open-LLaVa-Next-8B by +4.1 CIDEr with 1.92x speedup. On bidirectional tasks, it achieves +59% improvement on Constrained Poem Completion. These results demonstrate LaViDa as a strong alternative to AR VLMs. Code and models will be released in the camera-ready version.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.16839v3-abstract-full').style.display = 'none'; document.getElementById('2505.16839v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 June, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 22 May, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">26 pages, 8 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2505.14999">arXiv:2505.14999</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2505.14999">pdf</a>, <a href="https://arxiv.org/ps/2505.14999">ps</a>, <a href="https://arxiv.org/format/2505.14999">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning to Rank Chain-of-Thought: An Energy-Based Approach with Outcome Supervision
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+E+H">Eric Hanchen Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+H">Haozheng Luo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pang%2C+S">Shengyuan Pang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xiaomin Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qi%2C+Z">Zhenting Qi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+H">Hengli Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+C">Cheng-Fu Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+Z">Zongyu Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xinfeng Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+H">Hao Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+Y+N">Ying Nian Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2505.14999v2-abstract-short" style="display: inline;">
        Mathematical reasoning presents a significant challenge for Large Language Models (LLMs), often requiring robust multi step logical consistency. While Chain of Thought (CoT) prompting elicits reasoning steps, it doesn&#39;t guarantee correctness, and improving reliability via extensive sampling is computationally costly. This paper introduces the Energy Outcome Reward Model (EORM), an effective, light&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.14999v2-abstract-full').style.display = 'inline'; document.getElementById('2505.14999v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2505.14999v2-abstract-full" style="display: none;">
        Mathematical reasoning presents a significant challenge for Large Language Models (LLMs), often requiring robust multi step logical consistency. While Chain of Thought (CoT) prompting elicits reasoning steps, it doesn&#39;t guarantee correctness, and improving reliability via extensive sampling is computationally costly. This paper introduces the Energy Outcome Reward Model (EORM), an effective, lightweight, post hoc verifier. EORM leverages Energy Based Models (EBMs) to simplify the training of reward models by learning to assign a scalar energy score to CoT solutions using only outcome labels, thereby avoiding detailed annotations. It achieves this by interpreting discriminator output logits as negative energies, effectively ranking candidates where lower energy is assigned to solutions leading to correct final outcomes implicitly favoring coherent reasoning. On mathematical benchmarks (GSM8k, MATH), EORM significantly improves final answer accuracy (e.g., with Llama 3 8B, achieving 90.7% on GSM8k and 63.7% on MATH). EORM effectively leverages a given pool of candidate solutions to match or exceed the performance of brute force sampling, thereby enhancing LLM reasoning outcome reliability through its streamlined post hoc verification process.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.14999v2-abstract-full').style.display = 'none'; document.getElementById('2505.14999v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 June, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 May, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2505.11178">arXiv:2505.11178</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2505.11178">pdf</a>, <a href="https://arxiv.org/ps/2505.11178">ps</a>, <a href="https://arxiv.org/format/2505.11178">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CompAlign: Improving Compositional Text-to-Image Generation with a Complex Benchmark and Fine-Grained Feedback
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wan%2C+Y">Yixin Wan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2505.11178v1-abstract-short" style="display: inline;">
        State-of-the-art T2I models are capable of generating high-resolution images given textual prompts. However, they still struggle with accurately depicting compositional scenes that specify multiple objects, attributes, and spatial relations. We present CompAlign, a challenging benchmark with an emphasis on assessing the depiction of 3D-spatial relationships, for evaluating and improving models on&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.11178v1-abstract-full').style.display = 'inline'; document.getElementById('2505.11178v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2505.11178v1-abstract-full" style="display: none;">
        State-of-the-art T2I models are capable of generating high-resolution images given textual prompts. However, they still struggle with accurately depicting compositional scenes that specify multiple objects, attributes, and spatial relations. We present CompAlign, a challenging benchmark with an emphasis on assessing the depiction of 3D-spatial relationships, for evaluating and improving models on compositional image generation. CompAlign consists of 900 complex multi-subject image generation prompts that combine numerical and 3D-spatial relationships with varied attribute bindings. Our benchmark is remarkably challenging, incorporating generation tasks with 3+ generation subjects with complex 3D-spatial relationships. Additionally, we propose CompQuest, an interpretable and accurate evaluation framework that decomposes complex prompts into atomic sub-questions, then utilizes a MLLM to provide fine-grained binary feedback on the correctness of each aspect of generation elements in model-generated images. This enables precise quantification of alignment between generated images and compositional prompts. Furthermore, we propose an alignment framework that uses CompQuest&#39;s feedback as preference signals to improve diffusion models&#39; compositional image generation abilities. Using adjustable per-image preferences, our method is easily scalable and flexible for different tasks. Evaluation of 9 T2I models reveals that: (1) models remarkable struggle more with compositional tasks with more complex 3D-spatial configurations, and (2) a noticeable performance gap exists between open-source accessible models and closed-source commercial models. Further empirical study on using CompAlign for model alignment yield promising results: post-alignment diffusion models achieve remarkable improvements in compositional accuracy, especially on complex generation tasks, outperforming previous approaches.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.11178v1-abstract-full').style.display = 'none'; document.getElementById('2505.11178v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 May, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2504.17075">arXiv:2504.17075</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2504.17075">pdf</a>, <a href="https://arxiv.org/ps/2504.17075">ps</a>, <a href="https://arxiv.org/format/2504.17075">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Agree to Disagree? A Meta-Evaluation of LLM Misgendering
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Subramonian%2C+A">Arjun Subramonian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gautam%2C+V">Vagrant Gautam</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Seshadri%2C+P">Preethi Seshadri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Klakow%2C+D">Dietrich Klakow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+Y">Yizhou Sun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2504.17075v2-abstract-short" style="display: inline;">
        Numerous methods have been proposed to measure LLM misgendering, including probability-based evaluations (e.g., automatically with templatic sentences) and generation-based evaluations (e.g., with automatic heuristics or human validation). However, it has gone unexamined whether these evaluation methods have convergent validity, that is, whether their results align. Therefore, we conduct a systema&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2504.17075v2-abstract-full').style.display = 'inline'; document.getElementById('2504.17075v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2504.17075v2-abstract-full" style="display: none;">
        Numerous methods have been proposed to measure LLM misgendering, including probability-based evaluations (e.g., automatically with templatic sentences) and generation-based evaluations (e.g., with automatic heuristics or human validation). However, it has gone unexamined whether these evaluation methods have convergent validity, that is, whether their results align. Therefore, we conduct a systematic meta-evaluation of these methods across three existing datasets for LLM misgendering. We propose a method to transform each dataset to enable parallel probability- and generation-based evaluation. Then, by automatically evaluating a suite of 6 models from 3 families, we find that these methods can disagree with each other at the instance, dataset, and model levels, conflicting on 20.2% of evaluation instances. Finally, with a human evaluation of 2400 LLM generations, we show that misgendering behaviour is complex and goes far beyond pronouns, which automatic evaluations are not currently designed to capture, suggesting essential disagreement with human evaluations. Based on our findings, we provide recommendations for future evaluations of LLM misgendering. Our results are also more widely relevant, as they call into question broader methodological conventions in LLM evaluation, which often assume that different evaluation methods agree.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2504.17075v2-abstract-full').style.display = 'none'; document.getElementById('2504.17075v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 April, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to COLM 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2504.13203">arXiv:2504.13203</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2504.13203">pdf</a>, <a href="https://arxiv.org/ps/2504.13203">ps</a>, <a href="https://arxiv.org/format/2504.13203">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multiagent Systems">cs.MA</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        X-Teaming: Multi-Turn Jailbreaks and Defenses with Adaptive Multi-Agents
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Rahman%2C+S">Salman Rahman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+L">Liwei Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shiffer%2C+J">James Shiffer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+G">Genglin Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Issaka%2C+S">Sheriff Issaka</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Parvez%2C+M+R">Md Rizwan Parvez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Palangi%2C+H">Hamid Palangi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Choi%2C+Y">Yejin Choi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gabriel%2C+S">Saadia Gabriel</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2504.13203v2-abstract-short" style="display: inline;">
        Multi-turn interactions with language models (LMs) pose critical safety risks, as harmful intent can be strategically spread across exchanges. Yet, the vast majority of prior work has focused on single-turn safety, while adaptability and diversity remain among the key challenges of multi-turn red-teaming. To address these challenges, we present X-Teaming, a scalable framework that systematically e&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2504.13203v2-abstract-full').style.display = 'inline'; document.getElementById('2504.13203v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2504.13203v2-abstract-full" style="display: none;">
        Multi-turn interactions with language models (LMs) pose critical safety risks, as harmful intent can be strategically spread across exchanges. Yet, the vast majority of prior work has focused on single-turn safety, while adaptability and diversity remain among the key challenges of multi-turn red-teaming. To address these challenges, we present X-Teaming, a scalable framework that systematically explores how seemingly harmless interactions escalate into harmful outcomes and generates corresponding attack scenarios. X-Teaming employs collaborative agents for planning, attack optimization, and verification, achieving state-of-the-art multi-turn jailbreak effectiveness and diversity with success rates up to 98.1% across representative leading open-weight and closed-source models. In particular, X-Teaming achieves a 96.2% attack success rate against the latest Claude 3.7 Sonnet model, which has been considered nearly immune to single-turn attacks. Building on X-Teaming, we introduce XGuard-Train, an open-source multi-turn safety training dataset that is 20x larger than the previous best resource, comprising 30K interactive jailbreaks, designed to enable robust multi-turn safety alignment for LMs. Our work offers essential tools and insights for mitigating sophisticated conversational attacks, advancing the multi-turn safety of LMs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2504.13203v2-abstract-full').style.display = 'none'; document.getElementById('2504.13203v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 April, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2504.08528">arXiv:2504.08528</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2504.08528">pdf</a>, <a href="https://arxiv.org/format/2504.08528">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On The Landscape of Spoken Language Models: A Comprehensive Survey
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Arora%2C+S">Siddhant Arora</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chien%2C+C">Chung-Ming Chien</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+Y">Yifan Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+H">Haibin Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adi%2C+Y">Yossi Adi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dupoux%2C+E">Emmanuel Dupoux</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+H">Hung-Yi Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Livescu%2C+K">Karen Livescu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Watanabe%2C+S">Shinji Watanabe</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2504.08528v1-abstract-short" style="display: inline;">
        The field of spoken language processing is undergoing a shift from training custom-built, task-specific models toward using and optimizing spoken language models (SLMs) which act as universal speech processing systems. This trend is similar to the progression toward universal language models that has taken place in the field of (text) natural language processing. SLMs include both &#34;pure&#34; language&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2504.08528v1-abstract-full').style.display = 'inline'; document.getElementById('2504.08528v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2504.08528v1-abstract-full" style="display: none;">
        The field of spoken language processing is undergoing a shift from training custom-built, task-specific models toward using and optimizing spoken language models (SLMs) which act as universal speech processing systems. This trend is similar to the progression toward universal language models that has taken place in the field of (text) natural language processing. SLMs include both &#34;pure&#34; language models of speech -- models of the distribution of tokenized speech sequences -- and models that combine speech encoders with text language models, often including both spoken and written input or output. Work in this area is very diverse, with a range of terminology and evaluation settings. This paper aims to contribute an improved understanding of SLMs via a unifying literature survey of recent work in the context of the evolution of the field. Our survey categorizes the work in this area by model architecture, training, and evaluation choices, and describes some key challenges and directions for future work.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2504.08528v1-abstract-full').style.display = 'none'; document.getElementById('2504.08528v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 April, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2504.02883">arXiv:2504.02883</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2504.02883">pdf</a>, <a href="https://arxiv.org/format/2504.02883">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SemEval-2025 Task 4: Unlearning sensitive content from Large Language Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ramakrishna%2C+A">Anil Ramakrishna</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wan%2C+Y">Yixin Wan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jin%2C+X">Xiaomeng Jin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bu%2C+Z">Zhiqi Bu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vinzamuri%2C+B">Bhanukiran Vinzamuri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cevher%2C+V">Volkan Cevher</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hong%2C+M">Mingyi Hong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gupta%2C+R">Rahul Gupta</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2504.02883v1-abstract-short" style="display: inline;">
        We introduce SemEval-2025 Task 4: unlearning sensitive content from Large Language Models (LLMs). The task features 3 subtasks for LLM unlearning spanning different use cases: (1) unlearn long form synthetic creative documents spanning different genres; (2) unlearn short form synthetic biographies containing personally identifiable information (PII), including fake names, phone number, SSN, email&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2504.02883v1-abstract-full').style.display = 'inline'; document.getElementById('2504.02883v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2504.02883v1-abstract-full" style="display: none;">
        We introduce SemEval-2025 Task 4: unlearning sensitive content from Large Language Models (LLMs). The task features 3 subtasks for LLM unlearning spanning different use cases: (1) unlearn long form synthetic creative documents spanning different genres; (2) unlearn short form synthetic biographies containing personally identifiable information (PII), including fake names, phone number, SSN, email and home addresses, and (3) unlearn real documents sampled from the target model&#39;s training dataset. We received over 100 submissions from over 30 institutions and we summarize the key techniques and lessons in this paper.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2504.02883v1-abstract-full').style.display = 'none'; document.getElementById('2504.02883v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 April, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2504.01018">arXiv:2504.01018</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2504.01018">pdf</a>, <a href="https://arxiv.org/format/2504.01018">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Self-Routing RAG: Binding Selective Retrieval with Knowledge Verbalization
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+D">Di Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gu%2C+J">Jia-Chen Gu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+N">Nanyun Peng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2504.01018v1-abstract-short" style="display: inline;">
        Selective retrieval improves retrieval-augmented generation (RAG) by reducing distractions from low-quality retrievals and improving efficiency. However, existing approaches under-utilize the inherent knowledge of large language models (LLMs), leading to suboptimal retrieval decisions and degraded generation performance. To bridge this gap, we propose Self-Routing RAG (SR-RAG), a novel framework t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2504.01018v1-abstract-full').style.display = 'inline'; document.getElementById('2504.01018v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2504.01018v1-abstract-full" style="display: none;">
        Selective retrieval improves retrieval-augmented generation (RAG) by reducing distractions from low-quality retrievals and improving efficiency. However, existing approaches under-utilize the inherent knowledge of large language models (LLMs), leading to suboptimal retrieval decisions and degraded generation performance. To bridge this gap, we propose Self-Routing RAG (SR-RAG), a novel framework that binds selective retrieval with knowledge verbalization. SR-RAG enables an LLM to dynamically decide between external retrieval and verbalizing its own parametric knowledge. To this end, we design a multi-task objective that jointly optimizes an LLM on knowledge source selection, knowledge verbalization, and response generation. We further introduce dynamic knowledge source inference via nearest neighbor search to improve the accuracy of knowledge source decision under domain shifts. Fine-tuning three LLMs with SR-RAG significantly improves both their response accuracy and inference latency. Compared to the strongest selective retrieval baseline, SR-RAG reduces retrievals by 29% while improving the performance by 5.1%.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2504.01018v1-abstract-full').style.display = 'none'; document.getElementById('2504.01018v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 April, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Work in Progress</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2504.01005">arXiv:2504.01005</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2504.01005">pdf</a>, <a href="https://arxiv.org/format/2504.01005">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        When To Solve, When To Verify: Compute-Optimal Problem Solving and Generative Verification for LLM Reasoning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Singhi%2C+N">Nishad Singhi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bansal%2C+H">Hritik Bansal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hosseini%2C+A">Arian Hosseini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Grover%2C+A">Aditya Grover</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rohrbach%2C+M">Marcus Rohrbach</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rohrbach%2C+A">Anna Rohrbach</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2504.01005v1-abstract-short" style="display: inline;">
        Scaling test-time compute has emerged as a key strategy for enhancing the reasoning capabilities of large language models (LLMs), particularly in tasks like mathematical problem-solving. A traditional approach, Self-Consistency (SC), generates multiple solutions to a problem and selects the most common answer via majority voting. Another common method involves scoring each solution with a reward m&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2504.01005v1-abstract-full').style.display = 'inline'; document.getElementById('2504.01005v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2504.01005v1-abstract-full" style="display: none;">
        Scaling test-time compute has emerged as a key strategy for enhancing the reasoning capabilities of large language models (LLMs), particularly in tasks like mathematical problem-solving. A traditional approach, Self-Consistency (SC), generates multiple solutions to a problem and selects the most common answer via majority voting. Another common method involves scoring each solution with a reward model (verifier) and choosing the best one. Recent advancements in Generative Reward Models (GenRM) reframe verification as a next-token prediction task, enabling inference-time scaling along a new axis. Specifically, GenRM generates multiple verification chains-of-thought to score each solution. Under a limited inference budget, this introduces a fundamental trade-off: should you spend the budget on scaling solutions via SC or generate fewer solutions and allocate compute to verification via GenRM? To address this, we evaluate GenRM against SC under a fixed inference budget. Interestingly, we find that SC is more compute-efficient than GenRM for most practical inference budgets across diverse models and datasets. For instance, GenRM first matches SC after consuming up to 8x the inference compute and requires significantly more compute to outperform it. Furthermore, we derive inference scaling laws for the GenRM paradigm, revealing that compute-optimal inference favors scaling solution generation more aggressively than scaling the number of verifications. Our work provides practical guidance on optimizing test-time scaling by balancing solution generation and verification. The code is available at https://github.com/nishadsinghi/sc-genrm-scaling.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2504.01005v1-abstract-full').style.display = 'none'; document.getElementById('2504.01005v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 April, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">29 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2503.17352">arXiv:2503.17352</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2503.17352">pdf</a>, <a href="https://arxiv.org/ps/2503.17352">ps</a>, <a href="https://arxiv.org/format/2503.17352">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        OpenVLThinker: Complex Vision-Language Reasoning via Iterative SFT-RL Cycles
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+Y">Yihe Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bansal%2C+H">Hritik Bansal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+F">Fan Yin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+N">Nanyun Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+W">Wei Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2503.17352v2-abstract-short" style="display: inline;">
        We introduce OpenVLThinker, one of the first open-source large vision-language models (LVLMs) to exhibit sophisticated chain-of-thought reasoning, achieving notable performance gains on challenging visual reasoning tasks. While text-based reasoning models (e.g., Deepseek R1) show promising results in text-only tasks, distilling their reasoning into LVLMs via supervised fine-tuning (SFT) often resu&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.17352v2-abstract-full').style.display = 'inline'; document.getElementById('2503.17352v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2503.17352v2-abstract-full" style="display: none;">
        We introduce OpenVLThinker, one of the first open-source large vision-language models (LVLMs) to exhibit sophisticated chain-of-thought reasoning, achieving notable performance gains on challenging visual reasoning tasks. While text-based reasoning models (e.g., Deepseek R1) show promising results in text-only tasks, distilling their reasoning into LVLMs via supervised fine-tuning (SFT) often results in performance degradation due to imprecise visual grounding. Conversely, purely reinforcement learning (RL)-based methods face a large search space, hindering the emergence of reflective behaviors in smaller models (e.g., 7B LVLMs). Surprisingly, alternating between SFT and RL ultimately results in significant performance improvements after a few iterations. Our analysis reveals that the base model rarely exhibits reasoning behaviors initially, but SFT effectively surfaces these latent actions and narrows the RL search space, accelerating the development of reasoning capabilities. Each subsequent RL stage further refines the model&#39;s reasoning skills, producing higher-quality SFT data for continued self-improvement. OpenVLThinker-7B consistently advances performance across six benchmarks demanding mathematical and general reasoning, notably improving MathVista by 3.8%, EMMA by 2.4%, and HallusionBench by 1.6%. Beyond demonstrating the synergy between SFT and RL for complex reasoning tasks, our findings provide early evidence towards achieving R1-style reasoning in multimodal contexts. The code, model and data are held at https://github.com/yihedeng9/OpenVLThinker.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.17352v2-abstract-full').style.display = 'none'; document.getElementById('2503.17352v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 July, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 March, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">23 pages, 11 figures, 8 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2503.07826">arXiv:2503.07826</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2503.07826">pdf</a>, <a href="https://arxiv.org/format/2503.07826">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Magnet: Multi-turn Tool-use Data Synthesis and Distillation via Graph Translation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+F">Fan Yin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Z">Zifeng Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hsu%2C+I">I-Hung Hsu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yan%2C+J">Jun Yan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+K">Ke Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yanfei Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gu%2C+J">Jindong Gu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Le%2C+L+T">Long T. Le</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+C">Chen-Yu Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Palangi%2C+H">Hamid Palangi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pfister%2C+T">Tomas Pfister</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2503.07826v1-abstract-short" style="display: inline;">
        Large language models (LLMs) have exhibited the ability to effectively utilize external tools to address user queries. However, their performance may be limited in complex, multi-turn interactions involving users and multiple tools. To address this, we propose Magnet, a principled framework for synthesizing high-quality training trajectories to enhance the function calling capability of large lang&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.07826v1-abstract-full').style.display = 'inline'; document.getElementById('2503.07826v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2503.07826v1-abstract-full" style="display: none;">
        Large language models (LLMs) have exhibited the ability to effectively utilize external tools to address user queries. However, their performance may be limited in complex, multi-turn interactions involving users and multiple tools. To address this, we propose Magnet, a principled framework for synthesizing high-quality training trajectories to enhance the function calling capability of large language model agents in multi-turn conversations with humans. The framework is based on automatic and iterative translations from a function signature path to a sequence of queries and executable function calls. We model the complicated function interactions in multi-turn cases with graph and design novel node operations to build reliable signature paths. Motivated by context distillation, when guiding the generation of positive and negative trajectories using a teacher model, we provide reference function call sequences as positive hints in context and contrastive, incorrect function calls as negative hints. Experiments show that training with the positive trajectories with supervised fine-tuning and preference optimization against negative trajectories, our 14B model, Magnet-14B-mDPO, obtains 68.01 on BFCL-v3 and 73.30 on ToolQuery, surpassing the performance of the teacher model Gemini-1.5-pro-002 by a large margin in function calling.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.07826v1-abstract-full').style.display = 'none'; document.getElementById('2503.07826v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 March, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages, 3 figures, 4 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2503.06800">arXiv:2503.06800</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2503.06800">pdf</a>, <a href="https://arxiv.org/format/2503.06800">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        VideoPhy-2: A Challenging Action-Centric Physical Commonsense Evaluation in Video Generation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bansal%2C+H">Hritik Bansal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+C">Clark Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bitton%2C+Y">Yonatan Bitton</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goldenberg%2C+R">Roman Goldenberg</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Grover%2C+A">Aditya Grover</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2503.06800v1-abstract-short" style="display: inline;">
        Large-scale video generative models, capable of creating realistic videos of diverse visual concepts, are strong candidates for general-purpose physical world simulators. However, their adherence to physical commonsense across real-world actions remains unclear (e.g., playing tennis, backflip). Existing benchmarks suffer from limitations such as limited size, lack of human evaluation, sim-to-real&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.06800v1-abstract-full').style.display = 'inline'; document.getElementById('2503.06800v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2503.06800v1-abstract-full" style="display: none;">
        Large-scale video generative models, capable of creating realistic videos of diverse visual concepts, are strong candidates for general-purpose physical world simulators. However, their adherence to physical commonsense across real-world actions remains unclear (e.g., playing tennis, backflip). Existing benchmarks suffer from limitations such as limited size, lack of human evaluation, sim-to-real gaps, and absence of fine-grained physical rule analysis. To address this, we introduce VideoPhy-2, an action-centric dataset for evaluating physical commonsense in generated videos. We curate 200 diverse actions and detailed prompts for video synthesis from modern generative models. We perform human evaluation that assesses semantic adherence, physical commonsense, and grounding of physical rules in the generated videos. Our findings reveal major shortcomings, with even the best model achieving only 22% joint performance (i.e., high semantic and physical commonsense adherence) on the hard subset of VideoPhy-2. We find that the models particularly struggle with conservation laws like mass and momentum. Finally, we also train VideoPhy-AutoEval, an automatic evaluator for fast, reliable assessment on our dataset. Overall, VideoPhy-2 serves as a rigorous benchmark, exposing critical gaps in video generative models and guiding future research in physically-grounded video generation. The data and code is available at https://videophy2.github.io/.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.06800v1-abstract-full').style.display = 'none'; document.getElementById('2503.06800v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 March, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">41 pages, 33 Figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2502.17832">arXiv:2502.17832</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2502.17832">pdf</a>, <a href="https://arxiv.org/format/2502.17832">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MM-PoisonRAG: Disrupting Multimodal RAG with Local and Global Poisoning Attacks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ha%2C+H">Hyeonjeong Ha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhan%2C+Q">Qiusi Zhan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+J">Jeonghwan Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bralios%2C+D">Dimitrios Bralios</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sanniboina%2C+S">Saikrishna Sanniboina</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+N">Nanyun Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kang%2C+D">Daniel Kang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ji%2C+H">Heng Ji</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2502.17832v2-abstract-short" style="display: inline;">
        Multimodal large language models (MLLMs) equipped with Retrieval Augmented Generation (RAG) leverage both their rich parametric knowledge and the dynamic, external knowledge to excel in tasks such as Question Answering. While RAG enhances MLLMs by grounding responses in query-relevant external knowledge, this reliance poses a critical yet underexplored safety risk: knowledge poisoning attacks, whe&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.17832v2-abstract-full').style.display = 'inline'; document.getElementById('2502.17832v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2502.17832v2-abstract-full" style="display: none;">
        Multimodal large language models (MLLMs) equipped with Retrieval Augmented Generation (RAG) leverage both their rich parametric knowledge and the dynamic, external knowledge to excel in tasks such as Question Answering. While RAG enhances MLLMs by grounding responses in query-relevant external knowledge, this reliance poses a critical yet underexplored safety risk: knowledge poisoning attacks, where misinformation or irrelevant knowledge is intentionally injected into external knowledge bases to manipulate model outputs to be incorrect and even harmful. To expose such vulnerabilities in multimodal RAG, we propose MM-PoisonRAG, a novel knowledge poisoning attack framework with two attack strategies: Localized Poisoning Attack (LPA), which injects query-specific misinformation in both text and images for targeted manipulation, and Globalized Poisoning Attack (GPA) to provide false guidance during MLLM generation to elicit nonsensical responses across all queries. We evaluate our attacks across multiple tasks, models, and access settings, demonstrating that LPA successfully manipulates the MLLM to generate attacker-controlled answers, with a success rate of up to 56% on MultiModalQA. Moreover, GPA completely disrupts model generation to 0% accuracy with just a single irrelevant knowledge injection. Our results highlight the urgent need for robust defenses against knowledge poisoning to safeguard multimodal RAG frameworks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.17832v2-abstract-full').style.display = 'none'; document.getElementById('2502.17832v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 March, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 24 February, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Code is available at https://github.com/HyeonjeongHa/MM-PoisonRAG</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2502.17793">arXiv:2502.17793</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2502.17793">pdf</a>, <a href="https://arxiv.org/ps/2502.17793">ps</a>, <a href="https://arxiv.org/format/2502.17793">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SYNTHIA: Novel Concept Design with Affordance Composition
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ha%2C+H">Hyeonjeong Ha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jin%2C+X">Xiaomeng Jin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+J">Jeonghwan Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Jiateng Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Z">Zhenhailong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nguyen%2C+K+D">Khanh Duy Nguyen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Blume%2C+A">Ansel Blume</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+N">Nanyun Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ji%2C+H">Heng Ji</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2502.17793v3-abstract-short" style="display: inline;">
        Text-to-image (T2I) models enable rapid concept design, making them widely used in AI-driven design. While recent studies focus on generating semantic and stylistic variations of given design concepts, functional coherence--the integration of multiple affordances into a single coherent concept--remains largely overlooked. In this paper, we introduce SYNTHIA, a framework for generating novel, funct&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.17793v3-abstract-full').style.display = 'inline'; document.getElementById('2502.17793v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2502.17793v3-abstract-full" style="display: none;">
        Text-to-image (T2I) models enable rapid concept design, making them widely used in AI-driven design. While recent studies focus on generating semantic and stylistic variations of given design concepts, functional coherence--the integration of multiple affordances into a single coherent concept--remains largely overlooked. In this paper, we introduce SYNTHIA, a framework for generating novel, functionally coherent designs based on desired affordances. Our approach leverages a hierarchical concept ontology that decomposes concepts into parts and affordances, serving as a crucial building block for functionally coherent design. We also develop a curriculum learning scheme based on our ontology that contrastively fine-tunes T2I models to progressively learn affordance composition while maintaining visual novelty. To elaborate, we (i) gradually increase affordance distance, guiding models from basic concept-affordance association to complex affordance compositions that integrate parts of distinct affordances into a single, coherent form, and (ii) enforce visual novelty by employing contrastive objectives to push learned representations away from existing concepts. Experimental results show that SYNTHIA outperforms state-of-the-art T2I models, demonstrating absolute gains of 25.1% and 14.7% for novelty and functional coherence in human evaluation, respectively.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.17793v3-abstract-full').style.display = 'none'; document.getElementById('2502.17793v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 June, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 24 February, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ACL 2025 Main, Code is available https://github.com/HyeonjeongHa/SYNTHIA</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2502.17709">arXiv:2502.17709</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2502.17709">pdf</a>, <a href="https://arxiv.org/ps/2502.17709">ps</a>, <a href="https://arxiv.org/format/2502.17709">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Contrastive Visual Data Augmentation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+Y">Yu Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+B">Bingxuan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+M">Mohan Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jin%2C+X">Xiaomeng Jin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+T">Te-Lin Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+K">Kuan-Hao Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ji%2C+H">Heng Ji</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+N">Nanyun Peng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2502.17709v2-abstract-short" style="display: inline;">
        Large multimodal models (LMMs) often struggle to recognize novel concepts, as they rely on pre-trained knowledge and have limited ability to capture subtle visual details. Domain-specific knowledge gaps in training also make them prone to confusing visually similar, commonly misrepresented, or low-resource concepts. To help LMMs better align nuanced visual features with language, improving their a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.17709v2-abstract-full').style.display = 'inline'; document.getElementById('2502.17709v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2502.17709v2-abstract-full" style="display: none;">
        Large multimodal models (LMMs) often struggle to recognize novel concepts, as they rely on pre-trained knowledge and have limited ability to capture subtle visual details. Domain-specific knowledge gaps in training also make them prone to confusing visually similar, commonly misrepresented, or low-resource concepts. To help LMMs better align nuanced visual features with language, improving their ability to recognize and reason about novel or rare concepts, we propose a Contrastive visual Data Augmentation (CoDA) strategy. CoDA extracts key contrastive textual and visual features of target concepts against the known concepts they are misrecognized as, and then uses multimodal generative models to produce targeted synthetic data. Automatic filtering of extracted features and augmented images is implemented to guarantee their quality, as verified by human annotators. We show the effectiveness and efficiency of CoDA on low-resource concept and diverse scene recognition datasets including INaturalist and SUN. We additionally collect NovelSpecies, a benchmark dataset consisting of newly discovered animal species that are guaranteed to be unseen by LMMs. LLaVA-1.6 1-shot updating results on these three datasets show CoDA significantly improves SOTA visual data augmentation strategies by 12.3% (NovelSpecies), 5.1% (SUN), and 6.0% (iNat) absolute gains in accuracy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.17709v2-abstract-full').style.display = 'none'; document.getElementById('2502.17709v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 June, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 24 February, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2025.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        ICML 2025
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2502.17651">arXiv:2502.17651</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2502.17651">pdf</a>, <a href="https://arxiv.org/format/2502.17651">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        METAL: A Multi-Agent Framework for Chart Generation with Test-Time Scaling
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+B">Bingxuan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yiwei Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gu%2C+J">Jiuxiang Gu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+N">Nanyun Peng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2502.17651v3-abstract-short" style="display: inline;">
        Chart generation aims to generate code to produce charts satisfying the desired visual properties, e.g., texts, layout, color, and type. It has great potential to empower the automatic professional report generation in financial analysis, research presentation, education, and healthcare. In this work, we build a vision-language model (VLM) based multi-agent framework for effective automatic chart&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.17651v3-abstract-full').style.display = 'inline'; document.getElementById('2502.17651v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2502.17651v3-abstract-full" style="display: none;">
        Chart generation aims to generate code to produce charts satisfying the desired visual properties, e.g., texts, layout, color, and type. It has great potential to empower the automatic professional report generation in financial analysis, research presentation, education, and healthcare. In this work, we build a vision-language model (VLM) based multi-agent framework for effective automatic chart generation. Generating high-quality charts requires both strong visual design skills and precise coding capabilities that embed the desired visual properties into code. Such a complex multi-modal reasoning process is difficult for direct prompting of VLMs. To resolve these challenges, we propose METAL, a multi-agent framework that decomposes the task of chart generation into the iterative collaboration among specialized agents. METAL achieves 5.2% improvement over the current best result in the chart generation task. The METAL framework exhibits the phenomenon of test-time scaling: its performance increases monotonically as the logarithmic computational budget grows from 512 to 8192 tokens. In addition, we find that separating different modalities during the critique process of METAL boosts the self-correction capability of VLMs in the multimodal context.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.17651v3-abstract-full').style.display = 'none'; document.getElementById('2502.17651v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 March, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 24 February, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2502.17394">arXiv:2502.17394</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2502.17394">pdf</a>, <a href="https://arxiv.org/ps/2502.17394">ps</a>, <a href="https://arxiv.org/format/2502.17394">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SNaRe: Domain-aware Data Generation for Low-Resource Event Detection
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Parekh%2C+T">Tanmay Parekh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dong%2C+Y">Yuxuan Dong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bandarkar%2C+L">Lucas Bandarkar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+A">Artin Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hsu%2C+I">I-Hung Hsu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+N">Nanyun Peng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2502.17394v2-abstract-short" style="display: inline;">
        Event Detection (ED) -- the task of identifying event mentions from natural language text -- is critical for enabling reasoning in highly specialized domains such as biomedicine, law, and epidemiology. Data generation has proven to be effective in broadening its utility to wider applications without requiring expensive expert annotations. However, when existing generation approaches are applied to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.17394v2-abstract-full').style.display = 'inline'; document.getElementById('2502.17394v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2502.17394v2-abstract-full" style="display: none;">
        Event Detection (ED) -- the task of identifying event mentions from natural language text -- is critical for enabling reasoning in highly specialized domains such as biomedicine, law, and epidemiology. Data generation has proven to be effective in broadening its utility to wider applications without requiring expensive expert annotations. However, when existing generation approaches are applied to specialized domains, they struggle with label noise, where annotations are incorrect, and domain drift, characterized by a distributional mismatch between generated sentences and the target domain. To address these issues, we introduce SNaRe, a domain-aware synthetic data generation framework composed of three components: Scout, Narrator, and Refiner. Scout extracts triggers from unlabeled target domain data and curates a high-quality domain-specific trigger list using corpus-level statistics to mitigate domain drift. Narrator, conditioned on these triggers, generates high-quality domain-aligned sentences, and Refiner identifies additional event mentions, ensuring high annotation quality. Experimentation on three diverse domain ED datasets reveals how SNaRe outperforms the best baseline, achieving average F1 gains of 3-7% in the zero-shot/few-shot settings and 4-20% F1 improvement for multilingual generation. Analyzing the generated trigger hit rate and human evaluation substantiates SNaRe&#39;s stronger annotation quality and reduced domain drift.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.17394v2-abstract-full').style.display = 'none'; document.getElementById('2502.17394v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 June, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 24 February, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Under review at ACL ARR May 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2502.15097">arXiv:2502.15097</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2502.15097">pdf</a>, <a href="https://arxiv.org/format/2502.15097">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LUME: LLM Unlearning with Multitask Evaluations
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ramakrishna%2C+A">Anil Ramakrishna</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wan%2C+Y">Yixin Wan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jin%2C+X">Xiaomeng Jin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bu%2C+Z">Zhiqi Bu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vinzamuri%2C+B">Bhanukiran Vinzamuri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cevher%2C+V">Volkan Cevher</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hong%2C+M">Mingyi Hong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gupta%2C+R">Rahul Gupta</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2502.15097v3-abstract-short" style="display: inline;">
        Unlearning aims to remove copyrighted, sensitive, or private content from large language models (LLMs) without a full retraining. In this work, we develop a multi-task unlearning benchmark (LUME) which features three tasks: (1) unlearn synthetically generated creative short novels, (2) unlearn synthetic biographies with sensitive information, and (3) unlearn a collection of public biographies. We&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.15097v3-abstract-full').style.display = 'inline'; document.getElementById('2502.15097v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2502.15097v3-abstract-full" style="display: none;">
        Unlearning aims to remove copyrighted, sensitive, or private content from large language models (LLMs) without a full retraining. In this work, we develop a multi-task unlearning benchmark (LUME) which features three tasks: (1) unlearn synthetically generated creative short novels, (2) unlearn synthetic biographies with sensitive information, and (3) unlearn a collection of public biographies. We further release two fine-tuned LLMs of 1B and 7B parameter sizes as the target models. We conduct detailed evaluations of several recently proposed unlearning algorithms and present results on carefully crafted metrics to understand their behavior and limitations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.15097v3-abstract-full').style.display = 'none'; document.getElementById('2502.15097v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 February, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 February, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2502.14275">arXiv:2502.14275</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2502.14275">pdf</a>, <a href="https://arxiv.org/ps/2502.14275">ps</a>, <a href="https://arxiv.org/format/2502.14275">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Fact or Guesswork? Evaluating Large Language Models&#39; Medical Knowledge with Structured One-Hop Judgments
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Jiaxi Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yiwei Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+K">Kai Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+Y">Yujun Cai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hooi%2C+B">Bryan Hooi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+N">Nanyun Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+J">Jin Lu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2502.14275v2-abstract-short" style="display: inline;">
        Large language models (LLMs) have been widely adopted in various downstream task domains. However, their abilities to directly recall and apply factual medical knowledge remains under-explored. Most existing medical QA benchmarks assess complex reasoning or multi-hop inference, making it difficult to isolate LLMs&#39; inherent medical knowledge from their reasoning capabilities. Given the high-stakes&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.14275v2-abstract-full').style.display = 'inline'; document.getElementById('2502.14275v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2502.14275v2-abstract-full" style="display: none;">
        Large language models (LLMs) have been widely adopted in various downstream task domains. However, their abilities to directly recall and apply factual medical knowledge remains under-explored. Most existing medical QA benchmarks assess complex reasoning or multi-hop inference, making it difficult to isolate LLMs&#39; inherent medical knowledge from their reasoning capabilities. Given the high-stakes nature of medical applications, where incorrect information can have critical consequences, it is essential to evaluate the factuality of LLMs to retain medical knowledge.
  To address this challenge, we introduce the Medical Knowledge Judgment Dataset (MKJ), a dataset derived from the Unified Medical Language System (UMLS), a comprehensive repository of standardized biomedical vocabularies and knowledge graphs. Through a binary classification framework, MKJ evaluates LLMs&#39; grasp of fundamental medical facts by having them assess the validity of concise, one-hop statements, enabling direct measurement of their knowledge retention capabilities.
  Our experiments reveal that LLMs have difficulty accurately recalling medical facts, with performances varying substantially across semantic types and showing notable weakness in uncommon medical conditions. Furthermore, LLMs show poor calibration, often being overconfident in incorrect answers. To mitigate these issues, we explore retrieval-augmented generation, demonstrating its effectiveness in improving factual accuracy and reducing uncertainty in medical decision-making.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.14275v2-abstract-full').style.display = 'none'; document.getElementById('2502.14275v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 February, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">15 pages, 11 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2502.10626">arXiv:2502.10626</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2502.10626">pdf</a>, <a href="https://arxiv.org/format/2502.10626">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        K-Edit: Language Model Editing with Contextual Knowledge Awareness
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Markowitz%2C+E">Elan Markowitz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ramakrishna%2C+A">Anil Ramakrishna</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mehrabi%2C+N">Ninareh Mehrabi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peris%2C+C">Charith Peris</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gupta%2C+R">Rahul Gupta</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Galstyan%2C+A">Aram Galstyan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2502.10626v2-abstract-short" style="display: inline;">
        As the world changes, we need to be able to update our models and correct false information without costly retraining. Knowledge-based model editing enables precise modifications to the weights of large language models in order to modify the information encoded within. Recent approaches have seen success in enabling recall of edited information for thousands of edits at once. However, these approa&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.10626v2-abstract-full').style.display = 'inline'; document.getElementById('2502.10626v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2502.10626v2-abstract-full" style="display: none;">
        As the world changes, we need to be able to update our models and correct false information without costly retraining. Knowledge-based model editing enables precise modifications to the weights of large language models in order to modify the information encoded within. Recent approaches have seen success in enabling recall of edited information for thousands of edits at once. However, these approaches fail to produce edits that account for associated contextual information. We present K-Edit, an effective approach to generating contextually consistent knowledge edits. By using knowledge graphs, which maintain contextual consistency when an edge is edited, we are able to generate additional \textit{contextual edits} that ensure consistency of related information in the language model. Our experiments demonstrate significant improvements in multi-hop question answering while maintaining the general effectiveness and scalability of model edits.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.10626v2-abstract-full').style.display = 'none'; document.getElementById('2502.10626v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 February, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 14 February, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2502.05849">arXiv:2502.05849</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2502.05849">pdf</a>, <a href="https://arxiv.org/format/2502.05849">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Fact-or-Fair: A Checklist for Behavioral Testing of AI Models on Fairness-Related Queries
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+J">Jen-tse Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yan%2C+Y">Yuhang Yan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+L">Linqi Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wan%2C+Y">Yixin Wan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+W">Wenxuan Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lyu%2C+M+R">Michael R. Lyu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2502.05849v1-abstract-short" style="display: inline;">
        The generation of incorrect images, such as depictions of people of color in Nazi-era uniforms by Gemini, frustrated users and harmed Google&#39;s reputation, motivating us to investigate the relationship between accurately reflecting factuality and promoting diversity and equity. In this study, we focus on 19 real-world statistics collected from authoritative sources. Using these statistics, we devel&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.05849v1-abstract-full').style.display = 'inline'; document.getElementById('2502.05849v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2502.05849v1-abstract-full" style="display: none;">
        The generation of incorrect images, such as depictions of people of color in Nazi-era uniforms by Gemini, frustrated users and harmed Google&#39;s reputation, motivating us to investigate the relationship between accurately reflecting factuality and promoting diversity and equity. In this study, we focus on 19 real-world statistics collected from authoritative sources. Using these statistics, we develop a checklist comprising objective and subjective queries to analyze behavior of large language models (LLMs) and text-to-image (T2I) models. Objective queries assess the models&#39; ability to provide accurate world knowledge. In contrast, the design of subjective queries follows a key principle: statistical or experiential priors should not be overgeneralized to individuals, ensuring that models uphold diversity. These subjective queries are based on three common human cognitive errors that often result in social biases. We propose metrics to assess factuality and fairness, and formally prove the inherent trade-off between these two aspects. Results show that GPT-4o and DALL-E 3 perform notably well among six LLMs and four T2I models. Our code is publicly available at https://github.com/uclanlp/Fact-or-Fair.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.05849v1-abstract-full').style.display = 'none'; document.getElementById('2502.05849v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 February, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages of main text; 7 pages of appendices;</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2502.02584">arXiv:2502.02584</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2502.02584">pdf</a>, <a href="https://arxiv.org/format/2502.02584">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        QLASS: Boosting Language Agent Inference via Q-Guided Stepwise Search
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+Z">Zongyu Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+Y">Yao Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yao%2C+X">Xingcheng Yao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+D">Da Yin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+Z">Ziniu Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+Y">Yizhou Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2502.02584v1-abstract-short" style="display: inline;">
        Language agents have become a promising solution to complex interactive tasks. One of the key ingredients to the success of language agents is the reward model on the trajectory of the agentic workflow, which provides valuable guidance during training or inference. However, due to the lack of annotations of intermediate interactions, most existing works use an outcome reward model to optimize poli&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.02584v1-abstract-full').style.display = 'inline'; document.getElementById('2502.02584v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2502.02584v1-abstract-full" style="display: none;">
        Language agents have become a promising solution to complex interactive tasks. One of the key ingredients to the success of language agents is the reward model on the trajectory of the agentic workflow, which provides valuable guidance during training or inference. However, due to the lack of annotations of intermediate interactions, most existing works use an outcome reward model to optimize policies across entire trajectories. This may lead to sub-optimal policies and hinder the overall performance. To address this, we propose QLASS (Q-guided Language Agent Stepwise Search), to automatically generate annotations by estimating Q-values in a stepwise manner for open language agents. By introducing a reasoning tree and performing process reward modeling, QLASS provides effective intermediate guidance for each step. With the stepwise guidance, we propose a Q-guided generation strategy to enable language agents to better adapt to long-term value, resulting in significant performance improvement during model inference on complex interactive agent tasks. Notably, even with almost half the annotated data, QLASS retains strong performance, demonstrating its efficiency in handling limited supervision. We also empirically demonstrate that QLASS can lead to more effective decision making through qualitative analysis. We will release our code and data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.02584v1-abstract-full').style.display = 'none'; document.getElementById('2502.02584v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 February, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2412.07730">arXiv:2412.07730</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2412.07730">pdf</a>, <a href="https://arxiv.org/format/2412.07730">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        STIV: Scalable Text and Image Conditioned Video Generation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+Z">Zongyu Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+W">Wei Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+C">Chen Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+J">Jiasen Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+W">Wenze Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fu%2C+T">Tsu-Jui Fu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Allardice%2C+J">Jesse Allardice</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lai%2C+Z">Zhengfeng Lai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+L">Liangchen Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+B">Bowen Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+C">Cha Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fei%2C+Y">Yiran Fei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+Y">Yifan Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+L">Lezhi Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+Y">Yizhou Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Y">Yinfei Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2412.07730v1-abstract-short" style="display: inline;">
        The field of video generation has made remarkable advancements, yet there remains a pressing need for a clear, systematic recipe that can guide the development of robust and scalable models. In this work, we present a comprehensive study that systematically explores the interplay of model architectures, training recipes, and data curation strategies, culminating in a simple and scalable text-image&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.07730v1-abstract-full').style.display = 'inline'; document.getElementById('2412.07730v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2412.07730v1-abstract-full" style="display: none;">
        The field of video generation has made remarkable advancements, yet there remains a pressing need for a clear, systematic recipe that can guide the development of robust and scalable models. In this work, we present a comprehensive study that systematically explores the interplay of model architectures, training recipes, and data curation strategies, culminating in a simple and scalable text-image-conditioned video generation method, named STIV. Our framework integrates image condition into a Diffusion Transformer (DiT) through frame replacement, while incorporating text conditioning via a joint image-text conditional classifier-free guidance. This design enables STIV to perform both text-to-video (T2V) and text-image-to-video (TI2V) tasks simultaneously. Additionally, STIV can be easily extended to various applications, such as video prediction, frame interpolation, multi-view generation, and long video generation, etc. With comprehensive ablation studies on T2I, T2V, and TI2V, STIV demonstrate strong performance, despite its simple design. An 8.7B model with 512 resolution achieves 83.1 on VBench T2V, surpassing both leading open and closed-source models like CogVideoX-5B, Pika, Kling, and Gen-3. The same-sized model also achieves a state-of-the-art result of 90.1 on VBench I2V task at 512 resolution. By providing a transparent and extensible recipe for building cutting-edge video generation models, we aim to empower future research and accelerate progress toward more versatile and reliable video generation solutions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.07730v1-abstract-full').style.display = 'none'; document.getElementById('2412.07730v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 December, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2412.06483">arXiv:2412.06483</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2412.06483">pdf</a>, <a href="https://arxiv.org/format/2412.06483">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SafeWorld: Geo-Diverse Safety Alignment
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+D">Da Yin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qiu%2C+H">Haoyi Qiu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+K">Kung-Hsiang Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+N">Nanyun Peng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2412.06483v1-abstract-short" style="display: inline;">
        In the rapidly evolving field of Large Language Models (LLMs), ensuring safety is a crucial and widely discussed topic. However, existing works often overlook the geo-diversity of cultural and legal standards across the world. To demonstrate the challenges posed by geo-diverse safety standards, we introduce SafeWorld, a novel benchmark specifically designed to evaluate LLMs&#39; ability to generate re&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.06483v1-abstract-full').style.display = 'inline'; document.getElementById('2412.06483v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2412.06483v1-abstract-full" style="display: none;">
        In the rapidly evolving field of Large Language Models (LLMs), ensuring safety is a crucial and widely discussed topic. However, existing works often overlook the geo-diversity of cultural and legal standards across the world. To demonstrate the challenges posed by geo-diverse safety standards, we introduce SafeWorld, a novel benchmark specifically designed to evaluate LLMs&#39; ability to generate responses that are not only helpful but also culturally sensitive and legally compliant across diverse global contexts. SafeWorld encompasses 2,342 test user queries, each grounded in high-quality, human-verified cultural norms and legal policies from 50 countries and 493 regions/races. On top of it, we propose a multi-dimensional automatic safety evaluation framework that assesses the contextual appropriateness, accuracy, and comprehensiveness of responses. Our evaluations reveal that current LLMs struggle to meet these criteria. To enhance LLMs&#39; alignment with geo-diverse safety standards, we synthesize helpful preference pairs for Direct Preference Optimization (DPO) alignment training. The preference pair construction aims to encourage LLMs to behave appropriately and provide precise references to relevant cultural norms and policies when necessary. Our trained SafeWorldLM outperforms all competing models, including GPT-4o on all three evaluation dimensions by a large margin. Global human evaluators also note a nearly 20% higher winning rate in helpfulness and harmfulness evaluation. Our code and data can be found here: https://github.com/PlusLabNLP/SafeWorld.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.06483v1-abstract-full').style.display = 'none'; document.getElementById('2412.06483v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 December, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by NeurIPS 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2412.02172">arXiv:2412.02172</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2412.02172">pdf</a>, <a href="https://arxiv.org/format/2412.02172">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        VISCO: Benchmarking Fine-Grained Critique and Correction Towards Self-Improvement in Visual Reasoning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+X">Xueqing Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ding%2C+Y">Yuheng Ding</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+B">Bingxuan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+P">Pan Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+D">Da Yin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+N">Nanyun Peng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2412.02172v2-abstract-short" style="display: inline;">
        The ability of large vision-language models (LVLMs) to critique and correct their reasoning is an essential building block towards their self-improvement. However, a systematic analysis of such capabilities in LVLMs is still lacking. We propose VISCO, the first benchmark to extensively analyze the fine-grained critique and correction capabilities of LVLMs. Compared to existing work that uses a sin&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.02172v2-abstract-full').style.display = 'inline'; document.getElementById('2412.02172v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2412.02172v2-abstract-full" style="display: none;">
        The ability of large vision-language models (LVLMs) to critique and correct their reasoning is an essential building block towards their self-improvement. However, a systematic analysis of such capabilities in LVLMs is still lacking. We propose VISCO, the first benchmark to extensively analyze the fine-grained critique and correction capabilities of LVLMs. Compared to existing work that uses a single scalar value to critique the entire reasoning [4], VISCO features dense and fine-grained critique, requiring LVLMs to evaluate the correctness of each step in the chain-of-thought and provide natural language explanations to support their judgments. Extensive evaluation of 24 LVLMs demonstrates that human-written critiques significantly enhance the performance after correction, showcasing the potential of the self-improvement strategy. However, the model-generated critiques are less helpful and sometimes detrimental to the performance, suggesting that critique is the crucial bottleneck. We identified three common patterns in critique failures: failure to critique visual perception, reluctance to &#34;say no&#34;, and exaggerated assumption of error propagation. To address these issues, we propose an effective LookBack strategy that revisits the image to verify each piece of information in the initial reasoning. LookBack significantly improves critique and correction performance by up to 13.5%.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.02172v2-abstract-full').style.display = 'none'; document.getElementById('2412.02172v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 March, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 3 December, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">CVPR 2025. https://visco-benchmark.github.io/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.18651">arXiv:2411.18651</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.18651">pdf</a>, <a href="https://arxiv.org/ps/2411.18651">ps</a>, <a href="https://arxiv.org/format/2411.18651">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Verbalized Representation Learning for Interpretable Few-Shot Generalization
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+C">Cheng-Fu Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+D">Da Yin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+W">Wenbo Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ji%2C+H">Heng Ji</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+N">Nanyun Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+B">Bolei Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.18651v3-abstract-short" style="display: inline;">
        Humans recognize objects after observing only a few examples, a remarkable capability enabled by their inherent language understanding of the real-world environment. Developing verbalized and interpretable representation can significantly improve model generalization in low-data settings. In this work, we propose Verbalized Representation Learning (VRL), a novel approach for automatically extracti&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.18651v3-abstract-full').style.display = 'inline'; document.getElementById('2411.18651v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.18651v3-abstract-full" style="display: none;">
        Humans recognize objects after observing only a few examples, a remarkable capability enabled by their inherent language understanding of the real-world environment. Developing verbalized and interpretable representation can significantly improve model generalization in low-data settings. In this work, we propose Verbalized Representation Learning (VRL), a novel approach for automatically extracting human-interpretable features for object recognition using few-shot data. Our method uniquely captures inter-class differences and intra-class commonalities in the form of natural language by employing a Vision-Language Model (VLM) to identify key discriminative features between different classes and shared characteristics within the same class. These verbalized features are then mapped to numeric vectors through the VLM. The resulting feature vectors can be further utilized to train and infer with downstream classifiers. Experimental results show that, at the same model scale, VRL achieves a 24% absolute improvement over prior state-of-the-art methods while using 95% less data and a smaller mode. Furthermore, compared to human-labeled attributes, the features learned by VRL exhibit a 20% absolute gain when used for downstream classification tasks. Code is available at: https://github.com/joeyy5588/VRL/tree/main.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.18651v3-abstract-full').style.display = 'none'; document.getElementById('2411.18651v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 26 November, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to ICCV 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.18000">arXiv:2411.18000</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.18000">pdf</a>, <a href="https://arxiv.org/format/2411.18000">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Exploring Visual Vulnerabilities via Multi-Loss Adversarial Search for Jailbreaking Vision-Language Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hao%2C+S">Shuyang Hao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hooi%2C+B">Bryan Hooi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Jun Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+Z">Zi Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+Y">Yujun Cai</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.18000v2-abstract-short" style="display: inline;">
        Despite inheriting security measures from underlying language models, Vision-Language Models (VLMs) may still be vulnerable to safety alignment issues. Through empirical analysis, we uncover two critical findings: scenario-matched images can significantly amplify harmful outputs, and contrary to common assumptions in gradient-based attacks, minimal loss values do not guarantee optimal attack effec&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.18000v2-abstract-full').style.display = 'inline'; document.getElementById('2411.18000v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.18000v2-abstract-full" style="display: none;">
        Despite inheriting security measures from underlying language models, Vision-Language Models (VLMs) may still be vulnerable to safety alignment issues. Through empirical analysis, we uncover two critical findings: scenario-matched images can significantly amplify harmful outputs, and contrary to common assumptions in gradient-based attacks, minimal loss values do not guarantee optimal attack effectiveness. Building on these insights, we introduce MLAI (Multi-Loss Adversarial Images), a novel jailbreak framework that leverages scenario-aware image generation for semantic alignment, exploits flat minima theory for robust adversarial image selection, and employs multi-image collaborative attacks for enhanced effectiveness. Extensive experiments demonstrate MLAI&#39;s significant impact, achieving attack success rates of 77.75% on MiniGPT-4 and 82.80% on LLaVA-2, substantially outperforming existing methods by margins of 34.37% and 12.77% respectively. Furthermore, MLAI shows considerable transferability to commercial black-box VLMs, achieving up to 60.11% success rate. Our work reveals fundamental visual vulnerabilities in current VLMs safety mechanisms and underscores the need for stronger defenses. Warning: This paper contains potentially harmful example text.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.18000v2-abstract-full').style.display = 'none'; document.getElementById('2411.18000v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 November, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 26 November, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.17993">arXiv:2411.17993</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.17993">pdf</a>, <a href="https://arxiv.org/ps/2411.17993">ps</a>, <a href="https://arxiv.org/format/2411.17993">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DRS: Deep Question Reformulation With Structured Output
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zhecheng Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yiwei Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hooi%2C+B">Bryan Hooi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+Y">Yujun Cai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+N">Nanyun Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.17993v5-abstract-short" style="display: inline;">
        Question answering represents a core capability of large language models (LLMs). However, when individuals encounter unfamiliar knowledge in texts, they often formulate questions that the text itself cannot answer due to insufficient understanding of the underlying information. Recent studies reveal that while LLMs can detect unanswerable questions, they struggle to assist users in reformulating t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.17993v5-abstract-full').style.display = 'inline'; document.getElementById('2411.17993v5-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.17993v5-abstract-full" style="display: none;">
        Question answering represents a core capability of large language models (LLMs). However, when individuals encounter unfamiliar knowledge in texts, they often formulate questions that the text itself cannot answer due to insufficient understanding of the underlying information. Recent studies reveal that while LLMs can detect unanswerable questions, they struggle to assist users in reformulating these questions. Even advanced models like GPT-3.5 demonstrate limited effectiveness in this regard. To address this limitation, we propose DRS: Deep Question Reformulation with Structured Output, a novel zero-shot method aimed at enhancing LLMs ability to assist users in reformulating questions to extract relevant information from new documents. DRS combines the strengths of LLMs with a DFS-based algorithm to iteratively explore potential entity combinations and constrain outputs using predefined entities. This structured approach significantly enhances the reformulation capabilities of LLMs. Comprehensive experimental evaluations demonstrate that DRS improves the reformulation accuracy of GPT-3.5 from 23.03% to 70.42%, while also enhancing the performance of open-source models, such as Gemma2-9B, from 26.35% to 56.75%.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.17993v5-abstract-full').style.display = 'none'; document.getElementById('2411.17993v5-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 July, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 26 November, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Findings of the Association for Computational Linguistics (ACL 2025)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.05361">arXiv:2411.05361</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.05361">pdf</a>, <a href="https://arxiv.org/ps/2411.05361">ps</a>, <a href="https://arxiv.org/format/2411.05361">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dynamic-SUPERB Phase-2: A Collaboratively Expanding Benchmark for Measuring the Capabilities of Spoken Language Models with 180 Tasks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+C">Chien-yu Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+W">Wei-Chih Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+S">Shu-wen Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+A+T">Andy T. Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+C">Chen-An Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+Y">Yu-Xiang Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tseng%2C+W">Wei-Cheng Tseng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Diwan%2C+A">Anuj Diwan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shih%2C+Y">Yi-Jen Shih</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+J">Jiatong Shi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+W">William Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+C">Chih-Kai Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ren%2C+W">Wenze Ren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xuanjun Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hsiao%2C+C">Chi-Yuan Hsiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+P">Puyuan Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+S">Shih-Heng Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kuan%2C+C">Chun-Yi Kuan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+K">Ke-Han Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ritter-Gutierrez%2C+F">Fabian Ritter-Gutierrez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+K">Kuan-Po Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Arora%2C+S">Siddhant Arora</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+Y">You-Kuan Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chuang%2C+M+T">Ming To Chuang</a>
      , et al. (55 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.05361v2-abstract-short" style="display: inline;">
        Multimodal foundation models, such as Gemini and ChatGPT, have revolutionized human-machine interactions by seamlessly integrating various forms of data. Developing a universal spoken language model that comprehends a wide range of natural language instructions is critical for bridging communication gaps and facilitating more intuitive interactions. However, the absence of a comprehensive evaluati&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.05361v2-abstract-full').style.display = 'inline'; document.getElementById('2411.05361v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.05361v2-abstract-full" style="display: none;">
        Multimodal foundation models, such as Gemini and ChatGPT, have revolutionized human-machine interactions by seamlessly integrating various forms of data. Developing a universal spoken language model that comprehends a wide range of natural language instructions is critical for bridging communication gaps and facilitating more intuitive interactions. However, the absence of a comprehensive evaluation benchmark poses a significant challenge. We present Dynamic-SUPERB Phase-2, an open and evolving benchmark for the comprehensive evaluation of instruction-based universal speech models. Building upon the first generation, this second version incorporates 125 new tasks contributed collaboratively by the global research community, expanding the benchmark to a total of 180 tasks, making it the largest benchmark for speech and audio evaluation. While the first generation of Dynamic-SUPERB was limited to classification tasks, Dynamic-SUPERB Phase-2 broadens its evaluation capabilities by introducing a wide array of novel and diverse tasks, including regression and sequence generation, across speech, music, and environmental audio. Evaluation results show that no model performed well universally. SALMONN-13B excelled in English ASR and Qwen2-Audio-7B-Instruct showed high accuracy in emotion recognition, but current models still require further innovations to handle a broader range of tasks. We open-source all task data and the evaluation pipeline at https://github.com/dynamic-superb/dynamic-superb.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.05361v2-abstract-full').style.display = 'none'; document.getElementById('2411.05361v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 June, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 November, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ICLR 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.03700">arXiv:2411.03700</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.03700">pdf</a>, <a href="https://arxiv.org/format/2411.03700">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Root Shapes the Fruit: On the Persistence of Gender-Exclusive Harms in Aligned Language Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ovalle%2C+A">Anaelia Ovalle</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pavasovic%2C+K+L">Krunoslav Lehman Pavasovic</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Martin%2C+L">Louis Martin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zettlemoyer%2C+L">Luke Zettlemoyer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Smith%2C+E+M">Eric Michael Smith</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Williams%2C+A">Adina Williams</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sagun%2C+L">Levent Sagun</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.03700v2-abstract-short" style="display: inline;">
        Natural-language assistants are designed to provide users with helpful responses while avoiding harmful outputs, largely achieved through alignment to human preferences. Yet there is limited understanding of whether alignment techniques may inadvertently perpetuate or even amplify harmful biases inherited from their pre-aligned base models. This issue is compounded by the choice of bias evaluation&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.03700v2-abstract-full').style.display = 'inline'; document.getElementById('2411.03700v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.03700v2-abstract-full" style="display: none;">
        Natural-language assistants are designed to provide users with helpful responses while avoiding harmful outputs, largely achieved through alignment to human preferences. Yet there is limited understanding of whether alignment techniques may inadvertently perpetuate or even amplify harmful biases inherited from their pre-aligned base models. This issue is compounded by the choice of bias evaluation benchmarks in popular preference-finetuned models, which predominantly focus on dominant social categories, such as binary gender, thereby limiting insights into biases affecting underrepresented groups. Towards addressing this gap, we center transgender, nonbinary, and other gender-diverse identities to investigate how alignment procedures interact with pre-existing gender-diverse bias in LLMs. Our key contributions include: 1) a comprehensive survey of bias evaluation modalities across leading preference-finetuned LLMs, highlighting critical gaps in gender-diverse representation, 2) systematic evaluation of gender-diverse biases across 16 models spanning Direct Preference Optimization (DPO) stages, uncovering harms popular bias benchmarks fail to detect, and 3) a flexible framework for measuring harmful biases in implicit reward signals applicable to other social contexts. Our findings reveal that DPO-aligned models are particularly sensitive to supervised finetuning (SFT), and can amplify two forms of real-world gender-diverse harms from their base models: stigmatization and gender non-affirmative language. We conclude with recommendations tailored to DPO and broader alignment practices, advocating for the adoption of community-informed bias evaluation frameworks to more effectively identify and address underrepresented harms in LLMs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.03700v2-abstract-full').style.display = 'none'; document.getElementById('2411.03700v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 May, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 November, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to 2025 ACM FAccT</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.23277">arXiv:2410.23277</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.23277">pdf</a>, <a href="https://arxiv.org/format/2410.23277">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SlowFast-VGen: Slow-Fast Learning for Action-Driven Long Video Generation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hong%2C+Y">Yining Hong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+B">Beide Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+M">Maxine Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhai%2C+Y">Yuanhao Zhai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+L">Linjie Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+K">Kevin Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+C">Chung-Ching Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jianfeng Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Z">Zhengyuan Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+Y">Yingnian Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+L">Lijuan Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.23277v2-abstract-short" style="display: inline;">
        Human beings are endowed with a complementary learning system, which bridges the slow learning of general world dynamics with fast storage of episodic memory from a new experience. Previous video generation models, however, primarily focus on slow learning by pre-training on vast amounts of data, overlooking the fast learning phase crucial for episodic memory storage. This oversight leads to incon&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.23277v2-abstract-full').style.display = 'inline'; document.getElementById('2410.23277v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.23277v2-abstract-full" style="display: none;">
        Human beings are endowed with a complementary learning system, which bridges the slow learning of general world dynamics with fast storage of episodic memory from a new experience. Previous video generation models, however, primarily focus on slow learning by pre-training on vast amounts of data, overlooking the fast learning phase crucial for episodic memory storage. This oversight leads to inconsistencies across temporally distant frames when generating longer videos, as these frames fall beyond the model&#39;s context window. To this end, we introduce SlowFast-VGen, a novel dual-speed learning system for action-driven long video generation. Our approach incorporates a masked conditional video diffusion model for the slow learning of world dynamics, alongside an inference-time fast learning strategy based on a temporal LoRA module. Specifically, the fast learning process updates its temporal LoRA parameters based on local inputs and outputs, thereby efficiently storing episodic memory in its parameters. We further propose a slow-fast learning loop algorithm that seamlessly integrates the inner fast learning loop into the outer slow learning loop, enabling the recall of prior multi-episode experiences for context-aware skill learning. To facilitate the slow learning of an approximate world model, we collect a large-scale dataset of 200k videos with language action annotations, covering a wide range of scenarios. Extensive experiments show that SlowFast-VGen outperforms baselines across various metrics for action-driven video generation, achieving an FVD score of 514 compared to 782, and maintaining consistency in longer videos, with an average of 0.37 scene cuts versus 0.89. The slow-fast learning loop algorithm significantly enhances performances on long-horizon planning tasks as well. Project Website: https://slowfast-vgen.github.io
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.23277v2-abstract-full').style.display = 'none'; document.getElementById('2410.23277v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 October, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 30 October, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.22086">arXiv:2410.22086</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.22086">pdf</a>, <a href="https://arxiv.org/format/2410.22086">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Unlearning as multi-task optimization: A normalized gradient difference approach with an adaptive learning rate
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bu%2C+Z">Zhiqi Bu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jin%2C+X">Xiaomeng Jin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vinzamuri%2C+B">Bhanukiran Vinzamuri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ramakrishna%2C+A">Anil Ramakrishna</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-Wei Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cevher%2C+V">Volkan Cevher</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hong%2C+M">Mingyi Hong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.22086v3-abstract-short" style="display: inline;">
        Machine unlearning has been used to remove unwanted knowledge acquired by large language models (LLMs). In this paper, we examine machine unlearning from an optimization perspective, framing it as a regularized multi-task optimization problem, where one task optimizes a forgetting objective and another optimizes the model performance. In particular, we introduce a normalized gradient difference (N&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.22086v3-abstract-full').style.display = 'inline'; document.getElementById('2410.22086v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.22086v3-abstract-full" style="display: none;">
        Machine unlearning has been used to remove unwanted knowledge acquired by large language models (LLMs). In this paper, we examine machine unlearning from an optimization perspective, framing it as a regularized multi-task optimization problem, where one task optimizes a forgetting objective and another optimizes the model performance. In particular, we introduce a normalized gradient difference (NGDiff) algorithm, enabling us to have better control over the trade-off between the objectives, while integrating a new, automatic learning rate scheduler. We provide a theoretical analysis and empirically demonstrate the superior performance of NGDiff among state-of-the-art unlearning methods on the TOFU and MUSE datasets while exhibiting stable training.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.22086v3-abstract-full').style.display = 'none'; document.getElementById('2410.22086v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 May, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 29 October, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to NAACL 2025 main conference</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.20021">arXiv:2410.20021</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.20021">pdf</a>, <a href="https://arxiv.org/format/2410.20021">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Think Carefully and Check Again! Meta-Generation Unlocking LLMs for Low-Resource Cross-Lingual Summarization
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zhecheng Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yiwei Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hooi%2C+B">Bryan Hooi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+Y">Yujun Cai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheung%2C+N">Naifan Cheung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+N">Nanyun Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-wei Chang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.20021v2-abstract-short" style="display: inline;">
        Cross-lingual summarization (CLS) aims to generate a summary for the source text in a different target language. Currently, instruction-tuned large language models (LLMs) excel at various English tasks. However, unlike languages such as English, Chinese or Spanish, for those relatively low-resource languages with limited usage or data, recent studies have shown that LLMs&#39; performance on CLS tasks&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.20021v2-abstract-full').style.display = 'inline'; document.getElementById('2410.20021v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.20021v2-abstract-full" style="display: none;">
        Cross-lingual summarization (CLS) aims to generate a summary for the source text in a different target language. Currently, instruction-tuned large language models (LLMs) excel at various English tasks. However, unlike languages such as English, Chinese or Spanish, for those relatively low-resource languages with limited usage or data, recent studies have shown that LLMs&#39; performance on CLS tasks remains unsatisfactory even with few-shot settings. This raises the question: Are LLMs capable of handling cross-lingual summarization tasks for low-resource languages? To resolve this question, we fully explore the potential of large language models on cross-lingual summarization task for low-resource languages through our four-step zero-shot method: Summarization, Improvement, Translation and Refinement (SITR) with correspondingly designed prompts. We test our proposed method with multiple LLMs on two well-known cross-lingual summarization datasets with various low-resource target languages. The results show that: i) GPT-3.5 and GPT-4 significantly and consistently outperform other baselines when using our zero-shot SITR methods. ii) By employing our proposed method, we unlock the potential of LLMs, enabling them to effectively handle cross-lingual summarization tasks for relatively low-resource languages.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.20021v2-abstract-full').style.display = 'none'; document.getElementById('2410.20021v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 March, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 25 October, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.20016">arXiv:2410.20016</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.20016">pdf</a>, <a href="https://arxiv.org/ps/2410.20016">ps</a>, <a href="https://arxiv.org/format/2410.20016">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Vulnerability of LLMs to Vertically Aligned Text Manipulations
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zhecheng Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yiwei Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hooi%2C+B">Bryan Hooi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+Y">Yujun Cai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiong%2C+Z">Zhen Xiong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+N">Nanyun Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+K">Kai-wei Chang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.20016v3-abstract-short" style="display: inline;">
        Vertical text input is commonly encountered in various real-world applications, such as mathematical computations and word-based Sudoku puzzles. While current large language models (LLMs) have excelled in natural language tasks, they remain vulnerable to variations in text formatting. Recent research demonstrates that modifying input formats, such as vertically aligning words for encoder-based mod&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.20016v3-abstract-full').style.display = 'inline'; document.getElementById('2410.20016v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.20016v3-abstract-full" style="display: none;">
        Vertical text input is commonly encountered in various real-world applications, such as mathematical computations and word-based Sudoku puzzles. While current large language models (LLMs) have excelled in natural language tasks, they remain vulnerable to variations in text formatting. Recent research demonstrates that modifying input formats, such as vertically aligning words for encoder-based models, can substantially lower accuracy in text classification tasks. While easily understood by humans, these inputs can significantly mislead models, posing a potential risk of bypassing detection in real-world scenarios involving harmful or sensitive information. With the expanding application of LLMs, a crucial question arises: \textit{Do decoder-based LLMs exhibit similar vulnerabilities to vertically formatted text input?} In this paper, we investigate the impact of vertical text input on the performance of various LLMs across multiple text classification datasets and analyze the underlying causes. Our findings are as follows: (i) Vertical text input significantly degrades the accuracy of LLMs in text classification tasks. (ii) \textit{Chain of Thought (CoT)} reasoning does not help LLMs recognize vertical input or mitigate its vulnerability, but \textit{few-shot learning} with careful analysis does. (iii) We explore the underlying cause of the vulnerability by analyzing the inherent issues in tokenization and attention matrices.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.20016v3-abstract-full').style.display = 'none'; document.getElementById('2410.20016v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 July, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 25 October, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to ACL 2025 (Main)</span>
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?query=Kai-Wei+Chang&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?query=Kai-Wei+Chang&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?query=Kai-Wei+Chang&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
        <li>
          <a href="/search/?query=Kai-Wei+Chang&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=100"
            class="pagination-link "
            aria-label="Page 3"
            aria-current="page">3
          </a>
        </li>
        
        <li>
          <a href="/search/?query=Kai-Wei+Chang&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=150"
            class="pagination-link "
            aria-label="Page 4"
            aria-current="page">4
          </a>
        </li>
        
        <li>
          <a href="/search/?query=Kai-Wei+Chang&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=200"
            class="pagination-link "
            aria-label="Page 5"
            aria-current="page">5
          </a>
        </li>
        
        <li>
          <a href="/search/?query=Kai-Wei+Chang&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=250"
            class="pagination-link "
            aria-label="Page 6"
            aria-current="page">6
          </a>
        </li>
        
      
    </ul>
  </nav>
  

  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>