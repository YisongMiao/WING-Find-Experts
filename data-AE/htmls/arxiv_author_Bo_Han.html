<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;50 of 1,146 results for author: <span class="mathjax">Bo Han</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/"  aria-role="search">
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Bo Han">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Bo+Han&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Bo Han">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?query=Bo+Han&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?query=Bo+Han&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/?query=Bo+Han&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
              class="pagination-link "
              aria-label="Page 2"
              aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/?query=Bo+Han&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=100"
              class="pagination-link "
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/?query=Bo+Han&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=150"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/?query=Bo+Han&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=200"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2509.02544">arXiv:2509.02544</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2509.02544">pdf</a>, <a href="https://arxiv.org/ps/2509.02544">ps</a>, <a href="https://arxiv.org/format/2509.02544">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Haoming Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zou%2C+H">Haoyang Zou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+H">Huatong Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feng%2C+J">Jiazhan Feng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fang%2C+J">Junjie Fang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+J">Junting Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+L">Longxiang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+Q">Qinyu Luo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liang%2C+S">Shihao Liang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+S">Shijue Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhong%2C+W">Wanjun Zhong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ye%2C+Y">Yining Ye</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qin%2C+Y">Yujia Qin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiong%2C+Y">Yuwen Xiong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+Y">Yuxin Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+Z">Zhiyong Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+B">Bo Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dun%2C+C">Chen Dun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+C">Chong Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Leng%2C+F">Fuxing Leng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Hanbin Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+H">Hao Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+H">Haobin Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+H">Hongyi Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Su%2C+J">Jing Su</a>
      , et al. (81 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2509.02544v1-abstract-short" style="display: inline;">
        The development of autonomous agents for graphical user interfaces (GUIs) presents major challenges in artificial intelligence. While recent advances in native agent models have shown promise by unifying perception, reasoning, action, and memory through end-to-end learning, open problems remain in data scalability, multi-turn reinforcement learning (RL), the limitations of GUI-only operation, and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.02544v1-abstract-full').style.display = 'inline'; document.getElementById('2509.02544v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2509.02544v1-abstract-full" style="display: none;">
        The development of autonomous agents for graphical user interfaces (GUIs) presents major challenges in artificial intelligence. While recent advances in native agent models have shown promise by unifying perception, reasoning, action, and memory through end-to-end learning, open problems remain in data scalability, multi-turn reinforcement learning (RL), the limitations of GUI-only operation, and environment stability. In this technical report, we present UI-TARS-2, a native GUI-centered agent model that addresses these challenges through a systematic training methodology: a data flywheel for scalable data generation, a stabilized multi-turn RL framework, a hybrid GUI environment that integrates file systems and terminals, and a unified sandbox platform for large-scale rollouts. Empirical evaluation demonstrates that UI-TARS-2 achieves significant improvements over its predecessor UI-TARS-1.5. On GUI benchmarks, it reaches 88.2 on Online-Mind2Web, 47.5 on OSWorld, 50.6 on WindowsAgentArena, and 73.3 on AndroidWorld, outperforming strong baselines such as Claude and OpenAI agents. In game environments, it attains a mean normalized score of 59.8 across a 15-game suite-roughly 60% of human-level performance-and remains competitive with frontier proprietary models (e.g., OpenAI o3) on LMGame-Bench. Additionally, the model can generalize to long-horizon information-seeking tasks and software engineering benchmarks, highlighting its robustness across diverse agent tasks. Detailed analyses of training dynamics further provide insights into achieving stability and efficiency in large-scale agent RL. These results underscore UI-TARS-2&#39;s potential to advance the state of GUI agents and exhibit strong generalization to real-world interactive scenarios.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.02544v1-abstract-full').style.display = 'none'; document.getElementById('2509.02544v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 September, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2509.02041">arXiv:2509.02041</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2509.02041">pdf</a>, <a href="https://arxiv.org/ps/2509.02041">ps</a>, <a href="https://arxiv.org/format/2509.02041">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Instrumentation and Detectors">physics.ins-det</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Characterization of SiPMs at 40 K for neutrino coherent detection based on pure CsI
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tao Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+X">Xilei Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+F">Fengjiao Luo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ye%2C+J">Jingbo Ye</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+B">Bo Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+C">Cong Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hou%2C+Z">Zhilong Hou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+R">Rongbin Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+A">Aiqin Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+L">Lei Cao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+B">Bo Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+S">Sijia Han</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2509.02041v1-abstract-short" style="display: inline;">
        Silicon photomultiplier (SiPM), as the core photoelectric sensor for coherent neutrino detection in low-temperature pure CsI, its working performance directly determines the measurement accuracy of the scintillator light yield. Our previous research has fully demonstrated the performance of pure CsI at liquid nitrogen temperature. More intriguingly, its performance is expected to be even better at&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.02041v1-abstract-full').style.display = 'inline'; document.getElementById('2509.02041v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2509.02041v1-abstract-full" style="display: none;">
        Silicon photomultiplier (SiPM), as the core photoelectric sensor for coherent neutrino detection in low-temperature pure CsI, its working performance directly determines the measurement accuracy of the scintillator light yield. Our previous research has fully demonstrated the performance of pure CsI at liquid nitrogen temperature. More intriguingly, its performance is expected to be even better at 40 K. However, the performance characteristics of SiPM in the 40 K temperature range still remain to be explored. In this study, a self-developed adjustable temperature control system ranging from 30 K to 293 K was built to investigate the key performance parameters of SiPM at different temperatures, such as single photoelectron spectrum, gain, breakdown voltage, dark count rate, after-pulse, internal crosstalk, and single photoelectron resolution. Special emphasis was placed on examining the key performance parameters of SiPM in the 40 K temperature range to evaluate its feasibility for light yield measurement in this temperature range. The results show that this study obtained the parameter variation trends and optimal working conditions of 3 types of SiPM at different temperatures, thereby improving the sensitivity of the detector. This research provides important technical support for low-temperature detection in neutrino physics experiments.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.02041v1-abstract-full').style.display = 'none'; document.getElementById('2509.02041v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 September, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2509.01322">arXiv:2509.01322</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2509.01322">pdf</a>, <a href="https://arxiv.org/ps/2509.01322">ps</a>, <a href="https://arxiv.org/format/2509.01322">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LongCat-Flash Technical Report
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Meituan+LongCat+Team"> Meituan LongCat Team</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bayan"> Bayan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+B">Bei Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lei%2C+B">Bingye Lei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+B">Bo Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rong%2C+B">Bolin Rong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+C">Chao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+C">Chao Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+C">Chen Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+C">Chen Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+C">Cheng Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+C">Chengcheng Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xi%2C+C">Chenguang Xi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+C">Chi Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+C">Chong Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qin%2C+C">Chuan Qin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+C">Chuyu Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+C">Cong Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+C">Congkui Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+D">Dan Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pan%2C+D">Daoru Pan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bu%2C+D">Defei Bu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+D">Dengchang Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kong%2C+D">Deyang Kong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+D">Dishan Liu</a>
      , et al. (157 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2509.01322v1-abstract-short" style="display: inline;">
        We introduce LongCat-Flash, a 560-billion-parameter Mixture-of-Experts (MoE) language model designed for both computational efficiency and advanced agentic capabilities. Stemming from the need for scalable efficiency, LongCat-Flash adopts two novel designs: (a) Zero-computation Experts, which enables dynamic computational budget allocation and activates 18.6B-31.3B (27B on average) per token depen&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.01322v1-abstract-full').style.display = 'inline'; document.getElementById('2509.01322v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2509.01322v1-abstract-full" style="display: none;">
        We introduce LongCat-Flash, a 560-billion-parameter Mixture-of-Experts (MoE) language model designed for both computational efficiency and advanced agentic capabilities. Stemming from the need for scalable efficiency, LongCat-Flash adopts two novel designs: (a) Zero-computation Experts, which enables dynamic computational budget allocation and activates 18.6B-31.3B (27B on average) per token depending on contextual demands, optimizing resource usage. (b) Shortcut-connected MoE, which enlarges the computation-communication overlap window, demonstrating notable gains in inference efficiency and throughput compared to models of a comparable scale. We develop a comprehensive scaling framework for large models that combines hyperparameter transfer, model-growth initialization, a multi-pronged stability suite, and deterministic computation to achieve stable and reproducible training. Notably, leveraging the synergy among scalable architectural design and infrastructure efforts, we complete model training on more than 20 trillion tokens within 30 days, while achieving over 100 tokens per second (TPS) for inference at a cost of \$0.70 per million output tokens. To cultivate LongCat-Flash towards agentic intelligence, we conduct a large-scale pre-training on optimized mixtures, followed by targeted mid- and post-training on reasoning, code, and instructions, with further augmentation from synthetic data and tool use tasks. Comprehensive evaluations demonstrate that, as a non-thinking foundation model, LongCat-Flash delivers highly competitive performance among other leading models, with exceptional strengths in agentic tasks. The model checkpoint of LongCat-Flash is open-sourced to foster community research.
  LongCat Chat: https://longcat.ai
  Hugging Face: https://huggingface.co/meituan-longcat
  GitHub: https://github.com/meituan-longcat
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.01322v1-abstract-full').style.display = 'none'; document.getElementById('2509.01322v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 September, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2509.00289">arXiv:2509.00289</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2509.00289">pdf</a>, <a href="https://arxiv.org/ps/2509.00289">ps</a>, <a href="https://arxiv.org/format/2509.00289">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="High Energy Physics - Experiment">hep-ex</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Helicity amplitude and branching fraction measurement of $χ_{cJ} \rightarrow Λ\barΛ $
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=BESIII+Collaboration"> BESIII Collaboration</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ablikim%2C+M">M. Ablikim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Achasov%2C+M+N">M. N. Achasov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adlarson%2C+P">P. Adlarson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ai%2C+X+C">X. C. Ai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aliberti%2C+R">R. Aliberti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Amoroso%2C+A">A. Amoroso</a>, 
      
      <a href="/search/?searchtype=author&amp;query=An%2C+Q">Q. An</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+Y">Y. Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bakina%2C+O">O. Bakina</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ban%2C+Y">Y. Ban</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bao%2C+H+-">H. -R. Bao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Batozskaya%2C+V">V. Batozskaya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Begzsuren%2C+K">K. Begzsuren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berger%2C+N">N. Berger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berlowski%2C+M">M. Berlowski</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bertani%2C+M">M. Bertani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bettoni%2C+D">D. Bettoni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianchi%2C+F">F. Bianchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianco%2C+E">E. Bianco</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bortone%2C+A">A. Bortone</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boyko%2C+I">I. Boyko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Briere%2C+R+A">R. A. Briere</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brueggemann%2C+A">A. Brueggemann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+H">H. Cai</a>
      , et al. (697 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2509.00289v1-abstract-short" style="display: inline;">
        Utilizing $2712.4 \pm 14.3$ million $ψ(3686)$ events accumulated by the BESIII experiment, we perform a partial wave analysis of $ψ(3686)\rightarrowγχ_{cJ}\rightarrowγΛ\barΛ$ decay ($J=0,1,2$). The ratio of the helicity amplitudes with same (++) and opposite (+-) helicity for $χ_{c2}\rightarrowΛ\barΛ$ decay is determined for the first time to be $R_{χ_{c2}}=0.575 \pm 0.048 \pm 0.018 $, with a rela&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.00289v1-abstract-full').style.display = 'inline'; document.getElementById('2509.00289v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2509.00289v1-abstract-full" style="display: none;">
        Utilizing $2712.4 \pm 14.3$ million $ψ(3686)$ events accumulated by the BESIII experiment, we perform a partial wave analysis of $ψ(3686)\rightarrowγχ_{cJ}\rightarrowγΛ\barΛ$ decay ($J=0,1,2$). The ratio of the helicity amplitudes with same (++) and opposite (+-) helicity for $χ_{c2}\rightarrowΛ\barΛ$ decay is determined for the first time to be $R_{χ_{c2}}=0.575 \pm 0.048 \pm 0.018 $, with a relative phase angle $ΔΦ_{χ_{c2}} = 0.37 \pm 0.15 \pm 0.05 $~rad. The parameters of the angular distribution of $χ_{c2}$ are determined to be $α_{χ_{c2}} = -0.211 \pm 0.100 \pm 0.050 $ and $β_{χ_{c2}} = -0.039 \pm 0.089 \pm 0.033 $, based on the distribution $dN / d\cosθ= 1 + α_{χ_{c2}} \cos^2θ+ β_{χ_{c2}} \cos^4θ$. The width of $χ_{c0}$ is determined to be $12.31 \pm 0.26 \pm 0.12 $~MeV. Additionally, the branching fractions for $χ_{cJ} \rightarrow Λ\barΛ$ are measured to be $(3.662 \pm 0.048 \pm 0.111) \times 10^{-4}$, $(1.182 \pm 0.026 \pm 0.042) \times 10^{-4}$, and $(1.704 \pm 0.035 \pm 0.057) \times 10^{-4}$ for $χ_{c0}$, $χ_{c1}$ and $χ_{c2}$, respectively, where the first uncertainty is statistical and the second systematic.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.00289v1-abstract-full').style.display = 'none'; document.getElementById('2509.00289v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This is the first submission of the manuscript. 13 pages, 15 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.19092">arXiv:2508.19092</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.19092">pdf</a>, <a href="https://arxiv.org/ps/2508.19092">ps</a>, <a href="https://arxiv.org/format/2508.19092">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="High Energy Physics - Experiment">hep-ex</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Measurement of the branching fraction of $\psip \to ωηη$
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=BESIII+Collaboration"> BESIII Collaboration</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ablikim%2C+M">M. Ablikim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Achasov%2C+M+N">M. N. Achasov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adlarson%2C+P">P. Adlarson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ai%2C+X+C">X. C. Ai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aliberti%2C+R">R. Aliberti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Amoroso%2C+A">A. Amoroso</a>, 
      
      <a href="/search/?searchtype=author&amp;query=An%2C+Q">Q. An</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+Y">Y. Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bakina%2C+O">O. Bakina</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ban%2C+Y">Y. Ban</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bao%2C+H+-">H. -R. Bao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Batozskaya%2C+V">V. Batozskaya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Begzsuren%2C+K">K. Begzsuren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berger%2C+N">N. Berger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berlowski%2C+M">M. Berlowski</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bertani%2C+M">M. Bertani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bettoni%2C+D">D. Bettoni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianchi%2C+F">F. Bianchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianco%2C+E">E. Bianco</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bortone%2C+A">A. Bortone</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boyko%2C+I">I. Boyko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Briere%2C+R+A">R. A. Briere</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brueggemann%2C+A">A. Brueggemann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+H">H. Cai</a>
      , et al. (706 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.19092v1-abstract-short" style="display: inline;">
        Using a sample of (2.712 $\pm$ 0.014)$\times 10^{9}$ $\psip$ events collected with the BESIII detector at the BEPCII collider in 2009, 2012, and 2021, the decay $\psip \to ωηη$ is observed for the first time. The branching fraction of the $ψ(3686)\toωηη$ decay is measured to be (1.65 $\pm$ 0.02 $\pm$ 0.21)$\times 10^{-5}$, where the first uncertainty is statistical and the second systematic. Clear&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.19092v1-abstract-full').style.display = 'inline'; document.getElementById('2508.19092v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.19092v1-abstract-full" style="display: none;">
        Using a sample of (2.712 $\pm$ 0.014)$\times 10^{9}$ $\psip$ events collected with the BESIII detector at the BEPCII collider in 2009, 2012, and 2021, the decay $\psip \to ωηη$ is observed for the first time. The branching fraction of the $ψ(3686)\toωηη$ decay is measured to be (1.65 $\pm$ 0.02 $\pm$ 0.21)$\times 10^{-5}$, where the first uncertainty is statistical and the second systematic. Clear structures associated with the well-established $ω(1420)$ and $f_{0}(1710)$ resonances are observed in the $ωη$ and $ηη$ invariant-mass spectra, respectively.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.19092v1-abstract-full').style.display = 'none'; document.getElementById('2508.19092v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.19005">arXiv:2508.19005</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.19005">pdf</a>, <a href="https://arxiv.org/ps/2508.19005">ps</a>, <a href="https://arxiv.org/format/2508.19005">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+Y">Yuxuan Cai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hao%2C+Y">Yipeng Hao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+J">Jie Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yan%2C+H">Hang Yan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lei%2C+Z">Zhikai Lei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhen%2C+R">Rui Zhen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+Z">Zhenhua Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Y">Yutao Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Junsong Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pan%2C+Q">Qianjun Pan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huai%2C+T">Tianyu Huai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Q">Qin Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xin Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+K">Kai Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+B">Bo Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qiu%2C+X">Xipeng Qiu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+L">Liang He</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.19005v2-abstract-short" style="display: inline;">
        As AI advances toward general intelligence, the focus is shifting from systems optimized for static tasks to creating open-ended agents that learn continuously. In this paper, we introduce Experience-driven Lifelong Learning (ELL), a framework for building self-evolving agents capable of continuous growth through real-world interaction. The framework is built on four core principles: (1) Experienc&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.19005v2-abstract-full').style.display = 'inline'; document.getElementById('2508.19005v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.19005v2-abstract-full" style="display: none;">
        As AI advances toward general intelligence, the focus is shifting from systems optimized for static tasks to creating open-ended agents that learn continuously. In this paper, we introduce Experience-driven Lifelong Learning (ELL), a framework for building self-evolving agents capable of continuous growth through real-world interaction. The framework is built on four core principles: (1) Experience Exploration: Agents learn through continuous, self-motivated interaction with dynamic environments, navigating interdependent tasks and generating rich experiential trajectories. (2) Long-term Memory: Agents preserve and structure historical knowledge, including personal experiences, domain expertise, and commonsense reasoning, into a persistent memory system. (3) Skill Learning: Agents autonomously improve by abstracting recurring patterns from experience into reusable skills, which are actively refined and validated for application in new tasks. (4) Knowledge Internalization: Agents internalize explicit and discrete experiences into implicit and intuitive capabilities as &#34;second nature&#34;.
  We also introduce StuLife, a benchmark dataset for ELL that simulates a student&#39;s holistic college journey, from enrollment to academic and personal development, across three core phases and ten detailed sub-scenarios. StuLife is designed around three key paradigm
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.19005v2-abstract-full').style.display = 'none'; document.getElementById('2508.19005v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 26 August, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.18761">arXiv:2508.18761</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.18761">pdf</a>, <a href="https://arxiv.org/ps/2508.18761">ps</a>, <a href="https://arxiv.org/format/2508.18761">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="High Energy Physics - Experiment">hep-ex</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Study of the $χ_{cJ}\rightarrowΛ\barΛη^\prime$ decays
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=BESIII+Collaboration"> BESIII Collaboration</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ablikim%2C+M">M. Ablikim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Achasov%2C+M+N">M. N. Achasov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adlarson%2C+P">P. Adlarson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ai%2C+X+C">X. C. Ai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aliberti%2C+R">R. Aliberti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Amoroso%2C+A">A. Amoroso</a>, 
      
      <a href="/search/?searchtype=author&amp;query=An%2C+Q">Q. An</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+Y">Y. Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bakina%2C+O">O. Bakina</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ban%2C+Y">Y. Ban</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bao%2C+H+-">H. -R. Bao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Batozskaya%2C+V">V. Batozskaya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Begzsuren%2C+K">K. Begzsuren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berger%2C+N">N. Berger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berlowski%2C+M">M. Berlowski</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bertani%2C+M+B">M. B. Bertani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bettoni%2C+D">D. Bettoni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianchi%2C+F">F. Bianchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianco%2C+E">E. Bianco</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bortone%2C+A">A. Bortone</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boyko%2C+I">I. Boyko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Briere%2C+R+A">R. A. Briere</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brueggemann%2C+A">A. Brueggemann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+H">H. Cai</a>
      , et al. (683 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.18761v1-abstract-short" style="display: inline;">
        Using a data sample of $(2.712\pm0.014)\times10^{9}$ $ψ(3686)$ events collected with the BESIII detector at the BEPCII collider, we investigate the decays $χ_{cJ} \rightarrow Λ\barΛ η^\prime$ for $J=0,~1,~2$ via the radiative transition $ψ(3686) \rightarrow γχ_{cJ}$. The decays $χ_{c0,2}\rightarrowΛ\barΛη^\prime$ are observed for the first time, with statistical significances of 6.7$\,σ$ and 6.4&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.18761v1-abstract-full').style.display = 'inline'; document.getElementById('2508.18761v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.18761v1-abstract-full" style="display: none;">
        Using a data sample of $(2.712\pm0.014)\times10^{9}$ $ψ(3686)$ events collected with the BESIII detector at the BEPCII collider, we investigate the decays $χ_{cJ} \rightarrow Λ\barΛ η^\prime$ for $J=0,~1,~2$ via the radiative transition $ψ(3686) \rightarrow γχ_{cJ}$. The decays $χ_{c0,2}\rightarrowΛ\barΛη^\prime$ are observed for the first time, with statistical significances of 6.7$\,σ$ and 6.4$\,σ$, respectively. Evidence for the decay $χ_{c1}\rightarrowΛ\barΛη^\prime$ is found with a statistical significance of 3.3$\,σ$. The corresponding branching fractions are measured to be $\mathscr{B}(χ_{c0}\rightarrowΛ\barΛη^\prime)=(7.56\pm1.42\pm0.90)\times10^{-5}$, $\mathscr{B}(χ_{c1}\rightarrowΛ\barΛη^\prime)=(1.54\pm0.51\pm0.16)\times10^{-5}$, and $\mathscr{B}(χ_{c2}\rightarrowΛ\barΛη^\prime)=(3.03\pm0.61\pm0.29)\times10^{-5}$, where the first uncertainties are statistical and the second systematic. No significant excited $Λ$ baryon states or $Λ\barΛ$ near-threshold enhancements are observed.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.18761v1-abstract-full').style.display = 'none'; document.getElementById('2508.18761v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.18601">arXiv:2508.18601</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.18601">pdf</a>, <a href="https://arxiv.org/ps/2508.18601">ps</a>, <a href="https://arxiv.org/format/2508.18601">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="High Energy Physics - Experiment">hep-ex</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Search for $χ_{c1}\to π^{+}π^{-}η_c$ via $ψ(3686)\toγχ_{c1}$
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=BESIII+Collaboration"> BESIII Collaboration</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ablikim%2C+M">M. Ablikim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Achasov%2C+M+N">M. N. Achasov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adlarson%2C+P">P. Adlarson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ai%2C+X+C">X. C. Ai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aliberti%2C+R">R. Aliberti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Amoroso%2C+A">A. Amoroso</a>, 
      
      <a href="/search/?searchtype=author&amp;query=An%2C+Q">Q. An</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+Y">Y. Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bakina%2C+O">O. Bakina</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ban%2C+Y">Y. Ban</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bao%2C+H+-">H. -R. Bao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Batozskaya%2C+V">V. Batozskaya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Begzsuren%2C+K">K. Begzsuren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berger%2C+N">N. Berger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berlowski%2C+M">M. Berlowski</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bertani%2C+M">M. Bertani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bettoni%2C+D">D. Bettoni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianchi%2C+F">F. Bianchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianco%2C+E">E. Bianco</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bortone%2C+A">A. Bortone</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boyko%2C+I">I. Boyko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Briere%2C+R+A">R. A. Briere</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brueggemann%2C+A">A. Brueggemann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+H">H. Cai</a>
      , et al. (697 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.18601v1-abstract-short" style="display: inline;">
        Utilizing $(2712.4 \pm 14.3) \times 10^6$ $ψ(3686)$ events collected with the BESIII detector at the BEPCII collider, we search for the hadronic transition process $χ_{c1} \to π^+π^-η_c$ following the decay $ψ(3686)\to γχ_{c1}$. No significant signal is observed, and an upper limit of $\mathcal{B}(χ_{c1}\toπ^+π^-η_c)$ is determined to be $3.1 times 10^{-4}$~at 90\% confidence level, which is one o&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.18601v1-abstract-full').style.display = 'inline'; document.getElementById('2508.18601v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.18601v1-abstract-full" style="display: none;">
        Utilizing $(2712.4 \pm 14.3) \times 10^6$ $ψ(3686)$ events collected with the BESIII detector at the BEPCII collider, we search for the hadronic transition process $χ_{c1} \to π^+π^-η_c$ following the decay $ψ(3686)\to γχ_{c1}$. No significant signal is observed, and an upper limit of $\mathcal{B}(χ_{c1}\toπ^+π^-η_c)$ is determined to be $3.1 times 10^{-4}$~at 90\% confidence level, which is one order of magnitude more stringent than the previous measurement.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.18601v1-abstract-full').style.display = 'none'; document.getElementById('2508.18601v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.18594">arXiv:2508.18594</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.18594">pdf</a>, <a href="https://arxiv.org/ps/2508.18594">ps</a>, <a href="https://arxiv.org/format/2508.18594">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="High Energy Physics - Experiment">hep-ex</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Search for a bound state of $Λ_{c}\barΣ_{c}$ near threshold
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=BESIII+Collaboration"> BESIII Collaboration</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ablikim%2C+M">M. Ablikim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Achasov%2C+M+N">M. N. Achasov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adlarson%2C+P">P. Adlarson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ai%2C+X+C">X. C. Ai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aliberti%2C+R">R. Aliberti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Amoroso%2C+A">A. Amoroso</a>, 
      
      <a href="/search/?searchtype=author&amp;query=An%2C+Q">Q. An</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+Y">Y. Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bakina%2C+O">O. Bakina</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ban%2C+Y">Y. Ban</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bao%2C+H+-">H. -R. Bao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Batozskaya%2C+V">V. Batozskaya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Begzsuren%2C+K">K. Begzsuren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berger%2C+N">N. Berger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berlowski%2C+M">M. Berlowski</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bertani%2C+M">M. Bertani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bettoni%2C+D">D. Bettoni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianchi%2C+F">F. Bianchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianco%2C+E">E. Bianco</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bortone%2C+A">A. Bortone</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boyko%2C+I">I. Boyko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Briere%2C+R+A">R. A. Briere</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brueggemann%2C+A">A. Brueggemann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+H">H. Cai</a>
      , et al. (706 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.18594v1-abstract-short" style="display: inline;">
        We search for a possible $Λ_{c} \bar{Σ}_{c}$ bound state, denoted as $H_{c}^{\pm}$, via the $ e^{+}e^{-} \to π^{+} π^{-} Λ_{c}^{+}\barΛ_{c}^{-}$ process for the first time. This analysis utilizes 207.8 and 159.3 pb$^{-1}$ of $e^{+}e^{-}$ annihilation data at the center-of-mass energies of 4918.02 and 4950.93 MeV, respectively, collected with the BESIII detector at the BEPCII collider. No statistic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.18594v1-abstract-full').style.display = 'inline'; document.getElementById('2508.18594v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.18594v1-abstract-full" style="display: none;">
        We search for a possible $Λ_{c} \bar{Σ}_{c}$ bound state, denoted as $H_{c}^{\pm}$, via the $ e^{+}e^{-} \to π^{+} π^{-} Λ_{c}^{+}\barΛ_{c}^{-}$ process for the first time. This analysis utilizes 207.8 and 159.3 pb$^{-1}$ of $e^{+}e^{-}$ annihilation data at the center-of-mass energies of 4918.02 and 4950.93 MeV, respectively, collected with the BESIII detector at the BEPCII collider. No statistically significant signal is observed. The upper limits of the product of Born cross section and branching fraction $σ(e^{+}e^{-} \to π^{+} H_c^{-} + c.c.) \times \mathcal{B}(H_c^{-} \rightarrow π^{-}Λ_{c}^{+}\barΛ_{c}^{-})$ at a 90\% confidence level are reported at each energy point and for various $H_{c}$ mass hypotheses (4715, 4720, 4725, 4730, and 4735 MeV/$c^{2}$) and widths (5, 10, or 20 MeV), with the upper limits ranging from 1.1 pb to 6.4 pb.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.18594v1-abstract-full').style.display = 'none'; document.getElementById('2508.18594v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.18032">arXiv:2508.18032</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.18032">pdf</a>, <a href="https://arxiv.org/ps/2508.18032">ps</a>, <a href="https://arxiv.org/format/2508.18032">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yaqi Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+P">Peng Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+M">Mingyang Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bu%2C+P">Pi Bu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+H">Haoxiang Shi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+R">Runzhou Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yao%2C+Y">Yang Yao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+X">Xuan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+J">Jun Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+B">Bo Zheng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.18032v2-abstract-short" style="display: inline;">
        Despite the promising progress of recent autoregressive models in text-to-image (T2I) generation, their ability to handle multi-attribute and ambiguous prompts remains limited. To address these limitations, existing works have applied chain-of-thought (CoT) to enable stage-aware visual synthesis and employed reinforcement learning (RL) to improve reasoning capabilities. However, most models provid&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.18032v2-abstract-full').style.display = 'inline'; document.getElementById('2508.18032v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.18032v2-abstract-full" style="display: none;">
        Despite the promising progress of recent autoregressive models in text-to-image (T2I) generation, their ability to handle multi-attribute and ambiguous prompts remains limited. To address these limitations, existing works have applied chain-of-thought (CoT) to enable stage-aware visual synthesis and employed reinforcement learning (RL) to improve reasoning capabilities. However, most models provide reward signals only at the end of the generation stage. This monolithic final-only guidance makes it difficult to identify which stages contribute positively to the final outcome and may lead to suboptimal policies. To tackle this issue, we propose a Visual-Chain of Guidance (Visual-CoG) paradigm consisting of three stages: semantic reasoning, process refining, and outcome evaluation, with stage-aware rewards providing immediate guidance throughout the image generation pipeline. We further construct a visual cognition benchmark, VisCog-Bench, which comprises four subtasks to evaluate the effectiveness of semantic reasoning. Comprehensive evaluations on GenEval, T2I-CompBench, and the proposed VisCog-Bench show improvements of 15%, 5%, and 19%, respectively, demonstrating the superior performance of the proposed Visual-CoG. We will release all the resources soon.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.18032v2-abstract-full').style.display = 'none'; document.getElementById('2508.18032v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 25 August, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.17819">arXiv:2508.17819</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.17819">pdf</a>, <a href="https://arxiv.org/ps/2508.17819">ps</a>, <a href="https://arxiv.org/format/2508.17819">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="High Energy Physics - Experiment">hep-ex</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Search for CP violation in e+e- -&gt; psi(3770) -&gt; DDbar via D -&gt; KsPi0
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=BESIII+Collaboration"> BESIII Collaboration</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ablikim%2C+M">M. Ablikim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Achasov%2C+M+N">M. N. Achasov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adlarson%2C+P">P. Adlarson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ai%2C+X+C">X. C. Ai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aliberti%2C+R">R. Aliberti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Amoroso%2C+A">A. Amoroso</a>, 
      
      <a href="/search/?searchtype=author&amp;query=An%2C+Q">Q. An</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+Y">Y. Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bakina%2C+O">O. Bakina</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ban%2C+Y">Y. Ban</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bao%2C+H+-">H. -R. Bao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Batozskaya%2C+V">V. Batozskaya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Begzsuren%2C+K">K. Begzsuren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berger%2C+N">N. Berger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berlowski%2C+M">M. Berlowski</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bertani%2C+M+B">M. B. Bertani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bettoni%2C+D">D. Bettoni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianchi%2C+F">F. Bianchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianco%2C+E">E. Bianco</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bortone%2C+A">A. Bortone</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boyko%2C+I">I. Boyko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Briere%2C+R+A">R. A. Briere</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brueggemann%2C+A">A. Brueggemann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+H">H. Cai</a>
      , et al. (707 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.17819v2-abstract-short" style="display: inline;">
        Utilizing data sample of electron-positron collisions recorded with the BESIII detector at the center-of-mass energies of 3.773~GeV, corresponding to an integrated luminosity of 20.28~fb$^{-1}$, we report the first search for the CP forbidden process $e^+e^- \to ψ(3773) \to D^0\bar{D}^0 \to (K^0_Sπ^0)(K^0_Sπ^0)$. No significant signal is observed. We set the upper limit on the observed cross secti&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.17819v2-abstract-full').style.display = 'inline'; document.getElementById('2508.17819v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.17819v2-abstract-full" style="display: none;">
        Utilizing data sample of electron-positron collisions recorded with the BESIII detector at the center-of-mass energies of 3.773~GeV, corresponding to an integrated luminosity of 20.28~fb$^{-1}$, we report the first search for the CP forbidden process $e^+e^- \to ψ(3773) \to D^0\bar{D}^0 \to (K^0_Sπ^0)(K^0_Sπ^0)$. No significant signal is observed. We set the upper limit on the observed cross section to be 7.37~fb, and the upper limit on the joint branching fraction of the C-odd correlated neutral $D$ pair $\mathcal{B}[(D^0\bar{D}^0)_{\text{C-odd}} \to (K^0_Sπ^0)(K^0_Sπ^0)]$ to be $2.04 \times 10^{-6}$ at the 90\% confidence level.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.17819v2-abstract-full').style.display = 'none'; document.getElementById('2508.17819v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 25 August, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 4 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.15217">arXiv:2508.15217</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.15217">pdf</a>, <a href="https://arxiv.org/ps/2508.15217">ps</a>, <a href="https://arxiv.org/format/2508.15217">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        See Beyond a Single View: Multi-Attribution Learning Leads to Better Conversion Rate Prediction
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+S">Sishuo Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chan%2C+Z">Zhangming Chan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sheng%2C+X">Xiang-Rong Sheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+L">Lei Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+S">Sheng Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hou%2C+C">Chenghuan Hou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+H">Han Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+J">Jian Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+B">Bo Zheng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.15217v1-abstract-short" style="display: inline;">
        Conversion rate (CVR) prediction is a core component of online advertising systems, where the attribution mechanisms-rules for allocating conversion credit across user touchpoints-fundamentally determine label generation and model optimization. While many industrial platforms support diverse attribution mechanisms (e.g., First-Click, Last-Click, Linear, and Data-Driven Multi-Touch Attribution), co&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.15217v1-abstract-full').style.display = 'inline'; document.getElementById('2508.15217v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.15217v1-abstract-full" style="display: none;">
        Conversion rate (CVR) prediction is a core component of online advertising systems, where the attribution mechanisms-rules for allocating conversion credit across user touchpoints-fundamentally determine label generation and model optimization. While many industrial platforms support diverse attribution mechanisms (e.g., First-Click, Last-Click, Linear, and Data-Driven Multi-Touch Attribution), conventional approaches restrict model training to labels from a single production-critical attribution mechanism, discarding complementary signals in alternative attribution perspectives.
  To address this limitation, we propose a novel Multi-Attribution Learning (MAL) framework for CVR prediction that integrates signals from multiple attribution perspectives to better capture the underlying patterns driving user conversions. Specifically, MAL is a joint learning framework consisting of two core components: the Attribution Knowledge Aggregator (AKA) and the Primary Target Predictor (PTP). AKA is implemented as a multi-task learner that integrates knowledge extracted from diverse attribution labels. PTP, in contrast, focuses on the task of generating well-calibrated conversion probabilities that align with the system-optimized attribution metric (e.g., CVR under the Last-Click attribution), ensuring direct compatibility with industrial deployment requirements. Additionally, we propose CAT, a novel training strategy that leverages the Cartesian product of all attribution label combinations to generate enriched supervision signals. This design substantially enhances the performance of the attribution knowledge aggregator. Empirical evaluations demonstrate the superiority of MAL over single-attribution learning baselines, achieving +0.51% GAUC improvement on offline metrics. Online experiments demonstrate that MAL achieved a +2.6% increase in ROI (Return on Investment).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.15217v1-abstract-full').style.display = 'none'; document.getElementById('2508.15217v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at CIKM 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.13142">arXiv:2508.13142</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.13142">pdf</a>, <a href="https://arxiv.org/ps/2508.13142">ps</a>, <a href="https://arxiv.org/format/2508.13142">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Has GPT-5 Achieved Spatial Intelligence? An Empirical Study
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+Z">Zhongang Cai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yubo Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+Q">Qingping Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+R">Ruisi Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gu%2C+C">Chenyang Gu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+W">Wanqi Yin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+Z">Zhiqian Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Z">Zhitao Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wei%2C+C">Chen Wei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+X">Xuanke Shi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+K">Kewang Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+X">Xiaoyang Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Z">Zukai Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Jiaqi Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fan%2C+X">Xiangyu Fan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+H">Hanming Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+L">Lewei Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+B">Bo Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Z">Ziwei Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Q">Quan Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+D">Dahua Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+L">Lei Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.13142v1-abstract-short" style="display: inline;">
        Multi-modal models have achieved remarkable progress in recent years. Nevertheless, they continue to exhibit notable limitations in spatial understanding and reasoning, which are fundamental capabilities to achieving artificial general intelligence. With the recent release of GPT-5, allegedly the most powerful AI model to date, it is timely to examine where the leading models stand on the path tow&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.13142v1-abstract-full').style.display = 'inline'; document.getElementById('2508.13142v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.13142v1-abstract-full" style="display: none;">
        Multi-modal models have achieved remarkable progress in recent years. Nevertheless, they continue to exhibit notable limitations in spatial understanding and reasoning, which are fundamental capabilities to achieving artificial general intelligence. With the recent release of GPT-5, allegedly the most powerful AI model to date, it is timely to examine where the leading models stand on the path toward spatial intelligence. First, we propose a comprehensive taxonomy of spatial tasks that unifies existing benchmarks and discuss the challenges in ensuring fair evaluation. We then evaluate state-of-the-art proprietary and open-source models on eight key benchmarks, at a cost exceeding one billion total tokens. Our empirical study reveals that (1) GPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2) still falls short of human performance across a broad spectrum of tasks. Moreover, we (3) identify the more challenging spatial intelligence problems for multi-modal models, and (4) proprietary models do not exhibit a decisive advantage when facing the most difficult problems. In addition, we conduct a qualitative evaluation across a diverse set of scenarios that are intuitive for humans yet fail even the most advanced multi-modal models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.13142v1-abstract-full').style.display = 'none'; document.getElementById('2508.13142v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.11400">arXiv:2508.11400</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.11400">pdf</a>, <a href="https://arxiv.org/ps/2508.11400">ps</a>, <a href="https://arxiv.org/format/2508.11400">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="High Energy Physics - Experiment">hep-ex</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Production and Decay Dynamics of the Charmed Baryon $Λ_c^+$ in $e^+e^-$ Annihilations near Threshold
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=BESIII+Collaboration"> BESIII Collaboration</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ablikim%2C+M">M. Ablikim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Achasov%2C+M+N">M. N. Achasov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adlarson%2C+P">P. Adlarson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ai%2C+X+C">X. C. Ai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aliberti%2C+R">R. Aliberti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Amoroso%2C+A">A. Amoroso</a>, 
      
      <a href="/search/?searchtype=author&amp;query=An%2C+Q">Q. An</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+Y">Y. Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bakina%2C+O">O. Bakina</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ban%2C+Y">Y. Ban</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bao%2C+H+-">H. -R. Bao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Batozskaya%2C+V">V. Batozskaya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Begzsuren%2C+K">K. Begzsuren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berger%2C+N">N. Berger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berlowski%2C+M">M. Berlowski</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bertani%2C+M+B">M. B. Bertani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bettoni%2C+D">D. Bettoni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianchi%2C+F">F. Bianchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianco%2C+E">E. Bianco</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bortone%2C+A">A. Bortone</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boyko%2C+I">I. Boyko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Briere%2C+R+A">R. A. Briere</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brueggemann%2C+A">A. Brueggemann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+H">H. Cai</a>
      , et al. (706 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.11400v2-abstract-short" style="display: inline;">
        The study of the charmed baryons is crucial for investigating the strong and weak interactions in the Standard Model and for gaining insights into the internal structure of baryons. In an $e^+e^-$ experiment the lightest charmed baryon, $Λ_c^+$, can be produced in pairs through the single photon annihilation process. This process can be described by two complex electromagnetic form factors. The pr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.11400v2-abstract-full').style.display = 'inline'; document.getElementById('2508.11400v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.11400v2-abstract-full" style="display: none;">
        The study of the charmed baryons is crucial for investigating the strong and weak interactions in the Standard Model and for gaining insights into the internal structure of baryons. In an $e^+e^-$ experiment the lightest charmed baryon, $Λ_c^+$, can be produced in pairs through the single photon annihilation process. This process can be described by two complex electromagnetic form factors. The presence of a non-zero relative phase between these form factors gives rise to a transverse polarization of the charmed baryon and provides additional constraints on the dynamic parameters in the decays. In this article, we present the first observation of the transverse polarization of $Λ_{c}^{+}$ in the reaction $e^+e^- \to Λ_c^{+}\barΛ_c^-$, based on $6.4~\text{fb}^{-1}$ of $e^{+}e^{-}$ annihilation data collected at center-of-mass energies between 4600 MeV and 4951 MeV with the BESIII detector. The decay asymmetry parameters and strong phase shift in the decays $Λ_c^+ \to pK_S^0$, $Λπ^+$, $Σ^0π^+$, $Σ^+π^0$ are also simultaneously extracted from the joint angular distributions. These results are vital for understanding CP violation and its role in the matter-antimatter asymmetry of the Universe.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.11400v2-abstract-full').style.display = 'none'; document.getElementById('2508.11400v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 August, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">21 pages, 8 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.11276">arXiv:2508.11276</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.11276">pdf</a>, <a href="https://arxiv.org/ps/2508.11276">ps</a>, <a href="https://arxiv.org/format/2508.11276">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="High Energy Physics - Experiment">hep-ex</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Measurement of the Born cross section for $e^+e^- \to p K^- K^- \barΞ^+$ at $\sqrt{s} =$ 3.5-4.9 GeV
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=BESIII+Collaboration"> BESIII Collaboration</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ablikim%2C+M">M. Ablikim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Achasov%2C+M+N">M. N. Achasov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adlarson%2C+P">P. Adlarson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ai%2C+X+C">X. C. Ai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aliberti%2C+R">R. Aliberti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Amoroso%2C+A">A. Amoroso</a>, 
      
      <a href="/search/?searchtype=author&amp;query=An%2C+Q">Q. An</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+Y">Y. Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bakina%2C+O">O. Bakina</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ban%2C+Y">Y. Ban</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bao%2C+H+-">H. -R. Bao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Batozskaya%2C+V">V. Batozskaya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Begzsuren%2C+K">K. Begzsuren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berger%2C+N">N. Berger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berlowski%2C+M">M. Berlowski</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bertani%2C+M">M. Bertani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bettoni%2C+D">D. Bettoni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianchi%2C+F">F. Bianchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianco%2C+E">E. Bianco</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bortone%2C+A">A. Bortone</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boyko%2C+I">I. Boyko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Briere%2C+R+A">R. A. Briere</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brueggemann%2C+A">A. Brueggemann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+H">H. Cai</a>
      , et al. (701 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.11276v1-abstract-short" style="display: inline;">
        Using $e^+ e^-$ collision data corresponding to a total integrated luminosity of 20 ${\rm fb}^{-1}$ collected with the BESIII detector at the BEPCII collider, we present a measurement of the Born cross section for the process $e^+e^- \to p K^-K^-\barΞ^{+}$ at 39 center-of-mass energies between 3.5 and 4.9 GeV with a partial reconstruction technique. By performing a fit to the dressed cross section&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.11276v1-abstract-full').style.display = 'inline'; document.getElementById('2508.11276v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.11276v1-abstract-full" style="display: none;">
        Using $e^+ e^-$ collision data corresponding to a total integrated luminosity of 20 ${\rm fb}^{-1}$ collected with the BESIII detector at the BEPCII collider, we present a measurement of the Born cross section for the process $e^+e^- \to p K^-K^-\barΞ^{+}$ at 39 center-of-mass energies between 3.5 and 4.9 GeV with a partial reconstruction technique. By performing a fit to the dressed cross section of $e^{+}e^{-}\to p K^- K^-\barΞ^{+}$ with a power law function for continuum production and one resonance at a time for the $ψ(3770)$, $ψ(4040)$, $ψ(4160)$, $ψ(4230)$, $ψ(4360)$, $ψ(4415)$ or $ψ(4660)$, respectively, the upper limits for the product of partial electronic width and branching fraction into the final state $p K^- K^- \barΞ^+$ for these resonances are determined at the $90\%$ confidence level.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.11276v1-abstract-full').style.display = 'none'; document.getElementById('2508.11276v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">18 pages, 2 figures, 3 tables, etc</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.10029">arXiv:2508.10029</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.10029">pdf</a>, <a href="https://arxiv.org/ps/2508.10029">ps</a>, <a href="https://arxiv.org/format/2508.10029">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xing%2C+W">Wenpeng Xing</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+M">Mohan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+C">Chunqiang Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+H+X">Haitao XuNingyu Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+B">Bo Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+M">Meng Han</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.10029v1-abstract-short" style="display: inline;">
        Large language models (LLMs) demonstrate impressive capabilities in various language tasks but are susceptible to jailbreak attacks that circumvent their safety alignments. This paper introduces Latent Fusion Jailbreak (LFJ), a representation-based attack that interpolates hidden states from harmful and benign query pairs to elicit prohibited responses. LFJ begins by selecting query pairs with hig&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.10029v1-abstract-full').style.display = 'inline'; document.getElementById('2508.10029v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.10029v1-abstract-full" style="display: none;">
        Large language models (LLMs) demonstrate impressive capabilities in various language tasks but are susceptible to jailbreak attacks that circumvent their safety alignments. This paper introduces Latent Fusion Jailbreak (LFJ), a representation-based attack that interpolates hidden states from harmful and benign query pairs to elicit prohibited responses. LFJ begins by selecting query pairs with high thematic and syntactic similarity, then performs gradient-guided interpolation at influential layers and tokens, followed by optimization to balance attack success, output fluency, and computational efficiency. Evaluations on models such as Vicuna and LLaMA-2 across benchmarks like AdvBench and MaliciousInstruct yield an average attack success rate (ASR) of 94.01%, outperforming existing methods. To mitigate LFJ, we propose an adversarial training defense that fine-tunes models on interpolated examples, reducing ASR by over 80% without degrading performance on benign inputs. Ablation studies validate the importance of query pair selection, hidden state interpolation components, and optimization strategies in LFJ&#39;s effectiveness.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.10029v1-abstract-full').style.display = 'none'; document.getElementById('2508.10029v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.06069">arXiv:2508.06069</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.06069">pdf</a>, <a href="https://arxiv.org/ps/2508.06069">ps</a>, <a href="https://arxiv.org/format/2508.06069">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Lightweight Auto-bidding based on Traffic Prediction in Live Advertising
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+B">Bo Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+R">Ruixuan Luo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jin%2C+J">Junqi Jin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+H">Han Zhu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.06069v1-abstract-short" style="display: inline;">
        Internet live streaming is widely used in online entertainment and e-commerce, where live advertising is an important marketing tool for anchors. An advertising campaign hopes to maximize the effect (such as conversions) under constraints (such as budget and cost-per-click). The mainstream control of campaigns is auto-bidding, where the performance depends on the decision of the bidding algorithm&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.06069v1-abstract-full').style.display = 'inline'; document.getElementById('2508.06069v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.06069v1-abstract-full" style="display: none;">
        Internet live streaming is widely used in online entertainment and e-commerce, where live advertising is an important marketing tool for anchors. An advertising campaign hopes to maximize the effect (such as conversions) under constraints (such as budget and cost-per-click). The mainstream control of campaigns is auto-bidding, where the performance depends on the decision of the bidding algorithm in each request. The most widely used auto-bidding algorithms include Proportional-Integral-Derivative (PID) control, linear programming (LP), reinforcement learning (RL), etc. Existing methods either do not consider the entire time traffic, or have too high computational complexity. In this paper, the live advertising has high requirements for real-time bidding (second-level control) and faces the difficulty of unknown future traffic. Therefore, we propose a lightweight bidding algorithm Binary Constrained Bidding (BiCB), which neatly combines the optimal bidding formula given by mathematical analysis and the statistical method of future traffic estimation, and obtains good approximation to the optimal result through a low complexity solution. In addition, we complement the form of upper and lower bound constraints for traditional auto-bidding modeling and give theoretical analysis of BiCB. Sufficient offline and online experiments prove BiCB&#39;s good performance and low engineering cost.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.06069v1-abstract-full').style.display = 'none'; document.getElementById('2508.06069v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.05226">arXiv:2508.05226</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.05226">pdf</a>, <a href="https://arxiv.org/ps/2508.05226">ps</a>, <a href="https://arxiv.org/format/2508.05226">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Signal Processing">eess.SP</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Deep Learning Based Dynamic Environment Reconstruction for Vehicular ISAC Scenarios
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+J">Junzhe Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+R">Ruisi He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+M">Mi Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Z">Zhengyu Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+B">Bingcheng Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+J">Jiahui Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+H">Haoxiang Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ai%2C+B">Bo Ai</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.05226v1-abstract-short" style="display: inline;">
        Integrated Sensing and Communication (ISAC) technology plays a critical role in future intelligent transportation systems, by enabling vehicles to perceive and reconstruct the surrounding environment through reuse of wireless signals, thereby reducing or even eliminating the need for additional sensors such as LiDAR or radar. However, existing ISAC based reconstruction methods often lack the abili&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.05226v1-abstract-full').style.display = 'inline'; document.getElementById('2508.05226v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.05226v1-abstract-full" style="display: none;">
        Integrated Sensing and Communication (ISAC) technology plays a critical role in future intelligent transportation systems, by enabling vehicles to perceive and reconstruct the surrounding environment through reuse of wireless signals, thereby reducing or even eliminating the need for additional sensors such as LiDAR or radar. However, existing ISAC based reconstruction methods often lack the ability to track dynamic scenes with sufficient accuracy and temporal consistency, limiting the real world applicability. To address this limitation, we propose a deep learning based framework for vehicular environment reconstruction by using ISAC channels. We first establish a joint channel environment dataset based on multi modal measurements from real world urban street scenarios. Then, a multistage deep learning network is developed to reconstruct the environment. Specifically, a scene decoder identifies the environmental context such as buildings, trees and so on; a cluster center decoder predicts coarse spatial layouts by localizing dominant scattering centers; a point cloud decoder recovers fine grained geometry and structure of surrounding environments. Experimental results demonstrate that the proposed method achieves high-quality dynamic environment reconstruction with a Chamfer Distance of 0.29 and F Score@1% of 0.87. In addition, complexity analysis demonstrates the efficiency and practical applicability of the method in real time scenarios. This work provides a pathway toward low cost environment reconstruction based on ISAC for future intelligent transportation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.05226v1-abstract-full').style.display = 'none'; document.getElementById('2508.05226v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.05206">arXiv:2508.05206</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.05206">pdf</a>, <a href="https://arxiv.org/ps/2508.05206">ps</a>, <a href="https://arxiv.org/format/2508.05206">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Bidding-Aware Retrieval for Multi-Stage Consistency in Online Advertising
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+B">Bin Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yunfei Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+Z">Ziru Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+Z">Zhaoyu Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kou%2C+Z">Zhi Kou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Y">Yeqiu Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+H">Han Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+J">Jian Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+B">Bo Zheng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.05206v1-abstract-short" style="display: inline;">
        Online advertising systems typically use a cascaded architecture to manage massive requests and candidate volumes, where the ranking stages allocate traffic based on eCPM (predicted CTR $\times$ Bid). With the increasing popularity of auto-bidding strategies, the inconsistency between the computationally sensitive retrieval stage and the ranking stages becomes more pronounced, as the former cannot&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.05206v1-abstract-full').style.display = 'inline'; document.getElementById('2508.05206v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.05206v1-abstract-full" style="display: none;">
        Online advertising systems typically use a cascaded architecture to manage massive requests and candidate volumes, where the ranking stages allocate traffic based on eCPM (predicted CTR $\times$ Bid). With the increasing popularity of auto-bidding strategies, the inconsistency between the computationally sensitive retrieval stage and the ranking stages becomes more pronounced, as the former cannot access precise, real-time bids for the vast ad corpus. This discrepancy leads to sub-optimal platform revenue and advertiser outcomes. To tackle this problem, we propose Bidding-Aware Retrieval (BAR), a model-based retrieval framework that addresses multi-stage inconsistency by incorporating ad bid value into the retrieval scoring function. The core innovation is Bidding-Aware Modeling, incorporating bid signals through monotonicity-constrained learning and multi-task distillation to ensure economically coherent representations, while Asynchronous Near-Line Inference enables real-time updates to the embedding for market responsiveness. Furthermore, the Task-Attentive Refinement module selectively enhances feature interactions to disentangle user interest and commercial value signals. Extensive offline experiments and full-scale deployment across Alibaba&#39;s display advertising platform validated BAR&#39;s efficacy: 4.32% platform revenue increase with 22.2% impression lift for positively-operated advertisements.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.05206v1-abstract-full').style.display = 'none'; document.getElementById('2508.05206v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.04329">arXiv:2508.04329</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.04329">pdf</a>, <a href="https://arxiv.org/ps/2508.04329">ps</a>, <a href="https://arxiv.org/format/2508.04329">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Forgetting: A New Mechanism Towards Better Large Language Model Fine-tuning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ghahrizjani%2C+A+T">Ali Taheri Ghahrizjani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Taban%2C+A">Alireza Taban</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ye%2C+S">Shanshan Ye</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mirzaei%2C+A">Abdolreza Mirzaei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+B">Bo Han</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.04329v3-abstract-short" style="display: inline;">
        Supervised fine-tuning (SFT) plays a critical role for pretrained large language models (LLMs), notably enhancing their capacity to acquire domain-specific knowledge while preserving or potentially augmenting their general-purpose capabilities. However, the efficacy of SFT hinges on data quality as well as data volume, otherwise it may result in limited performance gains or even degradation relati&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.04329v3-abstract-full').style.display = 'inline'; document.getElementById('2508.04329v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.04329v3-abstract-full" style="display: none;">
        Supervised fine-tuning (SFT) plays a critical role for pretrained large language models (LLMs), notably enhancing their capacity to acquire domain-specific knowledge while preserving or potentially augmenting their general-purpose capabilities. However, the efficacy of SFT hinges on data quality as well as data volume, otherwise it may result in limited performance gains or even degradation relative to the associated baselines. To mitigate such reliance, we suggest categorizing tokens within each corpus into two parts -- positive and negative tokens -- based on whether they are useful to improve model performance. Positive tokens can be trained in common ways, whereas negative tokens, which may lack essential semantics or be misleading, should be explicitly forgotten. Overall, the token categorization facilitate the model to learn less informative message, and the forgetting process shapes a knowledge boundary to guide the model on what information to learn more precisely. We conduct experiments on well-established benchmarks, finding that this forgetting mechanism not only improves overall model performance and also facilitate more diverse model responses.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.04329v3-abstract-full').style.display = 'none'; document.getElementById('2508.04329v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 August, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.03029">arXiv:2508.03029</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.03029">pdf</a>, <a href="https://arxiv.org/ps/2508.03029">ps</a>, <a href="https://arxiv.org/format/2508.03029">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Strongly Correlated Electrons">cond-mat.str-el</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dichotomy of flat bands in the van der Waals ferromagnet Fe$_5$GeTe$_2$
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+H">Han Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+J">Jianwei Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+C">Chaowei Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+L">Lei Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hao%2C+Y">Yiqing Hao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+Y">Yue Shi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Malinowski%2C+P">Paul Malinowski</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Y">Yucheng Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jang%2C+B+G">Bo Gyu Jang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+J">Jian-Xin Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=May%2C+A+F">Andrew F. May</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+S">Siqi Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xiang Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+Y">Yaofeng Xie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+B">Bin Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yichen Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yue%2C+Z">Ziqin Yue</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ren%2C+Z">Zheng Ren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hashimoto%2C+M">Makoto Hashimoto</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+D">Donghui Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fedorov%2C+A">Alexei Fedorov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mo%2C+S">Sung-Kwan Mo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kono%2C+J">Junichiro Kono</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+Y">Yu He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Birgeneau%2C+R+J">Robert J. Birgeneau</a>
      , et al. (6 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.03029v2-abstract-short" style="display: inline;">
        Quantum materials with bands of narrow bandwidth near the Fermi level represent a promising platform for exploring a diverse range of fascinating physical phenomena, as the high density of states within the small energy window often enables the emergence of many-body physics. On one hand, flat bands can arise from strong Coulomb interactions that localize atomic orbitals. On the other hand, quantu&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.03029v2-abstract-full').style.display = 'inline'; document.getElementById('2508.03029v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.03029v2-abstract-full" style="display: none;">
        Quantum materials with bands of narrow bandwidth near the Fermi level represent a promising platform for exploring a diverse range of fascinating physical phenomena, as the high density of states within the small energy window often enables the emergence of many-body physics. On one hand, flat bands can arise from strong Coulomb interactions that localize atomic orbitals. On the other hand, quantum destructive interference can quench the electronic kinetic energy. Although both have a narrow bandwidth, the two types of flat bands should exhibit very distinct spectral properties arising from their distinctive origins. So far, the two types of flat bands have only been realized in very different material settings and chemical environments, preventing a direct comparison. Here, we report the observation of the two types of flat bands within the same material system--an above-room-temperature van der Waals ferromagnet, Fe$_{5-x}$GeTe$_2$, distinguishable by a switchable iron site order. The contrasting nature of the flat bands is also identified by the remarkably distinctive temperature-evolution of the spectral features, indicating that one arises from electron correlations in the Fe(1) site-disordered phase, while the other geometrical frustration in the Fe(1) site-ordered phase. Our results therefore provide a direct juxtaposition of the distinct formation mechanism of flat bands in quantum materials, and an avenue for understanding the distinctive roles flat bands play in the presence of magnetism, topology, and lattice geometrical frustration, utilizing sublattice ordering as a key control parameter.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.03029v2-abstract-full').style.display = 'none'; document.getElementById('2508.03029v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 4 August, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">The manuscript was submitted on June 12 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.02520">arXiv:2508.02520</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.02520">pdf</a>, <a href="https://arxiv.org/ps/2508.02520">ps</a>, <a href="https://arxiv.org/format/2508.02520">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        xDeepServe: Model-as-a-Service on Huawei CloudMatrix384
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xiao%2C+A">Ao Xiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+B">Bangzheng He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+B">Baoquan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huai%2C+B">Baoxing Huai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+B">Bingji Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+B">Bo Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+B">Bo Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hou%2C+B">Boyi Hou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+C">Chan Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+C">Changhong Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cui%2C+C">Cheng Cui</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+C">Chenyu Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feng%2C+C">Cong Feng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+D">Daohui Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+D">Dayun Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+D">Duo Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zou%2C+F">Fengshao Zou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+F">Fu Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+G">Gangqiang Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dan%2C+G">Gengyuan Dan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+G">Guanjie Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guan%2C+G">Guodong Guan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+G">Guodong Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+H">Haifeng Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+H">Haipei Zhu</a>
      , et al. (103 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.02520v5-abstract-short" style="display: inline;">
        The rise of scaled-out LLMs and scaled-up SuperPods signals a new era in large-scale AI infrastructure. LLMs continue to scale out via MoE, as seen in recent models like DeepSeek, Kimi, and Qwen. In parallel, AI hardware is scaling up, with Huawei&#39;s CloudMatrix384 SuperPod offering hundreds of GB/s high-speed interconnects. Running large MoE models on SuperPod-scale hardware brings new challenges.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.02520v5-abstract-full').style.display = 'inline'; document.getElementById('2508.02520v5-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.02520v5-abstract-full" style="display: none;">
        The rise of scaled-out LLMs and scaled-up SuperPods signals a new era in large-scale AI infrastructure. LLMs continue to scale out via MoE, as seen in recent models like DeepSeek, Kimi, and Qwen. In parallel, AI hardware is scaling up, with Huawei&#39;s CloudMatrix384 SuperPod offering hundreds of GB/s high-speed interconnects. Running large MoE models on SuperPod-scale hardware brings new challenges. It requires new execution models, scalable scheduling, efficient expert load balancing, and elimination of single points of failure. This paper presents xDeepServe, Huawei Cloud&#39;s LLM serving system designed for SuperPod-scale infrastructure. At its core is Transformerless, a disaggregated architecture that decomposes transformer models into modular units--attention, feedforward, and MoE--executed independently on NPUs connected via high-speed fabric. We implement this design in two forms: disaggregated prefill-decode and disaggregated MoE-attention. This fully disaggregated setup enables independent scaling of compute and memory without sacrificing performance. To support this architecture, we propose XCCL, a communication library that leverages CloudMatrix384&#39;s global shared memory to implement efficient point-to-point and all-to-all primitives. We also extend our serving engine FlowServe with system-level techniques, enabling scalable inference across hundreds of NPUs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.02520v5-abstract-full').style.display = 'none'; document.getElementById('2508.02520v5-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 4 August, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.01359">arXiv:2508.01359</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.01359">pdf</a>, <a href="https://arxiv.org/ps/2508.01359">ps</a>, <a href="https://arxiv.org/format/2508.01359">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="High Energy Physics - Experiment">hep-ex</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Measurement of Born Cross Sections and Effective Form Factors of $e^+e^-\to Ω^{-}\barΩ^{+}$ from$\sqrt{s}$ = 3.7 to 4.7 GeV
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=BESIII+Collaboration"> BESIII Collaboration</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ablikim%2C+M">M. Ablikim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Achasov%2C+M+N">M. N. Achasov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adlarson%2C+P">P. Adlarson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Afedulidis%2C+O">O. Afedulidis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ai%2C+X+C">X. C. Ai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aliberti%2C+R">R. Aliberti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Amoroso%2C+A">A. Amoroso</a>, 
      
      <a href="/search/?searchtype=author&amp;query=An%2C+Q">Q. An</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+Y">Y. Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bakina%2C+O">O. Bakina</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Balossino%2C+I">I. Balossino</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ban%2C+Y">Y. Ban</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bao%2C+H+-">H. -R. Bao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Batozskaya%2C+V">V. Batozskaya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Begzsuren%2C+K">K. Begzsuren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berger%2C+N">N. Berger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berlowski%2C+M">M. Berlowski</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bertani%2C+M">M. Bertani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bettoni%2C+D">D. Bettoni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianchi%2C+F">F. Bianchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianco%2C+E">E. Bianco</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bortone%2C+A">A. Bortone</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boyko%2C+I">I. Boyko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Briere%2C+R+A">R. A. Briere</a>
      , et al. (625 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.01359v1-abstract-short" style="display: inline;">
        Using $e^+e^-$ collision data corresponding to an integrated luminosity of 22.7 fb$^{-1}$, collected at center-of-mass energies between 3.7 and 4.7 GeV with the BESIII detector at the BEPCII storage ring, we measure the energy-dependent Born cross sections of $e^+e^-\to Ω^{-}\barΩ^+$ and the effective form factors of the $Ω^-$ baryon. The analysis employs a single baryon tagging method, and the re&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.01359v1-abstract-full').style.display = 'inline'; document.getElementById('2508.01359v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.01359v1-abstract-full" style="display: none;">
        Using $e^+e^-$ collision data corresponding to an integrated luminosity of 22.7 fb$^{-1}$, collected at center-of-mass energies between 3.7 and 4.7 GeV with the BESIII detector at the BEPCII storage ring, we measure the energy-dependent Born cross sections of $e^+e^-\to Ω^{-}\barΩ^+$ and the effective form factors of the $Ω^-$ baryon. The analysis employs a single baryon tagging method, and the results are consistent with theoretical predictions, providing critical constraints on the electromagnetic structure of the $Ω^-$ hyperon. No significant signal of charmonium or charmonium-like states decaying to $Ω^{-}\barΩ^+$ is observed in the investigated energy range.This paper supersedes the withdrawn work arXiv:2505.03180v1.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.01359v1-abstract-full').style.display = 'none'; document.getElementById('2508.01359v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.00410">arXiv:2508.00410</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.00410">pdf</a>, <a href="https://arxiv.org/ps/2508.00410">ps</a>, <a href="https://arxiv.org/format/2508.00410">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Co-Reward: Self-supervised Reinforcement Learning for Large Language Model Reasoning via Contrastive Agreement
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Z">Zizhuo Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+J">Jianing Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ge%2C+X">Xinmu Ge</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+Z">Zihua Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+Z">Zhanke Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xuan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feng%2C+X">Xiao Feng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yao%2C+J">Jiangchao Yao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+B">Bo Han</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.00410v1-abstract-short" style="display: inline;">
        Although reinforcement learning with verifiable rewards (RLVR) shows promise in improving the reasoning ability of large language models (LLMs), the scaling up dilemma remains due to the reliance on human annotated labels especially for complex tasks. Recent alternatives that explore various self-reward signals exhibit the eliciting potential of LLM reasoning, but suffer from the non-negligible co&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.00410v1-abstract-full').style.display = 'inline'; document.getElementById('2508.00410v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.00410v1-abstract-full" style="display: none;">
        Although reinforcement learning with verifiable rewards (RLVR) shows promise in improving the reasoning ability of large language models (LLMs), the scaling up dilemma remains due to the reliance on human annotated labels especially for complex tasks. Recent alternatives that explore various self-reward signals exhibit the eliciting potential of LLM reasoning, but suffer from the non-negligible collapse issue. Inspired by the success of self-supervised learning, we propose \textit{Co-Reward}, a novel RL framework that leverages contrastive agreement across semantically analogical questions as a reward basis. Specifically, we construct a similar question for each training sample (without labels) and synthesize their individual surrogate labels through a simple rollout voting, and then the reward is constructed by cross-referring the labels of each question pair to enforce the internal reasoning consistency across analogical inputs. Intuitively, such a self-supervised reward-shaping mechanism increases the difficulty of learning collapse into a trivial solution, and promotes stable reasoning elicitation and improvement through expanding the input sample variants. Empirically, Co-Reward achieves superior performance compared to other self-reward baselines on multiple reasoning benchmarks and LLM series, and reaches or even surpasses ground-truth (GT) labeled reward, with improvements of up to $+6.8\%$ on MATH500 over GT reward on Llama-3.2-3B-Instruct. Our code is publicly available at https://github.com/tmlr-group/Co-Reward.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.00410v1-abstract-full').style.display = 'none'; document.getElementById('2508.00410v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.20618">arXiv:2507.20618</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.20618">pdf</a>, <a href="https://arxiv.org/ps/2507.20618">ps</a>, <a href="https://arxiv.org/format/2507.20618">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="High Energy Physics - Experiment">hep-ex</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Precise Measurement of Chromo-Electric Dipole Moment of the Charm Quark
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=BESIII+Collaboration"> BESIII Collaboration</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ablikim%2C+M">M. Ablikim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Achasov%2C+M+N">M. N. Achasov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adlarson%2C+P">P. Adlarson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ai%2C+X+C">X. C. Ai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aliberti%2C+R">R. Aliberti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Amoroso%2C+A">A. Amoroso</a>, 
      
      <a href="/search/?searchtype=author&amp;query=An%2C+Q">Q. An</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+Y">Y. Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bakina%2C+O">O. Bakina</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ban%2C+Y">Y. Ban</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bao%2C+H+-">H. -R. Bao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Batozskaya%2C+V">V. Batozskaya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Begzsuren%2C+K">K. Begzsuren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berger%2C+N">N. Berger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berlowski%2C+M">M. Berlowski</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bertani%2C+M">M. Bertani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bettoni%2C+D">D. Bettoni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianchi%2C+F">F. Bianchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianco%2C+E">E. Bianco</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bortone%2C+A">A. Bortone</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boyko%2C+I">I. Boyko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Briere%2C+R+A">R. A. Briere</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brueggemann%2C+A">A. Brueggemann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+H">H. Cai</a>
      , et al. (697 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.20618v1-abstract-short" style="display: inline;">
        The combined symmetry of charge conjugation and parity ($C\!P$) is tested in the hadronic transition $ψ(3686)\toπ^+π^{-}J/ψ$, utilizing a dataset of 2.7 billion $ψ(3686)$ events collected by the BESIII detector at the BEPCII collider. The resulting asymmetry observable is $A_{cp} = (0.6\pm1.8_{\rm stat}\pm0.1_{\rm sys})\times10^{-4}$ by combining the two channels $J/ψ\to e^+e^-$ and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.20618v1-abstract-full').style.display = 'inline'; document.getElementById('2507.20618v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.20618v1-abstract-full" style="display: none;">
        The combined symmetry of charge conjugation and parity ($C\!P$) is tested in the hadronic transition $ψ(3686)\toπ^+π^{-}J/ψ$, utilizing a dataset of 2.7 billion $ψ(3686)$ events collected by the BESIII detector at the BEPCII collider. The resulting asymmetry observable is $A_{cp} = (0.6\pm1.8_{\rm stat}\pm0.1_{\rm sys})\times10^{-4}$ by combining the two channels $J/ψ\to e^+e^-$ and $J/ψ\toμ^+μ^-$ with unprecedented precision. Meanwhile, by considering the relationship between the chromo-electric dipole moment (CEDM) and the $A_{cp}$ observable derived from the quantum chromo-dynamics multipole expansion (QCDME) theory based on Chen-Kuang, as well as Cornell potential model, we yield the results of charm quark&#39;s CEDM with $d^{\prime}_{c} = (2.6\pm7.8_{\rm stat}\pm0.4_{\rm sys}\pm0.6_{\rm theo})\times10^{-16}$ $e\cdot$cm, and $d^{\prime}_{c} = (3.5\pm10.5_{\rm stat}\pm0.6_{\rm sys}\pm0.5_{\rm theo})\times10^{-16}$ $e\cdot$cm, respectively. These results correspond to an upper limit of $|d^{\prime}_{c} |&lt;2.1\times10^{-15}\ e\cdot$cm at a 90\% confidence level, an order of magnitude improvement in sensitivity compared to the previous direct bound using the same decay process. Our results provide insights into the dynamics of charmonium hadronic transitions, shedding light on their behavior in the context of $C\!P$ violation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.20618v1-abstract-full').style.display = 'none'; document.getElementById('2507.20618v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 July, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 3 figures, 1 table, etc</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.19427">arXiv:2507.19427</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.19427">pdf</a>, <a href="https://arxiv.org/ps/2507.19427">ps</a>, <a href="https://arxiv.org/format/2507.19427">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=StepFun"> StepFun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=%3A"> :</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+B">Bin Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+B">Bojun Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wan%2C+C">Changyi Wan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+G">Guanzhe Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+H">Hanpeng Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jia%2C+H">Haonan Jia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nie%2C+H">Hao Nie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+M">Mingliang Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+N">Nuo Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+S">Siyu Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yuan%2C+S">Song Yuan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+W">Wuxun Xie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+X">Xiaoniu Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xing Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+X">Xingping Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+X">Xuelin Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+Y">Yanbo Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yaoyu Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+Y">Yibo Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+Y">Yimin Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+Y">Yu Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+Y">Yuanwei Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+H">Houyi Li</a>
      , et al. (175 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.19427v1-abstract-short" style="display: inline;">
        Large language models (LLMs) face low hardware efficiency during decoding, especially for long-context reasoning tasks. This paper introduces Step-3, a 321B-parameter VLM with hardware-aware model-system co-design optimized for minimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel Multi-Matrix Factorization Attention (MFA) mechanism that significantly reduces both KV cache&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.19427v1-abstract-full').style.display = 'inline'; document.getElementById('2507.19427v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.19427v1-abstract-full" style="display: none;">
        Large language models (LLMs) face low hardware efficiency during decoding, especially for long-context reasoning tasks. This paper introduces Step-3, a 321B-parameter VLM with hardware-aware model-system co-design optimized for minimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel Multi-Matrix Factorization Attention (MFA) mechanism that significantly reduces both KV cache size and computation while maintaining high attention expressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed inference system that decouples attention and Feed-Forward Network (FFN) layers into specialized subsystems. This co-design achieves unprecedented cost efficiency: Step-3 significantly reduces theoretical decoding costs compared with models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at longer context. Step-3 achieves low cost while activating 38B parameters per token (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that hardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are critical to cost-effectiveness. We perform a head-to-head comparison with DeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs achieves a decoding throughput of up to 4,039 tokens per second per GPU under 50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3&#39;s 2,324 in the same setup and sets a new Pareto frontier for LLM decoding.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.19427v1-abstract-full').style.display = 'none'; document.getElementById('2507.19427v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 July, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.19017">arXiv:2507.19017</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.19017">pdf</a>, <a href="https://arxiv.org/ps/2507.19017">ps</a>, <a href="https://arxiv.org/format/2507.19017">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MindSpeed RL: Distributed Dataflow for Scalable and Efficient RL Training on Ascend NPU Cluster
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Feng%2C+L">Laingjun Feng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pan%2C+C">Chenyi Pan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+X">Xinjie Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mei%2C+F">Fei Mei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ning%2C+B">Benzhe Ning</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+J">Jianxiang Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+X">Xinyang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+B">Beirong Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shu%2C+Z">Zeng Shu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+C">Chang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+G">Guang Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+Z">Zhenyu Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jiangben Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+B">Bo Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.19017v1-abstract-short" style="display: inline;">
        Reinforcement learning (RL) is a paradigm increasingly used to align large language models. Popular RL algorithms utilize multiple workers and can be modeled as a graph, where each node is the status of a worker and each edge represents dataflow between nodes. Owing to the heavy cross-node dependencies, the RL training system usually suffers from poor cluster scalability and low memory utilization&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.19017v1-abstract-full').style.display = 'inline'; document.getElementById('2507.19017v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.19017v1-abstract-full" style="display: none;">
        Reinforcement learning (RL) is a paradigm increasingly used to align large language models. Popular RL algorithms utilize multiple workers and can be modeled as a graph, where each node is the status of a worker and each edge represents dataflow between nodes. Owing to the heavy cross-node dependencies, the RL training system usually suffers from poor cluster scalability and low memory utilization. In this article, we introduce MindSpeed RL, an effective and efficient system for large-scale RL training. Unlike existing centralized methods, MindSpeed RL organizes the essential data dependencies in RL training, i.e., sample flow and resharding flow, from a distributed view. On the one hand, a distributed transfer dock strategy, which sets controllers and warehouses on the basis of the conventional replay buffer, is designed to release the dispatch overhead in the sample flow. A practical allgather--swap strategy is presented to eliminate redundant memory usage in resharding flow. In addition, MindSpeed RL further integrates numerous parallelization strategies and acceleration techniques for systematic optimization. Compared with existing state-of-the-art systems, comprehensive experiments on the RL training of popular Qwen2.5-Dense-7B/32B, Qwen3-MoE-30B, and DeepSeek-R1-MoE-671B show that MindSpeed RL increases the throughput by 1.42 ~ 3.97 times. Finally, we open--source MindSpeed RL and perform all the experiments on a super pod of Ascend with 384 neural processing units (NPUs) to demonstrate the powerful performance and reliability of Ascend.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.19017v1-abstract-full').style.display = 'none'; document.getElementById('2507.19017v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 July, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          CS
        

        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.18576">arXiv:2507.18576</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.18576">pdf</a>, <a href="https://arxiv.org/ps/2507.18576">ps</a>, <a href="https://arxiv.org/format/2507.18576">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SafeWork-R1: Coevolving Safety and Intelligence under the AI-45$^{\circ}$ Law
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lab%2C+S+A">Shanghai AI Lab</a>, 
      
      <a href="/search/?searchtype=author&amp;query=%3A"> :</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bao%2C+Y">Yicheng Bao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+G">Guanxu Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+M">Mingkang Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yunhao Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+C">Chiyu Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+L">Lingjie Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+S">Sirui Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xinquan Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+J">Jie Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+Y">Yu Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+D">Dengke Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ding%2C+Y">Yizhuo Ding</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ding%2C+D">Dan Ding</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ding%2C+X">Xiaoshan Ding</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ding%2C+Y">Yi Ding</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dong%2C+Z">Zhichen Dong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+L">Lingxiao Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fan%2C+Y">Yuyu Fan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feng%2C+X">Xinshun Feng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fu%2C+Y">Yanwei Fu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+Y">Yuxuan Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ge%2C+R">Ruijun Ge</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gu%2C+T">Tianle Gu</a>
      , et al. (93 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.18576v3-abstract-short" style="display: inline;">
        We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that demonstrates the coevolution of capabilities and safety. It is developed by our proposed SafeLadder framework, which incorporates large-scale, progressive, safety-oriented reinforcement learning post-training, supported by a suite of multi-principled verifiers. Unlike previous alignment methods such as RLHF that simply learn&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.18576v3-abstract-full').style.display = 'inline'; document.getElementById('2507.18576v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.18576v3-abstract-full" style="display: none;">
        We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that demonstrates the coevolution of capabilities and safety. It is developed by our proposed SafeLadder framework, which incorporates large-scale, progressive, safety-oriented reinforcement learning post-training, supported by a suite of multi-principled verifiers. Unlike previous alignment methods such as RLHF that simply learn human preferences, SafeLadder enables SafeWork-R1 to develop intrinsic safety reasoning and self-reflection abilities, giving rise to safety `aha&#39; moments. Notably, SafeWork-R1 achieves an average improvement of $46.54\%$ over its base model Qwen2.5-VL-72B on safety-related benchmarks without compromising general capabilities, and delivers state-of-the-art safety performance compared to leading proprietary models such as GPT-4.1 and Claude Opus 4. To further bolster its reliability, we implement two distinct inference-time intervention methods and a deliberative search mechanism, enforcing step-level verification. Finally, we further develop SafeWork-R1-InternVL3-78B, SafeWork-R1-DeepSeek-70B, and SafeWork-R1-Qwen2.5VL-7B. All resulting models demonstrate that safety and capability can co-evolve synergistically, highlighting the generalizability of our framework in building robust, reliable, and trustworthy general-purpose AI.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.18576v3-abstract-full').style.display = 'none'; document.getElementById('2507.18576v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 24 July, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">47 pages, 18 figures, authors are listed in alphabetical order by their last names; v3 modifies minor issues</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.18028">arXiv:2507.18028</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.18028">pdf</a>, <a href="https://arxiv.org/ps/2507.18028">ps</a>, <a href="https://arxiv.org/format/2507.18028">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Fei%2C+W">Weizhi Fei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+H">Hao Shi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+J">Jing Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+J">Jingchen Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Jiazheng Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+J">Jingzhao Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+B">Bo Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+W">Wei Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Z">Zhenyuan Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Niu%2C+X">Xueyan Niu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.18028v1-abstract-short" style="display: inline;">
        Efficiently editing knowledge stored in large language models (LLMs) enables model updates without large-scale training. One possible solution is Locate-and-Edit (L\&amp;E), allowing simultaneous modifications of a massive number of facts. However, such editing may compromise the general abilities of LLMs and even result in forgetting edited facts when scaling up to thousands of edits. In this paper,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.18028v1-abstract-full').style.display = 'inline'; document.getElementById('2507.18028v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.18028v1-abstract-full" style="display: none;">
        Efficiently editing knowledge stored in large language models (LLMs) enables model updates without large-scale training. One possible solution is Locate-and-Edit (L\&amp;E), allowing simultaneous modifications of a massive number of facts. However, such editing may compromise the general abilities of LLMs and even result in forgetting edited facts when scaling up to thousands of edits. In this paper, we model existing linear L\&amp;E methods as querying a Key-Value (KV) database. From this perspective, we then propose NeuralDB, an editing framework that explicitly represents the edited facts as a neural KV database equipped with a non-linear gated retrieval module, % In particular, our gated module only operates when inference involves the edited facts, effectively preserving the general abilities of LLMs. Comprehensive experiments involving the editing of 10,000 facts were conducted on the ZsRE and CounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results demonstrate that NeuralDB not only excels in editing efficacy, generalization, specificity, fluency, and consistency, but also preserves overall performance across six representative text understanding and generation tasks. Further experiments indicate that NeuralDB maintains its effectiveness even when scaled to 100,000 facts (\textbf{50x} more than in prior work).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.18028v1-abstract-full').style.display = 'none'; document.getElementById('2507.18028v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 July, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.16632">arXiv:2507.16632</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.16632">pdf</a>, <a href="https://arxiv.org/ps/2507.16632">ps</a>, <a href="https://arxiv.org/format/2507.16632">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Step-Audio 2 Technical Report
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+B">Boyong Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yan%2C+C">Chao Yan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+C">Chen Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yi%2C+C">Cheng Yi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feng%2C+C">Chengli Feng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tian%2C+F">Fei Tian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shen%2C+F">Feiyu Shen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+G">Gang Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+H">Haoyang Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Jingbei Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+M">Mingrui Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+P">Peng Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=You%2C+W">Wang You</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+X+T">Xiangyu Tony Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xingyuan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+X">Xuerui Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+Y">Yayue Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+Y">Yechang Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yuxin Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yuxin Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=You%2C+Z">Zhao You</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+B">Brian Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wan%2C+C">Changyi Wan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+H">Hanpeng Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhen%2C+J">Jiangjie Zhen</a>
      , et al. (84 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.16632v3-abstract-short" style="display: inline;">
        This paper presents Step-Audio 2, an end-to-end multi-modal large language model designed for industry-strength audio understanding and speech conversation. By integrating a latent audio encoder and reasoning-centric reinforcement learning (RL), Step-Audio 2 achieves promising performance in automatic speech recognition (ASR) and audio understanding. To facilitate genuine end-to-end speech convers&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.16632v3-abstract-full').style.display = 'inline'; document.getElementById('2507.16632v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.16632v3-abstract-full" style="display: none;">
        This paper presents Step-Audio 2, an end-to-end multi-modal large language model designed for industry-strength audio understanding and speech conversation. By integrating a latent audio encoder and reasoning-centric reinforcement learning (RL), Step-Audio 2 achieves promising performance in automatic speech recognition (ASR) and audio understanding. To facilitate genuine end-to-end speech conversation, Step-Audio 2 incorporates the generation of discrete audio tokens into language modeling, significantly enhancing its responsiveness to paralinguistic information such as speaking styles and emotions. To effectively leverage the rich textual and acoustic knowledge in real-world data, Step-Audio 2 integrates retrieval-augmented generation (RAG) and is able to call external tools such as web search to mitigate hallucination and audio search to switch timbres. Trained on millions of hours of speech and audio data, Step-Audio 2 delivers intelligence and expressiveness across diverse conversational scenarios. Evaluation results demonstrate that Step-Audio 2 achieves state-of-the-art performance on various audio understanding and conversational benchmarks compared to other open-source and commercial solutions. Please visit https://github.com/stepfun-ai/Step-Audio2 for more information.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.16632v3-abstract-full').style.display = 'none'; document.getElementById('2507.16632v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 22 July, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">v3: Added introduction and evaluation results of Step-Audio 2 mini</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.16229">arXiv:2507.16229</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.16229">pdf</a>, <a href="https://arxiv.org/ps/2507.16229">ps</a>, <a href="https://arxiv.org/format/2507.16229">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Emerging Technologies">cs.ET</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Voice-based AI Agents: Filling the Economic Gaps in Digital Health Delivery
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wen%2C+B">Bo Wen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+C">Chen Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+Q">Qiwei Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Norel%2C+R">Raquel Norel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Julia Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Stappenbeck%2C+T">Thaddeus Stappenbeck</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rogers%2C+J+L">Jeffrey L. Rogers</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.16229v1-abstract-short" style="display: inline;">
        The integration of voice-based AI agents in healthcare presents a transformative opportunity to bridge economic and accessibility gaps in digital health delivery. This paper explores the role of large language model (LLM)-powered voice assistants in enhancing preventive care and continuous patient monitoring, particularly in underserved populations. Drawing insights from the development and pilot&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.16229v1-abstract-full').style.display = 'inline'; document.getElementById('2507.16229v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.16229v1-abstract-full" style="display: none;">
        The integration of voice-based AI agents in healthcare presents a transformative opportunity to bridge economic and accessibility gaps in digital health delivery. This paper explores the role of large language model (LLM)-powered voice assistants in enhancing preventive care and continuous patient monitoring, particularly in underserved populations. Drawing insights from the development and pilot study of Agent PULSE (Patient Understanding and Liaison Support Engine) -- a collaborative initiative between IBM Research, Cleveland Clinic Foundation, and Morehouse School of Medicine -- we present an economic model demonstrating how AI agents can provide cost-effective healthcare services where human intervention is economically unfeasible. Our pilot study with 33 inflammatory bowel disease patients revealed that 70\% expressed acceptance of AI-driven monitoring, with 37\% preferring it over traditional modalities. Technical challenges, including real-time conversational AI processing, integration with healthcare systems, and privacy compliance, are analyzed alongside policy considerations surrounding regulation, bias mitigation, and patient autonomy. Our findings suggest that AI-driven voice agents not only enhance healthcare scalability and efficiency but also improve patient engagement and accessibility. For healthcare executives, our cost-utility analysis demonstrates huge potential savings for routine monitoring tasks, while technologists can leverage our framework to prioritize improvements yielding the highest patient impact. By addressing current limitations and aligning AI development with ethical and regulatory frameworks, voice-based AI agents can serve as a critical entry point for equitable, sustainable digital healthcare solutions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.16229v1-abstract-full').style.display = 'none'; document.getElementById('2507.16229v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 July, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">IEEE International Conference on Digital Health (ICDH) 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.13241">arXiv:2507.13241</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.13241">pdf</a>, <a href="https://arxiv.org/ps/2507.13241">ps</a>, <a href="https://arxiv.org/format/2507.13241">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Nuclear Experiment">nucl-ex</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="High Energy Physics - Experiment">hep-ex</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Precise Measurement of $^{216}$Po Half-life with Exact Parent-daughter Pairing in PandaX-4T
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=PandaX+Collaboration"> PandaX Collaboration</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+C">Chenxiang Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bo%2C+Z">Zihao Bo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+W">Wei Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xun Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yunhua Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+C">Chen Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cui%2C+X">Xiangyi Cui</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+M">Manna Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fan%2C+Y">Yingjie Fan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fang%2C+D">Deqing Fang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fu%2C+X">Xuanye Fu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+Z">Zhixing Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ge%2C+Y">Yujie Ge</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Geng%2C+L">Lisheng Geng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Giboni%2C+K">Karl Giboni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+X">Xunan Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+X">Xuyuan Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Z">Zichao Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+C">Chencheng Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+K">Ke Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+C">Changda He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+J">Jinrong He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+H">Houqi Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+J">Junting Huang</a>
      , et al. (86 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.13241v1-abstract-short" style="display: inline;">
        We report a precise measurement of $^{216}\rm Po$ half-life using the PandaX-4T liquid xenon time projection chamber (TPC). $^{220}\rm Rn $, emanating from a $^{228}\rm Th $ calibration source, is injected to the detector and undergoes successive $α$ decays, first to $^{216}\rm Po$ and then to $^{212}\rm Pb$. PandaX-4T detector measures the 5-dimensional (5D) information of each decay, including t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.13241v1-abstract-full').style.display = 'inline'; document.getElementById('2507.13241v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.13241v1-abstract-full" style="display: none;">
        We report a precise measurement of $^{216}\rm Po$ half-life using the PandaX-4T liquid xenon time projection chamber (TPC). $^{220}\rm Rn $, emanating from a $^{228}\rm Th $ calibration source, is injected to the detector and undergoes successive $α$ decays, first to $^{216}\rm Po$ and then to $^{212}\rm Pb$. PandaX-4T detector measures the 5-dimensional (5D) information of each decay, including time, energy, and 3-dimensional positions. Therefore, we can identify the $^{220}\rm Rn $ and $^{216}\rm Po$ decay events and pair them exactly to extract the lifetime of each $^{216}\rm Po$. With a large data set and high-precision $^{220}\rm $Rn-$^{216}\rm $Po pairing technique, we measure the $^{216}\rm Po$ half-life to be $143.7\pm0.5$ ms, which is the most precise result to date and agrees with previously published values. The leading precision of this measurement demonstrates the power of 5D calorimeter and the potential of exact parent-daughter pairing in the xenon TPC.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.13241v1-abstract-full').style.display = 'none'; document.getElementById('2507.13241v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 July, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.11930">arXiv:2507.11930</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.11930">pdf</a>, <a href="https://arxiv.org/ps/2507.11930">ps</a>, <a href="https://arxiv.org/format/2507.11930">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="High Energy Physics - Experiment">hep-ex</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Search for Light Dark Matter with 259-day data in PandaX-4T
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+M">Minzhen Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bo%2C+Z">Zihao Bo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+W">Wei Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xun Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yunhua Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+C">Chen Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cui%2C+X">Xiangyi Cui</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+M">Manna Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fan%2C+Y">Yingjie Fan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fang%2C+D">Deqing Fang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fu%2C+X">Xuanye Fu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+Z">Zhixing Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ge%2C+Y">Yujie Ge</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Geng%2C+L">Lisheng Geng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Giboni%2C+K">Karl Giboni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+X">Xunan Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+X">Xuyuan Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Z">Zichao Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+C">Chencheng Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+K">Ke Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+C">Changda He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+J">Jinrong He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+H">Houqi Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+J">Junting Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+Y">Yule Huang</a>
      , et al. (86 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.11930v1-abstract-short" style="display: inline;">
        We present a search for light dark matter particles through their interactions with atomic electrons and nucleons, utilizing PandaX-4T data with an effective exposure of 1.04 tonne$\cdot$year for ionization-only data and 1.20 tonne$\cdot$year for paired data. Our analysis focuses on the energy range (efficiency$&gt;$0.01) of approximately 0.33 to 3 keV for nuclear recoils, and from 0.04 to 0.39 keV f&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.11930v1-abstract-full').style.display = 'inline'; document.getElementById('2507.11930v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.11930v1-abstract-full" style="display: none;">
        We present a search for light dark matter particles through their interactions with atomic electrons and nucleons, utilizing PandaX-4T data with an effective exposure of 1.04 tonne$\cdot$year for ionization-only data and 1.20 tonne$\cdot$year for paired data. Our analysis focuses on the energy range (efficiency$&gt;$0.01) of approximately 0.33 to 3 keV for nuclear recoils, and from 0.04 to 0.39 keV for electronic recoils. We establish the most stringent constraints on spin-independent dark matter-nucleon interactions within a mass range of 2.5 to 5.0 GeV/$c^2$, spin-dependent neutron-only interactions within 1.0 to 5.6 GeV/$c^2$, and spin-dependent proton-only interactions within 1.0 to 4.1 GeV/$c^2$. Additionally, our results improve the upper limits on the dark matter-electron scattering cross-section by a factor of 1.5 and 9.3 for heavy and light mediator scenarios respectively within 50 MeV/$c^2$ to 10 GeV/$c^2$, compared with previous best results.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.11930v1-abstract-full').style.display = 'none'; document.getElementById('2507.11930v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 July, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.11145">arXiv:2507.11145</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.11145">pdf</a>, <a href="https://arxiv.org/ps/2507.11145">ps</a>, <a href="https://arxiv.org/format/2507.11145">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="High Energy Physics - Experiment">hep-ex</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Observation of the electromagnetic radiative decays of the \boldmath{$Λ(1520)$} and \boldmath{$Λ(1670)$} to \boldmath{$γΣ^0$}
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=BESIII+Collaboration"> BESIII Collaboration</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ablikim%2C+M">M. Ablikim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Achasov%2C+M+N">M. N. Achasov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adlarson%2C+P">P. Adlarson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ai%2C+X+C">X. C. Ai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aliberti%2C+R">R. Aliberti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Amoroso%2C+A">A. Amoroso</a>, 
      
      <a href="/search/?searchtype=author&amp;query=An%2C+Q">Q. An</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+Y">Y. Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bakina%2C+O">O. Bakina</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ban%2C+Y">Y. Ban</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bao%2C+H+-">H. -R. Bao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Batozskaya%2C+V">V. Batozskaya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Begzsuren%2C+K">K. Begzsuren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berger%2C+N">N. Berger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berlowski%2C+M">M. Berlowski</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bertani%2C+M">M. Bertani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bettoni%2C+D">D. Bettoni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianchi%2C+F">F. Bianchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianco%2C+E">E. Bianco</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bortone%2C+A">A. Bortone</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boyko%2C+I">I. Boyko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Briere%2C+R+A">R. A. Briere</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brueggemann%2C+A">A. Brueggemann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+H">H. Cai</a>
      , et al. (697 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.11145v1-abstract-short" style="display: inline;">
        Using $(10087\pm 44)\times10^6$ $J/ψ$ events collected with the BESIII detector, we report the first observation of the electromagnetic radiative decays of the $Λ(1520)$ and $Λ(1670)$ to $γΣ^0$, with a statistical significance of $16.6σ$ and $23.5σ$, respectively. The ratio of the branching fractions $\frac{\mathcal{B}(Λ(1520)\toγΛ)}{\mathcal{B}(Λ(1520)\toγΣ^0)}$ is determined to be&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.11145v1-abstract-full').style.display = 'inline'; document.getElementById('2507.11145v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.11145v1-abstract-full" style="display: none;">
        Using $(10087\pm 44)\times10^6$ $J/ψ$ events collected with the BESIII detector, we report the first observation of the electromagnetic radiative decays of the $Λ(1520)$ and $Λ(1670)$ to $γΣ^0$, with a statistical significance of $16.6σ$ and $23.5σ$, respectively. The ratio of the branching fractions $\frac{\mathcal{B}(Λ(1520)\toγΛ)}{\mathcal{B}(Λ(1520)\toγΣ^0)}$ is determined to be $2.88\pm0.27(\text{stat.})\pm0.21(\text{syst.})$, which is in good agreement with flavor SU(3) symmetry. The branching fraction of $Λ(1520)\toγΣ^0$ is measured to be $\mathcal{B}(Λ(1520)\toγΣ^0)=(2.95\pm0.28(\text{stat.})\pm0.56(\text{syst.}))\times 10^{-3}$, corresponding to a partial width of $Γ(Λ(1520)\toγΣ^0)=(47.2\pm4.5(\text{stat.})\pm9.0(\text{syst.}))$ keV, which is inconsistent with predictions from the relativized constituent quark model and the Algebraic model. Additionally, we observe a clear resonant structure in the $γΣ^0$ mass spectrum around 1.67 GeV/$c^2$, attributed to the $Λ(1670)$. The product branching fraction $\mathcal{B}(J/ψ\to\barΛΛ(1670)+c.c.)\times\mathcal{B}(Λ(1670)\toγΣ^0)$ is measured for the first time as $(5.39\pm0.29(\text{stat.})\pm 0.44(\text{syst.}))\times 10^{-6}$. However, no corresponding structure is seen in the $γΛ$ mass spectrum, so an upper limit on the product branching fraction $\mathcal{B}(J/ψ\to\barΛΛ(1670)+c.c.)\times\mathcal{B}(Λ(1670)\toγΛ)$ is determined to be $5.97\times10^{-7}$ at the 90\% confidence level.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.11145v1-abstract-full').style.display = 'none'; document.getElementById('2507.11145v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 July, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.10331">arXiv:2507.10331</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.10331">pdf</a>, <a href="https://arxiv.org/ps/2507.10331">ps</a>, <a href="https://arxiv.org/format/2507.10331">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="High Energy Physics - Experiment">hep-ex</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Search for the charged lepton flavor violating decay $ψ(3686)\to e^{\pm}μ^{\mp}$
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=BESIII+Collaboration"> BESIII Collaboration</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ablikim%2C+M">M. Ablikim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Achasov%2C+M+N">M. N. Achasov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adlarson%2C+P">P. Adlarson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ai%2C+X+C">X. C. Ai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aliberti%2C+R">R. Aliberti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Amoroso%2C+A">A. Amoroso</a>, 
      
      <a href="/search/?searchtype=author&amp;query=An%2C+Q">Q. An</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+Y">Y. Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bakina%2C+O">O. Bakina</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ban%2C+Y">Y. Ban</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bao%2C+H+-">H. -R. Bao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Batozskaya%2C+V">V. Batozskaya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Begzsuren%2C+K">K. Begzsuren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berger%2C+N">N. Berger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berlowski%2C+M">M. Berlowski</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bertani%2C+M+B">M. B. Bertani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bettoni%2C+D">D. Bettoni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianchi%2C+F">F. Bianchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianco%2C+E">E. Bianco</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bortone%2C+A">A. Bortone</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boyko%2C+I">I. Boyko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Briere%2C+R+A">R. A. Briere</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brueggemann%2C+A">A. Brueggemann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+H">H. Cai</a>
      , et al. (706 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.10331v1-abstract-short" style="display: inline;">
        By analyzing $(2367.0\pm11.1)\times10^6$ $ψ(3686)$ events collected in $e^+e^-$ collisions at $\sqrt{s}=3.686~\rm GeV$ with the BESIII detector at the BEPCII collider, we report the first search for the charged lepton flavor violating decay $ψ(3686)\to e^{\pm}μ^{\mp}$. No signal is found. An upper limit on the branching fraction $\mathcal{B}(ψ(3686)\to e^{\pm}μ^{\mp})$ is determined to be&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.10331v1-abstract-full').style.display = 'inline'; document.getElementById('2507.10331v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.10331v1-abstract-full" style="display: none;">
        By analyzing $(2367.0\pm11.1)\times10^6$ $ψ(3686)$ events collected in $e^+e^-$ collisions at $\sqrt{s}=3.686~\rm GeV$ with the BESIII detector at the BEPCII collider, we report the first search for the charged lepton flavor violating decay $ψ(3686)\to e^{\pm}μ^{\mp}$. No signal is found. An upper limit on the branching fraction $\mathcal{B}(ψ(3686)\to e^{\pm}μ^{\mp})$ is determined to be $1.4\times10^{-8}$ at the 90\% confidence level.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.10331v1-abstract-full').style.display = 'none'; document.getElementById('2507.10331v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 July, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 4 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.06872">arXiv:2507.06872</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.06872">pdf</a>, <a href="https://arxiv.org/ps/2507.06872">ps</a>, <a href="https://arxiv.org/format/2507.06872">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="High Energy Physics - Experiment">hep-ex</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Search for the lepton number violating process $J/ψ\to K^+K^+e^-e^- +c.c.$
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=BESIII+Collaboration"> BESIII Collaboration</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ablikim%2C+M">M. Ablikim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Achasov%2C+M+N">M. N. Achasov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adlarson%2C+P">P. Adlarson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ai%2C+X+C">X. C. Ai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aliberti%2C+R">R. Aliberti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Amoroso%2C+A">A. Amoroso</a>, 
      
      <a href="/search/?searchtype=author&amp;query=An%2C+Q">Q. An</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+Y">Y. Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bakina%2C+O">O. Bakina</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ban%2C+Y">Y. Ban</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bao%2C+H+-">H. -R. Bao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Batozskaya%2C+V">V. Batozskaya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Begzsuren%2C+K">K. Begzsuren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berger%2C+N">N. Berger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berlowski%2C+M">M. Berlowski</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bertani%2C+M">M. Bertani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bettoni%2C+D">D. Bettoni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianchi%2C+F">F. Bianchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianco%2C+E">E. Bianco</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bortone%2C+A">A. Bortone</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boyko%2C+I">I. Boyko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Briere%2C+R+A">R. A. Briere</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brueggemann%2C+A">A. Brueggemann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+H">H. Cai</a>
      , et al. (697 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.06872v1-abstract-short" style="display: inline;">
        Based on $(10087\pm 44)\times10^{6}$ $J/ψ$ events collected with the BESIII detector, we search for the lepton number violating decay $J/ψ\to K^+K^+e^-e^- + c.c.$ for the first time. The upper limit on the branching fraction of this decay is set to be $2.1 \times 10^{-9}$ at the 90$\%$ confidence level with a frequentist method. This is the first search for $J/ψ$ decays with the lepton number chan&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.06872v1-abstract-full').style.display = 'inline'; document.getElementById('2507.06872v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.06872v1-abstract-full" style="display: none;">
        Based on $(10087\pm 44)\times10^{6}$ $J/ψ$ events collected with the BESIII detector, we search for the lepton number violating decay $J/ψ\to K^+K^+e^-e^- + c.c.$ for the first time. The upper limit on the branching fraction of this decay is set to be $2.1 \times 10^{-9}$ at the 90$\%$ confidence level with a frequentist method. This is the first search for $J/ψ$ decays with the lepton number change by two, offering valuable insights into the underlying physical processes.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.06872v1-abstract-full').style.display = 'none'; document.getElementById('2507.06872v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 July, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 3 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.06261">arXiv:2507.06261</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.06261">pdf</a>, <a href="https://arxiv.org/ps/2507.06261">ps</a>, <a href="https://arxiv.org/format/2507.06261">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Comanici%2C+G">Gheorghe Comanici</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bieber%2C+E">Eric Bieber</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schaekermann%2C+M">Mike Schaekermann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pasupat%2C+I">Ice Pasupat</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sachdeva%2C+N">Noveen Sachdeva</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dhillon%2C+I">Inderjit Dhillon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Blistein%2C+M">Marcel Blistein</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ram%2C+O">Ori Ram</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+D">Dan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rosen%2C+E">Evan Rosen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Marris%2C+L">Luke Marris</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Petulla%2C+S">Sam Petulla</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gaffney%2C+C">Colin Gaffney</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aharoni%2C+A">Asaf Aharoni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lintz%2C+N">Nathan Lintz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pais%2C+T+C">Tiago Cardal Pais</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jacobsson%2C+H">Henrik Jacobsson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Szpektor%2C+I">Idan Szpektor</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+N">Nan-Jiang Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Haridasan%2C+K">Krishna Haridasan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Omran%2C+A">Ahmed Omran</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Saunshi%2C+N">Nikunj Saunshi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bahri%2C+D">Dara Bahri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mishra%2C+G">Gaurav Mishra</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chu%2C+E">Eric Chu</a>
      , et al. (3284 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.06261v4-abstract-short" style="display: inline;">
        In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. In addition to its incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that excels at multimodal unde&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.06261v4-abstract-full').style.display = 'inline'; document.getElementById('2507.06261v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.06261v4-abstract-full" style="display: none;">
        In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. In addition to its incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that excels at multimodal understanding and it is now able to process up to 3 hours of video content. Its unique combination of long context, multimodal and reasoning capabilities can be combined to unlock new agentic workflows. Gemini 2.5 Flash provides excellent reasoning abilities at a fraction of the compute and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high performance at low latency and cost. Taken together, the Gemini 2.X model generation spans the full Pareto frontier of model capability vs cost, allowing users to explore the boundaries of what is possible with complex agentic problem solving.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.06261v4-abstract-full').style.display = 'none'; document.getElementById('2507.06261v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 July, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 July, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">72 pages, 17 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.04618">arXiv:2507.04618</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.04618">pdf</a>, <a href="https://arxiv.org/ps/2507.04618">ps</a>, <a href="https://arxiv.org/format/2507.04618">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Instrumentation and Methods for Astrophysics">astro-ph.IM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cosmology and Nongalactic Astrophysics">astro-ph.CO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Introduction to the Chinese Space Station Survey Telescope (CSST)
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=CSST+Collaboration"> CSST Collaboration</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gong%2C+Y">Yan Gong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Miao%2C+H">Haitao Miao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhan%2C+H">Hu Zhan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zhao-Yu Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shangguan%2C+J">Jinyi Shangguan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+H">Haining Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+C">Chao Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xuefei Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yuan%2C+H">Haibo Yuan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+J">Jilin Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+H">Hui-Gen Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+C">Cong Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ji%2C+J">Jianghui Ji</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qi%2C+Z">Zhaoxiang Qi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Jiacheng Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+Z">Zigao Dai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xiaofeng Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+Z">Zhenya Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hao%2C+L">Lei Hao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dou%2C+J">Jiangpei Dou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ao%2C+Y">Yiping Ao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+Z">Zhenhui Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+K">Kun Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+W">Wei Wang</a>
      , et al. (97 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.04618v2-abstract-short" style="display: inline;">
        The Chinese Space Station Survey Telescope (CSST) is a next-generation Stage-IV sky survey telescope, distinguished by its large field of view (FoV), high image quality, and multi-band observation capabilities. It can simultaneously conduct precise measurements of the Universe by performing multi-color photometric imaging and slitless spectroscopic surveys. The CSST is equipped with five scientifi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.04618v2-abstract-full').style.display = 'inline'; document.getElementById('2507.04618v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.04618v2-abstract-full" style="display: none;">
        The Chinese Space Station Survey Telescope (CSST) is a next-generation Stage-IV sky survey telescope, distinguished by its large field of view (FoV), high image quality, and multi-band observation capabilities. It can simultaneously conduct precise measurements of the Universe by performing multi-color photometric imaging and slitless spectroscopic surveys. The CSST is equipped with five scientific instruments, i.e. Multi-band Imaging and Slitless Spectroscopy Survey Camera (SC), Multi-Channel Imager (MCI), Integral Field Spectrograph (IFS), Cool Planet Imaging Coronagraph (CPI-C), and THz Spectrometer (TS). Using these instruments, CSST is expected to make significant contributions and discoveries across various astronomical fields, including cosmology, galaxy and active galactic nuclei (AGN), the Milky Way and nearby galaxies, stars, exoplanets, Solar System objects, astrometry, and transients and variable sources. This review aims to provide a comprehensive overview of the CSST instruments, observational capabilities, data products, and scientific potential.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.04618v2-abstract-full').style.display = 'none'; document.getElementById('2507.04618v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 July, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">45 pages, 12 figures, 1 table. Discussions, references, and contributors are added</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.04119">arXiv:2507.04119</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.04119">pdf</a>, <a href="https://arxiv.org/ps/2507.04119">ps</a>, <a href="https://arxiv.org/format/2507.04119">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        When Data-Free Knowledge Distillation Meets Non-Transferable Teacher: Escaping Out-of-Distribution Trap is All You Need
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hong%2C+Z">Ziming Hong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+R">Runnan Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Z">Zengmao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+B">Bo Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+B">Bo Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.04119v1-abstract-short" style="display: inline;">
        Data-free knowledge distillation (DFKD) transfers knowledge from a teacher to a student without access the real in-distribution (ID) data. Its common solution is to use a generator to synthesize fake data and use them as a substitute for real ID data. However, existing works typically assume teachers are trustworthy, leaving the robustness and security of DFKD from untrusted teachers largely unexp&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.04119v1-abstract-full').style.display = 'inline'; document.getElementById('2507.04119v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.04119v1-abstract-full" style="display: none;">
        Data-free knowledge distillation (DFKD) transfers knowledge from a teacher to a student without access the real in-distribution (ID) data. Its common solution is to use a generator to synthesize fake data and use them as a substitute for real ID data. However, existing works typically assume teachers are trustworthy, leaving the robustness and security of DFKD from untrusted teachers largely unexplored. In this work, we conduct the first investigation into distilling non-transferable learning (NTL) teachers using DFKD, where the transferability from an ID domain to an out-of-distribution (OOD) domain is prohibited. We find that NTL teachers fool DFKD through divert the generator&#39;s attention from the useful ID knowledge to the misleading OOD knowledge. This hinders ID knowledge transfer but prioritizes OOD knowledge transfer. To mitigate this issue, we propose Adversarial Trap Escaping (ATEsc) to benefit DFKD by identifying and filtering out OOD-like synthetic samples. Specifically, inspired by the evidence that NTL teachers show stronger adversarial robustness on OOD samples than ID samples, we split synthetic samples into two groups according to their robustness. The fragile group is treated as ID-like data and used for normal knowledge distillation, while the robust group is seen as OOD-like data and utilized for forgetting OOD knowledge. Extensive experiments demonstrate the effectiveness of ATEsc for improving DFKD against NTL teachers. Code is released at https://github.com/tmllab/2025_ICML_ATEsc.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.04119v1-abstract-full').style.display = 'none'; document.getElementById('2507.04119v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 July, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by ICML 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.01773">arXiv:2507.01773</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.01773">pdf</a>, <a href="https://arxiv.org/ps/2507.01773">ps</a>, <a href="https://arxiv.org/format/2507.01773">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Frontiers of Generative AI for Network Optimization: Theories, Limits, and Visions
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+B">Bo Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liang%2C+R">Ruihuai Liang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+W">Weixin Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Han Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+X">Xuelin Cao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+Z">Zhiwen Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lasaulce%2C+S">Samson Lasaulce</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Debbah%2C+M">Mérouane Debbah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Alouini%2C+M">Mohamed-Slim Alouini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Poor%2C+H+V">H. Vincent Poor</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yuen%2C+C">Chau Yuen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.01773v1-abstract-short" style="display: inline;">
        While interest in the application of generative AI (GenAI) in network optimization has surged in recent years, its rapid progress has often overshadowed critical limitations intrinsic to generative models that remain insufficiently examined in existing literature. This survey provides a comprehensive review and critical analysis of GenAI in network optimization. We focus on the two dominant paradi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.01773v1-abstract-full').style.display = 'inline'; document.getElementById('2507.01773v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.01773v1-abstract-full" style="display: none;">
        While interest in the application of generative AI (GenAI) in network optimization has surged in recent years, its rapid progress has often overshadowed critical limitations intrinsic to generative models that remain insufficiently examined in existing literature. This survey provides a comprehensive review and critical analysis of GenAI in network optimization. We focus on the two dominant paradigms of GenAI including generative diffusion models (GDMs) and large pre-trained models (LPTMs), and organize our discussion around a categorization we introduce, dividing network optimization problems into two primary formulations: one-shot optimization and Markov decision process (MDP). We first trace key works, including foundational contributions from the AI community, and categorize current efforts in network optimization. We also review frontier applications of GDMs and LPTMs in other networking tasks, providing additional context. Furthermore, we present theoretical generalization bounds for GDMs in both one-shot and MDP settings, offering insights into the fundamental factors affecting model performance. Most importantly, we reflect on the overestimated perception of GenAI&#39;s general capabilities and caution against the all-in-one illusion it may convey. We highlight critical limitations, including difficulties in constraint satisfying, limited concept understanding, and the inherent probabilistic nature of outputs. We also propose key future directions, such as bridging the gap between generation and optimization. Although they are increasingly integrated in implementations, they differ fundamentally in both objectives and underlying mechanisms, necessitating a deeper understanding of their theoretical connections. Ultimately, this survey aims to provide a structured overview and a deeper insight into the strengths, limitations, and potential of GenAI in network optimization.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.01773v1-abstract-full').style.display = 'none'; document.getElementById('2507.01773v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 July, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.23351">arXiv:2506.23351</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.23351">pdf</a>, <a href="https://arxiv.org/ps/2506.23351">ps</a>, <a href="https://arxiv.org/format/2506.23351">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multiagent Systems">cs.MA</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Benchmarking Generalizable Bimanual Manipulation: RoboTwin Dual-Arm Collaboration Challenge at CVPR 2025 MEIS Workshop
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+T">Tianxing Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+K">Kaixuan Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Z">Zhaohui Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yuhao Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Z">Zanxin Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+B">Baijun Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dong%2C+W">Wanxi Dong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Z">Ziyuan Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+D">Dong Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+T">Tianshuo Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+H">Haibao Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+X">Xiaokang Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qin%2C+Y">Yusen Qin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+Z">Zhiqiang Xie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mu%2C+Y">Yao Mu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+P">Ping Luo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nian%2C+T">Tian Nian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+W">Weiliang Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ge%2C+Y">Yiheng Ge</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yibin Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zixuan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+D">Dehui Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liang%2C+Z">Zhixuan Liang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+H">Haohui Xie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zeng%2C+R">Rijie Zeng</a>
      , et al. (74 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.23351v2-abstract-short" style="display: inline;">
        Embodied Artificial Intelligence (Embodied AI) is an emerging frontier in robotics, driven by the need for autonomous systems that can perceive, reason, and act in complex physical environments. While single-arm systems have shown strong task performance, collaborative dual-arm systems are essential for handling more intricate tasks involving rigid, deformable, and tactile-sensitive objects. To ad&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.23351v2-abstract-full').style.display = 'inline'; document.getElementById('2506.23351v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.23351v2-abstract-full" style="display: none;">
        Embodied Artificial Intelligence (Embodied AI) is an emerging frontier in robotics, driven by the need for autonomous systems that can perceive, reason, and act in complex physical environments. While single-arm systems have shown strong task performance, collaborative dual-arm systems are essential for handling more intricate tasks involving rigid, deformable, and tactile-sensitive objects. To advance this goal, we launched the RoboTwin Dual-Arm Collaboration Challenge at the 2nd MEIS Workshop, CVPR 2025. Built on the RoboTwin Simulation platform (1.0 and 2.0) and the AgileX COBOT-Magic Robot platform, the competition consisted of three stages: Simulation Round 1, Simulation Round 2, and a final Real-World Round. Participants totally tackled 17 dual-arm manipulation tasks, covering rigid, deformable, and tactile-based scenarios. The challenge attracted 64 global teams and over 400 participants, producing top-performing solutions like SEM and AnchorDP3 and generating valuable insights into generalizable bimanual policy learning. This report outlines the competition setup, task design, evaluation methodology, key findings and future direction, aiming to support future research on robust and generalizable bimanual manipulation policies. The Challenge Webpage is available at https://robotwin-benchmark.github.io/cvpr-2025-challenge/.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.23351v2-abstract-full').style.display = 'none'; document.getElementById('2506.23351v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 July, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 29 June, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Challenge Webpage: https://robotwin-benchmark.github.io/cvpr-2025-challenge/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.22554">arXiv:2506.22554</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.22554">pdf</a>, <a href="https://arxiv.org/ps/2506.22554">ps</a>, <a href="https://arxiv.org/format/2506.22554">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Agrawal%2C+V">Vasu Agrawal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Akinyemi%2C+A">Akinniyi Akinyemi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Alvero%2C+K">Kathryn Alvero</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Behrooz%2C+M">Morteza Behrooz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Buffalini%2C+J">Julia Buffalini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Carlucci%2C+F+M">Fabio Maria Carlucci</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+J">Joy Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+J">Junming Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Z">Zhang Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+S">Shiyang Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chowdary%2C+P">Praveen Chowdary</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chuang%2C+J">Joe Chuang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=D%27Avirro%2C+A">Antony D&#39;Avirro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Daly%2C+J">Jon Daly</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dong%2C+N">Ning Dong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Duppenthaler%2C+M">Mark Duppenthaler</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+C">Cynthia Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Girard%2C+J">Jeff Girard</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gleize%2C+M">Martin Gleize</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gomez%2C+S">Sahir Gomez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gong%2C+H">Hongyu Gong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Govindarajan%2C+S">Srivathsan Govindarajan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+B">Brandon Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+S">Sen He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hernandez%2C+D">Denise Hernandez</a>
      , et al. (59 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.22554v2-abstract-short" style="display: inline;">
        Human communication involves a complex interplay of verbal and nonverbal signals, essential for conveying meaning and achieving interpersonal goals. To develop socially intelligent AI technologies, it is crucial to develop models that can both comprehend and generate dyadic behavioral dynamics. To this end, we introduce the Seamless Interaction Dataset, a large-scale collection of over 4,000 hours&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.22554v2-abstract-full').style.display = 'inline'; document.getElementById('2506.22554v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.22554v2-abstract-full" style="display: none;">
        Human communication involves a complex interplay of verbal and nonverbal signals, essential for conveying meaning and achieving interpersonal goals. To develop socially intelligent AI technologies, it is crucial to develop models that can both comprehend and generate dyadic behavioral dynamics. To this end, we introduce the Seamless Interaction Dataset, a large-scale collection of over 4,000 hours of face-to-face interaction footage from over 4,000 participants in diverse contexts. This dataset enables the development of AI technologies that understand dyadic embodied dynamics, unlocking breakthroughs in virtual agents, telepresence experiences, and multimodal content analysis tools. We also develop a suite of models that utilize the dataset to generate dyadic motion gestures and facial expressions aligned with human speech. These models can take as input both the speech and visual behavior of their interlocutors. We present a variant with speech from an LLM model and integrations with 2D and 3D rendering methods, bringing us closer to interactive virtual agents. Additionally, we describe controllable variants of our motion models that can adapt emotional responses and expressivity levels, as well as generating more semantically-relevant gestures. Finally, we discuss methods for assessing the quality of these dyadic motion models, which are demonstrating the potential for more intuitive and responsive human-AI interactions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.22554v2-abstract-full').style.display = 'none'; document.getElementById('2506.22554v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 June, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 27 June, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.22518">arXiv:2506.22518</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.22518">pdf</a>, <a href="https://arxiv.org/ps/2506.22518">ps</a>, <a href="https://arxiv.org/format/2506.22518">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Weak-to-Strong GraphRAG: Aligning Weak Retrievers with Large Language Models for Graph-based Retrieval Augmented Generation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zou%2C+D">Deyu Zou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yongqiang Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+M">Mufei Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Miao%2C+S">Siqi Miao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+C">Chenxi Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+B">Bo Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+J">James Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+P">Pan Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.22518v1-abstract-short" style="display: inline;">
        Graph-based retrieval-augmented generation (RAG) enables large language models (LLMs) to ground responses with structured external knowledge from up-to-date knowledge graphs (KGs) and reduce hallucinations. However, LLMs often rely on a weak retriever in graph-based RAG: I) Due to the lack of ground truth, the retriever is often trained on weak supervision, which often introduces spurious signals&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.22518v1-abstract-full').style.display = 'inline'; document.getElementById('2506.22518v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.22518v1-abstract-full" style="display: none;">
        Graph-based retrieval-augmented generation (RAG) enables large language models (LLMs) to ground responses with structured external knowledge from up-to-date knowledge graphs (KGs) and reduce hallucinations. However, LLMs often rely on a weak retriever in graph-based RAG: I) Due to the lack of ground truth, the retriever is often trained on weak supervision, which often introduces spurious signals to the LLMs. II) Due to the abstraction of graph data, the retrieved knowledge is often presented in unorganized forms. To mitigate the issue, we present Refined Graph-based RAG (ReG) to align weak retrievers to LLMs for graph-based RAG. Specifically, ReG incorporates LLM feedback to get rid of spurious signals and improve the quality of the supervision. Meanwhile, ReG introduces a structure-aware reorganization module to refactor the retrieval results into logically coherent evidence chains. Experiments on prominent benchmarks demonstrate that ReG significantly and consistently brings improvements across different LLM backbones by up to 10%. The improved supervision quality enables ReG to match the state-of-the-art performance with 5% training data and to transfer to out-of-distribution KGs. Notably, when adopted to reasoning-based LLMs, ReG reduces the reasoning token cost by up to 30% and improves the performance by up to 4%.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.22518v1-abstract-full').style.display = 'none'; document.getElementById('2506.22518v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.21215">arXiv:2506.21215</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.21215">pdf</a>, <a href="https://arxiv.org/ps/2506.21215">ps</a>, <a href="https://arxiv.org/format/2506.21215">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chi%2C+H">Haoang Chi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+H">He Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+W">Wenjing Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+F">Feng Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lan%2C+L">Long Lan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ren%2C+X">Xiaoguang Ren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+B">Bo Han</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.21215v1-abstract-short" style="display: inline;">
        Causal reasoning capability is critical in advancing large language models (LLMs) toward strong artificial intelligence. While versatile LLMs appear to have demonstrated capabilities in understanding contextual causality and providing responses that obey the laws of causality, it remains unclear whether they perform genuine causal reasoning akin to humans. However, current evidence indicates the c&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.21215v1-abstract-full').style.display = 'inline'; document.getElementById('2506.21215v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.21215v1-abstract-full" style="display: none;">
        Causal reasoning capability is critical in advancing large language models (LLMs) toward strong artificial intelligence. While versatile LLMs appear to have demonstrated capabilities in understanding contextual causality and providing responses that obey the laws of causality, it remains unclear whether they perform genuine causal reasoning akin to humans. However, current evidence indicates the contrary. Specifically, LLMs are only capable of performing shallow (level-1) causal reasoning, primarily attributed to the causal knowledge embedded in their parameters, but they lack the capacity for genuine human-like (level-2) causal reasoning. To support this hypothesis, methodologically, we delve into the autoregression mechanism of transformer-based LLMs, revealing that it is not inherently causal. Empirically, we introduce a new causal Q&amp;A benchmark called CausalProbe-2024, whose corpora are fresh and nearly unseen for the studied LLMs. The LLMs exhibit a significant performance drop on CausalProbe-2024 compared to earlier benchmarks, indicating the fact that they primarily engage in level-1 causal reasoning. To bridge the gap towards level-2 causal reasoning, we draw inspiration from the fact that human reasoning is usually facilitated by general knowledge and intended goals. We propose G^2-Reasoner, a method that incorporates general knowledge and goal-oriented prompts into LLMs&#39; causal reasoning processes. Experiments demonstrate that G^2-Reasoner significantly enhances LLMs&#39; causal reasoning capability, particularly in fresh and counterfactual contexts. This work sheds light on a new path for LLMs to advance towards genuine causal reasoning, going beyond level-1 and making strides towards level-2.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.21215v1-abstract-full').style.display = 'none'; document.getElementById('2506.21215v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">24 pages, accepted at NeurIPS 2024</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Advances in Neural Information Processing Systems, 2024, 37: 96640-96670
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.19180">arXiv:2506.19180</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.19180">pdf</a>, <a href="https://arxiv.org/ps/2506.19180">ps</a>, <a href="https://arxiv.org/format/2506.19180">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="High Energy Physics - Experiment">hep-ex</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="High Energy Physics - Phenomenology">hep-ph</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Precise Measurement of the $Λ$ Electric Dipole Moment through the Entangled Strange Baryon-Antibaryon System
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=BESIII+Collaboration"> BESIII Collaboration</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ablikim%2C+M">M. Ablikim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Achasov%2C+M+N">M. N. Achasov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adlarson%2C+P">P. Adlarson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ai%2C+X+C">X. C. Ai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aliberti%2C+R">R. Aliberti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Amoroso%2C+A">A. Amoroso</a>, 
      
      <a href="/search/?searchtype=author&amp;query=An%2C+Q">Q. An</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+Y">Y. Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bakina%2C+O">O. Bakina</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ban%2C+Y">Y. Ban</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bao%2C+H+-">H. -R. Bao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Batozskaya%2C+V">V. Batozskaya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Begzsuren%2C+K">K. Begzsuren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berger%2C+N">N. Berger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berlowski%2C+M">M. Berlowski</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bertani%2C+M">M. Bertani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bettoni%2C+D">D. Bettoni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianchi%2C+F">F. Bianchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianco%2C+E">E. Bianco</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bortone%2C+A">A. Bortone</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boyko%2C+I">I. Boyko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Briere%2C+R+A">R. A. Briere</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brueggemann%2C+A">A. Brueggemann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+H">H. Cai</a>
      , et al. (696 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.19180v2-abstract-short" style="display: inline;">
        The dominance of matter over antimatter in the universe has consistently driven the pursuit of new physics beyond the Standard Model that violates charge-parity symmetry. Unlike the well-constrained electrons and neutrons, strange baryons (hyperons) remain a largely unexplored territory, in which interactions between hyperons and particles from new physics could induce a non-trivial electric dipol&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.19180v2-abstract-full').style.display = 'inline'; document.getElementById('2506.19180v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.19180v2-abstract-full" style="display: none;">
        The dominance of matter over antimatter in the universe has consistently driven the pursuit of new physics beyond the Standard Model that violates charge-parity symmetry. Unlike the well-constrained electrons and neutrons, strange baryons (hyperons) remain a largely unexplored territory, in which interactions between hyperons and particles from new physics could induce a non-trivial electric dipole moment (EDM). However, direct measurements of hyperon EDMs through spin precession are highly challenging due to their short lifetimes. In this paper, we present a novel method to extract the EDM of the lightest hyperon, $Λ$, using the entangled $Λ$$\overlineΛ$ system. Our result is consistent with zero, achieving a three-order-of-magnitude improvement over the previous upper limit established in the 1980s with comparable statistics, providing stringent constraints on potential new physics.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.19180v2-abstract-full').style.display = 'none'; document.getElementById('2506.19180v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 June, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 June, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.18902">arXiv:2506.18902</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.18902">pdf</a>, <a href="https://arxiv.org/ps/2506.18902">ps</a>, <a href="https://arxiv.org/format/2506.18902">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=G%C3%BCnther%2C+M">Michael Günther</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sturua%2C+S">Saba Sturua</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Akram%2C+M+K">Mohammad Kalim Akram</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mohr%2C+I">Isabelle Mohr</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ungureanu%2C+A">Andrei Ungureanu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+B">Bo Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Eslami%2C+S">Sedigheh Eslami</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Martens%2C+S">Scott Martens</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Werk%2C+M">Maximilian Werk</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+N">Nan Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiao%2C+H">Han Xiao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.18902v3-abstract-short" style="display: inline;">
        We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding model that unifies text and image representations through a novel architecture supporting both single-vector and multi-vector embeddings in the late interaction style. The model incorporates task-specific Low-Rank Adaptation (LoRA) adapters to optimize performance across diverse retrieval scenarios, including query-docum&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.18902v3-abstract-full').style.display = 'inline'; document.getElementById('2506.18902v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.18902v3-abstract-full" style="display: none;">
        We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding model that unifies text and image representations through a novel architecture supporting both single-vector and multi-vector embeddings in the late interaction style. The model incorporates task-specific Low-Rank Adaptation (LoRA) adapters to optimize performance across diverse retrieval scenarios, including query-document retrieval, semantic text similarity, and code search. Comprehensive evaluations demonstrate that jina-embeddings-v4 achieves state-of-the-art performance on both single-modal and cross-modal retrieval tasks, with particular strength in processing visually rich content such as tables, charts, diagrams, and mixed-media formats. To facilitate evaluation of this capability, we also introduce Jina-VDR, a novel benchmark specifically designed for visually rich image retrieval.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.18902v3-abstract-full').style.display = 'none'; document.getElementById('2506.18902v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 July, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 June, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">22 pages, 1-10 main, 14-22 experimental results, benchmark tables</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68T50
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.7
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.18407">arXiv:2506.18407</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.18407">pdf</a>, <a href="https://arxiv.org/ps/2506.18407">ps</a>, <a href="https://arxiv.org/format/2506.18407">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        What You Think Is What You Get: Bridge User Intent and Transfer Function Design through Multimodal Large Language Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yiyao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pan%2C+B">Bo Pan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+K">Ke Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+H">Han Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mao%2C+J">Jinyuan Mao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yuxin Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+M">Minfeng Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+B">Bo Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+W">Weifeng Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+X">Xiuqi Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+W">Wei Chen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.18407v1-abstract-short" style="display: inline;">
        Direct volume rendering (DVR) is a fundamental technique for visualizing volumetric data, with transfer functions (TFs) playing a crucial role in extracting meaningful structures. However, designing effective TFs remains unintuitive due to the semantic gap between user intent and TF parameter space. Researchers have developed numerous TF optimization methods to bridge this gap. However, existing m&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.18407v1-abstract-full').style.display = 'inline'; document.getElementById('2506.18407v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.18407v1-abstract-full" style="display: none;">
        Direct volume rendering (DVR) is a fundamental technique for visualizing volumetric data, with transfer functions (TFs) playing a crucial role in extracting meaningful structures. However, designing effective TFs remains unintuitive due to the semantic gap between user intent and TF parameter space. Researchers have developed numerous TF optimization methods to bridge this gap. However, existing methods still face two challenges: large exploration space and weak generalizability. To address these issues, we propose What You Think is What You Get (WYTWYG) framework, which leveraging Multi-model Large Language Models (MLLMs) to guide the TF optimization based on user intent. Specifically, we first introduce a novel TF optimization approach comprising two core components: (1) an evolution-based explorer for effective exploration of the TF space, and (2) a volume rendering quality evaluator based on MLLMs to provide generalizable visual guidance. We further propose a TF interactive design system based on this approach. We demonstrate the general applicability of our framework through three case studies, and validate the effectiveness of each component through extensive experiments. Our code is available at: https://github.com/wyysteelhead/TFevolve.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.18407v1-abstract-full').style.display = 'none'; document.getElementById('2506.18407v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.15533">arXiv:2506.15533</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.15533">pdf</a>, <a href="https://arxiv.org/ps/2506.15533">ps</a>, <a href="https://arxiv.org/format/2506.15533">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="High Energy Physics - Experiment">hep-ex</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Measurements of the absolute branching fractions of the doubly Cabibbo-suppressed decays $D^+\to K^+π^0$, $D^+\to K^+η$ and $D^+\to K^+η^{\prime}$
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=BESIII+Collaboration"> BESIII Collaboration</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ablikim%2C+M">M. Ablikim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Achasov%2C+M+N">M. N. Achasov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adlarson%2C+P">P. Adlarson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ai%2C+X+C">X. C. Ai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aliberti%2C+R">R. Aliberti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Amoroso%2C+A">A. Amoroso</a>, 
      
      <a href="/search/?searchtype=author&amp;query=An%2C+Q">Q. An</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+Y">Y. Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bakina%2C+O">O. Bakina</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ban%2C+Y">Y. Ban</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bao%2C+H+-">H. -R. Bao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Batozskaya%2C+V">V. Batozskaya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Begzsuren%2C+K">K. Begzsuren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berger%2C+N">N. Berger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berlowski%2C+M">M. Berlowski</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bertani%2C+M">M. Bertani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bettoni%2C+D">D. Bettoni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianchi%2C+F">F. Bianchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianco%2C+E">E. Bianco</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bortone%2C+A">A. Bortone</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boyko%2C+I">I. Boyko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Briere%2C+R+A">R. A. Briere</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brueggemann%2C+A">A. Brueggemann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+H">H. Cai</a>
      , et al. (697 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.15533v1-abstract-short" style="display: inline;">
        Using $20.3\,\rm fb^{-1}$ of $e^+e^-$ collision data collected at a center-of-mass energy of 3.773\,GeV with the BESIII detector, we present improved measurements of the absolute branching fractions of the doubly Cabibbo-suppressed decays $D^+\to K^+π^0$, $D^+\to K^+η$ and $ D^+ \to K^+ η^{\prime}$ with the double-tag method. The statistical significance of each signal decay exceeds $10σ$. The bra&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.15533v1-abstract-full').style.display = 'inline'; document.getElementById('2506.15533v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.15533v1-abstract-full" style="display: none;">
        Using $20.3\,\rm fb^{-1}$ of $e^+e^-$ collision data collected at a center-of-mass energy of 3.773\,GeV with the BESIII detector, we present improved measurements of the absolute branching fractions of the doubly Cabibbo-suppressed decays $D^+\to K^+π^0$, $D^+\to K^+η$ and $ D^+ \to K^+ η^{\prime}$ with the double-tag method. The statistical significance of each signal decay exceeds $10σ$. The branching fractions are determined to be ${\mathcal B}(D^+\to K^+ π^0) = (1.45 \pm 0.06 \pm 0.06)\times 10^{-4}$, ${\mathcal B}(D^+\to K^+ η) = (1.17 \pm 0.10 \pm 0.03)\times 10^{-4}$ and ${\mathcal B}(D^+\to K^+ η^{\prime}) = (1.88 \pm 0.15 \pm 0.06)\times 10^{-4}$, where the first uncertainties are statistical and the second systematic. These results are consistent with the world average values but with significantly improved precision.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.15533v1-abstract-full').style.display = 'none'; document.getElementById('2506.15533v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">20 pages, 4 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.15524">arXiv:2506.15524</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.15524">pdf</a>, <a href="https://arxiv.org/ps/2506.15524">ps</a>, <a href="https://arxiv.org/format/2506.15524">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        NTIRE 2025 Image Shadow Removal Challenge Report
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Vasluianu%2C+F">Florin-Alexandru Vasluianu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Seizinger%2C+T">Tim Seizinger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+Z">Zhuyun Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+C">Cailian Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+Z">Zongwei Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Timofte%2C+R">Radu Timofte</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+M">Mingjia Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+J">Jin Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Hainuo Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+H">Hengxing Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jiarui Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+Q">Qiming Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+X">Xiaojie Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+X">Xin Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+J">Jiarong Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bao%2C+Y">Yuanfei Bao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+A">Anya Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fan%2C+Z">Zihao Fan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+K">Kunyu Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiao%2C+J">Jie Xiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xi Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fu%2C+X">Xueyang Fu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zha%2C+Z">Zheng-Jun Zha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+Y">Yu-Fan Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+C">Chia-Ming Lee</a>
      , et al. (57 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.15524v1-abstract-short" style="display: inline;">
        This work examines the findings of the NTIRE 2025 Shadow Removal Challenge. A total of 306 participants have registered, with 17 teams successfully submitting their solutions during the final evaluation phase. Following the last two editions, this challenge had two evaluation tracks: one focusing on reconstruction fidelity and the other on visual perception through a user study. Both tracks were e&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.15524v1-abstract-full').style.display = 'inline'; document.getElementById('2506.15524v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.15524v1-abstract-full" style="display: none;">
        This work examines the findings of the NTIRE 2025 Shadow Removal Challenge. A total of 306 participants have registered, with 17 teams successfully submitting their solutions during the final evaluation phase. Following the last two editions, this challenge had two evaluation tracks: one focusing on reconstruction fidelity and the other on visual perception through a user study. Both tracks were evaluated with images from the WSRD+ dataset, simulating interactions between self- and cast-shadows with a large number of diverse objects, textures, and materials.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.15524v1-abstract-full').style.display = 'none'; document.getElementById('2506.15524v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.14127">arXiv:2506.14127</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.14127">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Superconductivity">cond-mat.supr-con</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Materials Science">cond-mat.mtrl-sci</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Resolving Phonons in Superconductor Bi2Sr2CaCu2O8+δ at Sub-Unit-Cell Resolution
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+X">Xiaowen Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Jiade Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+X">Xiaoyue Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+R">Ruochen Shi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+B">Bo Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xiaomei Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+J">Jinlong Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wen%2C+J">Jinsheng Wen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gu%2C+G">Genda Gu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+S">Shichong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+W">Wentao Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+P">Peng Gao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.14127v1-abstract-short" style="display: inline;">
        The role of phonons in cuprates remains controversial, with their complex lattice structure complicating the investigation. Here, we identify phonon modes originating from charge reservoir and superconducting layers of Bi2Sr2CaCu2O8+δ using sub-unit-cell resolved electron energy loss spectroscopy in a scanning transmission electron microscope. We reveal several phonon modes exhibiting layer-specif&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.14127v1-abstract-full').style.display = 'inline'; document.getElementById('2506.14127v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.14127v1-abstract-full" style="display: none;">
        The role of phonons in cuprates remains controversial, with their complex lattice structure complicating the investigation. Here, we identify phonon modes originating from charge reservoir and superconducting layers of Bi2Sr2CaCu2O8+δ using sub-unit-cell resolved electron energy loss spectroscopy in a scanning transmission electron microscope. We reveal several phonon modes exhibiting layer-specific localization: ~78 meV in-plane modes localized in the CuO2 planes, ~42 meV hybrid (in-plane and out-of-plane) modes in the CuO2 planes, and ~38 meV hybrid modes in the BiO layers. We also observe a periodic modulation of phonon frequencies induced by the superstructure, spatially correlating with the superconducting gap variation. Our findings offer new insights into the electron-phonon coupling in cuprates, fostering deeper exploration of the microscopic linkage between lattice dynamics and superconductivity.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.14127v1-abstract-full').style.display = 'none'; document.getElementById('2506.14127v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?query=Bo+Han&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?query=Bo+Han&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/?query=Bo+Han&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
              class="pagination-link "
              aria-label="Page 2"
              aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/?query=Bo+Han&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=100"
              class="pagination-link "
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/?query=Bo+Han&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=150"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/?query=Bo+Han&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=200"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  

  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>