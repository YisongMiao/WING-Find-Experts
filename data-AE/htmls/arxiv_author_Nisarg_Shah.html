<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;50 of 56 results for author: <span class="mathjax">Nisarg Shah</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/"  aria-role="search">
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Nisarg Shah">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Nisarg+Shah&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Nisarg Shah">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?query=Nisarg+Shah&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?query=Nisarg+Shah&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?query=Nisarg+Shah&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.22059">arXiv:2507.22059</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.22059">pdf</a>, <a href="https://arxiv.org/ps/2507.22059">ps</a>, <a href="https://arxiv.org/format/2507.22059">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        StepAL: Step-aware Active Learning for Cataract Surgical Videos
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N+A">Nisarg A. Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Safaei%2C+B">Bardia Safaei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sikder%2C+S">Shameema Sikder</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vedula%2C+S+S">S. Swaroop Vedula</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Patel%2C+V+M">Vishal M. Patel</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.22059v1-abstract-short" style="display: inline;">
        Active learning (AL) can reduce annotation costs in surgical video analysis while maintaining model performance. However, traditional AL methods, developed for images or short video clips, are suboptimal for surgical step recognition due to inter-step dependencies within long, untrimmed surgical videos. These methods typically select individual frames or clips for labeling, which is ineffective fo&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.22059v1-abstract-full').style.display = 'inline'; document.getElementById('2507.22059v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.22059v1-abstract-full" style="display: none;">
        Active learning (AL) can reduce annotation costs in surgical video analysis while maintaining model performance. However, traditional AL methods, developed for images or short video clips, are suboptimal for surgical step recognition due to inter-step dependencies within long, untrimmed surgical videos. These methods typically select individual frames or clips for labeling, which is ineffective for surgical videos where annotators require the context of the entire video for annotation. To address this, we propose StepAL, an active learning framework designed for full video selection in surgical step recognition. StepAL integrates a step-aware feature representation, which leverages pseudo-labels to capture the distribution of predicted steps within each video, with an entropy-weighted clustering strategy. This combination prioritizes videos that are both uncertain and exhibit diverse step compositions for annotation. Experiments on two cataract surgery datasets (Cataract-1k and Cataract-101) demonstrate that StepAL consistently outperforms existing active learning approaches, achieving higher accuracy in step recognition with fewer labeled videos. StepAL offers an effective approach for efficient surgical video analysis, reducing the annotation burden in developing computer-assisted surgical systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.22059v1-abstract-full').style.display = 'none'; document.getElementById('2507.22059v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 July, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to MICCAI 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.06261">arXiv:2507.06261</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.06261">pdf</a>, <a href="https://arxiv.org/ps/2507.06261">ps</a>, <a href="https://arxiv.org/format/2507.06261">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Comanici%2C+G">Gheorghe Comanici</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bieber%2C+E">Eric Bieber</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schaekermann%2C+M">Mike Schaekermann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pasupat%2C+I">Ice Pasupat</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sachdeva%2C+N">Noveen Sachdeva</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dhillon%2C+I">Inderjit Dhillon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Blistein%2C+M">Marcel Blistein</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ram%2C+O">Ori Ram</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+D">Dan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rosen%2C+E">Evan Rosen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Marris%2C+L">Luke Marris</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Petulla%2C+S">Sam Petulla</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gaffney%2C+C">Colin Gaffney</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aharoni%2C+A">Asaf Aharoni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lintz%2C+N">Nathan Lintz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pais%2C+T+C">Tiago Cardal Pais</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jacobsson%2C+H">Henrik Jacobsson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Szpektor%2C+I">Idan Szpektor</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+N">Nan-Jiang Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Haridasan%2C+K">Krishna Haridasan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Omran%2C+A">Ahmed Omran</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Saunshi%2C+N">Nikunj Saunshi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bahri%2C+D">Dara Bahri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mishra%2C+G">Gaurav Mishra</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chu%2C+E">Eric Chu</a>
      , et al. (3284 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.06261v4-abstract-short" style="display: inline;">
        In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. In addition to its incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that excels at multimodal unde&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.06261v4-abstract-full').style.display = 'inline'; document.getElementById('2507.06261v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.06261v4-abstract-full" style="display: none;">
        In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. In addition to its incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that excels at multimodal understanding and it is now able to process up to 3 hours of video content. Its unique combination of long context, multimodal and reasoning capabilities can be combined to unlock new agentic workflows. Gemini 2.5 Flash provides excellent reasoning abilities at a fraction of the compute and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high performance at low latency and cost. Taken together, the Gemini 2.X model generation spans the full Pareto frontier of model capability vs cost, allowing users to explore the boundaries of what is possible with complex agentic problem solving.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.06261v4-abstract-full').style.display = 'none'; document.getElementById('2507.06261v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 July, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 July, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">72 pages, 17 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2502.14143">arXiv:2502.14143</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2502.14143">pdf</a>, <a href="https://arxiv.org/format/2502.14143">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Multiagent Systems">cs.MA</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Emerging Technologies">cs.ET</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multi-Agent Risks from Advanced AI
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hammond%2C+L">Lewis Hammond</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chan%2C+A">Alan Chan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Clifton%2C+J">Jesse Clifton</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hoelscher-Obermaier%2C+J">Jason Hoelscher-Obermaier</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Khan%2C+A">Akbir Khan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McLean%2C+E">Euan McLean</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Smith%2C+C">Chandler Smith</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Barfuss%2C+W">Wolfram Barfuss</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Foerster%2C+J">Jakob Foerster</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gaven%C4%8Diak%2C+T">Tomáš Gavenčiak</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+T+A">The Anh Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hughes%2C+E">Edward Hughes</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kova%C5%99%C3%ADk%2C+V">Vojtěch Kovařík</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kulveit%2C+J">Jan Kulveit</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Leibo%2C+J+Z">Joel Z. Leibo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Oesterheld%2C+C">Caspar Oesterheld</a>, 
      
      <a href="/search/?searchtype=author&amp;query=de+Witt%2C+C+S">Christian Schroeder de Witt</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Nisarg Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wellman%2C+M">Michael Wellman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bova%2C+P">Paolo Bova</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cimpeanu%2C+T">Theodor Cimpeanu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ezell%2C+C">Carson Ezell</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feuillade-Montixi%2C+Q">Quentin Feuillade-Montixi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Franklin%2C+M">Matija Franklin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kran%2C+E">Esben Kran</a>
      , et al. (19 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2502.14143v1-abstract-short" style="display: inline;">
        The rapid development of advanced AI agents and the imminent deployment of many instances of these agents will give rise to multi-agent systems of unprecedented complexity. These systems pose novel and under-explored risks. In this report, we provide a structured taxonomy of these risks by identifying three key failure modes (miscoordination, conflict, and collusion) based on agents&#39; incentives, a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.14143v1-abstract-full').style.display = 'inline'; document.getElementById('2502.14143v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2502.14143v1-abstract-full" style="display: none;">
        The rapid development of advanced AI agents and the imminent deployment of many instances of these agents will give rise to multi-agent systems of unprecedented complexity. These systems pose novel and under-explored risks. In this report, we provide a structured taxonomy of these risks by identifying three key failure modes (miscoordination, conflict, and collusion) based on agents&#39; incentives, as well as seven key risk factors (information asymmetries, network effects, selection pressures, destabilising dynamics, commitment problems, emergent agency, and multi-agent security) that can underpin them. We highlight several important instances of each risk, as well as promising directions to help mitigate them. By anchoring our analysis in a range of real-world examples and experimental evidence, we illustrate the distinct challenges posed by multi-agent systems and their implications for the safety, governance, and ethics of advanced AI.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.14143v1-abstract-full').style.display = 'none'; document.getElementById('2502.14143v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 February, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Cooperative AI Foundation, Technical Report #1</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2502.08822">arXiv:2502.08822</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2502.08822">pdf</a>, <a href="https://arxiv.org/format/2502.08822">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        $\mathsf{CSMAE~}$:~Cataract Surgical Masked Autoencoder (MAE) based Pre-training
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N+A">Nisarg A. Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bandara%2C+W+G+C">Wele Gedara Chaminda Bandara</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Skider%2C+S">Shameema Skider</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vedula%2C+S+S">S. Swaroop Vedula</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Patel%2C+V+M">Vishal M. Patel</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2502.08822v1-abstract-short" style="display: inline;">
        Automated analysis of surgical videos is crucial for improving surgical training, workflow optimization, and postoperative assessment. We introduce a CSMAE, Masked Autoencoder (MAE)-based pretraining approach, specifically developed for Cataract Surgery video analysis, where instead of randomly selecting tokens for masking, they are selected based on the spatiotemporal importance of the token. We&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.08822v1-abstract-full').style.display = 'inline'; document.getElementById('2502.08822v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2502.08822v1-abstract-full" style="display: none;">
        Automated analysis of surgical videos is crucial for improving surgical training, workflow optimization, and postoperative assessment. We introduce a CSMAE, Masked Autoencoder (MAE)-based pretraining approach, specifically developed for Cataract Surgery video analysis, where instead of randomly selecting tokens for masking, they are selected based on the spatiotemporal importance of the token. We created a large dataset of cataract surgery videos to improve the model&#39;s learning efficiency and expand its robustness in low-data regimes. Our pre-trained model can be easily adapted for specific downstream tasks via fine-tuning, serving as a robust backbone for further analysis. Through rigorous testing on a downstream step-recognition task on two Cataract Surgery video datasets, D99 and Cataract-101, our approach surpasses current state-of-the-art self-supervised pretraining and adapter-based transfer learning methods by a significant margin. This advancement not only demonstrates the potential of our MAE-based pretraining in the field of surgical video analysis but also sets a new benchmark for future research.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.08822v1-abstract-full').style.display = 'none'; document.getElementById('2502.08822v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 February, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, Accepted to IEEE International Symposium on Biomedical Imaging (ISBI 2025)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.00133">arXiv:2411.00133</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.00133">pdf</a>, <a href="https://arxiv.org/ps/2411.00133">ps</a>, <a href="https://arxiv.org/format/2411.00133">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Science and Game Theory">cs.GT</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Constrained Fair and Efficient Allocations
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cookson%2C+B">Benjamin Cookson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ebadian%2C+S">Soroush Ebadian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Nisarg Shah</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.00133v1-abstract-short" style="display: inline;">
        Fairness and efficiency have become the pillars of modern fair division research, but prior work on achieving both simultaneously is largely limited to the unconstrained setting. We study fair and efficient allocations of indivisible goods under additive valuations and various types of allocation feasibility constraints, and demonstrate the unreasonable effectiveness of the maximum Nash welfare (M&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.00133v1-abstract-full').style.display = 'inline'; document.getElementById('2411.00133v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.00133v1-abstract-full" style="display: none;">
        Fairness and efficiency have become the pillars of modern fair division research, but prior work on achieving both simultaneously is largely limited to the unconstrained setting. We study fair and efficient allocations of indivisible goods under additive valuations and various types of allocation feasibility constraints, and demonstrate the unreasonable effectiveness of the maximum Nash welfare (MNW) solution in this previously uncharted territory.
  Our main result is that MNW allocations are 1/2-envy-free up to one good (EF1) and Pareto optimal under the broad family of (arbitrary) matroid constraints. We extend these guarantees to complete MNW allocations for base-orderable matroid constraints, and to a family of non-matroidal constraints (which includes balancedness) using a novel &#34;alternate worlds&#34; technique. We establish tightness of our results by providing counterexamples for the satisfiability of certain stronger desiderata, but show an improved result for the special case of goods with copies (Gafni et al. 2023). Finally, we also establish novel best-of-both-worlds guarantees for goods with copies and balancedness.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.00133v1-abstract-full').style.display = 'none'; document.getElementById('2411.00133v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.23416">arXiv:2410.23416</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.23416">pdf</a>, <a href="https://arxiv.org/ps/2410.23416">ps</a>, <a href="https://arxiv.org/format/2410.23416">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Science and Game Theory">cs.GT</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Temporal Fair Division
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cookson%2C+B">Benjamin Cookson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ebadian%2C+S">Soroush Ebadian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Nisarg Shah</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.23416v1-abstract-short" style="display: inline;">
        We study temporal fair division, whereby a set of agents are allocated a (possibly different) set of goods on each day for a period of days. We study this setting, as well as a number of its special cases formed by the restrictions to two agents, same goods on each day, identical preferences, or combinations thereof, and chart out the landscape of achieving two types of fairness guarantees simulta&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.23416v1-abstract-full').style.display = 'inline'; document.getElementById('2410.23416v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.23416v1-abstract-full" style="display: none;">
        We study temporal fair division, whereby a set of agents are allocated a (possibly different) set of goods on each day for a period of days. We study this setting, as well as a number of its special cases formed by the restrictions to two agents, same goods on each day, identical preferences, or combinations thereof, and chart out the landscape of achieving two types of fairness guarantees simultaneously: fairness on each day (per day) and fairness over time (up to each day, or the weaker version, overall).
  In the most general setting, we prove that there always exists an allocation that is stochastically-dominant envy-free up to one good (SD-EF1) per day and proportional up to one good (PROP1) overall, and when all the agents have identical preferences, we show that SD-EF1 per day and SD-EF1 overall can be guaranteed. For the case of two agents, we prove that SD-EF1 per day and EF1 up to each day can be guaranteed using an envy balancing technique. We provide counterexamples for other combinations that establish our results as among the best guarantees possible, but also leaving open some tantalizing questions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.23416v1-abstract-full').style.display = 'none'; document.getElementById('2410.23416v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.23273">arXiv:2410.23273</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.23273">pdf</a>, <a href="https://arxiv.org/format/2410.23273">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Science and Game Theory">cs.GT</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Proportional Fairness in Non-Centroid Clustering
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Caragiannis%2C+I">Ioannis Caragiannis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Micha%2C+E">Evi Micha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Nisarg Shah</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.23273v1-abstract-short" style="display: inline;">
        We revisit the recently developed framework of proportionally fair clustering, where the goal is to provide group fairness guarantees that become stronger for groups of data points (agents) that are large and cohesive. Prior work applies this framework to centroid clustering, where the loss of an agent is its distance to the centroid assigned to its cluster. We expand the framework to non-centroid&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.23273v1-abstract-full').style.display = 'inline'; document.getElementById('2410.23273v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.23273v1-abstract-full" style="display: none;">
        We revisit the recently developed framework of proportionally fair clustering, where the goal is to provide group fairness guarantees that become stronger for groups of data points (agents) that are large and cohesive. Prior work applies this framework to centroid clustering, where the loss of an agent is its distance to the centroid assigned to its cluster. We expand the framework to non-centroid clustering, where the loss of an agent is a function of the other agents in its cluster, by adapting two proportional fairness criteria -- the core and its relaxation, fully justified representation (FJR) -- to this setting.
  We show that the core can be approximated only under structured loss functions, and even then, the best approximation we are able to establish, using an adaptation of the GreedyCapture algorithm developed for centroid clustering [Chen et al., 2019; Micha and Shah, 2020], is unappealing for a natural loss function. In contrast, we design a new (inefficient) algorithm, GreedyCohesiveClustering, which achieves the relaxation FJR exactly under arbitrary loss functions, and show that the efficient GreedyCapture algorithm achieves a constant approximation of FJR. We also design an efficient auditing algorithm, which estimates the FJR approximation of any given clustering solution up to a constant factor. Our experiments on real data suggest that traditional clustering algorithms are highly unfair, whereas GreedyCapture is considerably fairer and incurs only a modest loss in common clustering objectives.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.23273v1-abstract-full').style.display = 'none'; document.getElementById('2410.23273v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">A preliminary version appeared at NeurIPS 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.23137">arXiv:2410.23137</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.23137">pdf</a>, <a href="https://arxiv.org/ps/2410.23137">ps</a>, <a href="https://arxiv.org/format/2410.23137">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Science and Game Theory">cs.GT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Fair Division with Market Values
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Barman%2C+S">Siddharth Barman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ebadian%2C+S">Soroush Ebadian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Latifian%2C+M">Mohamad Latifian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Nisarg Shah</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.23137v1-abstract-short" style="display: inline;">
        We introduce a model of fair division with market values, where indivisible goods must be partitioned among agents with (additive) subjective valuations, and each good additionally has a market value. The market valuation can be viewed as a separate additive valuation that holds identically across all the agents. We seek allocations that are simultaneously fair with respect to the subjective valua&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.23137v1-abstract-full').style.display = 'inline'; document.getElementById('2410.23137v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.23137v1-abstract-full" style="display: none;">
        We introduce a model of fair division with market values, where indivisible goods must be partitioned among agents with (additive) subjective valuations, and each good additionally has a market value. The market valuation can be viewed as a separate additive valuation that holds identically across all the agents. We seek allocations that are simultaneously fair with respect to the subjective valuations and with respect to the market valuation.
  We show that an allocation that satisfies stochastically-dominant envy-freeness up to one good (SD-EF1) with respect to both the subjective valuations and the market valuation does not always exist, but the weaker guarantee of EF1 with respect to the subjective valuations along with SD-EF1 with respect to the market valuation can be guaranteed. We also study a number of other guarantees such as Pareto optimality, EFX, and MMS. In addition, we explore non-additive valuations and extend our model to cake-cutting. Along the way, we identify several tantalizing open questions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.23137v1-abstract-full').style.display = 'none'; document.getElementById('2410.23137v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.03474">arXiv:2410.03474</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.03474">pdf</a>, <a href="https://arxiv.org/ps/2410.03474">ps</a>, <a href="https://arxiv.org/format/2410.03474">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Science and Game Theory">cs.GT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Physics and Society">physics.soc-ph</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Group Fairness in Peer Review
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Aziz%2C+H">Haris Aziz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Micha%2C+E">Evi Micha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Nisarg Shah</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.03474v1-abstract-short" style="display: inline;">
        Large conferences such as NeurIPS and AAAI serve as crossroads of various AI fields, since they attract submissions from a vast number of communities. However, in some cases, this has resulted in a poor reviewing experience for some communities, whose submissions get assigned to less qualified reviewers outside of their communities. An often-advocated solution is to break up any such large confere&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.03474v1-abstract-full').style.display = 'inline'; document.getElementById('2410.03474v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.03474v1-abstract-full" style="display: none;">
        Large conferences such as NeurIPS and AAAI serve as crossroads of various AI fields, since they attract submissions from a vast number of communities. However, in some cases, this has resulted in a poor reviewing experience for some communities, whose submissions get assigned to less qualified reviewers outside of their communities. An often-advocated solution is to break up any such large conference into smaller conferences, but this can lead to isolation of communities and harm interdisciplinary research. We tackle this challenge by introducing a notion of group fairness, called the core, which requires that every possible community (subset of researchers) to be treated in a way that prevents them from unilaterally benefiting by withdrawing from a large conference.
  We study a simple peer review model, prove that it always admits a reviewing assignment in the core, and design an efficient algorithm to find one such assignment. We use real data from CVPR and ICLR conferences to compare our algorithm to existing reviewing assignment algorithms on a number of metrics.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.03474v1-abstract-full').style.display = 'none'; document.getElementById('2410.03474v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">A preliminary version appeared at NeurIPS 2023</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.02977">arXiv:2410.02977</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.02977">pdf</a>, <a href="https://arxiv.org/format/2410.02977">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Science and Game Theory">cs.GT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Harm Ratio: A Novel and Versatile Fairness Criterion
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ebadian%2C+S">Soroush Ebadian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Freeman%2C+R">Rupert Freeman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Nisarg Shah</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.02977v1-abstract-short" style="display: inline;">
        Envy-freeness has become the cornerstone of fair division research. In settings where each individual is allocated a disjoint share of collective resources, it is a compelling fairness axiom which demands that no individual strictly prefer the allocation of another individual to their own. Unfortunately, in many real-life collective decision-making problems, the goal is to choose a (common) public&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.02977v1-abstract-full').style.display = 'inline'; document.getElementById('2410.02977v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.02977v1-abstract-full" style="display: none;">
        Envy-freeness has become the cornerstone of fair division research. In settings where each individual is allocated a disjoint share of collective resources, it is a compelling fairness axiom which demands that no individual strictly prefer the allocation of another individual to their own. Unfortunately, in many real-life collective decision-making problems, the goal is to choose a (common) public outcome that is equally applicable to all individuals, and the notion of envy becomes vacuous. Consequently, this literature has avoided studying fairness criteria that focus on individuals feeling a sense of jealousy or resentment towards other individuals (rather than towards the system), missing out on a key aspect of fairness.
  In this work, we propose a novel fairness criterion, individual harm ratio, which is inspired by envy-freeness but applies to a broad range of collective decision-making settings. Theoretically, we identify minimal conditions under which this criterion and its groupwise extensions can be guaranteed, and study the computational complexity of related problems. Empirically, we conduct experiments with real data to show that our fairness criterion is powerful enough to differentiate between prominent decision-making algorithms for a range of tasks from voting and fair division to participatory budgeting and peer review.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.02977v1-abstract-full').style.display = 'none'; document.getElementById('2410.02977v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To appear at EAAMO 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.15405">arXiv:2406.15405</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.15405">pdf</a>, <a href="https://arxiv.org/format/2406.15405">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Science and Game Theory">cs.GT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Applications">stat.AP</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        What is Best for Students, Numerical Scores or Letter Grades?
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Micha%2C+E">Evi Micha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sekar%2C+S">Shreyas Sekar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Nisarg Shah</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.15405v1-abstract-short" style="display: inline;">
        We study letter grading schemes, which are routinely employed for evaluating student performance. Typically, a numerical score obtained via one or more evaluations is converted into a letter grade (e.g., A+, B-, etc.) by associating a disjoint interval of numerical scores to each letter grade.
  We propose the first model for studying the (de)motivational effects of such grading on the students an&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.15405v1-abstract-full').style.display = 'inline'; document.getElementById('2406.15405v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.15405v1-abstract-full" style="display: none;">
        We study letter grading schemes, which are routinely employed for evaluating student performance. Typically, a numerical score obtained via one or more evaluations is converted into a letter grade (e.g., A+, B-, etc.) by associating a disjoint interval of numerical scores to each letter grade.
  We propose the first model for studying the (de)motivational effects of such grading on the students and, consequently, on their performance in future evaluations. We use the model to compare uniform letter grading schemes, in which the range of scores is divided into equal-length parts that are mapped to the letter grades, to numerical scoring, in which the score is not converted to any letter grade (equivalently, every score is its own letter grade).
  Theoretically, we identify realistic conditions under which numerical scoring is better than any uniform letter grading scheme. Our experiments confirm that this holds under even weaker conditions, but also find cases where the converse occurs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.15405v1-abstract-full').style.display = 'none'; document.getElementById('2406.15405v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to IJCAI 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.18822">arXiv:2403.18822</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.18822">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Trading and Market Microstructure">q-fin.TR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Enhancing Financial Data Visualization for Investment Decision-Making
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Patel%2C+N">Nisarg Patel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+H">Harmit Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mewada%2C+K">Kishan Mewada</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.18822v1-abstract-short" style="display: inline;">
        Navigating the intricate landscape of financial markets requires adept forecasting of stock price movements. This paper delves into the potential of Long Short-Term Memory (LSTM) networks for predicting stock dynamics, with a focus on discerning nuanced rise and fall patterns. Leveraging a dataset from the New York Stock Exchange (NYSE), the study incorporates multiple features to enhance LSTM&#39;s c&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.18822v1-abstract-full').style.display = 'inline'; document.getElementById('2403.18822v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.18822v1-abstract-full" style="display: none;">
        Navigating the intricate landscape of financial markets requires adept forecasting of stock price movements. This paper delves into the potential of Long Short-Term Memory (LSTM) networks for predicting stock dynamics, with a focus on discerning nuanced rise and fall patterns. Leveraging a dataset from the New York Stock Exchange (NYSE), the study incorporates multiple features to enhance LSTM&#39;s capacity in capturing complex patterns. Visualization of key attributes, such as opening, closing, low, and high prices, aids in unraveling subtle distinctions crucial for comprehensive market understanding. The meticulously crafted LSTM input structure, inspired by established guidelines, incorporates both price and volume attributes over a 25-day time step, enabling the model to capture temporal intricacies. A comprehensive methodology, including hyperparameter tuning with Grid Search, Early Stopping, and Callback mechanisms, leads to a remarkable 53% improvement in predictive accuracy. The study concludes with insights into model robustness, contributions to financial forecasting literature, and a roadmap for real-time stock market prediction. The amalgamation of LSTM networks, strategic hyperparameter tuning, and informed feature selection presents a potent framework for advancing the accuracy of stock price predictions, contributing substantively to financial time series forecasting discourse.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.18822v1-abstract-full').style.display = 'none'; document.getElementById('2403.18822v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 December, 2023; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5 pages, 10 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2309.04635">arXiv:2309.04635</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2309.04635">pdf</a>, <a href="https://arxiv.org/format/2309.04635">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Can NLP Models &#39;Identify&#39;, &#39;Distinguish&#39;, and &#39;Justify&#39; Questions that Don&#39;t have a Definitive Answer?
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Agarwal%2C+A">Ayushi Agarwal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Patel%2C+N">Nisarg Patel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Varshney%2C+N">Neeraj Varshney</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Parmar%2C+M">Mihir Parmar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mallina%2C+P">Pavan Mallina</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+A+B">Aryan Bhavin Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sangaraju%2C+S+R">Srihari Raju Sangaraju</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Patel%2C+T">Tirth Patel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Thakkar%2C+N">Nihar Thakkar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Baral%2C+C">Chitta Baral</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2309.04635v1-abstract-short" style="display: inline;">
        Though state-of-the-art (SOTA) NLP systems have achieved remarkable performance on a variety of language understanding tasks, they primarily focus on questions that have a correct and a definitive answer. However, in real-world applications, users often ask questions that don&#39;t have a definitive answer. Incorrectly answering such questions certainly hampers a system&#39;s reliability and trustworthine&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2309.04635v1-abstract-full').style.display = 'inline'; document.getElementById('2309.04635v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2309.04635v1-abstract-full" style="display: none;">
        Though state-of-the-art (SOTA) NLP systems have achieved remarkable performance on a variety of language understanding tasks, they primarily focus on questions that have a correct and a definitive answer. However, in real-world applications, users often ask questions that don&#39;t have a definitive answer. Incorrectly answering such questions certainly hampers a system&#39;s reliability and trustworthiness. Can SOTA models accurately identify such questions and provide a reasonable response?
  To investigate the above question, we introduce QnotA, a dataset consisting of five different categories of questions that don&#39;t have definitive answers. Furthermore, for each QnotA instance, we also provide a corresponding QA instance i.e. an alternate question that &#39;&#39;can be&#39;&#39; answered. With this data, we formulate three evaluation tasks that test a system&#39;s ability to &#39;identify&#39;, &#39;distinguish&#39;, and &#39;justify&#39; QnotA questions. Through comprehensive experiments, we show that even SOTA models including GPT-3 and Flan T5 do not fare well on these tasks and lack considerably behind the human performance baseline. We conduct a thorough analysis which further leads to several interesting findings. Overall, we believe our work and findings will encourage and facilitate further research in this important area and help develop more robust models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2309.04635v1-abstract-full').style.display = 'none'; document.getElementById('2309.04635v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 September, 2023; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2023.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">TrustNLP Workshop at ACL 2023</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2307.11081">arXiv:2307.11081</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2307.11081">pdf</a>, <a href="https://arxiv.org/format/2307.11081">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        GLSFormer: Gated - Long, Short Sequence Transformer for Step Recognition in Surgical Videos
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N+A">Nisarg A. Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sikder%2C+S">Shameema Sikder</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vedula%2C+S+S">S. Swaroop Vedula</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Patel%2C+V+M">Vishal M. Patel</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2307.11081v1-abstract-short" style="display: inline;">
        Automated surgical step recognition is an important task that can significantly improve patient safety and decision-making during surgeries. Existing state-of-the-art methods for surgical step recognition either rely on separate, multi-stage modeling of spatial and temporal information or operate on short-range temporal resolution when learned jointly. However, the benefits of joint modeling of sp&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2307.11081v1-abstract-full').style.display = 'inline'; document.getElementById('2307.11081v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2307.11081v1-abstract-full" style="display: none;">
        Automated surgical step recognition is an important task that can significantly improve patient safety and decision-making during surgeries. Existing state-of-the-art methods for surgical step recognition either rely on separate, multi-stage modeling of spatial and temporal information or operate on short-range temporal resolution when learned jointly. However, the benefits of joint modeling of spatio-temporal features and long-range information are not taken in account. In this paper, we propose a vision transformer-based approach to jointly learn spatio-temporal features directly from sequence of frame-level patches. Our method incorporates a gated-temporal attention mechanism that intelligently combines short-term and long-term spatio-temporal feature representations. We extensively evaluate our approach on two cataract surgery video datasets, namely Cataract-101 and D99, and demonstrate superior performance compared to various state-of-the-art methods. These results validate the suitability of our proposed approach for automated surgical step recognition. Our code is released at: https://github.com/nisargshah1999/GLSFormer
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2307.11081v1-abstract-full').style.display = 'none'; document.getElementById('2307.11081v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 July, 2023; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2023.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to MICCAI 2023 (Early Accept)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2305.19453">arXiv:2305.19453</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2305.19453">pdf</a>, <a href="https://arxiv.org/ps/2305.19453">ps</a>, <a href="https://arxiv.org/format/2305.19453">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Science and Game Theory">cs.GT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Best of Both Distortion Worlds
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gkatzelis%2C+V">Vasilis Gkatzelis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Latifian%2C+M">Mohamad Latifian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Nisarg Shah</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2305.19453v1-abstract-short" style="display: inline;">
        We study the problem of designing voting rules that take as input the ordinal preferences of $n$ agents over a set of $m$ alternatives and output a single alternative, aiming to optimize the overall happiness of the agents. The input to the voting rule is each agent&#39;s ranking of the alternatives from most to least preferred, yet the agents have more refined (cardinal) preferences that capture the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2305.19453v1-abstract-full').style.display = 'inline'; document.getElementById('2305.19453v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2305.19453v1-abstract-full" style="display: none;">
        We study the problem of designing voting rules that take as input the ordinal preferences of $n$ agents over a set of $m$ alternatives and output a single alternative, aiming to optimize the overall happiness of the agents. The input to the voting rule is each agent&#39;s ranking of the alternatives from most to least preferred, yet the agents have more refined (cardinal) preferences that capture the intensity with which they prefer one alternative over another. To quantify the extent to which voting rules can optimize over the cardinal preferences given access only to the ordinal ones, prior work has used the distortion measure, i.e., the worst-case approximation ratio between a voting rule&#39;s performance and the best performance achievable given the cardinal preferences.
  The work on the distortion of voting rules has been largely divided into two worlds: utilitarian distortion and metric distortion. In the former, the cardinal preferences of the agents correspond to general utilities and the goal is to maximize a normalized social welfare. In the latter, the agents&#39; cardinal preferences correspond to costs given by distances in an underlying metric space and the goal is to minimize the (unnormalized) social cost. Several deterministic and randomized voting rules have been proposed and evaluated for each of these worlds separately, gradually improving the achievable distortion bounds, but none of the known voting rules perform well in both worlds simultaneously.
  In this work, we prove that one can achieve the best of both worlds by designing new voting rules, that simultaneously achieve near-optimal distortion guarantees in both distortion worlds. We also prove that this positive result does not generalize to the case where the voting rule is provided with the rankings of only the top-$t$ alternatives of each agent, for $t&lt;m$.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2305.19453v1-abstract-full').style.display = 'none'; document.getElementById('2305.19453v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 May, 2023; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2023.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">EC&#39;23</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2209.15305">arXiv:2209.15305</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2209.15305">pdf</a>, <a href="https://arxiv.org/ps/2209.15305">ps</a>, <a href="https://arxiv.org/format/2209.15305">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Science and Game Theory">cs.GT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Data Structures and Algorithms">cs.DS</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Proportionally Fair Online Allocation of Public Goods with Predictions
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Banerjee%2C+S">Siddhartha Banerjee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gkatzelis%2C+V">Vasilis Gkatzelis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hossain%2C+S">Safwan Hossain</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jin%2C+B">Billy Jin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Micha%2C+E">Evi Micha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Nisarg Shah</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2209.15305v1-abstract-short" style="display: inline;">
        We design online algorithms for the fair allocation of public goods to a set of $N$ agents over a sequence of $T$ rounds and focus on improving their performance using predictions. In the basic model, a public good arrives in each round, the algorithm learns every agent&#39;s value for the good, and must irrevocably decide the amount of investment in the good without exceeding a total budget of $B$ ac&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2209.15305v1-abstract-full').style.display = 'inline'; document.getElementById('2209.15305v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2209.15305v1-abstract-full" style="display: none;">
        We design online algorithms for the fair allocation of public goods to a set of $N$ agents over a sequence of $T$ rounds and focus on improving their performance using predictions. In the basic model, a public good arrives in each round, the algorithm learns every agent&#39;s value for the good, and must irrevocably decide the amount of investment in the good without exceeding a total budget of $B$ across all rounds. The algorithm can utilize (potentially inaccurate) predictions of each agent&#39;s total value for all the goods to arrive. We measure the performance of the algorithm using a proportional fairness objective, which informally demands that every group of agents be rewarded in proportion to its size and the cohesiveness of its preferences.
  In the special case of binary agent preferences and a unit budget, we show that $O(\log N)$ proportional fairness can be achieved without using any predictions, and that this is optimal even if perfectly accurate predictions were available. However, for general preferences and budget no algorithm can achieve better than $Θ(T/B)$ proportional fairness without predictions. We show that algorithms with (reasonably accurate) predictions can do much better, achieving $Θ(\log (T/B))$ proportional fairness. We also extend this result to a general model in which a batch of $L$ public goods arrive in each round and achieve $O(\log (\min(N,L) \cdot T/B))$ proportional fairness. Our exact bounds are parametrized as a function of the error in the predictions and the performance degrades gracefully with increasing errors.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2209.15305v1-abstract-full').style.display = 'none'; document.getElementById('2209.15305v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 September, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2207.00506">arXiv:2207.00506</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2207.00506">pdf</a>, <a href="https://arxiv.org/format/2207.00506">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computational Geometry">cs.CG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        How Far Can I Go ? : A Self-Supervised Approach for Deterministic Video Depth Forecasting
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Nag%2C+S">Sauradip Nag</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Nisarg Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qi%2C+A">Anran Qi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ramachandra%2C+R">Raghavendra Ramachandra</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2207.00506v2-abstract-short" style="display: inline;">
        In this paper we present a novel self-supervised method to anticipate the depth estimate for a future, unobserved real-world urban scene. This work is the first to explore self-supervised learning for estimation of monocular depth of future unobserved frames of a video. Existing works rely on a large number of annotated samples to generate the probabilistic prediction of depth for unseen frames. H&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2207.00506v2-abstract-full').style.display = 'inline'; document.getElementById('2207.00506v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2207.00506v2-abstract-full" style="display: none;">
        In this paper we present a novel self-supervised method to anticipate the depth estimate for a future, unobserved real-world urban scene. This work is the first to explore self-supervised learning for estimation of monocular depth of future unobserved frames of a video. Existing works rely on a large number of annotated samples to generate the probabilistic prediction of depth for unseen frames. However, this makes it unrealistic due to its requirement for large amount of annotated depth samples of video. In addition, the probabilistic nature of the case, where one past can have multiple future outcomes often leads to incorrect depth estimates. Unlike previous methods, we model the depth estimation of the unobserved frame as a view-synthesis problem, which treats the depth estimate of the unseen video frame as an auxiliary task while synthesizing back the views using learned pose. This approach is not only cost effective - we do not use any ground truth depth for training (hence practical) but also deterministic (a sequence of past frames map to an immediate future). To address this task we first develop a novel depth forecasting network DeFNet which estimates depth of unobserved future by forecasting latent features. Second, we develop a channel-attention based pose estimation network that estimates the pose of the unobserved frame. Using this learned pose, estimated depth map is reconstructed back into the image domain, thus forming a self-supervised solution. Our proposed approach shows significant improvements in Abs Rel metric compared to state-of-the-art alternatives on both short and mid-term forecasting setting, benchmarked on KITTI and Cityscapes. Code is available at https://github.com/sauradip/depthForecasting
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2207.00506v2-abstract-full').style.display = 'none'; document.getElementById('2207.00506v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 July, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 1 July, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted in ML4AD Workshop, NeurIPS 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2205.15760">arXiv:2205.15760</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2205.15760">pdf</a>, <a href="https://arxiv.org/format/2205.15760">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Science and Game Theory">cs.GT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Theoretical Economics">econ.TH</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1145/3640760">10.1145/3640760 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Optimized Distortion and Proportional Fairness in Voting
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ebadian%2C+S">Soroush Ebadian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kahng%2C+A">Anson Kahng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peters%2C+D">Dominik Peters</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Nisarg Shah</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2205.15760v3-abstract-short" style="display: inline;">
        A voting rule decides on a probability distribution over a set of m alternatives, based on rankings of those alternatives provided by agents. We assume that agents have cardinal utility functions over the alternatives, but voting rules have access to only the rankings induced by these utilities. We evaluate how well voting rules do on measures of social welfare and of proportional fairness, comput&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.15760v3-abstract-full').style.display = 'inline'; document.getElementById('2205.15760v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2205.15760v3-abstract-full" style="display: none;">
        A voting rule decides on a probability distribution over a set of m alternatives, based on rankings of those alternatives provided by agents. We assume that agents have cardinal utility functions over the alternatives, but voting rules have access to only the rankings induced by these utilities. We evaluate how well voting rules do on measures of social welfare and of proportional fairness, computed based on the hidden utility functions.
  In particular, we study the distortion of voting rules, which is a worst-case measure. It is an approximation ratio comparing the utilitarian social welfare of the optimum outcome to the social welfare produced by the outcome selected by the voting rule, in the worst case over possible input profiles and utility functions that are consistent with the input. The previous literature has studied distortion with unit-sum utility functions (which are normalized to sum to 1), and left a small asymptotic gap in the best possible distortion. Using tools from the theory of fair multi-winner elections, we propose the first voting rule which achieves the optimal distortion $Θ(\sqrt{m})$ for unit-sum utilities. Our voting rule also achieves optimum $Θ(\sqrt{m})$ distortion for a larger class of utilities, including unit-range and approval (0/1) utilities.
  We then take a worst-case approach to a quantitative measure of the fairness of a voting rule, called proportional fairness. Informally, it measures whether the influence of cohesive groups of agents on the voting outcome is proportional to the group size. We show that there is a voting rule which, without knowledge of the utilities, can achieve a $Θ(\log m)$-approximation to proportional fairness, and thus also to Nash welfare and to the core, making it interesting for applications in participatory budgeting. For all three approximations, we show that $Θ(\log m)$ is the best possible.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.15760v3-abstract-full').style.display = 'none'; document.getElementById('2205.15760v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 January, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 31 May, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted version at ACM TEAC, 36 pages including appendix</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.10363">arXiv:2203.10363</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.10363">pdf</a>, <a href="https://arxiv.org/format/2203.10363">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards Device Efficient Conditional Image Generation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N+A">Nisarg A. Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bharaj%2C+G">Gaurav Bharaj</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.10363v2-abstract-short" style="display: inline;">
        We present a novel algorithm to reduce tensor compute required by a conditional image generation autoencoder without sacrificing quality of photo-realistic image generation. Our method is device agnostic, and can optimize an autoencoder for a given CPU-only, GPU compute device(s) in about normal time it takes to train an autoencoder on a generic workstation. We achieve this via a two-stage novel s&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.10363v2-abstract-full').style.display = 'inline'; document.getElementById('2203.10363v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.10363v2-abstract-full" style="display: none;">
        We present a novel algorithm to reduce tensor compute required by a conditional image generation autoencoder without sacrificing quality of photo-realistic image generation. Our method is device agnostic, and can optimize an autoencoder for a given CPU-only, GPU compute device(s) in about normal time it takes to train an autoencoder on a generic workstation. We achieve this via a two-stage novel strategy where, first, we condense the channel weights, such that, as few as possible channels are used. Then, we prune the nearly zeroed out weight activations, and fine-tune the autoencoder. To maintain image quality, fine-tuning is done via student-teacher training, where we reuse the condensed autoencoder as the teacher. We show performance gains for various conditional image generation tasks: segmentation mask to face images, face images to cartoonization, and finally CycleGAN-based model over multiple compute devices. We perform various ablation studies to justify the claims and design choices, and achieve real-time versions of various autoencoders on CPU-only devices while maintaining image quality, thus enabling at-scale deployment of such autoencoders.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.10363v2-abstract-full').style.display = 'none'; document.getElementById('2203.10363v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 October, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 March, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">British Machine Vision Conference 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.03751">arXiv:2203.03751</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.03751">pdf</a>, <a href="https://arxiv.org/ps/2203.03751">ps</a>, <a href="https://arxiv.org/format/2203.03751">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Science and Game Theory">cs.GT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multiagent Systems">cs.MA</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Theoretical Economics">econ.TH</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Class Fairness in Online Matching
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hosseini%2C+H">Hadi Hosseini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+Z">Zhiyi Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Igarashi%2C+A">Ayumi Igarashi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Nisarg Shah</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.03751v1-abstract-short" style="display: inline;">
        In the classical version of online bipartite matching, there is a given set of offline vertices (aka agents) and another set of vertices (aka items) that arrive online. When each item arrives, its incident edges -- the agents who like the item -- are revealed and the algorithm must irrevocably match the item to such agents. We initiate the study of class fairness in this setting, where agents are&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.03751v1-abstract-full').style.display = 'inline'; document.getElementById('2203.03751v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.03751v1-abstract-full" style="display: none;">
        In the classical version of online bipartite matching, there is a given set of offline vertices (aka agents) and another set of vertices (aka items) that arrive online. When each item arrives, its incident edges -- the agents who like the item -- are revealed and the algorithm must irrevocably match the item to such agents. We initiate the study of class fairness in this setting, where agents are partitioned into a set of classes and the matching is required to be fair with respect to the classes. We adopt popular fairness notions from the fair division literature such as envy-freeness (up to one item), proportionality, and maximin share fairness to our setting. Our class versions of these notions demand that all classes, regardless of their sizes, receive a fair treatment. We study deterministic and randomized algorithms for matching indivisible items (leading to integral matchings) and for matching divisible items (leading to fractional matchings). We design and analyze three novel algorithms. For matching indivisible items, we propose an adaptive-priority-based algorithm, MATCH-AND-SHIFT, prove that it achieves 1/2-approximation of both class envy-freeness up to one item and class maximin share fairness, and show that each guarantee is tight. For matching divisible items, we design a water-filling-based algorithm, EQUAL-FILLING, that achieves (1-1/e)-approximation of class envy-freeness and class proportionality; we prove (1-1/e) to be tight for class proportionality and establish a 3/4 upper bound on class envy-freeness. Finally, we build upon EQUAL-FILLING to design a randomized algorithm for matching indivisible items, EQAUL-FILLING-OCS, which achieves 0.593-approximation of class proportionality. The algorithm and its analysis crucially leverage the recently introduced technique of online correlated selection (OCS) [Fahrbach et al., 2020].
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.03751v1-abstract-full').style.display = 'none'; document.getElementById('2203.03751v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">31 pages</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.11
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.00845">arXiv:2203.00845</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.00845">pdf</a>, <a href="https://arxiv.org/format/2203.00845">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Can No-reference features help in Full-reference image quality estimation?
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dutta%2C+S">Saikat Dutta</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Das%2C+S+D">Sourya Dipta Das</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N+A">Nisarg A. Shah</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.00845v1-abstract-short" style="display: inline;">
        Development of perceptual image quality assessment (IQA) metrics has been of significant interest to computer vision community. The aim of these metrics is to model quality of an image as perceived by humans. Recent works in Full-reference IQA research perform pixelwise comparison between deep features corresponding to query and reference images for quality prediction. However, pixelwise feature c&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.00845v1-abstract-full').style.display = 'inline'; document.getElementById('2203.00845v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.00845v1-abstract-full" style="display: none;">
        Development of perceptual image quality assessment (IQA) metrics has been of significant interest to computer vision community. The aim of these metrics is to model quality of an image as perceived by humans. Recent works in Full-reference IQA research perform pixelwise comparison between deep features corresponding to query and reference images for quality prediction. However, pixelwise feature comparison may not be meaningful if distortion present in query image is severe. In this context, we explore utilization of no-reference features in Full-reference IQA task. Our model consists of both full-reference and no-reference branches. Full-reference branches use both distorted and reference images, whereas No-reference branch only uses distorted image. Our experiments show that use of no-reference features boosts performance of image quality assessment. Our model achieves higher SRCC and KRCC scores than a number of state-of-the-art algorithms on KADID-10K and PIPAL datasets.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.00845v1-abstract-full').style.display = 'none'; document.getElementById('2203.00845v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Code to be updated on: https://github.com/saikatdutta/nr-in-friqa</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2202.07983">arXiv:2202.07983</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2202.07983">pdf</a>, <a href="https://arxiv.org/format/2202.07983">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1109/TMI.2022.3172773">10.1109/TMI.2022.3172773 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ADAM Challenge: Detecting Age-related Macular Degeneration from Fundus Images
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Fang%2C+H">Huihui Fang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+F">Fei Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fu%2C+H">Huazhu Fu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+X">Xu Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+X">Xingxing Cao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+F">Fengbin Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Son%2C+J">Jaemin Son</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+S">Sunho Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Quellec%2C+G">Gwenole Quellec</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Matta%2C+S">Sarah Matta</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shankaranarayana%2C+S+M">Sharath M Shankaranarayana</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yi-Ting Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+C">Chuen-heng Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N+A">Nisarg A. Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+C">Chia-Yen Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hsu%2C+C">Chih-Chung Hsu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+H">Hai Xie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lei%2C+B">Baiying Lei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Baid%2C+U">Ujjwal Baid</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Innani%2C+S">Shubham Innani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dang%2C+K">Kang Dang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+W">Wenxiu Shi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kamble%2C+R">Ravi Kamble</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Singhal%2C+N">Nitin Singhal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+C">Ching-Wei Wang</a>
      , et al. (6 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2202.07983v3-abstract-short" style="display: inline;">
        Age-related macular degeneration (AMD) is the leading cause of visual impairment among elderly in the world. Early detection of AMD is of great importance, as the vision loss caused by this disease is irreversible and permanent. Color fundus photography is the most cost-effective imaging modality to screen for retinal disorders. Cutting edge deep learning based algorithms have been recently develo&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.07983v3-abstract-full').style.display = 'inline'; document.getElementById('2202.07983v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2202.07983v3-abstract-full" style="display: none;">
        Age-related macular degeneration (AMD) is the leading cause of visual impairment among elderly in the world. Early detection of AMD is of great importance, as the vision loss caused by this disease is irreversible and permanent. Color fundus photography is the most cost-effective imaging modality to screen for retinal disorders. Cutting edge deep learning based algorithms have been recently developed for automatically detecting AMD from fundus images. However, there are still lack of a comprehensive annotated dataset and standard evaluation benchmarks. To deal with this issue, we set up the Automatic Detection challenge on Age-related Macular degeneration (ADAM), which was held as a satellite event of the ISBI 2020 conference. The ADAM challenge consisted of four tasks which cover the main aspects of detecting and characterizing AMD from fundus images, including detection of AMD, detection and segmentation of optic disc, localization of fovea, and detection and segmentation of lesions. As part of the challenge, we have released a comprehensive dataset of 1200 fundus images with AMD diagnostic labels, pixel-wise segmentation masks for both optic disc and AMD-related lesions (drusen, exudates, hemorrhages and scars, among others), as well as the coordinates corresponding to the location of the macular fovea. A uniform evaluation framework has been built to make a fair comparison of different models using this dataset. During the challenge, 610 results were submitted for online evaluation, with 11 teams finally participating in the onsite challenge. This paper introduces the challenge, the dataset and the evaluation methods, as well as summarizes the participating methods and analyzes their results for each task. In particular, we observed that the ensembling strategy and the incorporation of clinical domain knowledge were the key to improve the performance of the deep learning models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.07983v3-abstract-full').style.display = 'none'; document.getElementById('2202.07983v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 May, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 February, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">31 pages, 17 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2201.13332">arXiv:2201.13332</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2201.13332">pdf</a>, <a href="https://arxiv.org/ps/2201.13332">ps</a>, <a href="https://arxiv.org/format/2201.13332">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Science and Game Theory">cs.GT</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Metric Distortion of Multiwinner Voting
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Caragiannis%2C+I">Ioannis Caragiannis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Nisarg Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Voudouris%2C+A+A">Alexandros A. Voudouris</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2201.13332v1-abstract-short" style="display: inline;">
        We extend the recently introduced framework of metric distortion to multiwinner voting. In this framework, $n$ agents and $m$ alternatives are located in an underlying metric space. The exact distances between agents and alternatives are unknown. Instead, each agent provides a ranking of the alternatives, ordered from the closest to the farthest. Typically, the goal is to select a single alternati&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.13332v1-abstract-full').style.display = 'inline'; document.getElementById('2201.13332v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2201.13332v1-abstract-full" style="display: none;">
        We extend the recently introduced framework of metric distortion to multiwinner voting. In this framework, $n$ agents and $m$ alternatives are located in an underlying metric space. The exact distances between agents and alternatives are unknown. Instead, each agent provides a ranking of the alternatives, ordered from the closest to the farthest. Typically, the goal is to select a single alternative that approximately minimizes the total distance from the agents, and the worst-case approximation ratio is termed distortion. In the case of multiwinner voting, the goal is to select a committee of $k$ alternatives that (approximately) minimizes the total cost to all agents. We consider the scenario where the cost of an agent for a committee is her distance from the $q$-th closest alternative in the committee. We reveal a surprising trichotomy on the distortion of multiwinner voting rules in terms of $k$ and $q$: The distortion is unbounded when $q \leq k/3$, asymptotically linear in the number of agents when $k/3 &lt; q \leq k/2$, and constant when $q &gt; k/2$.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.13332v1-abstract-full').style.display = 'none'; document.getElementById('2201.13332v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 January, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">A preliminary version of this paper appears in AAAI 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2201.11506">arXiv:2201.11506</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2201.11506">pdf</a>, <a href="https://arxiv.org/format/2201.11506">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Anomaly Detection in Retinal Images using Multi-Scale Deep Feature Sparse Coding
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Das%2C+S+D">Sourya Dipta Das</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dutta%2C+S">Saikat Dutta</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N+A">Nisarg A. Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mahapatra%2C+D">Dwarikanath Mahapatra</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ge%2C+Z">Zongyuan Ge</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2201.11506v1-abstract-short" style="display: inline;">
        Convolutional Neural Network models have successfully detected retinal illness from optical coherence tomography (OCT) and fundus images. These CNN models frequently rely on vast amounts of labeled data for training, difficult to obtain, especially for rare diseases. Furthermore, a deep learning system trained on a data set with only one or a few diseases cannot detect other diseases, limiting the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.11506v1-abstract-full').style.display = 'inline'; document.getElementById('2201.11506v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2201.11506v1-abstract-full" style="display: none;">
        Convolutional Neural Network models have successfully detected retinal illness from optical coherence tomography (OCT) and fundus images. These CNN models frequently rely on vast amounts of labeled data for training, difficult to obtain, especially for rare diseases. Furthermore, a deep learning system trained on a data set with only one or a few diseases cannot detect other diseases, limiting the system&#39;s practical use in disease identification. We have introduced an unsupervised approach for detecting anomalies in retinal images to overcome this issue. We have proposed a simple, memory efficient, easy to train method which followed a multi-step training technique that incorporated autoencoder training and Multi-Scale Deep Feature Sparse Coding (MDFSC), an extended version of normal sparse coding, to accommodate diverse types of retinal datasets. We achieve relative AUC score improvement of 7.8\%, 6.7\% and 12.1\% over state-of-the-art SPADE on Eye-Q, IDRiD and OCTID datasets respectively.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.11506v1-abstract-full').style.display = 'none'; document.getElementById('2201.11506v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 January, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to ISBI 2022.©IEEE</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2112.10074">arXiv:2112.10074</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2112.10074">pdf</a>, <a href="https://arxiv.org/format/2112.10074">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.59275/j.melba.2022-354b">10.59275/j.melba.2022-354b <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        QU-BraTS: MICCAI BraTS 2020 Challenge on Quantifying Uncertainty in Brain Tumor Segmentation - Analysis of Ranking Scores and Benchmarking Results
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mehta%2C+R">Raghav Mehta</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Filos%2C+A">Angelos Filos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Baid%2C+U">Ujjwal Baid</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sako%2C+C">Chiharu Sako</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McKinley%2C+R">Richard McKinley</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rebsamen%2C+M">Michael Rebsamen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Datwyler%2C+K">Katrin Datwyler</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Meier%2C+R">Raphael Meier</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Radojewski%2C+P">Piotr Radojewski</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Murugesan%2C+G+K">Gowtham Krishnan Murugesan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nalawade%2C+S">Sahil Nalawade</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ganesh%2C+C">Chandan Ganesh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wagner%2C+B">Ben Wagner</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+F+F">Fang F. Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fei%2C+B">Baowei Fei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Madhuranthakam%2C+A+J">Ananth J. Madhuranthakam</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Maldjian%2C+J+A">Joseph A. Maldjian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Daza%2C+L">Laura Daza</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gomez%2C+C">Catalina Gomez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Arbelaez%2C+P">Pablo Arbelaez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+C">Chengliang Dai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+S">Shuo Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Reynaud%2C+H">Hadrien Reynaud</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mo%2C+Y">Yuan-han Mo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Angelini%2C+E">Elsa Angelini</a>
      , et al. (67 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2112.10074v2-abstract-short" style="display: inline;">
        Deep learning (DL) models have provided state-of-the-art performance in various medical imaging benchmarking challenges, including the Brain Tumor Segmentation (BraTS) challenges. However, the task of focal pathology multi-compartment segmentation (e.g., tumor and lesion sub-regions) is particularly challenging, and potential errors hinder translating DL models into clinical workflows. Quantifying&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.10074v2-abstract-full').style.display = 'inline'; document.getElementById('2112.10074v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2112.10074v2-abstract-full" style="display: none;">
        Deep learning (DL) models have provided state-of-the-art performance in various medical imaging benchmarking challenges, including the Brain Tumor Segmentation (BraTS) challenges. However, the task of focal pathology multi-compartment segmentation (e.g., tumor and lesion sub-regions) is particularly challenging, and potential errors hinder translating DL models into clinical workflows. Quantifying the reliability of DL model predictions in the form of uncertainties could enable clinical review of the most uncertain regions, thereby building trust and paving the way toward clinical translation. Several uncertainty estimation methods have recently been introduced for DL medical image segmentation tasks. Developing scores to evaluate and compare the performance of uncertainty measures will assist the end-user in making more informed decisions. In this study, we explore and evaluate a score developed during the BraTS 2019 and BraTS 2020 task on uncertainty quantification (QU-BraTS) and designed to assess and rank uncertainty estimates for brain tumor multi-compartment segmentation. This score (1) rewards uncertainty estimates that produce high confidence in correct assertions and those that assign low confidence levels at incorrect assertions, and (2) penalizes uncertainty measures that lead to a higher percentage of under-confident correct assertions. We further benchmark the segmentation uncertainties generated by 14 independent participating teams of QU-BraTS 2020, all of which also participated in the main BraTS segmentation task. Overall, our findings confirm the importance and complementary value that uncertainty estimates provide to segmentation algorithms, highlighting the need for uncertainty quantification in medical image analyses. Finally, in favor of transparency and reproducibility, our evaluation code is made publicly available at: https://github.com/RagMeh11/QU-BraTS.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.10074v2-abstract-full').style.display = 'none'; document.getElementById('2112.10074v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 August, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 December, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted for publication at the Journal of Machine Learning for Biomedical Imaging (MELBA): https://www.melba-journal.org/papers/2022:026.html</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Machine.Learning.for.Biomedical.Imaging. 1 (2022)
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.11285">arXiv:2110.11285</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.11285">pdf</a>, <a href="https://arxiv.org/ps/2110.11285">ps</a>, <a href="https://arxiv.org/format/2110.11285">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Science and Game Theory">cs.GT</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        How to Fairly Allocate Easy and Difficult Chores
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ebadian%2C+S">Soroush Ebadian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peters%2C+D">Dominik Peters</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Nisarg Shah</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.11285v2-abstract-short" style="display: inline;">
        A major open question in fair allocation of indivisible items is whether there always exists an allocation of chores that is Pareto optimal (PO) and envy-free up to one item (EF1). We answer this question affirmatively for the natural class of bivalued utilities, where each agent partitions the chores into easy and difficult ones, and has cost $p &gt; 1$ for chores that are difficult for her and cost&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.11285v2-abstract-full').style.display = 'inline'; document.getElementById('2110.11285v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.11285v2-abstract-full" style="display: none;">
        A major open question in fair allocation of indivisible items is whether there always exists an allocation of chores that is Pareto optimal (PO) and envy-free up to one item (EF1). We answer this question affirmatively for the natural class of bivalued utilities, where each agent partitions the chores into easy and difficult ones, and has cost $p &gt; 1$ for chores that are difficult for her and cost $1$ for chores that are easy for her. Such an allocation can be found in polynomial time using an algorithm based on the Fisher market.
  We also show that for a slightly broader class of utilities, where each agent $i$ can have a potentially different integer $p_i$, an allocation that is maximin share fair (MMS) always exists and can be computed in polynomial time, provided that each $p_i$ is an integer. Our MMS arguments also hold when allocating goods instead of chores, and extend to another natural class of utilities, namely weakly lexicographic utilities.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.11285v2-abstract-full').style.display = 'none'; document.getElementById('2110.11285v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 February, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 October, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Full version of paper published at AAMAS 2022. 33 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.07404">arXiv:2107.07404</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.07404">pdf</a>, <a href="https://arxiv.org/format/2107.07404">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Science and Game Theory">cs.GT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Two-Sided Matching Meets Fair Division
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Freeman%2C+R">Rupert Freeman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Micha%2C+E">Evi Micha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Nisarg Shah</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.07404v1-abstract-short" style="display: inline;">
        We introduce a new model for two-sided matching which allows us to borrow popular fairness notions from the fair division literature such as envy-freeness up to one good and maximin share guarantee. In our model, each agent is matched to multiple agents on the other side over whom she has additive preferences. We demand fairness for each side separately, giving rise to notions such as double envy-&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.07404v1-abstract-full').style.display = 'inline'; document.getElementById('2107.07404v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.07404v1-abstract-full" style="display: none;">
        We introduce a new model for two-sided matching which allows us to borrow popular fairness notions from the fair division literature such as envy-freeness up to one good and maximin share guarantee. In our model, each agent is matched to multiple agents on the other side over whom she has additive preferences. We demand fairness for each side separately, giving rise to notions such as double envy-freeness up to one match (DEF1) and double maximin share guarantee (DMMS). We show that (a slight strengthening of) DEF1 cannot always be achieved, but in the special case where both sides have identical preferences, the round-robin algorithm with a carefully designed agent ordering achieves it. In contrast, DMMS cannot be achieved even when both sides have identical preferences.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.07404v1-abstract-full').style.display = 'none'; document.getElementById('2107.07404v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 July, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.06125">arXiv:2107.06125</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.06125">pdf</a>, <a href="https://arxiv.org/format/2107.06125">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MSR-Net: Multi-Scale Relighting Network for One-to-One Relighting
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Das%2C+S+D">Sourya Dipta Das</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N+A">Nisarg A. Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dutta%2C+S">Saikat Dutta</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.06125v1-abstract-short" style="display: inline;">
        Deep image relighting allows photo enhancement by illumination-specific retouching without human effort and so it is getting much interest lately. Most of the existing popular methods available for relighting are run-time intensive and memory inefficient. Keeping these issues in mind, we propose the use of Stacked Deep Multi-Scale Hierarchical Network, which aggregates features from each image at&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.06125v1-abstract-full').style.display = 'inline'; document.getElementById('2107.06125v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.06125v1-abstract-full" style="display: none;">
        Deep image relighting allows photo enhancement by illumination-specific retouching without human effort and so it is getting much interest lately. Most of the existing popular methods available for relighting are run-time intensive and memory inefficient. Keeping these issues in mind, we propose the use of Stacked Deep Multi-Scale Hierarchical Network, which aggregates features from each image at different scales. Our solution is differentiable and robust for translating image illumination setting from input image to target image. Additionally, we have also shown that using a multi-step training approach to this problem with two different loss functions can significantly boost performance and can achieve a high quality reconstruction of a relighted image.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.06125v1-abstract-full').style.display = 'none'; document.getElementById('2107.06125v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 July, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Workshop on Differentiable Vision, Graphics, and Physics in Machine Learning at NeurIPS 2020. arXiv admin note: text overlap with arXiv:2102.09242</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.10064">arXiv:2105.10064</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.10064">pdf</a>, <a href="https://arxiv.org/ps/2105.10064">ps</a>, <a href="https://arxiv.org/format/2105.10064">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Science and Game Theory">cs.GT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Fair and Efficient Resource Allocation with Partial Information
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Halpern%2C+D">Daniel Halpern</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Nisarg Shah</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.10064v2-abstract-short" style="display: inline;">
        We study the fundamental problem of allocating indivisible goods to agents with additive preferences. We consider eliciting from each agent only a ranking of her $k$ most preferred goods instead of her full cardinal valuations. We characterize the value of $k$ needed to achieve envy-freeness up to one good and approximate maximin share guarantee, two widely studied fairness notions. We also analyz&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.10064v2-abstract-full').style.display = 'inline'; document.getElementById('2105.10064v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.10064v2-abstract-full" style="display: none;">
        We study the fundamental problem of allocating indivisible goods to agents with additive preferences. We consider eliciting from each agent only a ranking of her $k$ most preferred goods instead of her full cardinal valuations. We characterize the value of $k$ needed to achieve envy-freeness up to one good and approximate maximin share guarantee, two widely studied fairness notions. We also analyze the multiplicative loss in social welfare incurred due to the lack of full information with and without the fairness requirements.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.10064v2-abstract-full').style.display = 'none'; document.getElementById('2105.10064v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 May, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 May, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Appears in the Proceedings of IJCAI 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.09386">arXiv:2105.09386</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.09386">pdf</a>, <a href="https://arxiv.org/format/2105.09386">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Science and Game Theory">cs.GT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multiagent Systems">cs.MA</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Surprisingly Popular Voting Recovers Rankings, Surprisingly!
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hosseini%2C+H">Hadi Hosseini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mandal%2C+D">Debmalya Mandal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Nisarg Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+K">Kevin Shi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.09386v1-abstract-short" style="display: inline;">
        The wisdom of the crowd has long become the de facto approach for eliciting information from individuals or experts in order to predict the ground truth. However, classical democratic approaches for aggregating individual \emph{votes} only work when the opinion of the majority of the crowd is relatively accurate. A clever recent approach, \emph{surprisingly popular voting}, elicits additional info&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.09386v1-abstract-full').style.display = 'inline'; document.getElementById('2105.09386v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.09386v1-abstract-full" style="display: none;">
        The wisdom of the crowd has long become the de facto approach for eliciting information from individuals or experts in order to predict the ground truth. However, classical democratic approaches for aggregating individual \emph{votes} only work when the opinion of the majority of the crowd is relatively accurate. A clever recent approach, \emph{surprisingly popular voting}, elicits additional information from the individuals, namely their \emph{prediction} of other individuals&#39; votes, and provably recovers the ground truth even when experts are in minority. This approach works well when the goal is to pick the correct option from a small list, but when the goal is to recover a true ranking of the alternatives, a direct application of the approach requires eliciting too much information. We explore practical techniques for extending the surprisingly popular algorithm to ranked voting by partial votes and predictions and designing robust aggregation rules. We experimentally demonstrate that even a little prediction information helps surprisingly popular voting outperform classical approaches.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.09386v1-abstract-full').style.display = 'none'; document.getElementById('2105.09386v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Forthcoming at IJCAI-2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.08819">arXiv:2105.08819</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.08819">pdf</a>, <a href="https://arxiv.org/format/2105.08819">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Fast and Accurate Quantized Camera Scene Detection on Smartphones, Mobile AI 2021 Challenge: Report
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ignatov%2C+A">Andrey Ignatov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Malivenko%2C+G">Grigory Malivenko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Timofte%2C+R">Radu Timofte</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+S">Sheng Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xia%2C+X">Xin Xia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Z">Zhaoyan Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yuwei Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+F">Feng Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Jiashi Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiao%2C+X">Xuefeng Xiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tian%2C+Y">Yuan Tian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+X">Xinglong Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kyrkou%2C+C">Christos Kyrkou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yixin Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Z">Zexin Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+Y">Yunbo Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+Y">Yue Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dutta%2C+S">Saikat Dutta</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Das%2C+S+D">Sourya Dipta Das</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N+A">Nisarg A. Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kumar%2C+H">Himanshu Kumar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ge%2C+C">Chao Ge</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+P">Pei-Lin Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+J">Jin-Hua Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Batutin%2C+A">Andrew Batutin</a>
      , et al. (6 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.08819v1-abstract-short" style="display: inline;">
        Camera scene detection is among the most popular computer vision problem on smartphones. While many custom solutions were developed for this task by phone vendors, none of the designed models were available publicly up until now. To address this problem, we introduce the first Mobile AI challenge, where the target is to develop quantized deep learning-based camera scene classification solutions th&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.08819v1-abstract-full').style.display = 'inline'; document.getElementById('2105.08819v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.08819v1-abstract-full" style="display: none;">
        Camera scene detection is among the most popular computer vision problem on smartphones. While many custom solutions were developed for this task by phone vendors, none of the designed models were available publicly up until now. To address this problem, we introduce the first Mobile AI challenge, where the target is to develop quantized deep learning-based camera scene classification solutions that can demonstrate a real-time performance on smartphones and IoT platforms. For this, the participants were provided with a large-scale CamSDD dataset consisting of more than 11K images belonging to the 30 most important scene categories. The runtime of all models was evaluated on the popular Apple Bionic A11 platform that can be found in many iOS devices. The proposed solutions are fully compatible with all major mobile AI accelerators and can demonstrate more than 100-200 FPS on the majority of recent smartphone platforms while achieving a top-3 accuracy of more than 98%. A detailed description of all models developed in the challenge is provided in this paper.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.08819v1-abstract-full').style.display = 'none'; document.getElementById('2105.08819v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Mobile AI 2021 Workshop and Challenges: https://ai-benchmark.com/workshops/mai/2021/. arXiv admin note: substantial text overlap with arXiv:2105.08630; text overlap with arXiv:2105.07825, arXiv:2105.07809, arXiv:2105.08629</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.07174">arXiv:2105.07174</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.07174">pdf</a>, <a href="https://arxiv.org/format/2105.07174">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Stacked Deep Multi-Scale Hierarchical Network for Fast Bokeh Effect Rendering from a Single Image
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dutta%2C+S">Saikat Dutta</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Das%2C+S+D">Sourya Dipta Das</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N+A">Nisarg A. Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tiwari%2C+A+K">Anil Kumar Tiwari</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.07174v1-abstract-short" style="display: inline;">
        The Bokeh Effect is one of the most desirable effects in photography for rendering artistic and aesthetic photos. Usually, it requires a DSLR camera with different aperture and shutter settings and certain photography skills to generate this effect. In smartphones, computational methods and additional sensors are used to overcome the physical lens and sensor limitations to achieve such effect. Mos&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.07174v1-abstract-full').style.display = 'inline'; document.getElementById('2105.07174v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.07174v1-abstract-full" style="display: none;">
        The Bokeh Effect is one of the most desirable effects in photography for rendering artistic and aesthetic photos. Usually, it requires a DSLR camera with different aperture and shutter settings and certain photography skills to generate this effect. In smartphones, computational methods and additional sensors are used to overcome the physical lens and sensor limitations to achieve such effect. Most of the existing methods utilized additional sensor&#39;s data or pretrained network for fine depth estimation of the scene and sometimes use portrait segmentation pretrained network module to segment salient objects in the image. Because of these reasons, networks have many parameters, become runtime intensive and unable to run in mid-range devices. In this paper, we used an end-to-end Deep Multi-Scale Hierarchical Network (DMSHN) model for direct Bokeh effect rendering of images captured from the monocular camera. To further improve the perceptual quality of such effect, a stacked model consisting of two DMSHN modules is also proposed. Our model does not rely on any pretrained network module for Monocular Depth Estimation or Saliency Detection, thus significantly reducing the size of model and run time. Stacked DMSHN achieves state-of-the-art results on a large scale EBB! dataset with around 6x less runtime compared to the current state-of-the-art model in processing HD quality images.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.07174v1-abstract-full').style.display = 'none'; document.getElementById('2105.07174v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to MAI workshop, CVPR 2021. Code and models: https://github.com/saikatdutta/Stacked_DMSHN_bokeh</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2104.05778">arXiv:2104.05778</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2104.05778">pdf</a>, <a href="https://arxiv.org/format/2104.05778">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Efficient Space-time Video Super Resolution using Low-Resolution Flow and Mask Upsampling
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dutta%2C+S">Saikat Dutta</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N+A">Nisarg A. Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mittal%2C+A">Anurag Mittal</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2104.05778v3-abstract-short" style="display: inline;">
        This paper explores an efficient solution for Space-time Super-Resolution, aiming to generate High-resolution Slow-motion videos from Low Resolution and Low Frame rate videos. A simplistic solution is the sequential running of Video Super Resolution and Video Frame interpolation models. However, this type of solutions are memory inefficient, have high inference time, and could not make the proper&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.05778v3-abstract-full').style.display = 'inline'; document.getElementById('2104.05778v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2104.05778v3-abstract-full" style="display: none;">
        This paper explores an efficient solution for Space-time Super-Resolution, aiming to generate High-resolution Slow-motion videos from Low Resolution and Low Frame rate videos. A simplistic solution is the sequential running of Video Super Resolution and Video Frame interpolation models. However, this type of solutions are memory inefficient, have high inference time, and could not make the proper use of space-time relation property. To this extent, we first interpolate in LR space using quadratic modeling. Input LR frames are super-resolved using a state-of-the-art Video Super-Resolution method. Flowmaps and blending mask which are used to synthesize LR interpolated frame is reused in HR space using bilinear upsampling. This leads to a coarse estimate of HR intermediate frame which often contains artifacts along motion boundaries. We use a refinement network to improve the quality of HR intermediate frame via residual learning. Our model is lightweight and performs better than current state-of-the-art models in REDS STSR Validation set.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2104.05778v3-abstract-full').style.display = 'none'; document.getElementById('2104.05778v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 June, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 12 April, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at NTIRE Workshop, CVPR 2021. Code and models: https://github.com/saikatdutta/FMU_STSR</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.09289">arXiv:2103.09289</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.09289">pdf</a>, <a href="https://arxiv.org/format/2103.09289">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Colorectal Cancer Segmentation using Atrous Convolution and Residual Enhanced UNet
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N+A">Nisarg A. Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gupta%2C+D">Divij Gupta</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lodaya%2C+R">Romil Lodaya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Baid%2C+U">Ujjwal Baid</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Talbar%2C+S">Sanjay Talbar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.09289v1-abstract-short" style="display: inline;">
        Colorectal cancer is a leading cause of death worldwide. However, early diagnosis dramatically increases the chances of survival, for which it is crucial to identify the tumor in the body. Since its imaging uses high-resolution techniques, annotating the tumor is time-consuming and requires particular expertise. Lately, methods built upon Convolutional Neural Networks(CNNs) have proven to be at pa&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.09289v1-abstract-full').style.display = 'inline'; document.getElementById('2103.09289v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.09289v1-abstract-full" style="display: none;">
        Colorectal cancer is a leading cause of death worldwide. However, early diagnosis dramatically increases the chances of survival, for which it is crucial to identify the tumor in the body. Since its imaging uses high-resolution techniques, annotating the tumor is time-consuming and requires particular expertise. Lately, methods built upon Convolutional Neural Networks(CNNs) have proven to be at par, if not better in many biomedical segmentation tasks. For the task at hand, we propose another CNN-based approach, which uses atrous convolutions and residual connections besides the conventional filters. The training and inference were made using an efficient patch-based approach, which significantly reduced unnecessary computations. The proposed AtResUNet was trained on the DigestPath 2019 Challenge dataset for colorectal cancer segmentation with results having a Dice Coefficient of 0.748.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.09289v1-abstract-full').style.display = 'none'; document.getElementById('2103.09289v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">5th IAPR International Conference on Computer Vision and Image Processing, 12 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.00911">arXiv:2103.00911</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.00911">pdf</a>, <a href="https://arxiv.org/ps/2103.00911">ps</a>, <a href="https://arxiv.org/format/2103.00911">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Science and Game Theory">cs.GT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Theoretical Economics">econ.TH</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Distortion in Social Choice Problems: The First 15 Years and Beyond
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Anshelevich%2C+E">Elliot Anshelevich</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Filos-Ratsikas%2C+A">Aris Filos-Ratsikas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Nisarg Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Voudouris%2C+A+A">Alexandros A. Voudouris</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.00911v1-abstract-short" style="display: inline;">
        The notion of distortion in social choice problems has been defined to measure the loss in efficiency -- typically measured by the utilitarian social welfare, the sum of utilities of the participating agents -- due to having access only to limited information about the preferences of the agents. We survey the most significant results of the literature on distortion from the past 15 years, and high&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.00911v1-abstract-full').style.display = 'inline'; document.getElementById('2103.00911v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.00911v1-abstract-full" style="display: none;">
        The notion of distortion in social choice problems has been defined to measure the loss in efficiency -- typically measured by the utilitarian social welfare, the sum of utilities of the participating agents -- due to having access only to limited information about the preferences of the agents. We survey the most significant results of the literature on distortion from the past 15 years, and highlight important open problems and the most promising avenues of ongoing and future work.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.00911v1-abstract-full').style.display = 'none'; document.getElementById('2103.00911v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Survey</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.09242">arXiv:2102.09242</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.09242">pdf</a>, <a href="https://arxiv.org/format/2102.09242">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DSRN: an Efficient Deep Network for Image Relighting
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Das%2C+S+D">Sourya Dipta Das</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N+A">Nisarg A. Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dutta%2C+S">Saikat Dutta</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kumar%2C+H">Himanshu Kumar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.09242v2-abstract-short" style="display: inline;">
        Custom and natural lighting conditions can be emulated in images of the scene during post-editing. Extraordinary capabilities of the deep learning framework can be utilized for such purpose. Deep image relighting allows automatic photo enhancement by illumination-specific retouching. Most of the state-of-the-art methods for relighting are run-time intensive and memory inefficient. In this paper, w&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.09242v2-abstract-full').style.display = 'inline'; document.getElementById('2102.09242v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.09242v2-abstract-full" style="display: none;">
        Custom and natural lighting conditions can be emulated in images of the scene during post-editing. Extraordinary capabilities of the deep learning framework can be utilized for such purpose. Deep image relighting allows automatic photo enhancement by illumination-specific retouching. Most of the state-of-the-art methods for relighting are run-time intensive and memory inefficient. In this paper, we propose an efficient, real-time framework Deep Stacked Relighting Network (DSRN) for image relighting by utilizing the aggregated features from input image at different scales. Our model is very lightweight with total size of about 42 MB and has an average inference time of about 0.0116s for image of resolution $1024 \times 1024$ which is faster as compared to other multi-scale models. Our solution is quite robust for translating image color temperature from input image to target image and also performs moderately for light gradient generation with respect to the target image. Additionally, we show that if images illuminated from opposite directions are used as input, the qualitative results improve over using a single input image.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.09242v2-abstract-full').style.display = 'none'; document.getElementById('2102.09242v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 June, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 February, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at ICIP 2021. $©$ IEEE</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2011.04988">arXiv:2011.04988</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2011.04988">pdf</a>, <a href="https://arxiv.org/format/2011.04988">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AIM 2020 Challenge on Rendering Realistic Bokeh
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ignatov%2C+A">Andrey Ignatov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Timofte%2C+R">Radu Timofte</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qian%2C+M">Ming Qian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qiao%2C+C">Congyu Qiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+J">Jiamin Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Z">Zhenyu Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+C">Chenghua Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Leng%2C+C">Cong Leng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+J">Jian Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+J">Juewen Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+X">Xianrui Luo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xian%2C+K">Ke Xian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+Z">Zijin Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+Z">Zhiguo Cao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Puthussery%2C+D">Densen Puthussery</a>, 
      
      <a href="/search/?searchtype=author&amp;query=C%2C+J">Jiji C V</a>, 
      
      <a href="/search/?searchtype=author&amp;query=S%2C+H+P">Hrishikesh P S</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kuriakose%2C+M">Melvin Kuriakose</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dutta%2C+S">Saikat Dutta</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Das%2C+S+D">Sourya Dipta Das</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N+A">Nisarg A. Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Purohit%2C+K">Kuldeep Purohit</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kandula%2C+P">Praveen Kandula</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Suin%2C+M">Maitreya Suin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rajagopalan%2C+A+N">A. N. Rajagopalan</a>
      , et al. (10 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2011.04988v1-abstract-short" style="display: inline;">
        This paper reviews the second AIM realistic bokeh effect rendering challenge and provides the description of the proposed solutions and results. The participating teams were solving a real-world bokeh simulation problem, where the goal was to learn a realistic shallow focus technique using a large-scale EBB! bokeh dataset consisting of 5K shallow / wide depth-of-field image pairs captured using th&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.04988v1-abstract-full').style.display = 'inline'; document.getElementById('2011.04988v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2011.04988v1-abstract-full" style="display: none;">
        This paper reviews the second AIM realistic bokeh effect rendering challenge and provides the description of the proposed solutions and results. The participating teams were solving a real-world bokeh simulation problem, where the goal was to learn a realistic shallow focus technique using a large-scale EBB! bokeh dataset consisting of 5K shallow / wide depth-of-field image pairs captured using the Canon 7D DSLR camera. The participants had to render bokeh effect based on only one single frame without any additional data from other cameras or sensors. The target metric used in this challenge combined the runtime and the perceptual quality of the solutions measured in the user study. To ensure the efficiency of the submitted models, we measured their runtime on standard desktop CPUs as well as were running the models on smartphone GPUs. The proposed solutions significantly improved the baseline results, defining the state-of-the-art for practical bokeh effect rendering problem.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.04988v1-abstract-full').style.display = 'none'; document.getElementById('2011.04988v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 November, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published in ECCV 2020 Workshop (Advances in Image Manipulation), https://data.vision.ee.ethz.ch/cvl/aim20/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2011.03910">arXiv:2011.03910</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2011.03910">pdf</a>, <a href="https://arxiv.org/format/2011.03910">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Faster object tracking pipeline for real time tracking
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Soni%2C+P">Parthesh Soni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+F">Falak Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vyas%2C+N">Nisarg Vyas</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2011.03910v1-abstract-short" style="display: inline;">
        Multi-object tracking (MOT) is a challenging practical problem for vision based applications. Most recent approaches for MOT use precomputed detections from models such as Faster RCNN, performing fine-tuning of bounding boxes and association in subsequent phases. However, this is not suitable for actual industrial applications due to unavailability of detections upfront. In their recent work, Wang&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.03910v1-abstract-full').style.display = 'inline'; document.getElementById('2011.03910v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2011.03910v1-abstract-full" style="display: none;">
        Multi-object tracking (MOT) is a challenging practical problem for vision based applications. Most recent approaches for MOT use precomputed detections from models such as Faster RCNN, performing fine-tuning of bounding boxes and association in subsequent phases. However, this is not suitable for actual industrial applications due to unavailability of detections upfront. In their recent work, Wang et al. proposed a tracking pipeline that uses a Joint detection and embedding model and performs target localization and association in realtime. Upon investigating the tracking by detection paradigm, we find that the tracking pipeline can be made faster by performing localization and association tasks parallely with model prediction. This, and other computational optimizations such as using mixed precision model and performing batchwise detection result in a speed-up of the tracking pipeline by 57.8\% (19 FPS to 30 FPS) on FullHD resolution. Moreover, the speed is independent of the object density in image sequence. The main contribution of this paper is showcasing a generic pipeline which can be used to speed up detection based object tracking methods. We also reviewed different batch sizes for optimal performance, taking into consideration GPU memory usage and speed.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.03910v1-abstract-full').style.display = 'none'; document.getElementById('2011.03910v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 November, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages, 6 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2009.12798">arXiv:2009.12798</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2009.12798">pdf</a>, <a href="https://arxiv.org/format/2009.12798">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AIM 2020: Scene Relighting and Illumination Estimation Challenge
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Helou%2C+M+E">Majed El Helou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+R">Ruofan Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=S%C3%BCsstrunk%2C+S">Sabine Süsstrunk</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Timofte%2C+R">Radu Timofte</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Afifi%2C+M">Mahmoud Afifi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brown%2C+M+S">Michael S. Brown</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+K">Kele Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+H">Hengxing Cai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yuzhong Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+L">Li-Wen Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Z">Zhi-Song Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+C">Chu-Tak Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Das%2C+S+D">Sourya Dipta Das</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N+A">Nisarg A. Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jassal%2C+A">Akashdeep Jassal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+T">Tongtong Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+S">Shanshan Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nathan%2C+S">Sabari Nathan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Beham%2C+M+P">M. Parisa Beham</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Suganya%2C+R">R. Suganya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Q">Qing Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+Z">Zhongyun Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+X">Xin Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yaning Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Suin%2C+M">Maitreya Suin</a>
      , et al. (12 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2009.12798v1-abstract-short" style="display: inline;">
        We review the AIM 2020 challenge on virtual image relighting and illumination estimation. This paper presents the novel VIDIT dataset used in the challenge and the different proposed solutions and final evaluation results over the 3 challenge tracks. The first track considered one-to-one relighting; the objective was to relight an input photo of a scene with a different color temperature and illum&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.12798v1-abstract-full').style.display = 'inline'; document.getElementById('2009.12798v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2009.12798v1-abstract-full" style="display: none;">
        We review the AIM 2020 challenge on virtual image relighting and illumination estimation. This paper presents the novel VIDIT dataset used in the challenge and the different proposed solutions and final evaluation results over the 3 challenge tracks. The first track considered one-to-one relighting; the objective was to relight an input photo of a scene with a different color temperature and illuminant orientation (i.e., light source position). The goal of the second track was to estimate illumination settings, namely the color temperature and orientation, from a given image. Lastly, the third track dealt with any-to-any relighting, thus a generalization of the first track. The target color temperature and orientation, rather than being pre-determined, are instead given by a guide image. Participants were allowed to make use of their track 1 and 2 solutions for track 3. The tracks had 94, 52, and 56 registered participants, respectively, leading to 20 confirmed submissions in the final competition stage.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2009.12798v1-abstract-full').style.display = 'none'; document.getElementById('2009.12798v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 September, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ECCVW 2020. Data and more information on https://github.com/majedelhelou/VIDIT</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2008.07742">arXiv:2008.07742</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2008.07742">pdf</a>, <a href="https://arxiv.org/format/2008.07742">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        UDC 2020 Challenge on Image Restoration of Under-Display Camera: Methods and Results
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+Y">Yuqian Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kwan%2C+M">Michael Kwan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tolentino%2C+K">Kyle Tolentino</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Emerton%2C+N">Neil Emerton</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lim%2C+S">Sehoon Lim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Large%2C+T">Tim Large</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fu%2C+L">Lijiang Fu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pan%2C+Z">Zhihong Pan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+B">Baopu Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Q">Qirui Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yihao Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+J">Jigang Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ku%2C+T">Tao Ku</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+S">Shibin Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+B">Bingnan Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jiarong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Puthussery%2C+D">Densen Puthussery</a>, 
      
      <a href="/search/?searchtype=author&amp;query=S%2C+H+P">Hrishikesh P S</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kuriakose%2C+M">Melvin Kuriakose</a>, 
      
      <a href="/search/?searchtype=author&amp;query=C%2C+J">Jiji C V</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sundar%2C+V">Varun Sundar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hegde%2C+S">Sumanth Hegde</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kothandaraman%2C+D">Divya Kothandaraman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mitra%2C+K">Kaushik Mitra</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jassal%2C+A">Akashdeep Jassal</a>
      , et al. (20 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2008.07742v1-abstract-short" style="display: inline;">
        This paper is the report of the first Under-Display Camera (UDC) image restoration challenge in conjunction with the RLQ workshop at ECCV 2020. The challenge is based on a newly-collected database of Under-Display Camera. The challenge tracks correspond to two types of display: a 4k Transparent OLED (T-OLED) and a phone Pentile OLED (P-OLED). Along with about 150 teams registered the challenge, ei&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.07742v1-abstract-full').style.display = 'inline'; document.getElementById('2008.07742v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2008.07742v1-abstract-full" style="display: none;">
        This paper is the report of the first Under-Display Camera (UDC) image restoration challenge in conjunction with the RLQ workshop at ECCV 2020. The challenge is based on a newly-collected database of Under-Display Camera. The challenge tracks correspond to two types of display: a 4k Transparent OLED (T-OLED) and a phone Pentile OLED (P-OLED). Along with about 150 teams registered the challenge, eight and nine teams submitted the results during the testing phase for each track. The results in the paper are state-of-the-art restoration performance of Under-Display Camera Restoration. Datasets and paper are available at https://yzhouas.github.io/projects/UDC/udc.html.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.07742v1-abstract-full').style.display = 'none'; document.getElementById('2008.07742v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 August, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">15 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2007.09079">arXiv:2007.09079</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2007.09079">pdf</a>, <a href="https://arxiv.org/ps/2007.09079">ps</a>, <a href="https://arxiv.org/format/2007.09079">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Science and Game Theory">cs.GT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Necessarily Optimal One-Sided Matchings
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hosseini%2C+H">Hadi Hosseini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Menon%2C+V">Vijay Menon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Nisarg Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sikdar%2C+S">Sujoy Sikdar</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2007.09079v3-abstract-short" style="display: inline;">
        We study the classical problem of matching $n$ agents to $n$ objects, where the agents have ranked preferences over the objects. We focus on two popular desiderata from the matching literature: Pareto optimality and rank-maximality. Instead of asking the agents to report their complete preferences, our goal is to learn a desirable matching from partial preferences, specifically a matching that is&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.09079v3-abstract-full').style.display = 'inline'; document.getElementById('2007.09079v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2007.09079v3-abstract-full" style="display: none;">
        We study the classical problem of matching $n$ agents to $n$ objects, where the agents have ranked preferences over the objects. We focus on two popular desiderata from the matching literature: Pareto optimality and rank-maximality. Instead of asking the agents to report their complete preferences, our goal is to learn a desirable matching from partial preferences, specifically a matching that is necessarily Pareto optimal (NPO) or necessarily rank-maximal (NRM) under any completion of the partial preferences. We focus on the top-$k$ model in which agents reveal a prefix of their preference rankings. We design efficient algorithms to check if a given matching is NPO or NRM, and to check whether such a matching exists given top-$k$ partial preferences. We also study online algorithms for eliciting partial preferences adaptively, and prove bounds on their competitive ratio.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.09079v3-abstract-full').style.display = 'none'; document.getElementById('2007.09079v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 April, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 17 July, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2007.07316">arXiv:2007.07316</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2007.07316">pdf</a>, <a href="https://arxiv.org/format/2007.07316">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Science and Game Theory">cs.GT</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Effect of Strategic Noise in Linear Regression
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hossain%2C+S">Safwan Hossain</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Nisarg Shah</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2007.07316v1-abstract-short" style="display: inline;">
        We build on an emerging line of work which studies strategic manipulations in training data provided to machine learning algorithms. Specifically, we focus on the ubiquitous task of linear regression. Prior work focused on the design of strategyproof algorithms, which aim to prevent such manipulations altogether by aligning the incentives of data sources. However, algorithms used in practice are o&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.07316v1-abstract-full').style.display = 'inline'; document.getElementById('2007.07316v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2007.07316v1-abstract-full" style="display: none;">
        We build on an emerging line of work which studies strategic manipulations in training data provided to machine learning algorithms. Specifically, we focus on the ubiquitous task of linear regression. Prior work focused on the design of strategyproof algorithms, which aim to prevent such manipulations altogether by aligning the incentives of data sources. However, algorithms used in practice are often not strategyproof, which induces a strategic game among the agents. We focus on a broad class of non-strategyproof algorithms for linear regression, namely $\ell_p$ norm minimization ($p &gt; 1$) with convex regularization. We show that when manipulations are bounded, every algorithm in this class admits a unique pure Nash equilibrium outcome. We also shed light on the structure of this equilibrium by uncovering a surprising connection between strategyproof algorithms and pure Nash equilibria of non-strategyproof algorithms in a broader setting, which may be of independent interest. Finally, we analyze the quality of equilibria under these algorithms in terms of the price of anarchy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.07316v1-abstract-full').style.display = 'none'; document.getElementById('2007.07316v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 July, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2007.06699">arXiv:2007.06699</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2007.06699">pdf</a>, <a href="https://arxiv.org/ps/2007.06699">ps</a>, <a href="https://arxiv.org/format/2007.06699">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Science and Game Theory">cs.GT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Fair Algorithms for Multi-Agent Multi-Armed Bandits
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hossain%2C+S">Safwan Hossain</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Micha%2C+E">Evi Micha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Nisarg Shah</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2007.06699v2-abstract-short" style="display: inline;">
        We propose a multi-agent variant of the classical multi-armed bandit problem, in which there are $N$ agents and $K$ arms, and pulling an arm generates a (possibly different) stochastic reward for each agent. Unlike the classical multi-armed bandit problem, the goal is not to learn the &#34;best arm&#34;; indeed, each agent may perceive a different arm to be the best for her personally. Instead, we seek to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.06699v2-abstract-full').style.display = 'inline'; document.getElementById('2007.06699v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2007.06699v2-abstract-full" style="display: none;">
        We propose a multi-agent variant of the classical multi-armed bandit problem, in which there are $N$ agents and $K$ arms, and pulling an arm generates a (possibly different) stochastic reward for each agent. Unlike the classical multi-armed bandit problem, the goal is not to learn the &#34;best arm&#34;; indeed, each agent may perceive a different arm to be the best for her personally. Instead, we seek to learn a fair distribution over the arms. Drawing on a long line of research in economics and computer science, we use the Nash social welfare as our notion of fairness. We design multi-agent variants of three classic multi-armed bandit algorithms and show that they achieve sublinear regret, which is now measured in terms of the lost Nash social welfare.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.06699v2-abstract-full').style.display = 'none'; document.getElementById('2007.06699v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 February, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 July, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2007.06242">arXiv:2007.06242</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2007.06242">pdf</a>, <a href="https://arxiv.org/format/2007.06242">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Science and Game Theory">cs.GT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Data Structures and Algorithms">cs.DS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Optimal Bounds on the Price of Fairness for Indivisible Goods
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Barman%2C+S">Siddharth Barman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bhaskar%2C+U">Umang Bhaskar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Nisarg Shah</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2007.06242v3-abstract-short" style="display: inline;">
        In the allocation of resources to a set of agents, how do fairness guarantees impact the social welfare? A quantitative measure of this impact is the price of fairness, which measures the worst-case loss of social welfare due to fairness constraints. While initially studied for divisible goods, recent work on the price of fairness also studies the setting of indivisible goods.
  In this paper, we&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.06242v3-abstract-full').style.display = 'inline'; document.getElementById('2007.06242v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2007.06242v3-abstract-full" style="display: none;">
        In the allocation of resources to a set of agents, how do fairness guarantees impact the social welfare? A quantitative measure of this impact is the price of fairness, which measures the worst-case loss of social welfare due to fairness constraints. While initially studied for divisible goods, recent work on the price of fairness also studies the setting of indivisible goods.
  In this paper, we resolve the price of two well-studied fairness notions for the allocation of indivisible goods: envy-freeness up to one good (EF1), and approximate maximin share (MMS). For both EF1 and 1/2-MMS guarantees, we show, via different techniques, that the price of fairness is $O(\sqrt{n})$, where $n$ is the number of agents. From previous work, it follows that our bounds are tight. Our bounds are obtained via efficient algorithms. For 1/2-MMS, our bound holds for additive valuations, whereas for EF1, our bound holds for the more general class of subadditive valuations. This resolves an open problem posed by Bei et al. (2019).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.06242v3-abstract-full').style.display = 'none'; document.getElementById('2007.06242v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 November, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 July, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">24 pages</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          91A68
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          F.2.0
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2007.06073">arXiv:2007.06073</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2007.06073">pdf</a>, <a href="https://arxiv.org/ps/2007.06073">ps</a>, <a href="https://arxiv.org/format/2007.06073">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Science and Game Theory">cs.GT</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Fair Division with Binary Valuations: One Rule to Rule Them All
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Halpern%2C+D">Daniel Halpern</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Procaccia%2C+A+D">Ariel D. Procaccia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Psomas%2C+A">Alexandros Psomas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Nisarg Shah</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2007.06073v2-abstract-short" style="display: inline;">
        We study fair allocation of indivisible goods among agents. Prior research focuses on additive agent preferences, which leads to an impossibility when seeking truthfulness, fairness, and efficiency. We show that when agents have binary additive preferences, a compelling rule -- maximum Nash welfare (MNW) -- provides all three guarantees.
  Specifically, we show that deterministic MNW with lexicogr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.06073v2-abstract-full').style.display = 'inline'; document.getElementById('2007.06073v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2007.06073v2-abstract-full" style="display: none;">
        We study fair allocation of indivisible goods among agents. Prior research focuses on additive agent preferences, which leads to an impossibility when seeking truthfulness, fairness, and efficiency. We show that when agents have binary additive preferences, a compelling rule -- maximum Nash welfare (MNW) -- provides all three guarantees.
  Specifically, we show that deterministic MNW with lexicographic tie-breaking is group strategyproof in addition to being envy-free up to one good and Pareto optimal. We also prove that fractional MNW -- known to be group strategyproof, envy-free, and Pareto optimal -- can be implemented as a distribution over deterministic MNW allocations, which are envy-free up to one good. Our work establishes maximum Nash welfare as the ultimate allocation rule in the realm of binary additive preferences.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.06073v2-abstract-full').style.display = 'none'; document.getElementById('2007.06073v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 September, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 12 July, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2005.14122">arXiv:2005.14122</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2005.14122">pdf</a>, <a href="https://arxiv.org/ps/2005.14122">ps</a>, <a href="https://arxiv.org/format/2005.14122">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Science and Game Theory">cs.GT</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Best of Both Worlds: Ex-Ante and Ex-Post Fairness in Resource Allocation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Freeman%2C+R">Rupert Freeman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Nisarg Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vaish%2C+R">Rohit Vaish</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2005.14122v1-abstract-short" style="display: inline;">
        We study the problem of allocating indivisible goods among agents with additive valuations. When randomization is allowed, it is possible to achieve compelling notions of fairness such as envy-freeness, which states that no agent should prefer any other agent&#39;s allocation to her own. When allocations must be deterministic, achieving exact fairness is impossible but approximate notions such as envy&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.14122v1-abstract-full').style.display = 'inline'; document.getElementById('2005.14122v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2005.14122v1-abstract-full" style="display: none;">
        We study the problem of allocating indivisible goods among agents with additive valuations. When randomization is allowed, it is possible to achieve compelling notions of fairness such as envy-freeness, which states that no agent should prefer any other agent&#39;s allocation to her own. When allocations must be deterministic, achieving exact fairness is impossible but approximate notions such as envy-freeness up to one good can be guaranteed. Our goal in this work is to achieve both simultaneously, by constructing a randomized allocation that is exactly fair ex-ante and approximately fair ex-post. The key question we address is whether ex-ante envy-freeness can be achieved in combination with ex-post envy-freeness up to one good. We settle this positively by designing an efficient algorithm that achieves both properties simultaneously. If we additionally require economic efficiency, we obtain an impossibility result. However, we show that economic efficiency and ex-ante envy-freeness can be simultaneously achieved if we slightly relax our ex-post fairness guarantee. On our way, we characterize the well-known Maximum Nash Welfare allocation rule in terms of a recently introduced fairness guarantee that applies to groups of agents, not just individuals.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.14122v1-abstract-full').style.display = 'none'; document.getElementById('2005.14122v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 May, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Full version of a paper published as an extended abstract at Economics and Computation (EC) 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2005.04117">arXiv:2005.04117</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2005.04117">pdf</a>, <a href="https://arxiv.org/format/2005.04117">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        NTIRE 2020 Challenge on Real Image Denoising: Dataset, Methods and Results
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Abdelhamed%2C+A">Abdelrahman Abdelhamed</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Afifi%2C+M">Mahmoud Afifi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Timofte%2C+R">Radu Timofte</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brown%2C+M+S">Michael S. Brown</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+Y">Yue Cao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Z">Zhilu Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zuo%2C+W">Wangmeng Zuo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+X">Xiaoling Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Jiye Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+W">Wendong Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wen%2C+C">Changyuan Wen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+M">Meng Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lv%2C+S">Shuailin Lv</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yunchao Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pan%2C+Z">Zhihong Pan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+B">Baopu Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xi%2C+T">Teng Xi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fan%2C+Y">Yanwen Fan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+X">Xiyu Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+G">Gang Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Jingtuo Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+J">Junyu Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ding%2C+E">Errui Ding</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+S">Songhyun Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Park%2C+B">Bumjun Park</a>
      , et al. (65 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2005.04117v1-abstract-short" style="display: inline;">
        This paper reviews the NTIRE 2020 challenge on real image denoising with focus on the newly introduced dataset, the proposed methods and their results. The challenge is a new version of the previous NTIRE 2019 challenge on real image denoising that was based on the SIDD benchmark. This challenge is based on a newly collected validation and testing image datasets, and hence, named SIDD+. This chall&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.04117v1-abstract-full').style.display = 'inline'; document.getElementById('2005.04117v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2005.04117v1-abstract-full" style="display: none;">
        This paper reviews the NTIRE 2020 challenge on real image denoising with focus on the newly introduced dataset, the proposed methods and their results. The challenge is a new version of the previous NTIRE 2019 challenge on real image denoising that was based on the SIDD benchmark. This challenge is based on a newly collected validation and testing image datasets, and hence, named SIDD+. This challenge has two tracks for quantitatively evaluating image denoising performance in (1) the Bayer-pattern rawRGB and (2) the standard RGB (sRGB) color spaces. Each track ~250 registered participants. A total of 22 teams, proposing 24 methods, competed in the final phase of the challenge. The proposed methods by the participating teams represent the current state-of-the-art performance in image denoising targeting real noisy images. The newly collected SIDD+ datasets are publicly available at: https://bit.ly/siddplus_data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.04117v1-abstract-full').style.display = 'none'; document.getElementById('2005.04117v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 May, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2004.07447">arXiv:2004.07447</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2004.07447">pdf</a>, <a href="https://arxiv.org/ps/2004.07447">ps</a>, <a href="https://arxiv.org/format/2004.07447">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Science and Game Theory">cs.GT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Data Structures and Algorithms">cs.DS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Resolving the Optimal Metric Distortion Conjecture
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gkatzelis%2C+V">Vasilis Gkatzelis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Halpern%2C+D">Daniel Halpern</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Nisarg Shah</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2004.07447v2-abstract-short" style="display: inline;">
        We study the following metric distortion problem: there are two finite sets of points, $V$ and $C$, that lie in the same metric space, and our goal is to choose a point in $C$ whose total distance from the points in $V$ is as small as possible. However, rather than having access to the underlying distance metric, we only know, for each point in $V$, a ranking of its distances to the points in $C$.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.07447v2-abstract-full').style.display = 'inline'; document.getElementById('2004.07447v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2004.07447v2-abstract-full" style="display: none;">
        We study the following metric distortion problem: there are two finite sets of points, $V$ and $C$, that lie in the same metric space, and our goal is to choose a point in $C$ whose total distance from the points in $V$ is as small as possible. However, rather than having access to the underlying distance metric, we only know, for each point in $V$, a ranking of its distances to the points in $C$. We propose algorithms that choose a point in $C$ using only these rankings as input and we provide bounds on their \emph{distortion} (worst-case approximation ratio). A prominent motivation for this problem comes from voting theory, where $V$ represents a set of voters, $C$ represents a set of candidates, and the rankings correspond to ordinal preferences of the voters. A major conjecture in this framework is that the optimal deterministic algorithm has distortion $3$. We resolve this conjecture by providing a polynomial-time algorithm that achieves distortion $3$, matching a known lower bound. We do so by proving a novel lemma about matching voters to candidates, which we refer to as the \emph{ranking-matching lemma}. This lemma induces a family of novel algorithms, which may be of independent interest, and we show that a special algorithm in this family achieves distortion $3$. We also provide more refined, parameterized, bounds using the notion of $α$-decisiveness, which quantifies the extent to which a voter may prefer her top choice relative to all others. Finally, we introduce a new randomized algorithm with improved distortion compared to known results, and also provide improved lower bounds on the distortion of all deterministic and randomized algorithms.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.07447v2-abstract-full').style.display = 'none'; document.getElementById('2004.07447v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 September, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 April, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">An extended abstract of this paper appears in the Proceedings of FOCS 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2003.00606">arXiv:2003.00606</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2003.00606">pdf</a>, <a href="https://arxiv.org/ps/2003.00606">ps</a>, <a href="https://arxiv.org/format/2003.00606">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Science and Game Theory">cs.GT</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Participatory Budgeting: Models and Approaches
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Aziz%2C+H">Haris Aziz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Nisarg Shah</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2003.00606v1-abstract-short" style="display: inline;">
        Participatory budgeting is a democratic approach to deciding the funding of public projects, which has been adopted in many cities across the world. We present a survey of research on participatory budgeting emerging from the computational social choice literature, which draws ideas from computer science and microeconomic theory. We present a mathematical model for participatory budgeting, which c&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.00606v1-abstract-full').style.display = 'inline'; document.getElementById('2003.00606v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2003.00606v1-abstract-full" style="display: none;">
        Participatory budgeting is a democratic approach to deciding the funding of public projects, which has been adopted in many cities across the world. We present a survey of research on participatory budgeting emerging from the computational social choice literature, which draws ideas from computer science and microeconomic theory. We present a mathematical model for participatory budgeting, which charts existing models across different axes including whether the projects are treated as &#34;divisible&#34; or &#34;indivisible&#34; and whether there are funding limits on individual projects. We then survey various approaches and methods from the literature, giving special emphasis on issues of preference elicitation, welfare objectives, fairness axioms, and voter incentives. Finally, we discuss several directions in which research on participatory budgeting can be extended in the future.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.00606v1-abstract-full').style.display = 'none'; document.getElementById('2003.00606v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 March, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2020.
      
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          91A12; 68Q15
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          F.2; J.4
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1805.10693">arXiv:1805.10693</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1805.10693">pdf</a>, <a href="https://arxiv.org/format/1805.10693">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Science and Game Theory">cs.GT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Strategyproof Linear Regression in High Dimensions
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yiling Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Podimata%2C+C">Chara Podimata</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Procaccia%2C+A+D">Ariel D. Procaccia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Nisarg Shah</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1805.10693v1-abstract-short" style="display: inline;">
        This paper is part of an emerging line of work at the intersection of machine learning and mechanism design, which aims to avoid noise in training data by correctly aligning the incentives of data sources. Specifically, we focus on the ubiquitous problem of linear regression, where strategyproof mechanisms have previously been identified in two dimensions. In our setting, agents have single-peaked&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1805.10693v1-abstract-full').style.display = 'inline'; document.getElementById('1805.10693v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1805.10693v1-abstract-full" style="display: none;">
        This paper is part of an emerging line of work at the intersection of machine learning and mechanism design, which aims to avoid noise in training data by correctly aligning the incentives of data sources. Specifically, we focus on the ubiquitous problem of linear regression, where strategyproof mechanisms have previously been identified in two dimensions. In our setting, agents have single-peaked preferences and can manipulate only their response variables. Our main contribution is the discovery of a family of group strategyproof linear regression mechanisms in any number of dimensions, which we call generalized resistant hyperplane mechanisms. The game-theoretic properties of these mechanisms -- and, in fact, their very existence -- are established through a connection to a discrete version of the Ham Sandwich Theorem.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1805.10693v1-abstract-full').style.display = 'none'; document.getElementById('1805.10693v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 May, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">In the Proceedings of the 19th ACM Conference on Economics and Computation (EC), 2018 (to appear)</span>
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?query=Nisarg+Shah&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?query=Nisarg+Shah&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?query=Nisarg+Shah&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
      
    </ul>
  </nav>
  

  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>