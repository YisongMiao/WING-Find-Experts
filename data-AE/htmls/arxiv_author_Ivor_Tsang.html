<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;50 of 166 results for author: <span class="mathjax">Ivor Tsang</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/"  aria-role="search">
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Ivor Tsang">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Ivor+Tsang&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Ivor Tsang">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?query=Ivor+Tsang&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?query=Ivor+Tsang&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?query=Ivor+Tsang&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
        <li>
          <a href="/search/?query=Ivor+Tsang&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=100"
            class="pagination-link "
            aria-label="Page 3"
            aria-current="page">3
          </a>
        </li>
        
        <li>
          <a href="/search/?query=Ivor+Tsang&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=150"
            class="pagination-link "
            aria-label="Page 4"
            aria-current="page">4
          </a>
        </li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.14437">arXiv:2508.14437</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.14437">pdf</a>, <a href="https://arxiv.org/ps/2508.14437">ps</a>, <a href="https://arxiv.org/format/2508.14437">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FOCUS: Frequency-Optimized Conditioning of DiffUSion Models for mitigating catastrophic forgetting during Test-Time Adaptation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tjio%2C+G">Gabriel Tjio</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+J">Jie Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+X">Xulei Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xing%2C+Y">Yun Xing</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chung%2C+N">Nhat Chung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+X">Xiaofeng Cao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I+W">Ivor W. Tsang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kwoh%2C+C+K">Chee Keong Kwoh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Q">Qing Guo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.14437v1-abstract-short" style="display: inline;">
        Test-time adaptation enables models to adapt to evolving domains. However, balancing the tradeoff between preserving knowledge and adapting to domain shifts remains challenging for model adaptation methods, since adapting to domain shifts can induce forgetting of task-relevant knowledge. To address this problem, we propose FOCUS, a novel frequency-based conditioning approach within a diffusion-dri&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.14437v1-abstract-full').style.display = 'inline'; document.getElementById('2508.14437v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.14437v1-abstract-full" style="display: none;">
        Test-time adaptation enables models to adapt to evolving domains. However, balancing the tradeoff between preserving knowledge and adapting to domain shifts remains challenging for model adaptation methods, since adapting to domain shifts can induce forgetting of task-relevant knowledge. To address this problem, we propose FOCUS, a novel frequency-based conditioning approach within a diffusion-driven input-adaptation framework. Utilising learned, spatially adaptive frequency priors, our approach conditions the reverse steps during diffusion-driven denoising to preserve task-relevant semantic information for dense prediction.
  FOCUS leverages a trained, lightweight, Y-shaped Frequency Prediction Network (Y-FPN) that disentangles high and low frequency information from noisy images. This minimizes the computational costs involved in implementing our approach in a diffusion-driven framework. We train Y-FPN with FrequencyMix, a novel data augmentation method that perturbs the images across diverse frequency bands, which improves the robustness of our approach to diverse corruptions.
  We demonstrate the effectiveness of FOCUS for semantic segmentation and monocular depth estimation across 15 corruption types and three datasets, achieving state-of-the-art averaged performance. In addition to improving standalone performance, FOCUS complements existing model adaptation methods since we can derive pseudo labels from FOCUS-denoised images for additional supervision. Even under limited, intermittent supervision with the pseudo labels derived from the FOCUS denoised images, we show that FOCUS mitigates catastrophic forgetting for recent model adaptation methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.14437v1-abstract-full').style.display = 'none'; document.getElementById('2508.14437v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.01784">arXiv:2508.01784</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.01784">pdf</a>, <a href="https://arxiv.org/ps/2508.01784">ps</a>, <a href="https://arxiv.org/format/2508.01784">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Emerging Technologies">cs.ET</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        RouteMark: A Fingerprint for Intellectual Property Attribution in Routing-based Model Merging
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=He%2C+X">Xin He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shen%2C+J">Junxi Shen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+Z">Zhenheng Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chu%2C+X">Xiaowen Chu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+B">Bo Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I+W">Ivor W. Tsang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ong%2C+Y">Yew-Soon Ong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.01784v1-abstract-short" style="display: inline;">
        Model merging via Mixture-of-Experts (MoE) has emerged as a scalable solution for consolidating multiple task-specific models into a unified sparse architecture, where each expert is derived from a model fine-tuned on a distinct task. While effective for multi-task integration, this paradigm introduces a critical yet underexplored challenge: how to attribute and protect the intellectual property (&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.01784v1-abstract-full').style.display = 'inline'; document.getElementById('2508.01784v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.01784v1-abstract-full" style="display: none;">
        Model merging via Mixture-of-Experts (MoE) has emerged as a scalable solution for consolidating multiple task-specific models into a unified sparse architecture, where each expert is derived from a model fine-tuned on a distinct task. While effective for multi-task integration, this paradigm introduces a critical yet underexplored challenge: how to attribute and protect the intellectual property (IP) of individual experts after merging. We propose RouteMark, a framework for IP protection in merged MoE models through the design of expert routing fingerprints. Our key insight is that task-specific experts exhibit stable and distinctive routing behaviors under probing inputs. To capture these patterns, we construct expert-level fingerprints using two complementary statistics: the Routing Score Fingerprint (RSF), quantifying the intensity of expert activation, and the Routing Preference Fingerprint (RPF), characterizing the input distribution that preferentially activates each expert. These fingerprints are reproducible, task-discriminative, and lightweight to construct. For attribution and tampering detection, we introduce a similarity-based matching algorithm that compares expert fingerprints between a suspect and a reference (victim) model. Extensive experiments across diverse tasks and CLIP-based MoE architectures show that RouteMark consistently yields high similarity for reused experts and clear separation from unrelated ones. Moreover, it remains robust against both structural tampering (expert replacement, addition, deletion) and parametric tampering (fine-tuning, pruning, permutation), outperforming weight- and activation-based baseliness. Our work lays the foundation for RouteMark as a practical and broadly applicable framework for IP verification in MoE-based model merging.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.01784v1-abstract-full').style.display = 'none'; document.getElementById('2508.01784v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">MoE, Model Merging, Fingerprint</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.00395">arXiv:2508.00395</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.00395">pdf</a>, <a href="https://arxiv.org/ps/2508.00395">ps</a>, <a href="https://arxiv.org/format/2508.00395">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Decouple before Align: Visual Disentanglement Enhances Prompt Tuning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+F">Fei Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+T">Tianfei Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yao%2C+J">Jiangchao Yao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Ya Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I+W">Ivor W. Tsang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yanfeng Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.00395v1-abstract-short" style="display: inline;">
        Prompt tuning (PT), as an emerging resource-efficient fine-tuning paradigm, has showcased remarkable effectiveness in improving the task-specific transferability of vision-language models. This paper delves into a previously overlooked information asymmetry issue in PT, where the visual modality mostly conveys more context than the object-oriented textual modality. Correspondingly, coarsely aligni&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.00395v1-abstract-full').style.display = 'inline'; document.getElementById('2508.00395v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.00395v1-abstract-full" style="display: none;">
        Prompt tuning (PT), as an emerging resource-efficient fine-tuning paradigm, has showcased remarkable effectiveness in improving the task-specific transferability of vision-language models. This paper delves into a previously overlooked information asymmetry issue in PT, where the visual modality mostly conveys more context than the object-oriented textual modality. Correspondingly, coarsely aligning these two modalities could result in the biased attention, driving the model to merely focus on the context area. To address this, we propose DAPT, an effective PT framework based on an intuitive decouple-before-align concept. First, we propose to explicitly decouple the visual modality into the foreground and background representation via exploiting coarse-and-fine visual segmenting cues, and then both of these decoupled patterns are aligned with the original foreground texts and the hand-crafted background classes, thereby symmetrically strengthening the modal alignment. To further enhance the visual concentration, we propose a visual pull-push regularization tailored for the foreground-background patterns, directing the original visual representation towards unbiased attention on the region-of-interest object. We demonstrate the power of architecture-free DAPT through few-shot learning, base-to-novel generalization, and data-efficient learning, all of which yield superior performance across prevailing benchmarks. Our code will be released at https://github.com/Ferenas/DAPT.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.00395v1-abstract-full').style.display = 'none'; document.getElementById('2508.00395v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">16 pages, Accepted at IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.06261">arXiv:2507.06261</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.06261">pdf</a>, <a href="https://arxiv.org/ps/2507.06261">ps</a>, <a href="https://arxiv.org/format/2507.06261">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Comanici%2C+G">Gheorghe Comanici</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bieber%2C+E">Eric Bieber</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schaekermann%2C+M">Mike Schaekermann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pasupat%2C+I">Ice Pasupat</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sachdeva%2C+N">Noveen Sachdeva</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dhillon%2C+I">Inderjit Dhillon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Blistein%2C+M">Marcel Blistein</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ram%2C+O">Ori Ram</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+D">Dan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rosen%2C+E">Evan Rosen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Marris%2C+L">Luke Marris</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Petulla%2C+S">Sam Petulla</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gaffney%2C+C">Colin Gaffney</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aharoni%2C+A">Asaf Aharoni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lintz%2C+N">Nathan Lintz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pais%2C+T+C">Tiago Cardal Pais</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jacobsson%2C+H">Henrik Jacobsson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Szpektor%2C+I">Idan Szpektor</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+N">Nan-Jiang Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Haridasan%2C+K">Krishna Haridasan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Omran%2C+A">Ahmed Omran</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Saunshi%2C+N">Nikunj Saunshi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bahri%2C+D">Dara Bahri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mishra%2C+G">Gaurav Mishra</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chu%2C+E">Eric Chu</a>
      , et al. (3284 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.06261v4-abstract-short" style="display: inline;">
        In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. In addition to its incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that excels at multimodal unde&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.06261v4-abstract-full').style.display = 'inline'; document.getElementById('2507.06261v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.06261v4-abstract-full" style="display: none;">
        In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. In addition to its incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that excels at multimodal understanding and it is now able to process up to 3 hours of video content. Its unique combination of long context, multimodal and reasoning capabilities can be combined to unlock new agentic workflows. Gemini 2.5 Flash provides excellent reasoning abilities at a fraction of the compute and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high performance at low latency and cost. Taken together, the Gemini 2.X model generation spans the full Pareto frontier of model capability vs cost, allowing users to explore the boundaries of what is possible with complex agentic problem solving.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.06261v4-abstract-full').style.display = 'none'; document.getElementById('2507.06261v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 July, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 July, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">72 pages, 17 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.23461">arXiv:2506.23461</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.23461">pdf</a>, <a href="https://arxiv.org/ps/2506.23461">ps</a>, <a href="https://arxiv.org/format/2506.23461">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Time-variant Image Inpainting via Interactive Distribution Transition Estimation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xing%2C+Y">Yun Xing</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Q">Qing Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xiaoguang Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+Y">Yihao Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+X">Xiaofeng Cao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+D">Di Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I">Ivor Tsang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+L">Lei Ma</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.23461v1-abstract-short" style="display: inline;">
        In this work, we focus on a novel and practical task, i.e., Time-vAriant iMage inPainting (TAMP). The aim of TAMP is to restore a damaged target image by leveraging the complementary information from a reference image, where both images captured the same scene but with a significant time gap in between, i.e., time-variant images. Different from conventional reference-guided image inpainting, the r&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.23461v1-abstract-full').style.display = 'inline'; document.getElementById('2506.23461v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.23461v1-abstract-full" style="display: none;">
        In this work, we focus on a novel and practical task, i.e., Time-vAriant iMage inPainting (TAMP). The aim of TAMP is to restore a damaged target image by leveraging the complementary information from a reference image, where both images captured the same scene but with a significant time gap in between, i.e., time-variant images. Different from conventional reference-guided image inpainting, the reference image under TAMP setup presents significant content distinction to the target image and potentially also suffers from damages. Such an application frequently happens in our daily lives to restore a damaged image by referring to another reference image, where there is no guarantee of the reference image&#39;s source and quality. In particular, our study finds that even state-of-the-art (SOTA) reference-guided image inpainting methods fail to achieve plausible results due to the chaotic image complementation. To address such an ill-posed problem, we propose a novel Interactive Distribution Transition Estimation (InDiTE) module which interactively complements the time-variant images with adaptive semantics thus facilitate the restoration of damaged regions. To further boost the performance, we propose our TAMP solution, namely Interactive Distribution Transition Estimation-driven Diffusion (InDiTE-Diff), which integrates InDiTE with SOTA diffusion model and conducts latent cross-reference during sampling. Moreover, considering the lack of benchmarks for TAMP task, we newly assembled a dataset, i.e., TAMP-Street, based on existing image and mask datasets. We conduct experiments on the TAMP-Street datasets under two different time-variant image inpainting settings, which show our method consistently outperform SOTA reference-guided image inpainting methods for solving TAMP.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.23461v1-abstract-full').style.display = 'none'; document.getElementById('2506.23461v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.23275">arXiv:2506.23275</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.23275">pdf</a>, <a href="https://arxiv.org/ps/2506.23275">ps</a>, <a href="https://arxiv.org/format/2506.23275">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Why Settle for One? Text-to-ImageSet Generation and Evaluation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Jia%2C+C">Chengyou Jia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shen%2C+X">Xin Shen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dang%2C+Z">Zhuohang Dang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dang%2C+Z">Zhuohang Dang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xia%2C+C">Changliang Xia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+W">Weijia Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+X">Xinyu Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qian%2C+H">Hangwei Qian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I+W">Ivor W. Tsang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+M">Minnan Luo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.23275v1-abstract-short" style="display: inline;">
        Despite remarkable progress in Text-to-Image models, many real-world applications require generating coherent image sets with diverse consistency requirements. Existing consistent methods often focus on a specific domain with specific aspects of consistency, which significantly constrains their generalizability to broader applications. In this paper, we propose a more challenging problem, Text-to-&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.23275v1-abstract-full').style.display = 'inline'; document.getElementById('2506.23275v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.23275v1-abstract-full" style="display: none;">
        Despite remarkable progress in Text-to-Image models, many real-world applications require generating coherent image sets with diverse consistency requirements. Existing consistent methods often focus on a specific domain with specific aspects of consistency, which significantly constrains their generalizability to broader applications. In this paper, we propose a more challenging problem, Text-to-ImageSet (T2IS) generation, which aims to generate sets of images that meet various consistency requirements based on user instructions. To systematically study this problem, we first introduce $\textbf{T2IS-Bench}$ with 596 diverse instructions across 26 subcategories, providing comprehensive coverage for T2IS generation. Building on this, we propose $\textbf{T2IS-Eval}$, an evaluation framework that transforms user instructions into multifaceted assessment criteria and employs effective evaluators to adaptively assess consistency fulfillment between criteria and generated sets. Subsequently, we propose $\textbf{AutoT2IS}$, a training-free framework that maximally leverages pretrained Diffusion Transformers&#39; in-context capabilities to harmonize visual elements to satisfy both image-level prompt alignment and set-level visual consistency. Extensive experiments on T2IS-Bench reveal that diverse consistency challenges all existing methods, while our AutoT2IS significantly outperforms current generalized and even specialized approaches. Our method also demonstrates the ability to enable numerous underexplored real-world applications, confirming its substantial practical value. Visit our project in https://chengyou-jia.github.io/T2IS-Home.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.23275v1-abstract-full').style.display = 'none'; document.getElementById('2506.23275v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.20307">arXiv:2506.20307</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.20307">pdf</a>, <a href="https://arxiv.org/ps/2506.20307">ps</a>, <a href="https://arxiv.org/format/2506.20307">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Beyond-Expert Performance with Limited Demonstrations: Efficient Imitation Learning with Double Exploration
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+H">Heyang Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+X">Xingrui Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bossens%2C+D+M">David M. Bossens</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I+W">Ivor W. Tsang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gu%2C+Q">Quanquan Gu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.20307v1-abstract-short" style="display: inline;">
        Imitation learning is a central problem in reinforcement learning where the goal is to learn a policy that mimics the expert&#39;s behavior. In practice, it is often challenging to learn the expert policy from a limited number of demonstrations accurately due to the complexity of the state space. Moreover, it is essential to explore the environment and collect data to achieve beyond-expert performance&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.20307v1-abstract-full').style.display = 'inline'; document.getElementById('2506.20307v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.20307v1-abstract-full" style="display: none;">
        Imitation learning is a central problem in reinforcement learning where the goal is to learn a policy that mimics the expert&#39;s behavior. In practice, it is often challenging to learn the expert policy from a limited number of demonstrations accurately due to the complexity of the state space. Moreover, it is essential to explore the environment and collect data to achieve beyond-expert performance. To overcome these challenges, we propose a novel imitation learning algorithm called Imitation Learning with Double Exploration (ILDE), which implements exploration in two aspects: (1) optimistic policy optimization via an exploration bonus that rewards state-action pairs with high uncertainty to potentially improve the convergence to the expert policy, and (2) curiosity-driven exploration of the states that deviate from the demonstration trajectories to potentially yield beyond-expert performance. Empirically, we demonstrate that ILDE outperforms the state-of-the-art imitation learning algorithms in terms of sample efficiency and achieves beyond-expert performance on Atari and MuJoCo tasks with fewer demonstrations than in previous work. We also provide a theoretical justification of ILDE as an uncertainty-regularized policy optimization method with optimistic exploration, leading to a regret growing sublinearly in the number of episodes.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.20307v1-abstract-full').style.display = 'none'; document.getElementById('2506.20307v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.16690">arXiv:2506.16690</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.16690">pdf</a>, <a href="https://arxiv.org/ps/2506.16690">ps</a>, <a href="https://arxiv.org/format/2506.16690">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DepthVanish: Optimizing Adversarial Interval Structures for Stereo-Depth-Invisible Patches
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xing%2C+Y">Yun Xing</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+Y">Yue Cao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chung%2C+N">Nhat Chung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+J">Jie Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I">Ivor Tsang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+M">Ming-Ming Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+L">Lei Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Q">Qing Guo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.16690v1-abstract-short" style="display: inline;">
        Stereo Depth estimation is a critical task in autonomous driving and robotics, where inaccuracies (such as misidentifying nearby objects as distant) can lead to dangerous situations. Adversarial attacks against stereo depth estimation can help reveal vulnerabilities before deployment. Previous work has shown that repeating optimized textures can effectively mislead stereo depth estimation in digit&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.16690v1-abstract-full').style.display = 'inline'; document.getElementById('2506.16690v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.16690v1-abstract-full" style="display: none;">
        Stereo Depth estimation is a critical task in autonomous driving and robotics, where inaccuracies (such as misidentifying nearby objects as distant) can lead to dangerous situations. Adversarial attacks against stereo depth estimation can help reveal vulnerabilities before deployment. Previous work has shown that repeating optimized textures can effectively mislead stereo depth estimation in digital settings. However, our research reveals that these naively repeated texture structures perform poorly in physical-world implementations, i.e., when deployed as patches, limiting their practical utility for testing stereo depth estimation systems. In this work, for the first time, we discover that introducing regular intervals between repeated textures, creating a striped structure, significantly enhances the patch attack effectiveness. Through extensive experimentation, we analyze how variations of this novel structure influence the performance. Based on these insights, we develop a novel stereo depth attack that jointly optimizes both the striped structure and texture elements. Our generated adversarial patches can be inserted into any scenes and successfully attack state-of-the-art stereo depth estimation methods, i.e., RAFT-Stereo and STTR. Most critically, our patch can also attack commercial RGB-D cameras (Intel RealSense) in real-world conditions, demonstrating their practical relevance for security assessment of stereo systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.16690v1-abstract-full').style.display = 'none'; document.getElementById('2506.16690v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.09538">arXiv:2506.09538</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.09538">pdf</a>, <a href="https://arxiv.org/ps/2506.09538">ps</a>, <a href="https://arxiv.org/format/2506.09538">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AngleRoCL: Angle-Robust Concept Learning for Physically View-Invariant T2I Adversarial Patches
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ji%2C+W">Wenjun Ji</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fu%2C+Y">Yuxiang Fu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ying%2C+L">Luyang Ying</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fan%2C+D">Deng-Ping Fan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yuyi Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+M">Ming-Ming Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I">Ivor Tsang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Q">Qing Guo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.09538v1-abstract-short" style="display: inline;">
        Cutting-edge works have demonstrated that text-to-image (T2I) diffusion models can generate adversarial patches that mislead state-of-the-art object detectors in the physical world, revealing detectors&#39; vulnerabilities and risks. However, these methods neglect the T2I patches&#39; attack effectiveness when observed from different views in the physical world (i.e., angle robustness of the T2I adversari&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.09538v1-abstract-full').style.display = 'inline'; document.getElementById('2506.09538v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.09538v1-abstract-full" style="display: none;">
        Cutting-edge works have demonstrated that text-to-image (T2I) diffusion models can generate adversarial patches that mislead state-of-the-art object detectors in the physical world, revealing detectors&#39; vulnerabilities and risks. However, these methods neglect the T2I patches&#39; attack effectiveness when observed from different views in the physical world (i.e., angle robustness of the T2I adversarial patches). In this paper, we study the angle robustness of T2I adversarial patches comprehensively, revealing their angle-robust issues, demonstrating that texts affect the angle robustness of generated patches significantly, and task-specific linguistic instructions fail to enhance the angle robustness. Motivated by the studies, we introduce Angle-Robust Concept Learning (AngleRoCL), a simple and flexible approach that learns a generalizable concept (i.e., text embeddings in implementation) representing the capability of generating angle-robust patches. The learned concept can be incorporated into textual prompts and guides T2I models to generate patches with their attack effectiveness inherently resistant to viewpoint variations. Through extensive simulation and physical-world experiments on five SOTA detectors across multiple views, we demonstrate that AngleRoCL significantly enhances the angle robustness of T2I adversarial patches compared to baseline methods. Our patches maintain high attack success rates even under challenging viewing conditions, with over 50% average relative improvement in attack effectiveness across multiple angles. This research advances the understanding of physically angle-robust patches and provides insights into the relationship between textual concepts and physical properties in T2I-generated contents.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.09538v1-abstract-full').style.display = 'none'; document.getElementById('2506.09538v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.09034">arXiv:2506.09034</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.09034">pdf</a>, <a href="https://arxiv.org/ps/2506.09034">ps</a>, <a href="https://arxiv.org/format/2506.09034">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FZOO: Fast Zeroth-Order Optimizer for Fine-Tuning Large Language Models towards Adam-Scale Speed
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dang%2C+S">Sizhe Dang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Y">Yangyang Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+Y">Yanjun Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ye%2C+H">Haishan Ye</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+X">Xiaodong Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+G">Guang Dai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I">Ivor Tsang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.09034v2-abstract-short" style="display: inline;">
        Fine-tuning large language models (LLMs) often faces GPU memory bottlenecks: the backward pass of first-order optimizers like Adam increases memory usage to more than 10 times the inference level (e.g., 633 GB for OPT-30B). Zeroth-order (ZO) optimizers avoid this cost by estimating gradients only from forward passes, yet existing methods like MeZO usually require many more steps to converge. Can t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.09034v2-abstract-full').style.display = 'inline'; document.getElementById('2506.09034v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.09034v2-abstract-full" style="display: none;">
        Fine-tuning large language models (LLMs) often faces GPU memory bottlenecks: the backward pass of first-order optimizers like Adam increases memory usage to more than 10 times the inference level (e.g., 633 GB for OPT-30B). Zeroth-order (ZO) optimizers avoid this cost by estimating gradients only from forward passes, yet existing methods like MeZO usually require many more steps to converge. Can this trade-off between speed and memory in ZO be fundamentally improved? Normalized-SGD demonstrates strong empirical performance with greater memory efficiency than Adam. In light of this, we introduce FZOO, a Fast Zeroth-Order Optimizer toward Adam-Scale Speed. FZOO reduces the total forward passes needed for convergence by employing batched one-sided estimates that adapt step sizes based on the standard deviation of batch losses. It also accelerates per-batch computation through the use of Rademacher random vector perturbations coupled with CUDA&#39;s parallel processing. Extensive experiments on diverse models, including RoBERTa-large, OPT (350M-66B), Phi-2, and Llama3, across 11 tasks validate FZOO&#39;s effectiveness. On average, FZOO outperforms MeZO by 3 percent in accuracy while requiring 3 times fewer forward passes. For RoBERTa-large, FZOO achieves average improvements of 5.6 percent in accuracy and an 18 times reduction in forward passes compared to MeZO, achieving convergence speeds comparable to Adam. We also provide theoretical analysis proving FZOO&#39;s formal equivalence to a normalized-SGD update rule and its convergence guarantees. FZOO integrates smoothly into PEFT techniques, enabling even larger memory savings. Overall, our results make single-GPU, high-speed, full-parameter fine-tuning practical and point toward future work on memory-efficient pre-training.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.09034v2-abstract-full').style.display = 'none'; document.getElementById('2506.09034v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 June, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 10 June, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.07351">arXiv:2506.07351</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.07351">pdf</a>, <a href="https://arxiv.org/ps/2506.07351">ps</a>, <a href="https://arxiv.org/format/2506.07351">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Systems and Control">eess.SY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Decentralized Optimization on Compact Submanifolds by Quantized Riemannian Gradient Tracking
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+J">Jun Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+L">Lina Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+T">Tianyi Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yong Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+G">Guang Dai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+Y">Yunliang Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I+W">Ivor W. Tsang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.07351v1-abstract-short" style="display: inline;">
        This paper considers the problem of decentralized optimization on compact submanifolds, where a finite sum of smooth (possibly non-convex) local functions is minimized by $n$ agents forming an undirected and connected graph. However, the efficiency of distributed optimization is often hindered by communication bottlenecks. To mitigate this, we propose the Quantized Riemannian Gradient Tracking (Q-&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.07351v1-abstract-full').style.display = 'inline'; document.getElementById('2506.07351v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.07351v1-abstract-full" style="display: none;">
        This paper considers the problem of decentralized optimization on compact submanifolds, where a finite sum of smooth (possibly non-convex) local functions is minimized by $n$ agents forming an undirected and connected graph. However, the efficiency of distributed optimization is often hindered by communication bottlenecks. To mitigate this, we propose the Quantized Riemannian Gradient Tracking (Q-RGT) algorithm, where agents update their local variables using quantized gradients. The introduction of quantization noise allows our algorithm to bypass the constraints of the accurate Riemannian projection operator (such as retraction), further improving iterative efficiency. To the best of our knowledge, this is the first algorithm to achieve an $\mathcal{O}(1/K)$ convergence rate in the presence of quantization, matching the convergence rate of methods without quantization. Additionally, we explicitly derive lower bounds on decentralized consensus associated with a function of quantization levels. Numerical experiments demonstrate that Q-RGT performs comparably to non-quantized methods while reducing communication bottlenecks and computational overhead.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.07351v1-abstract-full').style.display = 'none'; document.getElementById('2506.07351v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.02711">arXiv:2506.02711</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.02711">pdf</a>, <a href="https://arxiv.org/format/2506.02711">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Privacy Leaks by Adversaries: Adversarial Iterations for Membership Inference Attack
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xue%2C+J">Jing Xue</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+Z">Zhishen Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ye%2C+H">Haishan Ye</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+L">Luo Luo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+X">Xiangyu Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I">Ivor Tsang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+G">Guang Dai</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.02711v1-abstract-short" style="display: inline;">
        Membership inference attack (MIA) has become one of the most widely used and effective methods for evaluating the privacy risks of machine learning models. These attacks aim to determine whether a specific sample is part of the model&#39;s training set by analyzing the model&#39;s output. While traditional membership inference attacks focus on leveraging the model&#39;s posterior output, such as confidence on&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.02711v1-abstract-full').style.display = 'inline'; document.getElementById('2506.02711v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.02711v1-abstract-full" style="display: none;">
        Membership inference attack (MIA) has become one of the most widely used and effective methods for evaluating the privacy risks of machine learning models. These attacks aim to determine whether a specific sample is part of the model&#39;s training set by analyzing the model&#39;s output. While traditional membership inference attacks focus on leveraging the model&#39;s posterior output, such as confidence on the target sample, we propose IMIA, a novel attack strategy that utilizes the process of generating adversarial samples to infer membership. We propose to infer the member properties of the target sample using the number of iterations required to generate its adversarial sample. We conduct experiments across multiple models and datasets, and our results demonstrate that the number of iterations for generating an adversarial sample is a reliable feature for membership inference, achieving strong performance both in black-box and white-box attack scenarios. This work provides a new perspective for evaluating model privacy and highlights the potential of adversarial example-based features for privacy leakage assessment.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.02711v1-abstract-full').style.display = 'none'; document.getElementById('2506.02711v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.01586">arXiv:2506.01586</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.01586">pdf</a>, <a href="https://arxiv.org/ps/2506.01586">ps</a>, <a href="https://arxiv.org/format/2506.01586">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multi-Modal Dataset Distillation in the Wild
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dang%2C+Z">Zhuohang Dang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+M">Minnan Luo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jia%2C+C">Chengyou Jia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qian%2C+H">Hangwei Qian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+X">Xiaojun Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I+W">Ivor W. Tsang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.01586v1-abstract-short" style="display: inline;">
        Recent multi-modal models have shown remarkable versatility in real-world applications. However, their rapid development encounters two critical data challenges. First, the training process requires large-scale datasets, leading to substantial storage and computational costs. Second, these data are typically web-crawled with inevitable noise, i.e., partially mismatched pairs, severely degrading mo&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.01586v1-abstract-full').style.display = 'inline'; document.getElementById('2506.01586v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.01586v1-abstract-full" style="display: none;">
        Recent multi-modal models have shown remarkable versatility in real-world applications. However, their rapid development encounters two critical data challenges. First, the training process requires large-scale datasets, leading to substantial storage and computational costs. Second, these data are typically web-crawled with inevitable noise, i.e., partially mismatched pairs, severely degrading model performance. To these ends, we propose Multi-modal dataset Distillation in the Wild, i.e., MDW, the first framework to distill noisy multi-modal datasets into compact clean ones for effective and efficient model training. Specifically, MDW introduces learnable fine-grained correspondences during distillation and adaptively optimizes distilled data to emphasize correspondence-discriminative regions, thereby enhancing distilled data&#39;s information density and efficacy. Moreover, to capture robust cross-modal correspondence prior knowledge from real data, MDW proposes dual-track collaborative learning to avoid the risky data noise, alleviating information loss with certifiable noise tolerance. Extensive experiments validate MDW&#39;s theoretical and empirical efficacy with remarkable scalability, surpassing prior methods by over 15% across various compression ratios, highlighting its appealing practicality for applications with diverse efficacy and resource needs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.01586v1-abstract-full').style.display = 'none'; document.getElementById('2506.01586v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2505.22967">arXiv:2505.22967</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2505.22967">pdf</a>, <a href="https://arxiv.org/ps/2505.22967">ps</a>, <a href="https://arxiv.org/format/2505.22967">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multiagent Systems">cs.MA</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MermaidFlow: Redefining Agentic Workflow Generation via Safety-Constrained Evolutionary Programming
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+C">Chengqi Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+J">Jianda Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lyu%2C+Y">Yueming Lyu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ng%2C+W+Z+T">Wen Zheng Terence Ng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+H">Haopeng Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ong%2C+Y">Yew-Soon Ong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I">Ivor Tsang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+H">Haiyan Yin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2505.22967v1-abstract-short" style="display: inline;">
        Despite the promise of autonomous agentic reasoning, existing workflow generation methods frequently produce fragile, unexecutable plans due to unconstrained LLM-driven construction. We introduce MermaidFlow, a framework that redefines the agentic search space through safety-constrained graph evolution. At its core, MermaidFlow represent workflows as a verifiable intermediate representation using&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.22967v1-abstract-full').style.display = 'inline'; document.getElementById('2505.22967v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2505.22967v1-abstract-full" style="display: none;">
        Despite the promise of autonomous agentic reasoning, existing workflow generation methods frequently produce fragile, unexecutable plans due to unconstrained LLM-driven construction. We introduce MermaidFlow, a framework that redefines the agentic search space through safety-constrained graph evolution. At its core, MermaidFlow represent workflows as a verifiable intermediate representation using Mermaid, a structured and human-interpretable graph language. We formulate domain-aware evolutionary operators, i.e., crossover, mutation, insertion, and deletion, to preserve semantic correctness while promoting structural diversity, enabling efficient exploration of a high-quality, statically verifiable workflow space. Without modifying task settings or evaluation protocols, MermaidFlow achieves consistent improvements in success rates and faster convergence to executable plans on the agent reasoning benchmark. The experimental results demonstrate that safety-constrained graph evolution offers a scalable, modular foundation for robust and interpretable agentic reasoning systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.22967v1-abstract-full').style.display = 'none'; document.getElementById('2505.22967v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 May, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2505.14254">arXiv:2505.14254</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2505.14254">pdf</a>, <a href="https://arxiv.org/format/2505.14254">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Instructing Text-to-Image Diffusion Models via Classifier-Guided Semantic Optimization
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+Y">Yuanyuan Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yao%2C+Y">Yinghua Yao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qin%2C+T">Tao Qin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+M">Mengmeng Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I">Ivor Tsang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+G">Guang Dai</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2505.14254v1-abstract-short" style="display: inline;">
        Text-to-image diffusion models have emerged as powerful tools for high-quality image generation and editing. Many existing approaches rely on text prompts as editing guidance. However, these methods are constrained by the need for manual prompt crafting, which can be time-consuming, introduce irrelevant details, and significantly limit editing performance. In this work, we propose optimizing seman&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.14254v1-abstract-full').style.display = 'inline'; document.getElementById('2505.14254v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2505.14254v1-abstract-full" style="display: none;">
        Text-to-image diffusion models have emerged as powerful tools for high-quality image generation and editing. Many existing approaches rely on text prompts as editing guidance. However, these methods are constrained by the need for manual prompt crafting, which can be time-consuming, introduce irrelevant details, and significantly limit editing performance. In this work, we propose optimizing semantic embeddings guided by attribute classifiers to steer text-to-image models toward desired edits, without relying on text prompts or requiring any training or fine-tuning of the diffusion model. We utilize classifiers to learn precise semantic embeddings at the dataset level. The learned embeddings are theoretically justified as the optimal representation of attribute semantics, enabling disentangled and accurate edits. Experiments further demonstrate that our method achieves high levels of disentanglement and strong generalization across different domains of data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.14254v1-abstract-full').style.display = 'none'; document.getElementById('2505.14254v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 May, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2504.08808">arXiv:2504.08808</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2504.08808">pdf</a>, <a href="https://arxiv.org/format/2504.08808">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Exploring the Effectiveness and Interpretability of Texts in LLM-based Time Series Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+Z">Zhengke Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qian%2C+H">Hangwei Qian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I">Ivor Tsang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2504.08808v1-abstract-short" style="display: inline;">
        Large Language Models (LLMs) have been applied to time series forecasting tasks, leveraging pre-trained language models as the backbone and incorporating textual data to purportedly enhance the comprehensive capabilities of LLMs for time series. However, are these texts really helpful for interpretation? This study seeks to investigate the actual efficacy and interpretability of such textual incor&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2504.08808v1-abstract-full').style.display = 'inline'; document.getElementById('2504.08808v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2504.08808v1-abstract-full" style="display: none;">
        Large Language Models (LLMs) have been applied to time series forecasting tasks, leveraging pre-trained language models as the backbone and incorporating textual data to purportedly enhance the comprehensive capabilities of LLMs for time series. However, are these texts really helpful for interpretation? This study seeks to investigate the actual efficacy and interpretability of such textual incorporations. Through a series of empirical experiments on textual prompts and textual prototypes, our findings reveal that the misalignment between two modalities exists, and the textual information does not significantly improve time series forecasting performance in many cases. Furthermore, visualization analysis indicates that the textual representations learned by existing frameworks lack sufficient interpretability when applied to time series data. We further propose a novel metric named Semantic Matching Index (SMI) to better evaluate the matching degree between time series and texts during our post hoc interpretability investigation. Our analysis reveals the misalignment and limited interpretability of texts in current time-series LLMs, and we hope this study can raise awareness of the interpretability of texts for time series. The code is available at https://github.com/zachysun/TS-Lang-Exp.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2504.08808v1-abstract-full').style.display = 'none'; document.getElementById('2504.08808v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 April, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2503.07601">arXiv:2503.07601</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2503.07601">pdf</a>, <a href="https://arxiv.org/ps/2503.07601">ps</a>, <a href="https://arxiv.org/format/2503.07601">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Balanced Image Stylization with Style Matching Score
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+Y">Yuxin Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+L">Liming Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+S">Shuai Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Jia-Wei Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I">Ivor Tsang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shou%2C+M+Z">Mike Zheng Shou</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2503.07601v2-abstract-short" style="display: inline;">
        We present Style Matching Score (SMS), a novel optimization method for image stylization with diffusion models. Balancing effective style transfer with content preservation is a long-standing challenge. Unlike existing efforts, our method reframes image stylization as a style distribution matching problem. The target style distribution is estimated from off-the-shelf style-dependent LoRAs via care&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.07601v2-abstract-full').style.display = 'inline'; document.getElementById('2503.07601v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2503.07601v2-abstract-full" style="display: none;">
        We present Style Matching Score (SMS), a novel optimization method for image stylization with diffusion models. Balancing effective style transfer with content preservation is a long-standing challenge. Unlike existing efforts, our method reframes image stylization as a style distribution matching problem. The target style distribution is estimated from off-the-shelf style-dependent LoRAs via carefully designed score functions. To preserve content information adaptively, we propose Progressive Spectrum Regularization, which operates in the frequency domain to guide stylization progressively from low-frequency layouts to high-frequency details. In addition, we devise a Semantic-Aware Gradient Refinement technique that leverages relevance maps derived from diffusion semantic priors to selectively stylize semantically important regions. The proposed optimization formulation extends stylization from pixel space to parameter space, readily applicable to lightweight feedforward generators for efficient one-step stylization. SMS effectively balances style alignment and content preservation, outperforming state-of-the-art approaches, verified by extensive experiments.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.07601v2-abstract-full').style.display = 'none'; document.getElementById('2503.07601v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 July, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 10 March, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ICCV 2025. Code: https://github.com/showlab/SMS Project page: https://yuxinn-j.github.io/projects/SMS.html</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2503.05246">arXiv:2503.05246</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2503.05246">pdf</a>, <a href="https://arxiv.org/format/2503.05246">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Mastering Continual Reinforcement Learning through Fine-Grained Sparse Network Allocation and Dormant Neuron Exploration
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+C">Chengqi Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+H">Haiyan Yin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+J">Jianda Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ng%2C+T">Terence Ng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ong%2C+Y">Yew-Soon Ong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I">Ivor Tsang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2503.05246v2-abstract-short" style="display: inline;">
        Continual Reinforcement Learning (CRL) is essential for developing agents that can learn, adapt, and accumulate knowledge over time. However, a fundamental challenge persists as agents must strike a delicate balance between plasticity, which enables rapid skill acquisition, and stability, which ensures long-term knowledge retention while preventing catastrophic forgetting. In this paper, we introd&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.05246v2-abstract-full').style.display = 'inline'; document.getElementById('2503.05246v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2503.05246v2-abstract-full" style="display: none;">
        Continual Reinforcement Learning (CRL) is essential for developing agents that can learn, adapt, and accumulate knowledge over time. However, a fundamental challenge persists as agents must strike a delicate balance between plasticity, which enables rapid skill acquisition, and stability, which ensures long-term knowledge retention while preventing catastrophic forgetting. In this paper, we introduce SSDE, a novel structure-based approach that enhances plasticity through a fine-grained allocation strategy with Structured Sparsity and Dormant-guided Exploration. SSDE decomposes the parameter space into forward-transfer (frozen) parameters and task-specific (trainable) parameters. Crucially, these parameters are allocated by an efficient co-allocation scheme under sparse coding, ensuring sufficient trainable capacity for new tasks while promoting efficient forward transfer through frozen parameters. However, structure-based methods often suffer from rigidity due to the accumulation of non-trainable parameters, limiting exploration and adaptability. To address this, we further introduce a sensitivity-guided neuron reactivation mechanism that systematically identifies and resets dormant neurons, which exhibit minimal influence in the sparse policy network during inference. This approach effectively enhance exploration while preserving structural efficiency. Extensive experiments on the CW10-v1 Continual World benchmark demonstrate that SSDE achieves state-of-the-art performance, reaching a success rate of 95%, surpassing prior methods significantly in both plasticity and stability trade-offs (code is available at: https://github.com/chengqiArchy/SSDE).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.05246v2-abstract-full').style.display = 'none'; document.getElementById('2503.05246v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 March, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 March, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2503.00828">arXiv:2503.00828</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2503.00828">pdf</a>, <a href="https://arxiv.org/format/2503.00828">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Training-Free Dataset Pruning for Instance Segmentation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+Y">Yalun Dai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiao%2C+L">Lingao Xiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I+W">Ivor W. Tsang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+Y">Yang He</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2503.00828v1-abstract-short" style="display: inline;">
        Existing dataset pruning techniques primarily focus on classification tasks, limiting their applicability to more complex and practical tasks like instance segmentation. Instance segmentation presents three key challenges: pixel-level annotations, instance area variations, and class imbalances, which significantly complicate dataset pruning efforts. Directly adapting existing classification-based&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.00828v1-abstract-full').style.display = 'inline'; document.getElementById('2503.00828v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2503.00828v1-abstract-full" style="display: none;">
        Existing dataset pruning techniques primarily focus on classification tasks, limiting their applicability to more complex and practical tasks like instance segmentation. Instance segmentation presents three key challenges: pixel-level annotations, instance area variations, and class imbalances, which significantly complicate dataset pruning efforts. Directly adapting existing classification-based pruning methods proves ineffective due to their reliance on time-consuming model training process. To address this, we propose a novel Training-Free Dataset Pruning (TFDP) method for instance segmentation. Specifically, we leverage shape and class information from image annotations to design a Shape Complexity Score (SCS), refining it into a Scale-Invariant (SI-SCS) and Class-Balanced (CB-SCS) versions to address instance area variations and class imbalances, all without requiring model training. We achieve state-of-the-art results on VOC 2012, Cityscapes, and COCO datasets, generalizing well across CNN and Transformer architectures. Remarkably, our approach accelerates the pruning process by an average of 1349$\times$ on COCO compared to the adapted baselines. Source code is available at: https://github.com/he-y/dataset-pruning-for-instance-segmentation
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.00828v1-abstract-full').style.display = 'none'; document.getElementById('2503.00828v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 March, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by ICLR 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2502.01692">arXiv:2502.01692</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2502.01692">pdf</a>, <a href="https://arxiv.org/format/2502.01692">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Fast Direct: Query-Efficient Online Black-box Guidance for Diffusion-model Target Generation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tan%2C+K+Y">Kim Yong Tan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lyu%2C+Y">Yueming Lyu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I">Ivor Tsang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ong%2C+Y">Yew-Soon Ong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2502.01692v5-abstract-short" style="display: inline;">
        Guided diffusion-model generation is a promising direction for customizing the generation process of a pre-trained diffusion model to address specific downstream tasks. Existing guided diffusion models either rely on training the guidance model with pre-collected datasets or require the objective functions to be differentiable. However, for most real-world tasks, offline datasets are often unavail&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.01692v5-abstract-full').style.display = 'inline'; document.getElementById('2502.01692v5-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2502.01692v5-abstract-full" style="display: none;">
        Guided diffusion-model generation is a promising direction for customizing the generation process of a pre-trained diffusion model to address specific downstream tasks. Existing guided diffusion models either rely on training the guidance model with pre-collected datasets or require the objective functions to be differentiable. However, for most real-world tasks, offline datasets are often unavailable, and their objective functions are often not differentiable, such as image generation with human preferences, molecular generation for drug discovery, and material design. Thus, we need an $\textbf{online}$ algorithm capable of collecting data during runtime and supporting a $\textbf{black-box}$ objective function. Moreover, the $\textbf{query efficiency}$ of the algorithm is also critical because the objective evaluation of the query is often expensive in real-world scenarios. In this work, we propose a novel and simple algorithm, $\textbf{Fast Direct}$, for query-efficient online black-box target generation. Our Fast Direct builds a pseudo-target on the data manifold to update the noise sequence of the diffusion model with a universal direction, which is promising to perform query-efficient guided generation. Extensive experiments on twelve high-resolution ($\small {1024 \times 1024}$) image target generation tasks and six 3D-molecule target generation tasks show $\textbf{6}\times$ up to $\textbf{10}\times$ query efficiency improvement and $\textbf{11}\times$ up to $\textbf{44}\times$ query efficiency improvement, respectively. Our implementation is publicly available at: https://github.com/kimyong95/guide-stable-diffusion/tree/fast-direct
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.01692v5-abstract-full').style.display = 'none'; document.getElementById('2502.01692v5-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 March, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 2 February, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2501.08450">arXiv:2501.08450</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2501.08450">pdf</a>, <a href="https://arxiv.org/format/2501.08450">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Active Sampling for Node Attribute Completion on Graphs
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+B">Benyuan Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xu Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yanfeng Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Ya Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+Z">Zhi Cao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I">Ivor Tsang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2501.08450v1-abstract-short" style="display: inline;">
        Node attribute, a type of crucial information for graph analysis, may be partially or completely missing for certain nodes in real world applications. Restoring the missing attributes is expected to benefit downstream graph learning. Few attempts have been made on node attribute completion, but a novel framework called Structure-attribute Transformer (SAT) was recently proposed by using a decouple&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2501.08450v1-abstract-full').style.display = 'inline'; document.getElementById('2501.08450v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2501.08450v1-abstract-full" style="display: none;">
        Node attribute, a type of crucial information for graph analysis, may be partially or completely missing for certain nodes in real world applications. Restoring the missing attributes is expected to benefit downstream graph learning. Few attempts have been made on node attribute completion, but a novel framework called Structure-attribute Transformer (SAT) was recently proposed by using a decoupled scheme to leverage structures and attributes. SAT ignores the differences in contributing to the learning schedule and finding a practical way to model the different importance of nodes with observed attributes is challenging. This paper proposes a novel AcTive Sampling algorithm (ATS) to restore missing node attributes. The representativeness and uncertainty of each node&#39;s information are first measured based on graph structure, representation similarity and learning bias. To select nodes as train samples in the next optimization step, a weighting scheme controlled by Beta distribution is then introduced to linearly combine the two properties. Extensive experiments on four public benchmark datasets and two downstream tasks have shown the superiority of ATS in node attribute completion.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2501.08450v1-abstract-full').style.display = 'none'; document.getElementById('2501.08450v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 January, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2412.08014">arXiv:2412.08014</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2412.08014">pdf</a>, <a href="https://arxiv.org/format/2412.08014">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MAGIC: Mastering Physical Adversarial Generation in Context through Collaborative LLM Agents
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xing%2C+Y">Yun Xing</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chung%2C+N">Nhat Chung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+J">Jie Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+Y">Yue Cao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I">Ivor Tsang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+L">Lei Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Q">Qing Guo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2412.08014v2-abstract-short" style="display: inline;">
        Physical adversarial attacks in driving scenarios can expose critical vulnerabilities in visual perception models. However, developing such attacks remains challenging due to diverse real-world environments and the requirement for maintaining visual naturality. Building upon this challenge, we reformulate physical adversarial attacks as a one-shot patch generation problem. Our approach generates a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.08014v2-abstract-full').style.display = 'inline'; document.getElementById('2412.08014v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2412.08014v2-abstract-full" style="display: none;">
        Physical adversarial attacks in driving scenarios can expose critical vulnerabilities in visual perception models. However, developing such attacks remains challenging due to diverse real-world environments and the requirement for maintaining visual naturality. Building upon this challenge, we reformulate physical adversarial attacks as a one-shot patch generation problem. Our approach generates adversarial patches through a deep generative model that considers the specific scene context, enabling direct physical deployment in matching environments. The primary challenge lies in simultaneously achieving two objectives: generating adversarial patches that effectively mislead object detection systems while determining contextually appropriate deployment within the scene. We propose MAGIC (Mastering Physical Adversarial Generation In Context), a novel framework powered by multi-modal LLM agents to address these challenges. MAGIC automatically understands scene context and generates adversarial patch through the synergistic interaction of language and vision capabilities. In particular, MAGIC orchestrates three specialized LLM agents: The adv-patch generation agent (GAgent) masters the creation of deceptive patches through strategic prompt engineering for text-to-image models. The adv-patch deployment agent (DAgent) ensures contextual coherence by determining optimal deployment strategies based on scene understanding. The self-examination agent (EAgent) completes this trilogy by providing critical oversight and iterative refinement of both processes. We validate our method on both digital and physical levels, i.e., nuImage and manually captured real-world scenes, where both statistical and visual results prove that our MAGIC is powerful and effective for attacking widely applied object detection systems, i.e., YOLO and DETR series.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.08014v2-abstract-full').style.display = 'none'; document.getElementById('2412.08014v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 March, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 10 December, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2412.00114">arXiv:2412.00114</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2412.00114">pdf</a>, <a href="https://arxiv.org/format/2412.00114">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SceneTAP: Scene-Coherent Typographic Adversarial Planner against Vision-Language Models in Real-World Environments
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+Y">Yue Cao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xing%2C+Y">Yun Xing</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+J">Jie Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+D">Di Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+T">Tianwei Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I">Ivor Tsang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Q">Qing Guo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2412.00114v2-abstract-short" style="display: inline;">
        Large vision-language models (LVLMs) have shown remarkable capabilities in interpreting visual content. While existing works demonstrate these models&#39; vulnerability to deliberately placed adversarial texts, such texts are often easily identifiable as anomalous. In this paper, we present the first approach to generate scene-coherent typographic adversarial attacks that mislead advanced LVLMs while&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.00114v2-abstract-full').style.display = 'inline'; document.getElementById('2412.00114v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2412.00114v2-abstract-full" style="display: none;">
        Large vision-language models (LVLMs) have shown remarkable capabilities in interpreting visual content. While existing works demonstrate these models&#39; vulnerability to deliberately placed adversarial texts, such texts are often easily identifiable as anomalous. In this paper, we present the first approach to generate scene-coherent typographic adversarial attacks that mislead advanced LVLMs while maintaining visual naturalness through the capability of the LLM-based agent. Our approach addresses three critical questions: what adversarial text to generate, where to place it within the scene, and how to integrate it seamlessly. We propose a training-free, multi-modal LLM-driven scene-coherent typographic adversarial planning (SceneTAP) that employs a three-stage process: scene understanding, adversarial planning, and seamless integration. The SceneTAP utilizes chain-of-thought reasoning to comprehend the scene, formulate effective adversarial text, strategically plan its placement, and provide detailed instructions for natural integration within the image. This is followed by a scene-coherent TextDiffuser that executes the attack using a local diffusion mechanism. We extend our method to real-world scenarios by printing and placing generated patches in physical environments, demonstrating its practical implications. Extensive experiments show that our scene-coherent adversarial text successfully misleads state-of-the-art LVLMs, including ChatGPT-4o, even after capturing new images of physical setups. Our evaluations demonstrate a significant increase in attack success rates while maintaining visual naturalness and contextual appropriateness. This work highlights vulnerabilities in current vision-language models to sophisticated, scene-coherent adversarial attacks and provides insights into potential defense mechanisms.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.00114v2-abstract-full').style.display = 'none'; document.getElementById('2412.00114v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 April, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 28 November, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.13057">arXiv:2411.13057</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.13057">pdf</a>, <a href="https://arxiv.org/ps/2411.13057">ps</a>, <a href="https://arxiv.org/format/2411.13057">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Multi-Branch Cooperation for Enhanced Click-Through Rate Prediction at Taobao
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xu Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+Z">Zida Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pan%2C+Y">Yuangang Pan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiao%2C+S">Shuai Xiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+X">Xiaoming Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lan%2C+J">Jinsong Lan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+X">Xiaoyong Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+B">Bo Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I+W">Ivor W. Tsang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.13057v2-abstract-short" style="display: inline;">
        Existing click-through rate (CTR) prediction works have studied the role of feature interaction through a variety of techniques. Each interaction technique exhibits its own strength, and solely using one type usually constrains the model&#39;s capability to capture the complex feature relationships, especially for industrial data with enormous input feature fields. Recent research shows that effective&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.13057v2-abstract-full').style.display = 'inline'; document.getElementById('2411.13057v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.13057v2-abstract-full" style="display: none;">
        Existing click-through rate (CTR) prediction works have studied the role of feature interaction through a variety of techniques. Each interaction technique exhibits its own strength, and solely using one type usually constrains the model&#39;s capability to capture the complex feature relationships, especially for industrial data with enormous input feature fields. Recent research shows that effective CTR models often combine an MLP network with a dedicated feature interaction network in a two-parallel structure. However, the interplay and cooperative dynamics between different streams or branches remain under-researched. In this work, we introduce a novel Multi-Branch Cooperation Network (MBCnet) which enables multiple branch networks to collaborate with each other for better complex feature interaction modeling. Specifically, MBCnet consists of three branches: the Extensible Feature Grouping and Crossing (EFGC) branch that promotes the model&#39;s memorization ability of specific feature fields, the low rank Cross Net branch and Deep branch to enhance explicit and implicit feature crossing for improved generalization. Among these branches, a novel cooperation scheme is proposed based on two principles: Branch co-teaching and moderate differentiation. Branch co-teaching encourages well-learned branches to support poorly-learned ones on specific training samples. Moderate differentiation advocates branches to maintain a reasonable level of difference in their feature representations on the same inputs. This cooperation strategy improves learning through mutual knowledge sharing and boosts the discovery of diverse feature interactions across branches. Experiments on large-scale industrial datasets and online A/B test at Taobao app demonstrate MBCnet&#39;s superior performance, delivering a 0.09 point increase in CTR, 1.49% growth in deals, and 1.62% rise in GMV. Core codes are available online.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.13057v2-abstract-full').style.display = 'none'; document.getElementById('2411.13057v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 June, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 November, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">14 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.06965">arXiv:2411.06965</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.06965">pdf</a>, <a href="https://arxiv.org/format/2411.06965">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Imitation from Diverse Behaviors: Wasserstein Quality Diversity Imitation Learning with Single-Step Archive Exploration
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+X">Xingrui Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wan%2C+Z">Zhenglin Wan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bossens%2C+D+M">David Mark Bossens</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lyu%2C+Y">Yueming Lyu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Q">Qing Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I+W">Ivor W. Tsang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.06965v2-abstract-short" style="display: inline;">
        Learning diverse and high-performance behaviors from a limited set of demonstrations is a grand challenge. Traditional imitation learning methods usually fail in this task because most of them are designed to learn one specific behavior even with multiple demonstrations. Therefore, novel techniques for \textit{quality diversity imitation learning}, which bridges the quality diversity optimization&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.06965v2-abstract-full').style.display = 'inline'; document.getElementById('2411.06965v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.06965v2-abstract-full" style="display: none;">
        Learning diverse and high-performance behaviors from a limited set of demonstrations is a grand challenge. Traditional imitation learning methods usually fail in this task because most of them are designed to learn one specific behavior even with multiple demonstrations. Therefore, novel techniques for \textit{quality diversity imitation learning}, which bridges the quality diversity optimization and imitation learning methods, are needed to solve the above challenge. This work introduces Wasserstein Quality Diversity Imitation Learning (WQDIL), which 1) improves the stability of imitation learning in the quality diversity setting with latent adversarial training based on a Wasserstein Auto-Encoder (WAE), and 2) mitigates a behavior-overfitting issue using a measure-conditioned reward function with a single-step archive exploration bonus. Empirically, our method significantly outperforms state-of-the-art IL methods, achieving near-expert or beyond-expert QD performance on the challenging continuous control tasks derived from MuJoCo environments.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.06965v2-abstract-full').style.display = 'none'; document.getElementById('2411.06965v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 April, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 November, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.04708">arXiv:2411.04708</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.04708">pdf</a>, <a href="https://arxiv.org/format/2411.04708">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Exploring Hierarchical Molecular Graph Representation in Multimodal LLMs
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+C">Chengxin Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+H">Hao Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yuan%2C+Y">Yihe Yuan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Jing Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I">Ivor Tsang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.04708v2-abstract-short" style="display: inline;">
        Following the milestones in large language models (LLMs) and multimodal models, we have seen a surge in applying LLMs to biochemical tasks. Leveraging graph features and molecular text representations, LLMs can tackle various tasks, such as predicting chemical reaction outcomes and describing molecular properties. However, most current work overlooks the *multi-level nature* of the graph modality,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.04708v2-abstract-full').style.display = 'inline'; document.getElementById('2411.04708v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.04708v2-abstract-full" style="display: none;">
        Following the milestones in large language models (LLMs) and multimodal models, we have seen a surge in applying LLMs to biochemical tasks. Leveraging graph features and molecular text representations, LLMs can tackle various tasks, such as predicting chemical reaction outcomes and describing molecular properties. However, most current work overlooks the *multi-level nature* of the graph modality, even though different chemistry tasks may benefit from different feature levels. In this work, we first study the effect of feature granularity and reveal that even reducing all GNN-generated feature tokens to a single one does not significantly impact model performance. We then investigate the effect of various graph feature levels and demonstrate that both the quality of LLM-generated molecules and model performance across different tasks depend on different graph feature levels. Therefore, we conclude with two key insights: (1) current molecular-related multimodal LLMs lack a comprehensive understanding of graph features, and (2) static processing is not sufficient for hierarchical graph feature. We share our findings in detail, with the hope of paving the way for the community to develop more advanced multimodal LLMs for incorporating molecular graphs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.04708v2-abstract-full').style.display = 'none'; document.getElementById('2411.04708v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 February, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 November, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 4 tables, 1 figure, paper under review</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.03068">arXiv:2411.03068</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.03068">pdf</a>, <a href="https://arxiv.org/format/2411.03068">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Alpha and Prejudice: Improving $α$-sized Worst-case Fairness via Intrinsic Reweighting
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Jing Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yao%2C+Y">Yinghua Yao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pan%2C+Y">Yuangang Pan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xuanqian Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I+W">Ivor W. Tsang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fu%2C+X">Xiuju Fu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.03068v1-abstract-short" style="display: inline;">
        Worst-case fairness with off-the-shelf demographics achieves group parity by maximizing the model utility of the worst-off group. Nevertheless, demographic information is often unavailable in practical scenarios, which impedes the use of such a direct max-min formulation. Recent advances have reframed this learning problem by introducing the lower bound of minimal partition ratio, denoted as $α$,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.03068v1-abstract-full').style.display = 'inline'; document.getElementById('2411.03068v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.03068v1-abstract-full" style="display: none;">
        Worst-case fairness with off-the-shelf demographics achieves group parity by maximizing the model utility of the worst-off group. Nevertheless, demographic information is often unavailable in practical scenarios, which impedes the use of such a direct max-min formulation. Recent advances have reframed this learning problem by introducing the lower bound of minimal partition ratio, denoted as $α$, as side information, referred to as ``$α$-sized worst-case fairness&#39;&#39; in this paper. We first justify the practical significance of this setting by presenting noteworthy evidence from the data privacy perspective, which has been overlooked by existing research. Without imposing specific requirements on loss functions, we propose reweighting the training samples based on their intrinsic importance to fairness. Given the global nature of the worst-case formulation, we further develop a stochastic learning scheme to simplify the training process without compromising model performance. Additionally, we address the issue of outliers and provide a robust variant to handle potential outliers during model training. Our theoretical analysis and experimental observations reveal the connections between the proposed approaches and existing ``fairness-through-reweighting&#39;&#39; studies, with extensive experimental results on fairness benchmarks demonstrating the superiority of our methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.03068v1-abstract-full').style.display = 'none'; document.getElementById('2411.03068v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.02669">arXiv:2411.02669</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.02669">pdf</a>, <a href="https://arxiv.org/format/2411.02669">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Semantic-Aligned Adversarial Evolution Triangle for High-Transferability Vision-Language Attack
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Jia%2C+X">Xiaojun Jia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+S">Sensen Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Q">Qing Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+K">Ke Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+Y">Yihao Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qin%2C+S">Simeng Qin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fellow%2C+I+T">Ivor Tsang Fellow</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+X">Xiaochun Cao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.02669v1-abstract-short" style="display: inline;">
        Vision-language pre-training (VLP) models excel at interpreting both images and text but remain vulnerable to multimodal adversarial examples (AEs). Advancing the generation of transferable AEs, which succeed across unseen models, is key to developing more robust and practical VLP models. Previous approaches augment image-text pairs to enhance diversity within the adversarial example generation pr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.02669v1-abstract-full').style.display = 'inline'; document.getElementById('2411.02669v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.02669v1-abstract-full" style="display: none;">
        Vision-language pre-training (VLP) models excel at interpreting both images and text but remain vulnerable to multimodal adversarial examples (AEs). Advancing the generation of transferable AEs, which succeed across unseen models, is key to developing more robust and practical VLP models. Previous approaches augment image-text pairs to enhance diversity within the adversarial example generation process, aiming to improve transferability by expanding the contrast space of image-text features. However, these methods focus solely on diversity around the current AEs, yielding limited gains in transferability. To address this issue, we propose to increase the diversity of AEs by leveraging the intersection regions along the adversarial trajectory during optimization. Specifically, we propose sampling from adversarial evolution triangles composed of clean, historical, and current adversarial examples to enhance adversarial diversity. We provide a theoretical analysis to demonstrate the effectiveness of the proposed adversarial evolution triangle. Moreover, we find that redundant inactive dimensions can dominate similarity calculations, distorting feature matching and making AEs model-dependent with reduced transferability. Hence, we propose to generate AEs in the semantic image-text feature contrast space, which can project the original feature space into a semantic corpus subspace. The proposed semantic-aligned subspace can reduce the image feature redundancy, thereby improving adversarial transferability. Extensive experiments across different datasets and models demonstrate that the proposed method can effectively improve adversarial transferability and outperform state-of-the-art adversarial attack methods. The code is released at https://github.com/jiaxiaojunQAQ/SA-AET.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.02669v1-abstract-full').style.display = 'none'; document.getElementById('2411.02669v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.02467">arXiv:2411.02467</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.02467">pdf</a>, <a href="https://arxiv.org/format/2411.02467">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards Harmless Rawlsian Fairness Regardless of Demographic Prior
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xuanqian Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Jing Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I+W">Ivor W. Tsang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ong%2C+Y">Yew-Soon Ong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.02467v2-abstract-short" style="display: inline;">
        Due to privacy and security concerns, recent advancements in group fairness advocate for model training regardless of demographic information. However, most methods still require prior knowledge of demographics. In this study, we explore the potential for achieving fairness without compromising its utility when no prior demographics are provided to the training set, namely \emph{harmless Rawlsian&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.02467v2-abstract-full').style.display = 'inline'; document.getElementById('2411.02467v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.02467v2-abstract-full" style="display: none;">
        Due to privacy and security concerns, recent advancements in group fairness advocate for model training regardless of demographic information. However, most methods still require prior knowledge of demographics. In this study, we explore the potential for achieving fairness without compromising its utility when no prior demographics are provided to the training set, namely \emph{harmless Rawlsian fairness}. We ascertain that such a fairness requirement with no prior demographic information essential promotes training losses to exhibit a Dirac delta distribution. To this end, we propose a simple but effective method named VFair to minimize the variance of training losses inside the optimal set of empirical losses. This problem is then optimized by a tailored dynamic update approach that operates in both loss and gradient dimensions, directing the model towards relatively fairer solutions while preserving its intact utility. Our experimental findings indicate that regression tasks, which are relatively unexplored from literature, can achieve significant fairness improvement through VFair regardless of any prior, whereas classification tasks usually do not because of their quantized utility measurements. The implementation of our method is publicly available at \url{https://github.com/wxqpxw/VFair}.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.02467v2-abstract-full').style.display = 'none'; document.getElementById('2411.02467v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 November, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 4 November, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted as a Poster in Neurips 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.20580">arXiv:2410.20580</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.20580">pdf</a>, <a href="https://arxiv.org/ps/2410.20580">ps</a>, <a href="https://arxiv.org/format/2410.20580">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Coherence-guided Preference Disentanglement for Cross-domain Recommendations
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xiang%2C+Z">Zongyi Xiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Duan%2C+L">Lixin Duan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+H">Hongzhi Yin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I+W">Ivor W. Tsang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.20580v2-abstract-short" style="display: inline;">
        Discovering user preferences across different domains is pivotal in cross-domain recommendation systems, particularly when platforms lack comprehensive user-item interactive data. The limited presence of shared users often hampers the effective modeling of common preferences. While leveraging shared items&#39; attributes, such as category and popularity, can enhance cross-domain recommendation perform&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.20580v2-abstract-full').style.display = 'inline'; document.getElementById('2410.20580v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.20580v2-abstract-full" style="display: none;">
        Discovering user preferences across different domains is pivotal in cross-domain recommendation systems, particularly when platforms lack comprehensive user-item interactive data. The limited presence of shared users often hampers the effective modeling of common preferences. While leveraging shared items&#39; attributes, such as category and popularity, can enhance cross-domain recommendation performance, the scarcity of shared items between domains has limited research in this area. To address this, we propose a Coherence-guided Preference Disentanglement (CoPD) method aimed at improving cross-domain recommendation by i) explicitly extracting shared item attributes to guide the learning of shared user preferences and ii) disentangling these preferences to identify specific user interests transferred between domains. CoPD introduces coherence constraints on item embeddings of shared and specific domains, aiding in extracting shared attributes. Moreover, it utilizes these attributes to guide the disentanglement of user preferences into separate embeddings for interest and conformity through a popularity-weighted loss. Experiments conducted on real-world datasets demonstrate the superior performance of our proposed CoPD over existing competitive baselines, highlighting its effectiveness in enhancing cross-domain recommendation performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.20580v2-abstract-full').style.display = 'none'; document.getElementById('2410.20580v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 June, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 27 October, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">28 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.17954">arXiv:2410.17954</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.17954">pdf</a>, <a href="https://arxiv.org/format/2410.17954">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ExpertFlow: Optimized Expert Activation and Token Allocation for Efficient Mixture-of-Experts Inference
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=He%2C+X">Xin He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+S">Shunkang Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yuxin Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+H">Haiyan Yin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zeng%2C+Z">Zihao Zeng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+S">Shaohuai Shi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+Z">Zhenheng Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chu%2C+X">Xiaowen Chu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I">Ivor Tsang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Soon%2C+O+Y">Ong Yew Soon</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.17954v1-abstract-short" style="display: inline;">
        Sparse Mixture of Experts (MoE) models, while outperforming dense Large Language Models (LLMs) in terms of performance, face significant deployment challenges during inference due to their high memory demands. Existing offloading techniques, which involve swapping activated and idle experts between the GPU and CPU, often suffer from rigid expert caching mechanisms. These mechanisms fail to adapt t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.17954v1-abstract-full').style.display = 'inline'; document.getElementById('2410.17954v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.17954v1-abstract-full" style="display: none;">
        Sparse Mixture of Experts (MoE) models, while outperforming dense Large Language Models (LLMs) in terms of performance, face significant deployment challenges during inference due to their high memory demands. Existing offloading techniques, which involve swapping activated and idle experts between the GPU and CPU, often suffer from rigid expert caching mechanisms. These mechanisms fail to adapt to dynamic routing, leading to inefficient cache utilization, or incur prohibitive costs for prediction training. To tackle these inference-specific challenges, we introduce ExpertFlow, a comprehensive system specifically designed to enhance inference efficiency by accommodating flexible routing and enabling efficient expert scheduling between CPU and GPU. This reduces overhead and boosts system performance. Central to our approach is a predictive routing path-based offloading mechanism that utilizes a lightweight predictor to accurately forecast routing paths before computation begins. This proactive strategy allows for real-time error correction in expert caching, significantly increasing cache hit ratios and reducing the frequency of expert transfers, thereby minimizing I/O overhead. Additionally, we implement a dynamic token scheduling strategy that optimizes MoE inference by rearranging input tokens across different batches. This method not only reduces the number of activated experts per batch but also improves computational efficiency. Our extensive experiments demonstrate that ExpertFlow achieves up to 93.72\% GPU memory savings and enhances inference speed by 2 to 10 times compared to baseline methods, highlighting its effectiveness and utility as a robust solution for resource-constrained inference scenarios.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.17954v1-abstract-full').style.display = 'none'; document.getElementById('2410.17954v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Mixture-of-Experts, Inference, Offloading</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.12457">arXiv:2410.12457</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.12457">pdf</a>, <a href="https://arxiv.org/format/2410.12457">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Sharpness-Aware Black-Box Optimization
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ye%2C+F">Feiyang Ye</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lyu%2C+Y">Yueming Lyu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xuehao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sugiyama%2C+M">Masashi Sugiyama</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yu Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I">Ivor Tsang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.12457v1-abstract-short" style="display: inline;">
        Black-box optimization algorithms have been widely used in various machine learning problems, including reinforcement learning and prompt fine-tuning. However, directly optimizing the training loss value, as commonly done in existing black-box optimization methods, could lead to suboptimal model quality and generalization performance. To address those problems in black-box optimization, we propose&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.12457v1-abstract-full').style.display = 'inline'; document.getElementById('2410.12457v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.12457v1-abstract-full" style="display: none;">
        Black-box optimization algorithms have been widely used in various machine learning problems, including reinforcement learning and prompt fine-tuning. However, directly optimizing the training loss value, as commonly done in existing black-box optimization methods, could lead to suboptimal model quality and generalization performance. To address those problems in black-box optimization, we propose a novel Sharpness-Aware Black-box Optimization (SABO) algorithm, which applies a sharpness-aware minimization strategy to improve the model generalization. Specifically, the proposed SABO method first reparameterizes the objective function by its expectation over a Gaussian distribution. Then it iteratively updates the parameterized distribution by approximated stochastic gradients of the maximum objective value within a small neighborhood around the current solution in the Gaussian distribution space. Theoretically, we prove the convergence rate and generalization bound of the proposed SABO algorithm. Empirically, extensive experiments on the black-box prompt fine-tuning tasks demonstrate the effectiveness of the proposed SABO method in improving model generalization performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.12457v1-abstract-full').style.display = 'none'; document.getElementById('2410.12457v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">27 pages, 5 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.10453">arXiv:2410.10453</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.10453">pdf</a>, <a href="https://arxiv.org/format/2410.10453">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Self-Assessed Generation: Trustworthy Label Generation for Optical Flow and Stereo Matching in Real-world
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ling%2C+H">Han Ling</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+Y">Yinghui Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+Q">Quansen Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I">Ivor Tsang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+Y">Yuhui Zheng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.10453v1-abstract-short" style="display: inline;">
        A significant challenge facing current optical flow and stereo methods is the difficulty in generalizing them well to the real world. This is mainly due to the high costs required to produce datasets, and the limitations of existing self-supervised methods on fuzzy results and complex model training problems. To address the above challenges, we propose a unified self-supervised generalization fram&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.10453v1-abstract-full').style.display = 'inline'; document.getElementById('2410.10453v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.10453v1-abstract-full" style="display: none;">
        A significant challenge facing current optical flow and stereo methods is the difficulty in generalizing them well to the real world. This is mainly due to the high costs required to produce datasets, and the limitations of existing self-supervised methods on fuzzy results and complex model training problems. To address the above challenges, we propose a unified self-supervised generalization framework for optical flow and stereo tasks: Self-Assessed Generation (SAG). Unlike previous self-supervised methods, SAG is data-driven, using advanced reconstruction techniques to construct a reconstruction field from RGB images and generate datasets based on it. Afterward, we quantified the confidence level of the generated results from multiple perspectives, such as reconstruction field distribution, geometric consistency, and structural similarity, to eliminate inevitable defects in the generation process. We also designed a 3D flight foreground automatic rendering pipeline in SAG to encourage the network to learn occlusion and motion foreground. Experimentally, because SAG does not involve changes to methods or loss functions, it can directly self-supervised train the state-of-the-art deep networks, greatly improving the generalization performance of self-supervised methods on current mainstream optical flow and stereo-matching datasets. Compared to previous training modes, SAG is more generalized, cost-effective, and accurate.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.10453v1-abstract-full').style.display = 'none'; document.getElementById('2410.10453v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.06151">arXiv:2410.06151</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.06151">pdf</a>, <a href="https://arxiv.org/ps/2410.06151">ps</a>, <a href="https://arxiv.org/format/2410.06151">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Diversifying Policy Behaviors with Extrinsic Behavioral Curiosity
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wan%2C+Z">Zhenglin Wan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+X">Xingrui Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bossens%2C+D+M">David Mark Bossens</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lyu%2C+Y">Yueming Lyu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Q">Qing Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fan%2C+F+X">Flint Xiaofeng Fan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ong%2C+Y+S">Yew Soon Ong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I">Ivor Tsang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.06151v4-abstract-short" style="display: inline;">
        Imitation learning (IL) has shown promise in various applications (e.g. robot locomotion) but is often limited to learning a single expert policy, constraining behavior diversity and robustness in unpredictable real-world scenarios. To address this, we introduce Quality Diversity Inverse Reinforcement Learning (QD-IRL), a novel framework that integrates quality-diversity optimization with IRL meth&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.06151v4-abstract-full').style.display = 'inline'; document.getElementById('2410.06151v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.06151v4-abstract-full" style="display: none;">
        Imitation learning (IL) has shown promise in various applications (e.g. robot locomotion) but is often limited to learning a single expert policy, constraining behavior diversity and robustness in unpredictable real-world scenarios. To address this, we introduce Quality Diversity Inverse Reinforcement Learning (QD-IRL), a novel framework that integrates quality-diversity optimization with IRL methods, enabling agents to learn diverse behaviors from limited demonstrations. This work introduces Extrinsic Behavioral Curiosity (EBC), which allows agents to receive additional curiosity rewards from an external critic based on how novel the behaviors are with respect to a large behavioral archive. To validate the effectiveness of EBC in exploring diverse locomotion behaviors, we evaluate our method on multiple robot locomotion tasks. EBC improves the performance of QD-IRL instances with GAIL, VAIL, and DiffAIL across all included environments by up to 185%, 42%, and 150%, even surpassing expert performance by 20% in Humanoid. Furthermore, we demonstrate that EBC is applicable to Gradient-Arborescence-based Quality Diversity Reinforcement Learning (QD-RL) algorithms, where it substantially improves performance and provides a generic technique for learning behavioral-diverse policies. The source code of this work is provided at https://github.com/vanzll/EBC.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.06151v4-abstract-full').style.display = 'none'; document.getElementById('2410.06151v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 October, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">20 pages, conference paper</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        International Conference on Machine Learning (ICML 2025)
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.04493">arXiv:2407.04493</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.04493">pdf</a>, <a href="https://arxiv.org/format/2407.04493">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1007/s10994-024-06575-2">10.1007/s10994-024-06575-2 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PROUD: PaRetO-gUided Diffusion Model for Multi-objective Generation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yao%2C+Y">Yinghua Yao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pan%2C+Y">Yuangang Pan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Jing Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I">Ivor Tsang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yao%2C+X">Xin Yao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.04493v1-abstract-short" style="display: inline;">
        Recent advancements in the realm of deep generative models focus on generating samples that satisfy multiple desired properties. However, prevalent approaches optimize these property functions independently, thus omitting the trade-offs among them. In addition, the property optimization is often improperly integrated into the generative models, resulting in an unnecessary compromise on generation&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.04493v1-abstract-full').style.display = 'inline'; document.getElementById('2407.04493v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.04493v1-abstract-full" style="display: none;">
        Recent advancements in the realm of deep generative models focus on generating samples that satisfy multiple desired properties. However, prevalent approaches optimize these property functions independently, thus omitting the trade-offs among them. In addition, the property optimization is often improperly integrated into the generative models, resulting in an unnecessary compromise on generation quality (i.e., the quality of generated samples). To address these issues, we formulate a constrained optimization problem. It seeks to optimize generation quality while ensuring that generated samples reside at the Pareto front of multiple property objectives. Such a formulation enables the generation of samples that cannot be further improved simultaneously on the conflicting property functions and preserves good quality of generated samples. Building upon this formulation, we introduce the PaRetO-gUided Diffusion model (PROUD), wherein the gradients in the denoising process are dynamically adjusted to enhance generation quality while the generated samples adhere to Pareto optimality. Experimental evaluations on image generation and protein generation tasks demonstrate that our PROUD consistently maintains superior generation quality while approaching Pareto optimality across multiple property functions compared to various baselines.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.04493v1-abstract-full').style.display = 'none'; document.getElementById('2407.04493v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Machine Learning 2024
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.04872">arXiv:2406.04872</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.04872">pdf</a>, <a href="https://arxiv.org/format/2406.04872">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Diversified Batch Selection for Training Acceleration
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hong%2C+F">Feng Hong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lyu%2C+Y">Yueming Lyu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yao%2C+J">Jiangchao Yao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Ya Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I+W">Ivor W. Tsang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yanfeng Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.04872v1-abstract-short" style="display: inline;">
        The remarkable success of modern machine learning models on large datasets often demands extensive training time and resource consumption. To save cost, a prevalent research line, known as online batch selection, explores selecting informative subsets during the training process. Although recent efforts achieve advancements by measuring the impact of each sample on generalization, their reliance o&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.04872v1-abstract-full').style.display = 'inline'; document.getElementById('2406.04872v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.04872v1-abstract-full" style="display: none;">
        The remarkable success of modern machine learning models on large datasets often demands extensive training time and resource consumption. To save cost, a prevalent research line, known as online batch selection, explores selecting informative subsets during the training process. Although recent efforts achieve advancements by measuring the impact of each sample on generalization, their reliance on additional reference models inherently limits their practical applications, when there are no such ideal models available. On the other hand, the vanilla reference-model-free methods involve independently scoring and selecting data in a sample-wise manner, which sacrifices the diversity and induces the redundancy. To tackle this dilemma, we propose Diversified Batch Selection (DivBS), which is reference-model-free and can efficiently select diverse and representative samples. Specifically, we define a novel selection objective that measures the group-wise orthogonalized representativeness to combat the redundancy issue of previous sample-wise criteria, and provide a principled selection-efficient realization. Extensive experiments across various tasks demonstrate the significant superiority of DivBS in the performance-speedup trade-off. The code is publicly available.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.04872v1-abstract-full').style.display = 'none'; document.getElementById('2406.04872v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ICML 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.00812">arXiv:2406.00812</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.00812">pdf</a>, <a href="https://arxiv.org/format/2406.00812">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Covariance-Adaptive Sequential Black-box Optimization for Diffusion Targeted Generation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lyu%2C+Y">Yueming Lyu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tan%2C+K+Y">Kim Yong Tan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ong%2C+Y+S">Yew Soon Ong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I+W">Ivor W. Tsang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.00812v2-abstract-short" style="display: inline;">
        Diffusion models have demonstrated great potential in generating high-quality content for images, natural language, protein domains, etc. However, how to perform user-preferred targeted generation via diffusion models with only black-box target scores of users remains challenging. To address this issue, we first formulate the fine-tuning of the targeted reserve-time stochastic differential equatio&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.00812v2-abstract-full').style.display = 'inline'; document.getElementById('2406.00812v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.00812v2-abstract-full" style="display: none;">
        Diffusion models have demonstrated great potential in generating high-quality content for images, natural language, protein domains, etc. However, how to perform user-preferred targeted generation via diffusion models with only black-box target scores of users remains challenging. To address this issue, we first formulate the fine-tuning of the targeted reserve-time stochastic differential equation (SDE) associated with a pre-trained diffusion model as a sequential black-box optimization problem. Furthermore, we propose a novel covariance-adaptive sequential optimization algorithm to optimize cumulative black-box scores under unknown transition dynamics. Theoretically, we prove a $O(\frac{d^2}{\sqrt{T}})$ convergence rate for cumulative convex functions without smooth and strongly convex assumptions. Empirically, experiments on both numerical test problems and target-guided 3D-molecule generation tasks show the superior performance of our method in achieving better target scores.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.00812v2-abstract-full').style.display = 'none'; document.getElementById('2406.00812v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 June, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 2 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.17984">arXiv:2405.17984</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.17984">pdf</a>, <a href="https://arxiv.org/format/2405.17984">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Cross-Context Backdoor Attacks against Graph Prompt Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lyu%2C+X">Xiaoting Lyu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+Y">Yufei Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+W">Wei Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qian%2C+H">Hangwei Qian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I">Ivor Tsang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+X">Xiangliang Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.17984v1-abstract-short" style="display: inline;">
        Graph Prompt Learning (GPL) bridges significant disparities between pretraining and downstream applications to alleviate the knowledge transfer bottleneck in real-world graph learning. While GPL offers superior effectiveness in graph knowledge transfer and computational efficiency, the security risks posed by backdoor poisoning effects embedded in pretrained models remain largely unexplored. Our s&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.17984v1-abstract-full').style.display = 'inline'; document.getElementById('2405.17984v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.17984v1-abstract-full" style="display: none;">
        Graph Prompt Learning (GPL) bridges significant disparities between pretraining and downstream applications to alleviate the knowledge transfer bottleneck in real-world graph learning. While GPL offers superior effectiveness in graph knowledge transfer and computational efficiency, the security risks posed by backdoor poisoning effects embedded in pretrained models remain largely unexplored. Our study provides a comprehensive analysis of GPL&#39;s vulnerability to backdoor attacks. We introduce \textit{CrossBA}, the first cross-context backdoor attack against GPL, which manipulates only the pretraining phase without requiring knowledge of downstream applications. Our investigation reveals both theoretically and empirically that tuning trigger graphs, combined with prompt transformations, can seamlessly transfer the backdoor threat from pretrained encoders to downstream applications. Through extensive experiments involving 3 representative GPL methods across 5 distinct cross-context scenarios and 5 benchmark datasets of node and graph classification tasks, we demonstrate that \textit{CrossBA} consistently achieves high attack success rates while preserving the functionality of downstream applications over clean input. We also explore potential countermeasures against \textit{CrossBA} and conclude that current defenses are insufficient to mitigate \textit{CrossBA}. Our study highlights the persistent backdoor threats to GPL systems, raising trustworthiness concerns in the practices of GPL techniques.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.17984v1-abstract-full').style.display = 'none'; document.getElementById('2405.17984v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by KDD 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.17761">arXiv:2405.17761</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.17761">pdf</a>, <a href="https://arxiv.org/format/2405.17761">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Double Variance Reduction: A Smoothing Trick for Composite Optimization Problems without First-Order Gradient
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Di%2C+H">Hao Di</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ye%2C+H">Haishan Ye</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yueling Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+X">Xiangyu Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+G">Guang Dai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I+W">Ivor W. Tsang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.17761v1-abstract-short" style="display: inline;">
        Variance reduction techniques are designed to decrease the sampling variance, thereby accelerating convergence rates of first-order (FO) and zeroth-order (ZO) optimization methods. However, in composite optimization problems, ZO methods encounter an additional variance called the coordinate-wise variance, which stems from the random gradient estimation. To reduce this variance, prior works require&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.17761v1-abstract-full').style.display = 'inline'; document.getElementById('2405.17761v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.17761v1-abstract-full" style="display: none;">
        Variance reduction techniques are designed to decrease the sampling variance, thereby accelerating convergence rates of first-order (FO) and zeroth-order (ZO) optimization methods. However, in composite optimization problems, ZO methods encounter an additional variance called the coordinate-wise variance, which stems from the random gradient estimation. To reduce this variance, prior works require estimating all partial derivatives, essentially approximating FO information. This approach demands O(d) function evaluations (d is the dimension size), which incurs substantial computational costs and is prohibitive in high-dimensional scenarios. This paper proposes the Zeroth-order Proximal Double Variance Reduction (ZPDVR) method, which utilizes the averaging trick to reduce both sampling and coordinate-wise variances. Compared to prior methods, ZPDVR relies solely on random gradient estimates, calls the stochastic zeroth-order oracle (SZO) in expectation $\mathcal{O}(1)$ times per iteration, and achieves the optimal $\mathcal{O}(d(n + κ)\log (\frac{1}ε))$ SZO query complexity in the strongly convex and smooth setting, where $κ$ represents the condition number and $ε$ is the desired accuracy. Empirical results validate ZPDVR&#39;s linear convergence and demonstrate its superior performance over other related methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.17761v1-abstract-full').style.display = 'none'; document.getElementById('2405.17761v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.06204">arXiv:2405.06204</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.06204">pdf</a>, <a href="https://arxiv.org/format/2405.06204">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        HC$^2$L: Hybrid and Cooperative Contrastive Learning for Cross-lingual Spoken Language Understanding
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xing%2C+B">Bowen Xing</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I+W">Ivor W. Tsang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.06204v1-abstract-short" style="display: inline;">
        State-of-the-art model for zero-shot cross-lingual spoken language understanding performs cross-lingual unsupervised contrastive learning to achieve the label-agnostic semantic alignment between each utterance and its code-switched data. However, it ignores the precious intent/slot labels, whose label information is promising to help capture the label-aware semantics structure and then leverage su&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.06204v1-abstract-full').style.display = 'inline'; document.getElementById('2405.06204v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.06204v1-abstract-full" style="display: none;">
        State-of-the-art model for zero-shot cross-lingual spoken language understanding performs cross-lingual unsupervised contrastive learning to achieve the label-agnostic semantic alignment between each utterance and its code-switched data. However, it ignores the precious intent/slot labels, whose label information is promising to help capture the label-aware semantics structure and then leverage supervised contrastive learning to improve both source and target languages&#39; semantics. In this paper, we propose Hybrid and Cooperative Contrastive Learning to address this problem. Apart from cross-lingual unsupervised contrastive learning, we design a holistic approach that exploits source language supervised contrastive learning, cross-lingual supervised contrastive learning and multilingual supervised contrastive learning to perform label-aware semantics alignments in a comprehensive manner. Each kind of supervised contrastive learning mechanism includes both single-task and joint-task scenarios. In our model, one contrastive learning mechanism&#39;s input is enhanced by others. Thus the total four contrastive learning mechanisms are cooperative to learn more consistent and discriminative representations in the virtuous cycle during the training process. Experiments show that our model obtains consistent improvements over 9 languages, achieving new state-of-the-art performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.06204v1-abstract-full').style.display = 'none'; document.getElementById('2405.06204v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI). arXiv admin note: text overlap with arXiv:2312.03716</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.19219">arXiv:2403.19219</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.19219">pdf</a>, <a href="https://arxiv.org/format/2403.19219">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.26599/BDMA.2024.9020021">10.26599/BDMA.2024.9020021 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Collaborative Knowledge Infusion for Low-resource Stance Detection
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yan%2C+M">Ming Yan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+J+T">Joey Tianyi Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I+W">Ivor W. Tsang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.19219v1-abstract-short" style="display: inline;">
        Stance detection is the view towards a specific target by a given context (\textit{e.g.} tweets, commercial reviews). Target-related knowledge is often needed to assist stance detection models in understanding the target well and making detection correctly. However, prevailing works for knowledge-infused stance detection predominantly incorporate target knowledge from a singular source that lacks&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.19219v1-abstract-full').style.display = 'inline'; document.getElementById('2403.19219v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.19219v1-abstract-full" style="display: none;">
        Stance detection is the view towards a specific target by a given context (\textit{e.g.} tweets, commercial reviews). Target-related knowledge is often needed to assist stance detection models in understanding the target well and making detection correctly. However, prevailing works for knowledge-infused stance detection predominantly incorporate target knowledge from a singular source that lacks knowledge verification in limited domain knowledge. The low-resource training data further increases the challenge for the data-driven large models in this task. To address those challenges, we propose a collaborative knowledge infusion approach for low-resource stance detection tasks, employing a combination of aligned knowledge enhancement and efficient parameter learning techniques. Specifically, our stance detection approach leverages target background knowledge collaboratively from different knowledge sources with the help of knowledge alignment. Additionally, we also introduce the parameter-efficient collaborative adaptor with a staged optimization algorithm, which collaboratively addresses the challenges associated with low-resource stance detection tasks from both network structure and learning perspectives. To assess the effectiveness of our method, we conduct extensive experiments on three public stance detection datasets, including low-resource and cross-target settings. The results demonstrate significant performance improvements compared to the existing stance detection approaches.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.19219v1-abstract-full').style.display = 'none'; document.getElementById('2403.19219v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">13 pages, 3 figures, Big Data Mining and Analysis</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.12445">arXiv:2403.12445</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.12445">pdf</a>, <a href="https://arxiv.org/format/2403.12445">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Boosting Transferability in Vision-Language Attacks via Diversification along the Intersection Region of Adversarial Trajectory
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+S">Sensen Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jia%2C+X">Xiaojun Jia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ren%2C+X">Xuhong Ren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I">Ivor Tsang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Q">Qing Guo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.12445v3-abstract-short" style="display: inline;">
        Vision-language pre-training (VLP) models exhibit remarkable capabilities in comprehending both images and text, yet they remain susceptible to multimodal adversarial examples (AEs). Strengthening attacks and uncovering vulnerabilities, especially common issues in VLP models (e.g., high transferable AEs), can advance reliable and practical VLP models. A recent work (i.e., Set-level guidance attack&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.12445v3-abstract-full').style.display = 'inline'; document.getElementById('2403.12445v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.12445v3-abstract-full" style="display: none;">
        Vision-language pre-training (VLP) models exhibit remarkable capabilities in comprehending both images and text, yet they remain susceptible to multimodal adversarial examples (AEs). Strengthening attacks and uncovering vulnerabilities, especially common issues in VLP models (e.g., high transferable AEs), can advance reliable and practical VLP models. A recent work (i.e., Set-level guidance attack) indicates that augmenting image-text pairs to increase AE diversity along the optimization path enhances the transferability of adversarial examples significantly. However, this approach predominantly emphasizes diversity around the online adversarial examples (i.e., AEs in the optimization period), leading to the risk of overfitting the victim model and affecting the transferability. In this study, we posit that the diversity of adversarial examples towards the clean input and online AEs are both pivotal for enhancing transferability across VLP models. Consequently, we propose using diversification along the intersection region of adversarial trajectory to expand the diversity of AEs. To fully leverage the interaction between modalities, we introduce text-guided adversarial example selection during optimization. Furthermore, to further mitigate the potential overfitting, we direct the adversarial text deviating from the last intersection region along the optimization path, rather than adversarial images as in existing methods. Extensive experiments affirm the effectiveness of our method in improving transferability across various VLP models and downstream vision-and-language tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.12445v3-abstract-full').style.display = 'none'; document.getElementById('2403.12445v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 July, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 March, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ECCV2024. Code is available at https://github.com/SensenGao/VLPTransferAttack</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.06075">arXiv:2403.06075</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.06075">pdf</a>, <a href="https://arxiv.org/format/2403.06075">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multisize Dataset Condensation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=He%2C+Y">Yang He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiao%2C+L">Lingao Xiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+J+T">Joey Tianyi Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I">Ivor Tsang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.06075v2-abstract-short" style="display: inline;">
        While dataset condensation effectively enhances training efficiency, its application in on-device scenarios brings unique challenges. 1) Due to the fluctuating computational resources of these devices, there&#39;s a demand for a flexible dataset size that diverges from a predefined size. 2) The limited computational power on devices often prevents additional condensation operations. These two challeng&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.06075v2-abstract-full').style.display = 'inline'; document.getElementById('2403.06075v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.06075v2-abstract-full" style="display: none;">
        While dataset condensation effectively enhances training efficiency, its application in on-device scenarios brings unique challenges. 1) Due to the fluctuating computational resources of these devices, there&#39;s a demand for a flexible dataset size that diverges from a predefined size. 2) The limited computational power on devices often prevents additional condensation operations. These two challenges connect to the &#34;subset degradation problem&#34; in traditional dataset condensation: a subset from a larger condensed dataset is often unrepresentative compared to directly condensing the whole dataset to that smaller size. In this paper, we propose Multisize Dataset Condensation (MDC) by compressing N condensation processes into a single condensation process to obtain datasets with multiple sizes. Specifically, we introduce an &#34;adaptive subset loss&#34; on top of the basic condensation loss to mitigate the &#34;subset degradation problem&#34;. Our MDC method offers several benefits: 1) No additional condensation process is required; 2) reduced storage requirement by reusing condensed images. Experiments validate our findings on networks including ConvNet, ResNet and DenseNet, and datasets including SVHN, CIFAR-10, CIFAR-100 and ImageNet. For example, we achieved 5.22%-6.40% average accuracy gains on condensing CIFAR-10 to ten images per class. Code is available at: https://github.com/he-y/Multisize-Dataset-Condensation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.06075v2-abstract-full').style.display = 'none'; document.getElementById('2403.06075v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 April, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 9 March, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by ICLR 2024 Oral</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.15173">arXiv:2402.15173</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.15173">pdf</a>, <a href="https://arxiv.org/format/2402.15173">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Second-Order Fine-Tuning without Pain for LLMs:A Hessian Informed Zeroth-Order Optimizer
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+Y">Yanjun Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dang%2C+S">Sizhe Dang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ye%2C+H">Haishan Ye</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+G">Guang Dai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qian%2C+Y">Yi Qian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I+W">Ivor W. Tsang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.15173v4-abstract-short" style="display: inline;">
        Fine-tuning large language models (LLMs) with classic first-order optimizers entails prohibitive GPU memory due to the backpropagation process. Recent works have turned to zeroth-order optimizers for fine-tuning, which save substantial memory by using two forward passes. However, these optimizers are plagued by the heterogeneity of parameter curvatures across different dimensions. In this work, we&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.15173v4-abstract-full').style.display = 'inline'; document.getElementById('2402.15173v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.15173v4-abstract-full" style="display: none;">
        Fine-tuning large language models (LLMs) with classic first-order optimizers entails prohibitive GPU memory due to the backpropagation process. Recent works have turned to zeroth-order optimizers for fine-tuning, which save substantial memory by using two forward passes. However, these optimizers are plagued by the heterogeneity of parameter curvatures across different dimensions. In this work, we propose HiZOO, a diagonal Hessian informed zeroth-order optimizer which is the first work to leverage the diagonal Hessian to enhance zeroth-order optimizer for fine-tuning LLMs. What&#39;s more, HiZOO avoids the expensive memory cost and only increases one forward pass per step. Extensive experiments on various models (350M~66B parameters) indicate that HiZOO improves model convergence, significantly reducing training steps and effectively enhancing model accuracy. Moreover, we visualize the optimization trajectories of HiZOO on test functions, illustrating its effectiveness in handling heterogeneous curvatures. Lastly, we provide theoretical proofs of convergence for HiZOO. Code is publicly available at https://anonymous.4open.science/r/HiZOO27F8.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.15173v4-abstract-full').style.display = 'none'; document.getElementById('2402.15173v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 February, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 February, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.03661">arXiv:2402.03661</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.03661">pdf</a>, <a href="https://arxiv.org/format/2402.03661">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Transductive Reward Inference on Graph
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Qu%2C+B">Bohao Qu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+X">Xiaofeng Cao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Q">Qing Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chang%2C+Y">Yi Chang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I+W">Ivor W. Tsang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+C">Chengqi Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.03661v1-abstract-short" style="display: inline;">
        In this study, we present a transductive inference approach on that reward information propagation graph, which enables the effective estimation of rewards for unlabelled data in offline reinforcement learning. Reward inference is the key to learning effective policies in practical scenarios, while direct environmental interactions are either too costly or unethical and the reward functions are ra&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.03661v1-abstract-full').style.display = 'inline'; document.getElementById('2402.03661v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.03661v1-abstract-full" style="display: none;">
        In this study, we present a transductive inference approach on that reward information propagation graph, which enables the effective estimation of rewards for unlabelled data in offline reinforcement learning. Reward inference is the key to learning effective policies in practical scenarios, while direct environmental interactions are either too costly or unethical and the reward functions are rarely accessible, such as in healthcare and robotics. Our research focuses on developing a reward inference method based on the contextual properties of information propagation on graphs that capitalizes on a constrained number of human reward annotations to infer rewards for unlabelled data. We leverage both the available data and limited reward annotations to construct a reward propagation graph, wherein the edge weights incorporate various influential factors pertaining to the rewards. Subsequently, we employ the constructed graph for transductive reward inference, thereby estimating rewards for unlabelled data. Furthermore, we establish the existence of a fixed point during several iterations of the transductive inference process and demonstrate its at least convergence to a local optimum. Empirical evaluations on locomotion and robotic manipulation tasks validate the effectiveness of our approach. The application of our inferred rewards improves the performance in offline reinforcement learning tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.03661v1-abstract-full').style.display = 'none'; document.getElementById('2402.03661v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 February, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2401.11929">arXiv:2401.11929</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2401.11929">pdf</a>, <a href="https://arxiv.org/format/2401.11929">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Parsimony or Capability? Decomposition Delivers Both in Long-term Time Series Forecasting
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+J">Jinliang Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ye%2C+F">Feiyang Ye</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+D">Du Yin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+X">Xuan Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I+W">Ivor W. Tsang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiong%2C+H">Hui Xiong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2401.11929v4-abstract-short" style="display: inline;">
        Long-term time series forecasting (LTSF) represents a critical frontier in time series analysis, characterized by extensive input sequences, as opposed to the shorter spans typical of traditional approaches. While longer sequences inherently offer richer information for enhanced predictive precision, prevailing studies often respond by escalating model complexity. These intricate models can inflat&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2401.11929v4-abstract-full').style.display = 'inline'; document.getElementById('2401.11929v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2401.11929v4-abstract-full" style="display: none;">
        Long-term time series forecasting (LTSF) represents a critical frontier in time series analysis, characterized by extensive input sequences, as opposed to the shorter spans typical of traditional approaches. While longer sequences inherently offer richer information for enhanced predictive precision, prevailing studies often respond by escalating model complexity. These intricate models can inflate into millions of parameters, resulting in prohibitive parameter scales. Our study demonstrates, through both analytical and empirical evidence, that decomposition is key to containing excessive model inflation while achieving uniformly superior and robust results across various datasets. Remarkably, by tailoring decomposition to the intrinsic dynamics of time series data, our proposed model outperforms existing benchmarks, using over 99 \% fewer parameters than the majority of competing methods. Through this work, we aim to unleash the power of a restricted set of parameters by capitalizing on domain characteristics--a timely reminder that in the realm of LTSF, bigger is not invariably better.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2401.11929v4-abstract-full').style.display = 'none'; document.getElementById('2401.11929v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 October, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 22 January, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2401.09257">arXiv:2401.09257</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2401.09257">pdf</a>, <a href="https://arxiv.org/format/2401.09257">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A First-Order Multi-Gradient Algorithm for Multi-Objective Bi-Level Optimization
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ye%2C+F">Feiyang Ye</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+B">Baijiong Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+X">Xiaofeng Cao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yu Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I">Ivor Tsang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2401.09257v2-abstract-short" style="display: inline;">
        In this paper, we study the Multi-Objective Bi-Level Optimization (MOBLO) problem, where the upper-level subproblem is a multi-objective optimization problem and the lower-level subproblem is for scalar optimization. Existing gradient-based MOBLO algorithms need to compute the Hessian matrix, causing the computational inefficient problem. To address this, we propose an efficient first-order multi-&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2401.09257v2-abstract-full').style.display = 'inline'; document.getElementById('2401.09257v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2401.09257v2-abstract-full" style="display: none;">
        In this paper, we study the Multi-Objective Bi-Level Optimization (MOBLO) problem, where the upper-level subproblem is a multi-objective optimization problem and the lower-level subproblem is for scalar optimization. Existing gradient-based MOBLO algorithms need to compute the Hessian matrix, causing the computational inefficient problem. To address this, we propose an efficient first-order multi-gradient method for MOBLO, called FORUM. Specifically, we reformulate MOBLO problems as a constrained multi-objective optimization (MOO) problem via the value-function approach. Then we propose a novel multi-gradient aggregation method to solve the challenging constrained MOO problem. Theoretically, we provide the complexity analysis to show the efficiency of the proposed method and a non-asymptotic convergence result. Empirically, extensive experiments demonstrate the effectiveness and efficiency of the proposed FORUM method in different learning problems. In particular, it achieves state-of-the-art performance on three multi-task learning benchmark datasets. The code is available at https://github.com/Baijiong-Lin/FORUM.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2401.09257v2-abstract-full').style.display = 'none'; document.getElementById('2401.09257v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 July, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 17 January, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ECAI 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2312.17505">arXiv:2312.17505</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2312.17505">pdf</a>, <a href="https://arxiv.org/format/2312.17505">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Leveraging Open-Vocabulary Diffusion to Camouflaged Instance Segmentation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Vu%2C+T">Tuan-Anh Vu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nguyen%2C+D+T">Duc Thanh Nguyen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Q">Qing Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hua%2C+B">Binh-Son Hua</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chung%2C+N+M">Nhat Minh Chung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I+W">Ivor W. Tsang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yeung%2C+S">Sai-Kit Yeung</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2312.17505v1-abstract-short" style="display: inline;">
        Text-to-image diffusion techniques have shown exceptional capability of producing high-quality images from text descriptions. This indicates that there exists a strong correlation between the visual and textual domains. In addition, text-image discriminative models such as CLIP excel in image labelling from text prompts, thanks to the rich and diverse information available from open concepts. In t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2312.17505v1-abstract-full').style.display = 'inline'; document.getElementById('2312.17505v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2312.17505v1-abstract-full" style="display: none;">
        Text-to-image diffusion techniques have shown exceptional capability of producing high-quality images from text descriptions. This indicates that there exists a strong correlation between the visual and textual domains. In addition, text-image discriminative models such as CLIP excel in image labelling from text prompts, thanks to the rich and diverse information available from open concepts. In this paper, we leverage these technical advances to solve a challenging problem in computer vision: camouflaged instance segmentation. Specifically, we propose a method built upon a state-of-the-art diffusion model, empowered by open-vocabulary to learn multi-scale textual-visual features for camouflaged object representations. Such cross-domain representations are desirable in segmenting camouflaged objects where visual cues are subtle to distinguish the objects from the background, especially in segmenting novel objects which are not seen in training. We also develop technically supportive components to effectively fuse cross-domain features and engage relevant features towards respective foreground objects. We validate our method and compare it with existing ones on several benchmark datasets of camouflaged instance segmentation and generic open-vocabulary instance segmentation. Experimental results confirm the advances of our method over existing ones. We will publish our code and pre-trained models to support future research.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2312.17505v1-abstract-full').style.display = 'none'; document.getElementById('2312.17505v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 December, 2023; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2023.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This work is under review</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2312.13766">arXiv:2312.13766</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2312.13766">pdf</a>, <a href="https://arxiv.org/format/2312.13766">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Exploiting Contextual Target Attributes for Target Sentiment Classification
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xing%2C+B">Bowen Xing</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I+W">Ivor W. Tsang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2312.13766v1-abstract-short" style="display: inline;">
        Existing PTLM-based models for TSC can be categorized into two groups: 1) fine-tuning-based models that adopt PTLM as the context encoder; 2) prompting-based models that transfer the classification task to the text/word generation task. In this paper, we present a new perspective of leveraging PTLM for TSC: simultaneously leveraging the merits of both language modeling and explicit target-context&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2312.13766v1-abstract-full').style.display = 'inline'; document.getElementById('2312.13766v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2312.13766v1-abstract-full" style="display: none;">
        Existing PTLM-based models for TSC can be categorized into two groups: 1) fine-tuning-based models that adopt PTLM as the context encoder; 2) prompting-based models that transfer the classification task to the text/word generation task. In this paper, we present a new perspective of leveraging PTLM for TSC: simultaneously leveraging the merits of both language modeling and explicit target-context interactions via contextual target attributes. Specifically, we design the domain- and target-constrained cloze test, which can leverage the PTLMs&#39; strong language modeling ability to generate the given target&#39;s attributes pertaining to the review context. The attributes contain the background and property information of the target, which can help to enrich the semantics of the review context and the target. To exploit the attributes for tackling TSC, we first construct a heterogeneous information graph by treating the attributes as nodes and combining them with (1) the syntax graph automatically produced by the off-the-shelf dependency parser and (2) the semantics graph of the review context, which is derived from the self-attention mechanism. Then we propose a heterogeneous information gated graph convolutional network to model the interactions among the attribute information, the syntactic information, and the contextual information. The experimental results on three benchmark datasets demonstrate the superiority of our model, which achieves new state-of-the-art performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2312.13766v1-abstract-full').style.display = 'none'; document.getElementById('2312.13766v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 December, 2023; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2023.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by Journal of Artificial Intelligence Research (JAIR)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2312.03716">arXiv:2312.03716</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2312.03716">pdf</a>, <a href="https://arxiv.org/format/2312.03716">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Co-guiding for Multi-intent Spoken Language Understanding
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xing%2C+B">Bowen Xing</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsang%2C+I+W">Ivor W. Tsang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2312.03716v1-abstract-short" style="display: inline;">
        Recent graph-based models for multi-intent SLU have obtained promising results through modeling the guidance from the prediction of intents to the decoding of slot filling. However, existing methods (1) only model the unidirectional guidance from intent to slot, while there are bidirectional inter-correlations between intent and slot; (2) adopt homogeneous graphs to model the interactions between&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2312.03716v1-abstract-full').style.display = 'inline'; document.getElementById('2312.03716v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2312.03716v1-abstract-full" style="display: none;">
        Recent graph-based models for multi-intent SLU have obtained promising results through modeling the guidance from the prediction of intents to the decoding of slot filling. However, existing methods (1) only model the unidirectional guidance from intent to slot, while there are bidirectional inter-correlations between intent and slot; (2) adopt homogeneous graphs to model the interactions between the slot semantics nodes and intent label nodes, which limit the performance. In this paper, we propose a novel model termed Co-guiding Net, which implements a two-stage framework achieving the mutual guidances between the two tasks. In the first stage, the initial estimated labels of both tasks are produced, and then they are leveraged in the second stage to model the mutual guidances. Specifically, we propose two heterogeneous graph attention networks working on the proposed two heterogeneous semantics label graphs, which effectively represent the relations among the semantics nodes and label nodes. Besides, we further propose Co-guiding-SCL Net, which exploits the single-task and dual-task semantics contrastive relations. For the first stage, we propose single-task supervised contrastive learning, and for the second stage, we propose co-guiding supervised contrastive learning, which considers the two tasks&#39; mutual guidances in the contrastive learning procedure. Experiment results on multi-intent SLU show that our model outperforms existing models by a large margin, obtaining a relative improvement of 21.3% over the previous best model on MixATIS dataset in overall accuracy. We also evaluate our model on the zero-shot cross-lingual scenario and the results show that our model can relatively improve the state-of-the-art model by 33.5% on average in terms of overall accuracy for the total 9 languages.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2312.03716v1-abstract-full').style.display = 'none'; document.getElementById('2312.03716v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 November, 2023; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2023.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by IEEE Transactions on Pattern Analysis and Machine Intelligence (TPAMI). arXiv admin note: substantial text overlap with arXiv:2210.10375</span>
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?query=Ivor+Tsang&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?query=Ivor+Tsang&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?query=Ivor+Tsang&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
        <li>
          <a href="/search/?query=Ivor+Tsang&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=100"
            class="pagination-link "
            aria-label="Page 3"
            aria-current="page">3
          </a>
        </li>
        
        <li>
          <a href="/search/?query=Ivor+Tsang&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=150"
            class="pagination-link "
            aria-label="Page 4"
            aria-current="page">4
          </a>
        </li>
        
      
    </ul>
  </nav>
  

  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>