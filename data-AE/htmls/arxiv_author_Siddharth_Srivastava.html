<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;50 of 72 results for author: <span class="mathjax">Siddharth Srivastava</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/"  aria-role="search">
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Siddharth Srivastava">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Siddharth+Srivastava&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Siddharth Srivastava">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?query=Siddharth+Srivastava&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?query=Siddharth+Srivastava&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?query=Siddharth+Srivastava&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.17045">arXiv:2508.17045</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.17045">pdf</a>, <a href="https://arxiv.org/ps/2508.17045">ps</a>, <a href="https://arxiv.org/format/2508.17045">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Styleclone: Face Stylization with Diffusion Based Data Augmentation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Matiyali%2C+N">Neeraj Matiyali</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sharma%2C+G">Gaurav Sharma</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.17045v1-abstract-short" style="display: inline;">
        We present StyleClone, a method for training image-to-image translation networks to stylize faces in a specific style, even with limited style images. Our approach leverages textual inversion and diffusion-based guided image generation to augment small style datasets. By systematically generating diverse style samples guided by both the original style images and real face images, we significantly&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.17045v1-abstract-full').style.display = 'inline'; document.getElementById('2508.17045v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.17045v1-abstract-full" style="display: none;">
        We present StyleClone, a method for training image-to-image translation networks to stylize faces in a specific style, even with limited style images. Our approach leverages textual inversion and diffusion-based guided image generation to augment small style datasets. By systematically generating diverse style samples guided by both the original style images and real face images, we significantly enhance the diversity of the style dataset. Using this augmented dataset, we train fast image-to-image translation networks that outperform diffusion-based methods in speed and quality. Experiments on multiple styles demonstrate that our method improves stylization quality, better preserves source image content, and significantly accelerates inference. Additionally, we provide a systematic evaluation of the augmentation techniques and their impact on stylization performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.17045v1-abstract-full').style.display = 'none'; document.getElementById('2508.17045v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.17031">arXiv:2508.17031</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.17031">pdf</a>, <a href="https://arxiv.org/ps/2508.17031">ps</a>, <a href="https://arxiv.org/format/2508.17031">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        RephraseTTS: Dynamic Length Text based Speech Insertion with Speaker Style Transfer
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Matiyali%2C+N">Neeraj Matiyali</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sharma%2C+G">Gaurav Sharma</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.17031v1-abstract-short" style="display: inline;">
        We propose a method for the task of text-conditioned speech insertion, i.e. inserting a speech sample in an input speech sample, conditioned on the corresponding complete text transcript. An example use case of the task would be to update the speech audio when corrections are done on the corresponding text transcript. The proposed method follows a transformer-based non-autoregressive approach that&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.17031v1-abstract-full').style.display = 'inline'; document.getElementById('2508.17031v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.17031v1-abstract-full" style="display: none;">
        We propose a method for the task of text-conditioned speech insertion, i.e. inserting a speech sample in an input speech sample, conditioned on the corresponding complete text transcript. An example use case of the task would be to update the speech audio when corrections are done on the corresponding text transcript. The proposed method follows a transformer-based non-autoregressive approach that allows speech insertions of variable lengths, which are dynamically determined during inference, based on the text transcript and tempo of the available partial input. It is capable of maintaining the speaker&#39;s voice characteristics, prosody and other spectral properties of the available speech input. Results from our experiments and user study on LibriTTS show that our method outperforms baselines based on an existing adaptive text to speech method. We also provide numerous qualitative results to appreciate the quality of the output from the proposed method.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.17031v1-abstract-full').style.display = 'none'; document.getElementById('2508.17031v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.13364">arXiv:2507.13364</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.13364">pdf</a>, <a href="https://arxiv.org/ps/2507.13364">ps</a>, <a href="https://arxiv.org/format/2507.13364">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        OmniVec2 -- A Novel Transformer based Network for Large Scale Multimodal and Multitask Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sharma%2C+G">Gaurav Sharma</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.13364v1-abstract-short" style="display: inline;">
        We present a novel multimodal multitask network and associated training algorithm. The method is capable of ingesting data from approximately 12 different modalities namely image, video, audio, text, depth, point cloud, time series, tabular, graph, X-ray, infrared, IMU, and hyperspectral. The proposed approach utilizes modality specialized tokenizers, a shared transformer architecture, and cross-a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.13364v1-abstract-full').style.display = 'inline'; document.getElementById('2507.13364v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.13364v1-abstract-full" style="display: none;">
        We present a novel multimodal multitask network and associated training algorithm. The method is capable of ingesting data from approximately 12 different modalities namely image, video, audio, text, depth, point cloud, time series, tabular, graph, X-ray, infrared, IMU, and hyperspectral. The proposed approach utilizes modality specialized tokenizers, a shared transformer architecture, and cross-attention mechanisms to project the data from different modalities into a unified embedding space. It addresses multimodal and multitask scenarios by incorporating modality-specific task heads for different tasks in respective modalities. We propose a novel pretraining strategy with iterative modality switching to initialize the network, and a training algorithm which trades off fully joint training over all modalities, with training on pairs of modalities at a time. We provide comprehensive evaluation across 25 datasets from 12 modalities and show state of the art performances, demonstrating the effectiveness of the proposed architecture, pretraining strategy and adapted multitask training.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.13364v1-abstract-full').style.display = 'none'; document.getElementById('2507.13364v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 July, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        CVPR 2024
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.06261">arXiv:2507.06261</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.06261">pdf</a>, <a href="https://arxiv.org/ps/2507.06261">ps</a>, <a href="https://arxiv.org/format/2507.06261">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Comanici%2C+G">Gheorghe Comanici</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bieber%2C+E">Eric Bieber</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schaekermann%2C+M">Mike Schaekermann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pasupat%2C+I">Ice Pasupat</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sachdeva%2C+N">Noveen Sachdeva</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dhillon%2C+I">Inderjit Dhillon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Blistein%2C+M">Marcel Blistein</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ram%2C+O">Ori Ram</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+D">Dan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rosen%2C+E">Evan Rosen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Marris%2C+L">Luke Marris</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Petulla%2C+S">Sam Petulla</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gaffney%2C+C">Colin Gaffney</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aharoni%2C+A">Asaf Aharoni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lintz%2C+N">Nathan Lintz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pais%2C+T+C">Tiago Cardal Pais</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jacobsson%2C+H">Henrik Jacobsson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Szpektor%2C+I">Idan Szpektor</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+N">Nan-Jiang Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Haridasan%2C+K">Krishna Haridasan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Omran%2C+A">Ahmed Omran</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Saunshi%2C+N">Nikunj Saunshi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bahri%2C+D">Dara Bahri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mishra%2C+G">Gaurav Mishra</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chu%2C+E">Eric Chu</a>
      , et al. (3284 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.06261v4-abstract-short" style="display: inline;">
        In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. In addition to its incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that excels at multimodal unde&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.06261v4-abstract-full').style.display = 'inline'; document.getElementById('2507.06261v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.06261v4-abstract-full" style="display: none;">
        In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. In addition to its incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that excels at multimodal understanding and it is now able to process up to 3 hours of video content. Its unique combination of long context, multimodal and reasoning capabilities can be combined to unlock new agentic workflows. Gemini 2.5 Flash provides excellent reasoning abilities at a fraction of the compute and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high performance at low latency and cost. Taken together, the Gemini 2.X model generation spans the full Pareto frontier of model capability vs cost, allowing users to explore the boundaries of what is possible with complex agentic problem solving.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.06261v4-abstract-full').style.display = 'none'; document.getElementById('2507.06261v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 July, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 July, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">72 pages, 17 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.22531">arXiv:2506.22531</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.22531">pdf</a>, <a href="https://arxiv.org/ps/2506.22531">ps</a>, <a href="https://arxiv.org/format/2506.22531">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Preserve Anything: Controllable Image Synthesis with Object Preservation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sharma%2C+P+K">Prasen Kumar Sharma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Matiyali%2C+N">Neeraj Matiyali</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sharma%2C+G">Gaurav Sharma</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.22531v2-abstract-short" style="display: inline;">
        We introduce \textit{Preserve Anything}, a novel method for controlled image synthesis that addresses key limitations in object preservation and semantic consistency in text-to-image (T2I) generation. Existing approaches often fail (i) to preserve multiple objects with fidelity, (ii) maintain semantic alignment with prompts, or (iii) provide explicit control over scene composition. To overcome the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.22531v2-abstract-full').style.display = 'inline'; document.getElementById('2506.22531v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.22531v2-abstract-full" style="display: none;">
        We introduce \textit{Preserve Anything}, a novel method for controlled image synthesis that addresses key limitations in object preservation and semantic consistency in text-to-image (T2I) generation. Existing approaches often fail (i) to preserve multiple objects with fidelity, (ii) maintain semantic alignment with prompts, or (iii) provide explicit control over scene composition. To overcome these challenges, the proposed method employs an N-channel ControlNet that integrates (i) object preservation with size and placement agnosticism, color and detail retention, and artifact elimination, (ii) high-resolution, semantically consistent backgrounds with accurate shadows, lighting, and prompt adherence, and (iii) explicit user control over background layouts and lighting conditions. Key components of our framework include object preservation and background guidance modules, enforcing lighting consistency and a high-frequency overlay module to retain fine details while mitigating unwanted artifacts. We introduce a benchmark dataset consisting of 240K natural images filtered for aesthetic quality and 18K 3D-rendered synthetic images with metadata such as lighting, camera angles, and object relationships. This dataset addresses the deficiencies of existing benchmarks and allows a complete evaluation. Empirical results demonstrate that our method achieves state-of-the-art performance, significantly improving feature-space fidelity (FID 15.26) and semantic alignment (CLIP-S 32.85) while maintaining competitive aesthetic quality. We also conducted a user study to demonstrate the efficacy of the proposed work on unseen benchmark and observed a remarkable improvement of $\sim25\%$, $\sim19\%$, $\sim13\%$, and $\sim14\%$ in terms of prompt alignment, photorealism, the presence of AI artifacts, and natural aesthetics over existing works.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.22531v2-abstract-full').style.display = 'none'; document.getElementById('2506.22531v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 July, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 27 June, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at ICCV 2025 (main conference)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2503.12184">arXiv:2503.12184</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2503.12184">pdf</a>, <a href="https://arxiv.org/format/2503.12184">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Combinatorics">math.CO</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Line Graph Characterization of Cyclic Subgroup Graph
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Malviy%2C+S">Siddharth Malviy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kakkar%2C+V">Vipul Kakkar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Swapnil Srivastava</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2503.12184v1-abstract-short" style="display: inline;">
        The cyclic subgroup graph ${Γ(G)}$ of a group $G$ is the simple undirected graph with cyclic subgroups as a vertex set and two distinct vertices $H_1$ and $H_2$ are adjacent if and only if $H_1 \leq H_2$ and there does not exist any cyclic subgroup $K$ such that $H_1 &lt; K &lt; H_2$. In this paper, we classify all the finite groups $G$ such that $Γ(G)$ is the line graph of some graph.
        
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2503.12184v1-abstract-full" style="display: none;">
        The cyclic subgroup graph ${Γ(G)}$ of a group $G$ is the simple undirected graph with cyclic subgroups as a vertex set and two distinct vertices $H_1$ and $H_2$ are adjacent if and only if $H_1 \leq H_2$ and there does not exist any cyclic subgroup $K$ such that $H_1 &lt; K &lt; H_2$. In this paper, we classify all the finite groups $G$ such that $Γ(G)$ is the line graph of some graph.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.12184v1-abstract-full').style.display = 'none'; document.getElementById('2503.12184v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 March, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2412.16395">arXiv:2412.16395</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2412.16395">pdf</a>, <a href="https://arxiv.org/format/2412.16395">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Autonomous Option Invention for Continual Hierarchical Reinforcement Learning and Planning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Nayyar%2C+R+K">Rashmeet Kaur Nayyar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2412.16395v1-abstract-short" style="display: inline;">
        Abstraction is key to scaling up reinforcement learning (RL). However, autonomously learning abstract state and action representations to enable transfer and generalization remains a challenging open problem. This paper presents a novel approach for inventing, representing, and utilizing options, which represent temporally extended behaviors, in continual RL settings. Our approach addresses stream&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.16395v1-abstract-full').style.display = 'inline'; document.getElementById('2412.16395v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2412.16395v1-abstract-full" style="display: none;">
        Abstraction is key to scaling up reinforcement learning (RL). However, autonomously learning abstract state and action representations to enable transfer and generalization remains a challenging open problem. This paper presents a novel approach for inventing, representing, and utilizing options, which represent temporally extended behaviors, in continual RL settings. Our approach addresses streams of stochastic problems characterized by long horizons, sparse rewards, and unknown transition and reward functions.
  Our approach continually learns and maintains an interpretable state abstraction, and uses it to invent high-level options with abstract symbolic representations. These options meet three key desiderata: (1) composability for solving tasks effectively with lookahead planning, (2) reusability across problem instances for minimizing the need for relearning, and (3) mutual independence for reducing interference among options. Our main contributions are approaches for continually learning transferable, generalizable options with symbolic representations, and for integrating search techniques with RL to efficiently plan over these learned options to solve new problems. Empirical results demonstrate that the resulting approach effectively learns and transfers abstract knowledge across problem instances, achieving superior sample efficiency compared to state-of-the-art methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.16395v1-abstract-full').style.display = 'none'; document.getElementById('2412.16395v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 December, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2412.05528">arXiv:2412.05528</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2412.05528">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AI Planning: A Primer and Survey (Preliminary Report)
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+D+Z">Dillon Z. Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Verma%2C+P">Pulkit Verma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Katz%2C+M">Michael Katz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Thi%C3%A9baux%2C+S">Sylvie Thiébaux</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2412.05528v1-abstract-short" style="display: inline;">
        Automated decision-making is a fundamental topic that spans multiple sub-disciplines in AI: reinforcement learning (RL), AI planning (AP), foundation models, and operations research, among others. Despite recent efforts to ``bridge the gaps&#39;&#39; between these communities, there remain many insights that have not yet transcended the boundaries. Our goal in this paper is to provide a brief and non-exha&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.05528v1-abstract-full').style.display = 'inline'; document.getElementById('2412.05528v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2412.05528v1-abstract-full" style="display: none;">
        Automated decision-making is a fundamental topic that spans multiple sub-disciplines in AI: reinforcement learning (RL), AI planning (AP), foundation models, and operations research, among others. Despite recent efforts to ``bridge the gaps&#39;&#39; between these communities, there remain many insights that have not yet transcended the boundaries. Our goal in this paper is to provide a brief and non-exhaustive primer on ideas well-known in AP, but less so in other sub-disciplines. We do so by introducing the classical AP problem and representation, and extensions that handle uncertainty and time through the Markov Decision Process formalism. Next, we survey state-of-the-art techniques and ideas for solving AP problems, focusing on their ability to exploit problem structure. Lastly, we cover subfields within AP for learning structure from unstructured inputs and learning to generalise to unseen scenarios and situations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.05528v1-abstract-full').style.display = 'none'; document.getElementById('2412.05528v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 December, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.08437">arXiv:2410.08437</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.08437">pdf</a>, <a href="https://arxiv.org/format/2410.08437">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Autonomous Evaluation of LLMs for Truth Maintenance and Reasoning Tasks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Karia%2C+R">Rushang Karia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bramblett%2C+D">Daniel Bramblett</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dobhal%2C+D">Daksh Dobhal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.08437v3-abstract-short" style="display: inline;">
        This paper presents AutoEval, a novel benchmark for scaling Large Language Model (LLM) assessment in formal tasks with clear notions of correctness, such as truth maintenance in translation and logical reasoning. AutoEval is the first benchmarking paradigm that offers several key advantages necessary for scaling objective evaluation of LLMs without human labeling: (a) ability to evaluate LLMs of i&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.08437v3-abstract-full').style.display = 'inline'; document.getElementById('2410.08437v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.08437v3-abstract-full" style="display: none;">
        This paper presents AutoEval, a novel benchmark for scaling Large Language Model (LLM) assessment in formal tasks with clear notions of correctness, such as truth maintenance in translation and logical reasoning. AutoEval is the first benchmarking paradigm that offers several key advantages necessary for scaling objective evaluation of LLMs without human labeling: (a) ability to evaluate LLMs of increasing sophistication by auto-generating tasks at different levels of difficulty; (b) auto-generation of ground truth that eliminates dependence on expensive and time-consuming human annotation; (c) the use of automatically generated, randomized datasets that mitigate the ability of successive LLMs to overfit to static datasets used in many contemporary benchmarks. Empirical analysis shows that an LLM&#39;s performance on AutoEval is highly indicative of its performance on a diverse array of other benchmarks focusing on translation and reasoning tasks, making it a valuable autonomous evaluation paradigm in settings where hand-curated datasets can be hard to obtain and/or update.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.08437v3-abstract-full').style.display = 'none'; document.getElementById('2410.08437v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 April, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 10 October, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.12002">arXiv:2409.12002</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.12002">pdf</a>, <a href="https://arxiv.org/format/2409.12002">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards Global Localization using Multi-Modal Object-Instance Re-Identification
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chavan%2C+A">Aneesh Chavan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agrawal%2C+V">Vaibhav Agrawal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bhat%2C+V">Vineeth Bhat</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chittawar%2C+S">Sarthak Chittawar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Arora%2C+C">Chetan Arora</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Krishna%2C+K+M">K Madhava Krishna</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.12002v2-abstract-short" style="display: inline;">
        Re-identification (ReID) is a critical challenge in computer vision, predominantly studied in the context of pedestrians and vehicles. However, robust object-instance ReID, which has significant implications for tasks such as autonomous exploration, long-term perception, and scene understanding, remains underexplored. In this work, we address this gap by proposing a novel dual-path object-instance&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.12002v2-abstract-full').style.display = 'inline'; document.getElementById('2409.12002v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.12002v2-abstract-full" style="display: none;">
        Re-identification (ReID) is a critical challenge in computer vision, predominantly studied in the context of pedestrians and vehicles. However, robust object-instance ReID, which has significant implications for tasks such as autonomous exploration, long-term perception, and scene understanding, remains underexplored. In this work, we address this gap by proposing a novel dual-path object-instance re-identification transformer architecture that integrates multimodal RGB and depth information. By leveraging depth data, we demonstrate improvements in ReID across scenes that are cluttered or have varying illumination conditions. Additionally, we develop a ReID-based localization framework that enables accurate camera localization and pose identification across different viewpoints. We validate our methods using two custom-built RGB-D datasets, as well as multiple sequences from the open-source TUM RGB-D datasets. Our approach demonstrates significant improvements in both object instance ReID (mAP of 75.18) and localization accuracy (success rate of 83% on TUM-RGBD), highlighting the essential role of object ReID in advancing robotic perception. Our models, frameworks, and datasets have been made publicly available.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.12002v2-abstract-full').style.display = 'none'; document.getElementById('2409.12002v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 May, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages, 5 figures, 3 tables. Accepted at Advances in Robotics, AIR 2025 (Oral)</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68T40
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.9; I.2.10
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.02831">arXiv:2409.02831</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.02831">pdf</a>, <a href="https://arxiv.org/ps/2409.02831">ps</a>, <a href="https://arxiv.org/format/2409.02831">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Instrumentation and Methods for Astrophysics">astro-ph.IM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="General Relativity and Quantum Cosmology">gr-qc</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1088/1361-6382/adc4b6">10.1088/1361-6382/adc4b6 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LIGO Detector Characterization in the first half of the fourth Observing run
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Soni%2C+S">S. Soni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berger%2C+B+K">B. K. Berger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Davis%2C+D">D. Davis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Renzo%2C+F+D">F. Di. Renzo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Effler%2C+A">A. Effler</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ferreira%2C+T+A">T. A. Ferreira</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Glanzer%2C+J">J. Glanzer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goetz%2C+E">E. Goetz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gonz%C3%A1lez%2C+G">G. González</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Helmling-Cornell%2C+A">A. Helmling-Cornell</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hughey%2C+B">B. Hughey</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huxford%2C+R">R. Huxford</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mannix%2C+B">B. Mannix</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mo%2C+G">G. Mo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nandi%2C+D">D. Nandi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Neunzert%2C+A">A. Neunzert</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nichols%2C+S">S. Nichols</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pham%2C+K">K. Pham</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Renzini%2C+A+I">A. I. Renzini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schofield%2C+R+M+S">R. M. S. Schofield</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Stuver%2C+A">A Stuver</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Trevor%2C+M">M. Trevor</a>, 
      
      <a href="/search/?searchtype=author&amp;query=%C3%81lvarez-L%C3%B3pez%2C+S">S. Álvarez-López</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Beda%2C+R">R. Beda</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berry%2C+C+P+L">C. P. L. Berry</a>
      , et al. (211 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.02831v2-abstract-short" style="display: inline;">
        Progress in gravitational-wave astronomy depends upon having sensitive detectors with good data quality. Since the end of the LIGO-Virgo-KAGRA third Observing run in March 2020, detector-characterization efforts have lead to increased sensitivity of the detectors, swifter validation of gravitational-wave candidates and improved tools used for data-quality products. In this article, we discuss thes&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.02831v2-abstract-full').style.display = 'inline'; document.getElementById('2409.02831v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.02831v2-abstract-full" style="display: none;">
        Progress in gravitational-wave astronomy depends upon having sensitive detectors with good data quality. Since the end of the LIGO-Virgo-KAGRA third Observing run in March 2020, detector-characterization efforts have lead to increased sensitivity of the detectors, swifter validation of gravitational-wave candidates and improved tools used for data-quality products. In this article, we discuss these efforts in detail and their impact on our ability to detect and study gravitational-waves. These include the multiple instrumental investigations that led to reduction in transient noise, along with the work to improve software tools used to examine the detectors data-quality. We end with a brief discussion on the role and requirements of detector characterization as the sensitivity of our detectors further improves in the future Observing runs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.02831v2-abstract-full').style.display = 'none'; document.getElementById('2409.02831v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 July, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 4 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">35 pages, 18 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.15907">arXiv:2405.15907</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.15907">pdf</a>, <a href="https://arxiv.org/format/2405.15907">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Belief-State Query Policies for User-Aligned POMDPs
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bramblett%2C+D">Daniel Bramblett</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.15907v2-abstract-short" style="display: inline;">
        Planning in real-world settings often entails addressing partial observability while aligning with users&#39; requirements. We present a novel framework for expressing users&#39; constraints and preferences about agent behavior in a partially observable setting using parameterized belief-state query (BSQ) policies in the setting of goal-oriented partially observable Markov decision processes (gPOMDPs). We&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.15907v2-abstract-full').style.display = 'inline'; document.getElementById('2405.15907v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.15907v2-abstract-full" style="display: none;">
        Planning in real-world settings often entails addressing partial observability while aligning with users&#39; requirements. We present a novel framework for expressing users&#39; constraints and preferences about agent behavior in a partially observable setting using parameterized belief-state query (BSQ) policies in the setting of goal-oriented partially observable Markov decision processes (gPOMDPs). We present the first formal analysis of such constraints and prove that while the expected cost function of a parameterized BSQ policy w.r.t its parameters is not convex, it is piecewise constant and yields an implicit discrete parameter search space that is finite for finite horizons. This theoretical result leads to novel algorithms that optimize gPOMDP agent behavior with guaranteed user alignment. Analysis proves that our algorithms converge to the optimal user-aligned behavior in the limit. Empirical results show that parameterized BSQ policies provide a computationally feasible approach for user-aligned planning in partially observable settings.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.15907v2-abstract-full').style.display = 'none'; document.getElementById('2405.15907v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 April, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 24 May, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.00808">arXiv:2404.00808</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.00808">pdf</a>, <a href="https://arxiv.org/ps/2404.00808">ps</a>, <a href="https://arxiv.org/format/2404.00808">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Using Explainable AI and Hierarchical Planning for Outreach with Robots
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Karia%2C+R">Rushang Karia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nagpal%2C+J">Jayesh Nagpal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dobhal%2C+D">Daksh Dobhal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Verma%2C+P">Pulkit Verma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nayyar%2C+R+K">Rashmeet Kaur Nayyar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Naman Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.00808v3-abstract-short" style="display: inline;">
        Understanding how robots plan and execute tasks is crucial in today&#39;s world, where they are becoming more prevalent in our daily lives. However, teaching non-experts, such as K-12 students, the complexities of robot planning can be challenging. This work presents an open-source platform, JEDAI.Ed, that simplifies the process using a visual interface that abstracts the details of various planning p&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.00808v3-abstract-full').style.display = 'inline'; document.getElementById('2404.00808v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.00808v3-abstract-full" style="display: none;">
        Understanding how robots plan and execute tasks is crucial in today&#39;s world, where they are becoming more prevalent in our daily lives. However, teaching non-experts, such as K-12 students, the complexities of robot planning can be challenging. This work presents an open-source platform, JEDAI.Ed, that simplifies the process using a visual interface that abstracts the details of various planning processes that robots use for performing complex mobile manipulation tasks. Using principles developed in the field of explainable AI, this intuitive platform enables students to use a high-level intuitive instruction set to perform complex tasks, visualize them on an in-built simulator, and to obtain helpful hints and natural language explanations for errors. Finally, JEDAI.Ed, includes an adaptive curriculum generation method that provides students with customized learning ramps. This platform&#39;s efficacy was tested through a user study with university students who had little to no computer science background. Our results show that JEDAI.Ed is highly effective in increasing student engagement, teaching robotics programming, and decreasing the time need to solve tasks as compared to baselines.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.00808v3-abstract-full').style.display = 'none'; document.getElementById('2404.00808v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 June, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 31 March, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.18327">arXiv:2403.18327</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.18327">pdf</a>, <a href="https://arxiv.org/format/2403.18327">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        $\forall$uto$\exists$val: Autonomous Assessment of LLMs in Formal Synthesis and Interpretation Tasks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Karia%2C+R">Rushang Karia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bramblett%2C+D">Daniel Bramblett</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dobhal%2C+D">Daksh Dobhal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Verma%2C+P">Pulkit Verma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.18327v2-abstract-short" style="display: inline;">
        This paper presents $\forall$uto$\exists$val, a new approach for scaling LLM assessment in translating formal syntax -- such as first-order logic, regular expressions, etc -- to natural language (interpretation) or vice versa (compilation), thereby facilitating their use in applications such as generating/explaining logic and control flow for programs etc. Existing approaches for LLM assessment in&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.18327v2-abstract-full').style.display = 'inline'; document.getElementById('2403.18327v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.18327v2-abstract-full" style="display: none;">
        This paper presents $\forall$uto$\exists$val, a new approach for scaling LLM assessment in translating formal syntax -- such as first-order logic, regular expressions, etc -- to natural language (interpretation) or vice versa (compilation), thereby facilitating their use in applications such as generating/explaining logic and control flow for programs etc. Existing approaches for LLM assessment in these areas require labor-intensive ground-truth creation, the availability of which undermines the separation of training and test sets. Furthermore, such datasets typically include relatively few hand-coded test cases over which LLM accuracy is determined, thus making them inadequate for determining the safety or correctness of their generated outputs. We introduce a new approach that utilizes context-free grammars (CFGs) to generate out-of-distribution datasets on the fly and perform closed-loop testing of LLM capabilities using formal verifiers to guarantee the correctness of LLM outputs without any human intervention. We release our dataset and benchmark as open-source code at \url{https://github.com/AAIR-lab/auto-llm-assessment}. We also conduct an assessment of several SOTA closed and open-source LLMs to showcase the feasibility and scalability of this paradigm. Our experiments reveal that SOTA LLMs are unable to solve the formal translation task adequately.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.18327v2-abstract-full').style.display = 'none'; document.getElementById('2403.18327v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 July, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 27 March, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.11871">arXiv:2402.11871</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.11871">pdf</a>, <a href="https://arxiv.org/ps/2402.11871">ps</a>, <a href="https://arxiv.org/format/2402.11871">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        From Real World to Logic and Back: Learning Generalizable Relational Concepts For Long Horizon Robot Planning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Naman Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nagpal%2C+J">Jayesh Nagpal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.11871v5-abstract-short" style="display: inline;">
        Humans efficiently generalize from limited demonstrations, but robots still struggle to transfer learned knowledge to complex, unseen tasks with longer horizons and increased complexity. We propose the first known method enabling robots to autonomously invent relational concepts directly from small sets of unannotated, unsegmented demonstrations. The learned symbolic concepts are grounded into log&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.11871v5-abstract-full').style.display = 'inline'; document.getElementById('2402.11871v5-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.11871v5-abstract-full" style="display: none;">
        Humans efficiently generalize from limited demonstrations, but robots still struggle to transfer learned knowledge to complex, unseen tasks with longer horizons and increased complexity. We propose the first known method enabling robots to autonomously invent relational concepts directly from small sets of unannotated, unsegmented demonstrations. The learned symbolic concepts are grounded into logic-based world models, facilitating efficient zero-shot generalization to significantly more complex tasks. Empirical results demonstrate that our approach achieves performance comparable to hand-crafted models, successfully scaling execution horizons and handling up to 18 times more objects than seen in training, providing the first autonomous framework for learning transferable symbolic abstractions from raw robot trajectories.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.11871v5-abstract-full').style.display = 'none'; document.getElementById('2402.11871v5-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 June, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 February, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.08145">arXiv:2402.08145</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.08145">pdf</a>, <a href="https://arxiv.org/format/2402.08145">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1609/icaps.v34i1.31489">10.1609/icaps.v34i1.31489 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Epistemic Exploration for Generalizable Planning and Learning in Non-Stationary Settings
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Karia%2C+R">Rushang Karia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Verma%2C+P">Pulkit Verma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Speranzon%2C+A">Alberto Speranzon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.08145v2-abstract-short" style="display: inline;">
        This paper introduces a new approach for continual planning and model learning in relational, non-stationary stochastic environments. Such capabilities are essential for the deployment of sequential decision-making systems in the uncertain and constantly evolving real world. Working in such practical settings with unknown (and non-stationary) transition systems and changing tasks, the proposed fra&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.08145v2-abstract-full').style.display = 'inline'; document.getElementById('2402.08145v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.08145v2-abstract-full" style="display: none;">
        This paper introduces a new approach for continual planning and model learning in relational, non-stationary stochastic environments. Such capabilities are essential for the deployment of sequential decision-making systems in the uncertain and constantly evolving real world. Working in such practical settings with unknown (and non-stationary) transition systems and changing tasks, the proposed framework models gaps in the agent&#39;s current state of knowledge and uses them to conduct focused, investigative explorations. Data collected using these explorations is used for learning generalizable probabilistic models for solving the current task despite continual changes in the environment dynamics. Empirical evaluations on several non-stationary benchmark domains show that this approach significantly outperforms planning and RL baselines in terms of sample complexity. Theoretical results show that the system exhibits desirable convergence properties when stationarity holds.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.08145v2-abstract-full').style.display = 'none'; document.getElementById('2402.08145v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 June, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 12 February, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To appear at ICAPS-24</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Proceedings of the International Conference on Automated Planning and Scheduling, 34(1), 310-318, 2024
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2311.05709">arXiv:2311.05709</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2311.05709">pdf</a>, <a href="https://arxiv.org/ps/2311.05709">ps</a>, <a href="https://arxiv.org/format/2311.05709">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        OmniVec: Learning robust representations with cross modal sharing
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sharma%2C+G">Gaurav Sharma</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2311.05709v1-abstract-short" style="display: inline;">
        Majority of research in learning based methods has been towards designing and training networks for specific tasks. However, many of the learning based tasks, across modalities, share commonalities and could be potentially tackled in a joint framework. We present an approach in such direction, to learn multiple tasks, in multiple modalities, with a unified architecture. The proposed network is com&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2311.05709v1-abstract-full').style.display = 'inline'; document.getElementById('2311.05709v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2311.05709v1-abstract-full" style="display: none;">
        Majority of research in learning based methods has been towards designing and training networks for specific tasks. However, many of the learning based tasks, across modalities, share commonalities and could be potentially tackled in a joint framework. We present an approach in such direction, to learn multiple tasks, in multiple modalities, with a unified architecture. The proposed network is composed of task specific encoders, a common trunk in the middle, followed by task specific prediction heads. We first pre-train it by self-supervised masked training, followed by sequential training for the different tasks. We train the network on all major modalities, e.g.\ visual, audio, text and 3D, and report results on $22$ diverse and challenging public benchmarks. We demonstrate empirically that, using a joint network to train across modalities leads to meaningful information sharing and this allows us to achieve state-of-the-art results on most of the benchmarks. We also show generalization of the trained network on cross-modal tasks as well as unseen datasets and tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2311.05709v1-abstract-full').style.display = 'none'; document.getElementById('2311.05709v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 November, 2023; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2023.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to WACV 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2310.02251">arXiv:2310.02251</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2310.02251">pdf</a>, <a href="https://arxiv.org/format/2310.02251">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Talk2BEV: Language-enhanced Bird&#39;s-eye View Maps for Autonomous Driving
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Choudhary%2C+T">Tushar Choudhary</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dewangan%2C+V">Vikrant Dewangan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chandhok%2C+S">Shivam Chandhok</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Priyadarshan%2C+S">Shubham Priyadarshan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jain%2C+A">Anushka Jain</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Singh%2C+A+K">Arun K. Singh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jatavallabhula%2C+K+M">Krishna Murthy Jatavallabhula</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Krishna%2C+K+M">K. Madhava Krishna</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2310.02251v2-abstract-short" style="display: inline;">
        Talk2BEV is a large vision-language model (LVLM) interface for bird&#39;s-eye view (BEV) maps in autonomous driving contexts. While existing perception systems for autonomous driving scenarios have largely focused on a pre-defined (closed) set of object categories and driving scenarios, Talk2BEV blends recent advances in general-purpose language and vision models with BEV-structured map representation&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2310.02251v2-abstract-full').style.display = 'inline'; document.getElementById('2310.02251v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2310.02251v2-abstract-full" style="display: none;">
        Talk2BEV is a large vision-language model (LVLM) interface for bird&#39;s-eye view (BEV) maps in autonomous driving contexts. While existing perception systems for autonomous driving scenarios have largely focused on a pre-defined (closed) set of object categories and driving scenarios, Talk2BEV blends recent advances in general-purpose language and vision models with BEV-structured map representations, eliminating the need for task-specific models. This enables a single system to cater to a variety of autonomous driving tasks encompassing visual and spatial reasoning, predicting the intents of traffic actors, and decision-making based on visual cues. We extensively evaluate Talk2BEV on a large number of scene understanding tasks that rely on both the ability to interpret free-form natural language queries, and in grounding these queries to the visual context embedded into the language-enhanced BEV map. To enable further research in LVLMs for autonomous driving scenarios, we develop and release Talk2BEV-Bench, a benchmark encompassing 1000 human-annotated BEV scenarios, with more than 20,000 questions and ground-truth responses from the NuScenes dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2310.02251v2-abstract-full').style.display = 'none'; document.getElementById('2310.02251v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 November, 2023; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 3 October, 2023;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2023.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project page at https://llmbev.github.io/talk2bev/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2306.04806">arXiv:2306.04806</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2306.04806">pdf</a>, <a href="https://arxiv.org/format/2306.04806">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Autonomous Capability Assessment of Sequential Decision-Making Systems in Stochastic Settings (Extended Version)
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Verma%2C+P">Pulkit Verma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Karia%2C+R">Rushang Karia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2306.04806v2-abstract-short" style="display: inline;">
        It is essential for users to understand what their AI systems can and can&#39;t do in order to use them safely. However, the problem of enabling users to assess AI systems with sequential decision-making (SDM) capabilities is relatively understudied. This paper presents a new approach for modeling the capabilities of black-box AI systems that can plan and act, along with the possible effects and requi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2306.04806v2-abstract-full').style.display = 'inline'; document.getElementById('2306.04806v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2306.04806v2-abstract-full" style="display: none;">
        It is essential for users to understand what their AI systems can and can&#39;t do in order to use them safely. However, the problem of enabling users to assess AI systems with sequential decision-making (SDM) capabilities is relatively understudied. This paper presents a new approach for modeling the capabilities of black-box AI systems that can plan and act, along with the possible effects and requirements for executing those capabilities in stochastic settings. We present an active-learning approach that can effectively interact with a black-box SDM system and learn an interpretable probabilistic model describing its capabilities. Theoretical analysis of the approach identifies the conditions under which the learning process is guaranteed to converge to the correct model of the agent; empirical evaluations on different agents and simulated scenarios show that this approach is few-shot generalizable and can effectively describe the capabilities of arbitrary black-box SDM agents in a sample-efficient manner.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2306.04806v2-abstract-full').style.display = 'none'; document.getElementById('2306.04806v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 October, 2023; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 June, 2023;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2023.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">NeurIPS 2023</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2212.09175">arXiv:2212.09175</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2212.09175">pdf</a>, <a href="https://arxiv.org/format/2212.09175">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Predicting Citi Bike Demand Evolution Using Dynamic Graphs
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Saff%2C+A">Alexander Saff</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bhandary%2C+M">Mayur Bhandary</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2212.09175v1-abstract-short" style="display: inline;">
        Bike sharing systems often suffer from poor capacity management as a result of variable demand. These bike sharing systems would benefit from models to predict demand in order to moderate the number of bikes stored at each station. In this paper, we attempt to apply a graph neural network model to predict bike demand in the New York City, Citi Bike dataset.
        
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2212.09175v1-abstract-full" style="display: none;">
        Bike sharing systems often suffer from poor capacity management as a result of variable demand. These bike sharing systems would benefit from models to predict demand in order to moderate the number of bikes stored at each station. In this paper, we attempt to apply a graph neural network model to predict bike demand in the New York City, Citi Bike dataset.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2212.09175v1-abstract-full').style.display = 'none'; document.getElementById('2212.09175v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 December, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2212.02823">arXiv:2212.02823</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2212.02823">pdf</a>, <a href="https://arxiv.org/format/2212.02823">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Hierarchical Decomposition and Analysis for Generalized Planning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2212.02823v2-abstract-short" style="display: inline;">
        This paper presents new methods for analyzing and evaluating generalized plans that can solve broad classes of related planning problems. Although synthesis and learning of generalized plans has been a longstanding goal in AI, it remains challenging due to fundamental gaps in methods for analyzing the scope and utility of a given generalized plan. This paper addresses these gaps by developing a ne&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2212.02823v2-abstract-full').style.display = 'inline'; document.getElementById('2212.02823v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2212.02823v2-abstract-full" style="display: none;">
        This paper presents new methods for analyzing and evaluating generalized plans that can solve broad classes of related planning problems. Although synthesis and learning of generalized plans has been a longstanding goal in AI, it remains challenging due to fundamental gaps in methods for analyzing the scope and utility of a given generalized plan. This paper addresses these gaps by developing a new conceptual framework along with proof techniques and algorithmic processes for assessing termination and goal-reachability related properties of generalized plans. We build upon classic results from graph theory to decompose generalized plans into smaller components that are then used to derive hierarchical termination arguments. These methods can be used to determine the utility of a given generalized plan, as well as to guide the synthesis and learning processes for generalized plans. We present theoretical as well as empirical results illustrating the scope of this new approach. Our analysis shows that this approach significantly extends the class of generalized plans that can be assessed automatically, thereby reducing barriers in the synthesis and learning of reliable generalized plans.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2212.02823v2-abstract-full').style.display = 'none'; document.getElementById('2212.02823v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 June, 2023; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 December, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted for publication at JAIR</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2210.01955">arXiv:2210.01955</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2210.01955">pdf</a>, <a href="https://arxiv.org/format/2210.01955">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Dynamic Abstract Representations for Sample-Efficient Reinforcement Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dadvar%2C+M">Mehdi Dadvar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nayyar%2C+R+K">Rashmeet Kaur Nayyar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2210.01955v2-abstract-short" style="display: inline;">
        In many real-world problems, the learning agent needs to learn a problem&#39;s abstractions and solution simultaneously. However, most such abstractions need to be designed and refined by hand for different problems and domains of application. This paper presents a novel top-down approach for constructing state abstractions while carrying out reinforcement learning. Starting with state variables and a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2210.01955v2-abstract-full').style.display = 'inline'; document.getElementById('2210.01955v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2210.01955v2-abstract-full" style="display: none;">
        In many real-world problems, the learning agent needs to learn a problem&#39;s abstractions and solution simultaneously. However, most such abstractions need to be designed and refined by hand for different problems and domains of application. This paper presents a novel top-down approach for constructing state abstractions while carrying out reinforcement learning. Starting with state variables and a simulator, it presents a novel domain-independent approach for dynamically computing an abstraction based on the dispersion of Q-values in abstract states as the agent continues acting and learning. Extensive empirical evaluation on multiple domains and problems shows that this approach automatically learns abstractions that are finely-tuned to the problem, yield powerful sample efficiency, and result in the RL agent significantly outperforming existing approaches.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2210.01955v2-abstract-full').style.display = 'none'; document.getElementById('2210.01955v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 December, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 4 October, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2210.00068">arXiv:2210.00068</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2210.00068">pdf</a>, <a href="https://arxiv.org/format/2210.00068">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multi-Task Option Learning and Discovery for Stochastic Path Planning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Naman Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2210.00068v2-abstract-short" style="display: inline;">
        This paper addresses the problem of reliably and efficiently solving broad classes of long-horizon stochastic path planning problems. Starting with a vanilla RL formulation with a stochastic dynamics simulator and an occupancy matrix of the environment, our approach computes useful options with policies as well as high-level paths that compose the discovered options. Our main contributions are (1)&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2210.00068v2-abstract-full').style.display = 'inline'; document.getElementById('2210.00068v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2210.00068v2-abstract-full" style="display: none;">
        This paper addresses the problem of reliably and efficiently solving broad classes of long-horizon stochastic path planning problems. Starting with a vanilla RL formulation with a stochastic dynamics simulator and an occupancy matrix of the environment, our approach computes useful options with policies as well as high-level paths that compose the discovered options. Our main contributions are (1) data-driven methods for creating abstract states that serve as endpoints for helpful options, (2) methods for computing option policies using auto-generated option guides in the form of dense pseudo-reward functions, and (3) an overarching algorithm for composing the computed options. We show that this approach yields strong guarantees of executability and solvability: under fairly general conditions, the computed option guides lead to composable option policies and consequently ensure downward refinability. Empirical evaluation on a range of robots, environments, and tasks shows that this approach effectively transfers knowledge across related tasks and that it outperforms existing approaches by a significant margin.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2210.00068v2-abstract-full').style.display = 'none'; document.getElementById('2210.00068v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 December, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 30 September, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.12665">arXiv:2204.12665</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.12665">pdf</a>, <a href="https://arxiv.org/format/2204.12665">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Relational Abstractions for Generalized Reinforcement Learning on Symbolic Problems
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Karia%2C+R">Rushang Karia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.12665v1-abstract-short" style="display: inline;">
        Reinforcement learning in problems with symbolic state spaces is challenging due to the need for reasoning over long horizons. This paper presents a new approach that utilizes relational abstractions in conjunction with deep learning to learn a generalizable Q-function for such problems. The learned Q-function can be efficiently transferred to related problems that have different object names and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.12665v1-abstract-full').style.display = 'inline'; document.getElementById('2204.12665v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.12665v1-abstract-full" style="display: none;">
        Reinforcement learning in problems with symbolic state spaces is challenging due to the need for reasoning over long horizons. This paper presents a new approach that utilizes relational abstractions in conjunction with deep learning to learn a generalizable Q-function for such problems. The learned Q-function can be efficiently transferred to related problems that have different object names and object quantities, and thus, entirely different state spaces. We show that the learned generalized Q-function can be utilized for zero-shot transfer to related problems without an explicit, hand-coded curriculum. Empirical evaluations on a range of problems show that our method facilitates efficient zero-shot transfer of learned knowledge to much larger problem instances containing many objects.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.12665v1-abstract-full').style.display = 'none'; document.getElementById('2204.12665v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 April, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To be published in IJCAI-22</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.04301">arXiv:2204.04301</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.04301">pdf</a>, <a href="https://arxiv.org/format/2204.04301">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Generalized Policy Automata for Relational Stochastic Shortest Path Problems
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Karia%2C+R">Rushang Karia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nayyar%2C+R+K">Rashmeet Kaur Nayyar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.04301v3-abstract-short" style="display: inline;">
        Several goal-oriented problems in the real-world can be naturally expressed as Stochastic Shortest Path Problems (SSPs). However, the computational complexity of solving SSPs makes finding solutions to even moderately sized problems intractable. Currently, existing state-of-the-art planners and heuristics often fail to exploit knowledge learned from solving other instances. This paper presents an&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.04301v3-abstract-full').style.display = 'inline'; document.getElementById('2204.04301v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.04301v3-abstract-full" style="display: none;">
        Several goal-oriented problems in the real-world can be naturally expressed as Stochastic Shortest Path Problems (SSPs). However, the computational complexity of solving SSPs makes finding solutions to even moderately sized problems intractable. Currently, existing state-of-the-art planners and heuristics often fail to exploit knowledge learned from solving other instances. This paper presents an approach for learning \emph{Generalized Policy Automata} (GPA): non-deterministic partial policies that can be used to catalyze the solution process. GPAs are learned using relational, feature-based abstractions, which makes them applicable on broad classes of related problems with different object names and quantities. Theoretical analysis of this approach shows that it guarantees completeness and hierarchical optimality. Empirical analysis shows that this approach effectively learns broadly applicable policy knowledge in a few-shot fashion and significantly outperforms state-of-the-art SSP solvers on test problems whose object counts are far greater than those used during training.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.04301v3-abstract-full').style.display = 'none'; document.getElementById('2204.04301v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 October, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 April, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.13236">arXiv:2203.13236</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.13236">pdf</a>, <a href="https://arxiv.org/format/2203.13236">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Differential Assessment of Black-Box AI Agents
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Nayyar%2C+R+K">Rashmeet Kaur Nayyar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Verma%2C+P">Pulkit Verma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.13236v2-abstract-short" style="display: inline;">
        Much of the research on learning symbolic models of AI agents focuses on agents with stationary models. This assumption fails to hold in settings where the agent&#39;s capabilities may change as a result of learning, adaptation, or other post-deployment modifications. Efficient assessment of agents in such settings is critical for learning the true capabilities of an AI system and for ensuring its saf&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.13236v2-abstract-full').style.display = 'inline'; document.getElementById('2203.13236v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.13236v2-abstract-full" style="display: none;">
        Much of the research on learning symbolic models of AI agents focuses on agents with stationary models. This assumption fails to hold in settings where the agent&#39;s capabilities may change as a result of learning, adaptation, or other post-deployment modifications. Efficient assessment of agents in such settings is critical for learning the true capabilities of an AI system and for ensuring its safe usage. In this work, we propose a novel approach to &#34;differentially&#34; assess black-box AI agents that have drifted from their previously known models. As a starting point, we consider the fully observable and deterministic setting. We leverage sparse observations of the drifted agent&#39;s current behavior and knowledge of its initial model to generate an active querying policy that selectively queries the agent and computes an updated model of its functionality. Empirical evaluation shows that our approach is much more efficient than re-learning the agent model from scratch. We also show that the cost of differential assessment using our method is proportional to the amount of drift in the agent&#39;s functionality.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.13236v2-abstract-full').style.display = 'none'; document.getElementById('2203.13236v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 May, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 24 March, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">AAAI 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2202.00907">arXiv:2202.00907</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2202.00907">pdf</a>, <a href="https://arxiv.org/format/2202.00907">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Using Deep Learning to Bootstrap Abstractions for Hierarchical Robot Planning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Naman Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2202.00907v4-abstract-short" style="display: inline;">
        This paper addresses the problem of learning abstractions that boost robot planning performance while providing strong guarantees of reliability. Although state-of-the-art hierarchical robot planning algorithms allow robots to efficiently compute long-horizon motion plans for achieving user desired tasks, these methods typically rely upon environment-dependent state and action abstractions that ne&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.00907v4-abstract-full').style.display = 'inline'; document.getElementById('2202.00907v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2202.00907v4-abstract-full" style="display: none;">
        This paper addresses the problem of learning abstractions that boost robot planning performance while providing strong guarantees of reliability. Although state-of-the-art hierarchical robot planning algorithms allow robots to efficiently compute long-horizon motion plans for achieving user desired tasks, these methods typically rely upon environment-dependent state and action abstractions that need to be hand-designed by experts.
  We present a new approach for bootstrapping the entire hierarchical planning process. This allows us to compute abstract states and actions for new environments automatically using the critical regions predicted by a deep neural network with an auto-generated robot-specific architecture. We show that the learned abstractions can be used with a novel multi-source bi-directional hierarchical robot planning algorithm that is sound and probabilistically complete. An extensive empirical evaluation on twenty different settings using holonomic and non-holonomic robots shows that (a) our learned abstractions provide the information necessary for efficient multi-source hierarchical planning; and that (b) this approach of learning, abstractions, and planning outperforms state-of-the-art baselines by nearly a factor of ten in terms of planning time on test environments not seen during training.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.00907v4-abstract-full').style.display = 'none'; document.getElementById('2202.00907v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 April, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 2 February, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.08046">arXiv:2111.08046</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.08046">pdf</a>, <a href="https://arxiv.org/format/2111.08046">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Beyond Mono to Binaural: Generating Binaural Audio from Mono Audio with Depth and Cross Modal Attention
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Parida%2C+K+K">Kranti Kumar Parida</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sharma%2C+G">Gaurav Sharma</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.08046v1-abstract-short" style="display: inline;">
        Binaural audio gives the listener an immersive experience and can enhance augmented and virtual reality. However, recording binaural audio requires specialized setup with a dummy human head having microphones in left and right ears. Such a recording setup is difficult to build and setup, therefore mono audio has become the preferred choice in common devices. To obtain the same impact as binaural a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.08046v1-abstract-full').style.display = 'inline'; document.getElementById('2111.08046v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.08046v1-abstract-full" style="display: none;">
        Binaural audio gives the listener an immersive experience and can enhance augmented and virtual reality. However, recording binaural audio requires specialized setup with a dummy human head having microphones in left and right ears. Such a recording setup is difficult to build and setup, therefore mono audio has become the preferred choice in common devices. To obtain the same impact as binaural audio, recent efforts have been directed towards lifting mono audio to binaural audio conditioned on the visual input from the scene. Such approaches have not used an important cue for the task: the distance of different sound producing objects from the microphones. In this work, we argue that depth map of the scene can act as a proxy for inducing distance information of different objects in the scene, for the task of audio binauralization. We propose a novel encoder-decoder architecture with a hierarchical attention mechanism to encode image, depth and audio feature jointly. We design the network on top of state-of-the-art transformer networks for image and depth representation. We show empirically that the proposed method outperforms state-of-the-art methods comfortably for two challenging public datasets FAIR-Play and MUSIC-Stereo. We also demonstrate with qualitative results that the method is able to focus on the right information required for the task. The project details are available at \url{https://krantiparida.github.io/projects/bmonobinaural.html}
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.08046v1-abstract-full').style.display = 'none'; document.getElementById('2111.08046v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 November, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To appear in WACV 2022. arXiv admin note: text overlap with arXiv:2108.04906</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.03205">arXiv:2111.03205</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.03205">pdf</a>, <a href="https://arxiv.org/format/2111.03205">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LILA: Language-Informed Latent Actions
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Karamcheti%2C+S">Siddharth Karamcheti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+M">Megha Srivastava</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liang%2C+P">Percy Liang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sadigh%2C+D">Dorsa Sadigh</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.03205v1-abstract-short" style="display: inline;">
        We introduce Language-Informed Latent Actions (LILA), a framework for learning natural language interfaces in the context of human-robot collaboration. LILA falls under the shared autonomy paradigm: in addition to providing discrete language inputs, humans are given a low-dimensional controller $-$ e.g., a 2 degree-of-freedom (DoF) joystick that can move left/right and up/down $-$ for operating th&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.03205v1-abstract-full').style.display = 'inline'; document.getElementById('2111.03205v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.03205v1-abstract-full" style="display: none;">
        We introduce Language-Informed Latent Actions (LILA), a framework for learning natural language interfaces in the context of human-robot collaboration. LILA falls under the shared autonomy paradigm: in addition to providing discrete language inputs, humans are given a low-dimensional controller $-$ e.g., a 2 degree-of-freedom (DoF) joystick that can move left/right and up/down $-$ for operating the robot. LILA learns to use language to modulate this controller, providing users with a language-informed control space: given an instruction like &#34;place the cereal bowl on the tray,&#34; LILA may learn a 2-DoF space where one dimension controls the distance from the robot&#39;s end-effector to the bowl, and the other dimension controls the robot&#39;s end-effector pose relative to the grasp point on the bowl. We evaluate LILA with real-world user studies, where users can provide a language instruction while operating a 7-DoF Franka Emika Panda Arm to complete a series of complex manipulation tasks. We show that LILA models are not only more sample efficient and performant than imitation learning and end-effector control baselines, but that they are also qualitatively preferred by users.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.03205v1-abstract-full').style.display = 'none'; document.getElementById('2111.03205v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 November, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at the 5th Conference on Robot Learning (CoRL). Joint first authorship. 21 Pages, 11 Figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.00585">arXiv:2111.00585</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.00585">pdf</a>, <a href="https://arxiv.org/format/2111.00585">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        JEDAI: A System for Skill-Aligned Explainable Robot Planning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Naman Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Verma%2C+P">Pulkit Verma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Angle%2C+T">Trevor Angle</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.00585v3-abstract-short" style="display: inline;">
        This paper presents JEDAI, an AI system designed for outreach and educational efforts aimed at non-AI experts. JEDAI features a novel synthesis of research ideas from integrated task and motion planning and explainable AI. JEDAI helps users create high-level, intuitive plans while ensuring that they will be executable by the robot. It also provides users customized explanations about errors and he&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.00585v3-abstract-full').style.display = 'inline'; document.getElementById('2111.00585v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.00585v3-abstract-full" style="display: none;">
        This paper presents JEDAI, an AI system designed for outreach and educational efforts aimed at non-AI experts. JEDAI features a novel synthesis of research ideas from integrated task and motion planning and explainable AI. JEDAI helps users create high-level, intuitive plans while ensuring that they will be executable by the robot. It also provides users customized explanations about errors and helps improve their understanding of AI planning as well as the limits and capabilities of the underlying robot system.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.00585v3-abstract-full').style.display = 'none'; document.getElementById('2111.00585v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 March, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 31 October, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">AAAMS 2022 (Demonstration Track)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2109.14004">arXiv:2109.14004</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2109.14004">pdf</a>, <a href="https://arxiv.org/format/2109.14004">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Joint Communication and Motion Planning for Cobots
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dadvar%2C+M">Mehdi Dadvar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Majd%2C+K">Keyvan Majd</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Oikonomou%2C+E">Elena Oikonomou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fainekos%2C+G">Georgios Fainekos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2109.14004v3-abstract-short" style="display: inline;">
        The increasing deployment of robots in co-working scenarios with humans has revealed complex safety and efficiency challenges in the computation robot behavior. Movement among humans is one of the most fundamental -- and yet critical -- problems in this frontier. While several approaches have addressed this problem from a purely navigational point of view, the absence of a unified paradigm for com&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.14004v3-abstract-full').style.display = 'inline'; document.getElementById('2109.14004v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2109.14004v3-abstract-full" style="display: none;">
        The increasing deployment of robots in co-working scenarios with humans has revealed complex safety and efficiency challenges in the computation robot behavior. Movement among humans is one of the most fundamental -- and yet critical -- problems in this frontier. While several approaches have addressed this problem from a purely navigational point of view, the absence of a unified paradigm for communicating with humans limits their ability to prevent deadlocks and compute feasible solutions. This paper presents a joint communication and motion planning framework that selects from an arbitrary input set of robot&#39;s communication signals while computing robot motion plans. It models a human co-worker&#39;s imperfect perception of these communications using a noisy sensor model and facilitates the specification of a variety of social/workplace compliance priorities with a flexible cost function. Theoretical results and simulator-based empirical evaluations show that our approach efficiently computes motion plans and communication strategies that reduce conflicts between agents and resolve potential deadlocks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.14004v3-abstract-full').style.display = 'none'; document.getElementById('2109.14004v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 March, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 28 September, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2108.12537">arXiv:2108.12537</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2108.12537">pdf</a>, <a href="https://arxiv.org/format/2108.12537">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An Anytime Hierarchical Approach for Stochastic Task and Motion Planning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Naman Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2108.12537v2-abstract-short" style="display: inline;">
        In order to solve complex, long-horizon tasks, intelligent robots need to carry out high-level, abstract planning and reasoning in conjunction with motion planning. However, abstract models are typically lossy and plans or policies computed using them can be inexecutable. These problems are exacerbated in stochastic situations where the robot needs to reason about and plan for multiple contingenci&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.12537v2-abstract-full').style.display = 'inline'; document.getElementById('2108.12537v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2108.12537v2-abstract-full" style="display: none;">
        In order to solve complex, long-horizon tasks, intelligent robots need to carry out high-level, abstract planning and reasoning in conjunction with motion planning. However, abstract models are typically lossy and plans or policies computed using them can be inexecutable. These problems are exacerbated in stochastic situations where the robot needs to reason about and plan for multiple contingencies. We present a new approach for integrated task and motion planning in stochastic settings. In contrast to prior work in this direction, we show that our approach can effectively compute integrated task and motion policies whose branching structures encode agent behaviors that handle multiple execution-time contingencies. We prove that our algorithm is probabilistically complete and can compute feasible solution policies in an anytime fashion so that the probability of encountering an unresolved contingency decreases over time. Empirical results on a set of challenging problems show the utility and scope of our method.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.12537v2-abstract-full').style.display = 'none'; document.getElementById('2108.12537v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 May, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 27 August, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2108.09586">arXiv:2108.09586</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2108.09586">pdf</a>, <a href="https://arxiv.org/format/2108.09586">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Causal Models of Autonomous Agents using Interventions
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Verma%2C+P">Pulkit Verma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2108.09586v1-abstract-short" style="display: inline;">
        One of the several obstacles in the widespread use of AI systems is the lack of requirements of interpretability that can enable a layperson to ensure the safe and reliable behavior of such systems. We extend the analysis of an agent assessment module that lets an AI system execute high-level instruction sequences in simulators and answer the user queries about its execution of sequences of action&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.09586v1-abstract-full').style.display = 'inline'; document.getElementById('2108.09586v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2108.09586v1-abstract-full" style="display: none;">
        One of the several obstacles in the widespread use of AI systems is the lack of requirements of interpretability that can enable a layperson to ensure the safe and reliable behavior of such systems. We extend the analysis of an agent assessment module that lets an AI system execute high-level instruction sequences in simulators and answer the user queries about its execution of sequences of actions. We show that such a primitive query-response capability is sufficient to efficiently derive a user-interpretable causal model of the system in stationary, fully observable, and deterministic settings. We also introduce dynamic causal decision networks (DCDNs) that capture the causal structure of STRIPS-like domains. A comparative analysis of different classes of queries is also presented in terms of the computational requirements needed to answer them and the efforts required to evaluate their responses to learn the correct model.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.09586v1-abstract-full').style.display = 'none'; document.getElementById('2108.09586v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 August, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">IJCAI 2021 Workshop on Generalization in Planning</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2108.04906">arXiv:2108.04906</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2108.04906">pdf</a>, <a href="https://arxiv.org/format/2108.04906">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Depth Infused Binaural Audio Generation using Hierarchical Cross-Modal Attention
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Parida%2C+K+K">Kranti Kumar Parida</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Matiyali%2C+N">Neeraj Matiyali</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sharma%2C+G">Gaurav Sharma</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2108.04906v1-abstract-short" style="display: inline;">
        Binaural audio gives the listener the feeling of being in the recording place and enhances the immersive experience if coupled with AR/VR. But the problem with binaural audio recording is that it requires a specialized setup which is not possible to fabricate within handheld devices as compared to traditional mono audio that can be recorded with a single microphone. In order to overcome this drawb&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.04906v1-abstract-full').style.display = 'inline'; document.getElementById('2108.04906v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2108.04906v1-abstract-full" style="display: none;">
        Binaural audio gives the listener the feeling of being in the recording place and enhances the immersive experience if coupled with AR/VR. But the problem with binaural audio recording is that it requires a specialized setup which is not possible to fabricate within handheld devices as compared to traditional mono audio that can be recorded with a single microphone. In order to overcome this drawback, prior works have tried to uplift the mono recorded audio to binaural audio as a post processing step conditioning on the visual input. But all the prior approaches missed other most important information required for the task, i.e. distance of different sound producing objects from the recording setup. In this work, we argue that the depth map of the scene can act as a proxy for encoding distance information of objects in the scene and show that adding depth features along with image features improves the performance both qualitatively and quantitatively. We propose a novel encoder-decoder architecture, where we use a hierarchical attention mechanism to encode the image and depth feature extracted from individual transformer backbone, with audio features at each layer of the decoder.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.04906v1-abstract-full').style.display = 'none'; document.getElementById('2108.04906v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 August, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Presented at Sight and Sound Workshop, CVPR 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2107.13668">arXiv:2107.13668</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2107.13668">pdf</a>, <a href="https://arxiv.org/format/2107.13668">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Discovering User-Interpretable Capabilities of Black-Box Planning Agents
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Verma%2C+P">Pulkit Verma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Marpally%2C+S+R">Shashank Rao Marpally</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2107.13668v3-abstract-short" style="display: inline;">
        Several approaches have been developed for answering users&#39; specific questions about AI behavior and for assessing their core functionality in terms of primitive executable actions. However, the problem of summarizing an AI agent&#39;s broad capabilities for a user is comparatively new. This paper presents an algorithm for discovering from scratch the suite of high-level &#34;capabilities&#34; that an AI syst&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.13668v3-abstract-full').style.display = 'inline'; document.getElementById('2107.13668v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2107.13668v3-abstract-full" style="display: none;">
        Several approaches have been developed for answering users&#39; specific questions about AI behavior and for assessing their core functionality in terms of primitive executable actions. However, the problem of summarizing an AI agent&#39;s broad capabilities for a user is comparatively new. This paper presents an algorithm for discovering from scratch the suite of high-level &#34;capabilities&#34; that an AI system with arbitrary internal planning algorithms/policies can perform. It computes conditions describing the applicability and effects of these capabilities in user-interpretable terms. Starting from a set of user-interpretable state properties, an AI agent, and a simulator that the agent can interact with, our algorithm returns a set of high-level capabilities with their parameterized descriptions. Empirical evaluation on several game-based scenarios shows that this approach efficiently learns descriptions of various types of AI agents in deterministic, fully observable settings. User studies show that such descriptions are easier to understand and reason with than the agent&#39;s primitive actions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2107.13668v3-abstract-full').style.display = 'none'; document.getElementById('2107.13668v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 May, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 28 July, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">KR 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.03837">arXiv:2106.03837</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.03837">pdf</a>, <a href="https://arxiv.org/format/2106.03837">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MemStream: Memory-Based Streaming Anomaly Detection
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bhatia%2C+S">Siddharth Bhatia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jain%2C+A">Arjit Jain</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Shivin Srivastava</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kawaguchi%2C+K">Kenji Kawaguchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hooi%2C+B">Bryan Hooi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.03837v2-abstract-short" style="display: inline;">
        Given a stream of entries over time in a multi-dimensional data setting where concept drift is present, how can we detect anomalous activities? Most of the existing unsupervised anomaly detection approaches seek to detect anomalous events in an offline fashion and require a large amount of data for training. This is not practical in real-life scenarios where we receive the data in a streaming mann&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.03837v2-abstract-full').style.display = 'inline'; document.getElementById('2106.03837v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.03837v2-abstract-full" style="display: none;">
        Given a stream of entries over time in a multi-dimensional data setting where concept drift is present, how can we detect anomalous activities? Most of the existing unsupervised anomaly detection approaches seek to detect anomalous events in an offline fashion and require a large amount of data for training. This is not practical in real-life scenarios where we receive the data in a streaming manner and do not know the size of the stream beforehand. Thus, we need a data-efficient method that can detect and adapt to changing data trends, or concept drift, in an online manner. In this work, we propose MemStream, a streaming anomaly detection framework, allowing us to detect unusual events as they occur while being resilient to concept drift. We leverage the power of a denoising autoencoder to learn representations and a memory module to learn the dynamically changing trend in data without the need for labels. We prove the optimum memory size required for effective drift handling. Furthermore, MemStream makes use of two architecture design choices to be robust to memory poisoning. Experimental results show the effectiveness of our approach compared to state-of-the-art streaming baselines using $2$ synthetic datasets and $11$ real-world datasets.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.03837v2-abstract-full').style.display = 'none'; document.getElementById('2106.03837v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 March, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 June, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">The Web Conference (WWW), 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.00525">arXiv:2105.00525</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.00525">pdf</a>, <a href="https://arxiv.org/format/2105.00525">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Planning for Proactive Assistance in Environments with Partial Observability
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kulkarni%2C+A">Anagha Kulkarni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kambhampati%2C+S">Subbarao Kambhampati</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.00525v2-abstract-short" style="display: inline;">
        This paper addresses the problem of synthesizing the behavior of an AI agent that provides proactive task assistance to a human in settings like factory floors where they may coexist in a common environment. Unlike in the case of requested assistance, the human may not be expecting proactive assistance and hence it is crucial for the agent to ensure that the human is aware of how the assistance af&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.00525v2-abstract-full').style.display = 'inline'; document.getElementById('2105.00525v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.00525v2-abstract-full" style="display: none;">
        This paper addresses the problem of synthesizing the behavior of an AI agent that provides proactive task assistance to a human in settings like factory floors where they may coexist in a common environment. Unlike in the case of requested assistance, the human may not be expecting proactive assistance and hence it is crucial for the agent to ensure that the human is aware of how the assistance affects her task. This becomes harder when there is a possibility that the human may neither have full knowledge of the AI agent&#39;s capabilities nor have full observability of its activities. Therefore, our \textit{proactive assistant} is guided by the following three principles: \textbf{(1)} its activity decreases the human&#39;s cost towards her goal; \textbf{(2)} the human is able to recognize the potential reduction in her cost; \textbf{(3)} its activity optimizes the human&#39;s overall cost (time/resources) of achieving her goal. Through empirical evaluation and user studies, we demonstrate the usefulness of our approach.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.00525v2-abstract-full').style.display = 'none'; document.getElementById('2105.00525v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 September, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 2 May, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.15226">arXiv:2103.15226</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.15226">pdf</a>, <a href="https://arxiv.org/format/2103.15226">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Exploiting Local Geometry for Feature and Graph Construction for Better 3D Point Cloud Processing with Graph Neural Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sharma%2C+G">Gaurav Sharma</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.15226v1-abstract-short" style="display: inline;">
        We propose simple yet effective improvements in point representations and local neighborhood graph construction within the general framework of graph neural networks (GNNs) for 3D point cloud processing. As a first contribution, we propose to augment the vertex representations with important local geometric information of the points, followed by nonlinear projection using a MLP. As a second contri&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.15226v1-abstract-full').style.display = 'inline'; document.getElementById('2103.15226v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.15226v1-abstract-full" style="display: none;">
        We propose simple yet effective improvements in point representations and local neighborhood graph construction within the general framework of graph neural networks (GNNs) for 3D point cloud processing. As a first contribution, we propose to augment the vertex representations with important local geometric information of the points, followed by nonlinear projection using a MLP. As a second contribution, we propose to improve the graph construction for GNNs for 3D point clouds. The existing methods work with a k-nn based approach for constructing the local neighborhood graph. We argue that it might lead to reduction in coverage in case of dense sampling by sensors in some regions of the scene. The proposed methods aims to counter such problems and improve coverage in such cases. As the traditional GNNs were designed to work with general graphs, where vertices may have no geometric interpretations, we see both our proposals as augmenting the general graphs to incorporate the geometric nature of 3D point clouds. While being simple, we demonstrate with multiple challenging benchmarks, with relatively clean CAD models, as well as with real world noisy scans, that the proposed method achieves state of the art results on benchmarks for 3D classification (ModelNet40) , part segmentation (ShapeNet) and semantic segmentation (Stanford 3D Indoor Scenes Dataset). We also show that the proposed network achieves faster training convergence, i.e. ~40% less epochs for classification. The project details are available at https://siddharthsrivastava.github.io/publication/geomgcnn/
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.15226v1-abstract-full').style.display = 'none'; document.getElementById('2103.15226v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ICRA 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.08468">arXiv:2103.08468</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.08468">pdf</a>, <a href="https://arxiv.org/format/2103.08468">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Beyond Image to Depth: Improving Depth Prediction using Echoes
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Parida%2C+K+K">Kranti Kumar Parida</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sharma%2C+G">Gaurav Sharma</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.08468v2-abstract-short" style="display: inline;">
        We address the problem of estimating depth with multi modal audio visual data. Inspired by the ability of animals, such as bats and dolphins, to infer distance of objects with echolocation, some recent methods have utilized echoes for depth estimation. We propose an end-to-end deep learning based pipeline utilizing RGB images, binaural echoes and estimated material properties of various objects wi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.08468v2-abstract-full').style.display = 'inline'; document.getElementById('2103.08468v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.08468v2-abstract-full" style="display: none;">
        We address the problem of estimating depth with multi modal audio visual data. Inspired by the ability of animals, such as bats and dolphins, to infer distance of objects with echolocation, some recent methods have utilized echoes for depth estimation. We propose an end-to-end deep learning based pipeline utilizing RGB images, binaural echoes and estimated material properties of various objects within a scene. We argue that the relation between image, echoes and depth, for different scene elements, is greatly influenced by the properties of those elements, and a method designed to leverage this information can lead to significantly improved depth estimation from audio visual inputs. We propose a novel multi modal fusion technique, which incorporates the material properties explicitly while combining audio (echoes) and visual modalities to predict the scene depth. We show empirically, with experiments on Replica dataset, that the proposed method obtains 28% improvement in RMSE compared to the state-of-the-art audio-visual depth prediction method. To demonstrate the effectiveness of our method on larger dataset, we report competitive performance on Matterport3D, proposing to use it as a multimodal depth prediction benchmark with echoes for the first time. We also analyse the proposed method with exhaustive ablation experiments and qualitative results. The code and models are available at https://krantiparida.github.io/projects/bimgdepth.html
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.08468v2-abstract-full').style.display = 'none'; document.getElementById('2103.08468v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 April, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 March, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To appear in CVPR 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.11872">arXiv:2102.11872</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.11872">pdf</a>, <a href="https://arxiv.org/format/2102.11872">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Clustering Aware Classification for Risk Prediction and Subtyping in Clinical Data
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Shivin Srivastava</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bhatia%2C+S">Siddharth Bhatia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+L">Lingxiao Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Heng%2C+L+J">Lim Jun Heng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kawaguchi%2C+K">Kenji Kawaguchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rajan%2C+V">Vaibhav Rajan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.11872v5-abstract-short" style="display: inline;">
        In data containing heterogeneous subpopulations, classification performance benefits from incorporating the knowledge of cluster structure in the classifier. Previous methods for such combined clustering and classification either 1) are classifier-specific and not generic, or 2) independently perform clustering and classifier training, which may not form clusters that can potentially benefit class&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.11872v5-abstract-full').style.display = 'inline'; document.getElementById('2102.11872v5-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.11872v5-abstract-full" style="display: none;">
        In data containing heterogeneous subpopulations, classification performance benefits from incorporating the knowledge of cluster structure in the classifier. Previous methods for such combined clustering and classification either 1) are classifier-specific and not generic, or 2) independently perform clustering and classifier training, which may not form clusters that can potentially benefit classifier performance. The question of how to perform clustering to improve the performance of classifiers trained on the clusters has received scant attention in previous literature, despite its importance in several real-world applications. In this paper, first, we theoretically analyze the generalization performance of classifiers trained on clustered data and find conditions under which clustering can potentially aid classification. This motivates the design of a simple k-means-based classification algorithm called Clustering Aware Classification (CAC) and its neural variant {DeepCAC}. DeepCAC effectively leverages deep representation learning to learn latent embeddings and finds clusters in a manner that make the clustered data suitable for training classifiers for each underlying subpopulation. Our experiments on synthetic and real benchmark datasets demonstrate the efficacy of DeepCAC over previous methods for combined clustering and classification.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.11872v5-abstract-full').style.display = 'none'; document.getElementById('2102.11872v5-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 January, 2023; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 February, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">19 Pages, 5 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2012.00658">arXiv:2012.00658</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2012.00658">pdf</a>, <a href="https://arxiv.org/ps/2012.00658">ps</a>, <a href="https://arxiv.org/format/2012.00658">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning and Using Abstractions for Robot Planning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Naman Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srinet%2C+A">Abhyudaya Srinet</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2012.00658v2-abstract-short" style="display: inline;">
        Robot motion planning involves computing a sequence of valid robot configurations that take the robot from its initial state to a goal state. Solving a motion planning problem optimally using analytical methods is proven to be PSPACE-Hard. Sampling-based approaches have tried to approximate the optimal solution efficiently. Generally, sampling-based planners use uniform samplers to cover the entir&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.00658v2-abstract-full').style.display = 'inline'; document.getElementById('2012.00658v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2012.00658v2-abstract-full" style="display: none;">
        Robot motion planning involves computing a sequence of valid robot configurations that take the robot from its initial state to a goal state. Solving a motion planning problem optimally using analytical methods is proven to be PSPACE-Hard. Sampling-based approaches have tried to approximate the optimal solution efficiently. Generally, sampling-based planners use uniform samplers to cover the entire state space. In this paper, we propose a deep-learning-based framework that identifies robot configurations in the environment that are important to solve the given motion planning problem. These states are used to bias the sampling distribution in order to reduce the planning time. Our approach works with a unified network and generates domain-dependent network parameters based on the environment and the robot. We evaluate our approach with Learn and Link planner in three different settings. Results show significant improvement in motion planning times when compared with current sampling-based motion planners.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.00658v2-abstract-full').style.display = 'none'; document.getElementById('2012.00658v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 July, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 1 December, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2008.04849">arXiv:2008.04849</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2008.04849">pdf</a>, <a href="https://arxiv.org/format/2008.04849">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Populations and Evolution">q-bio.PE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Other Computer Science">cs.OH</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Physics and Society">physics.soc-ph</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Quantitative Methods">q-bio.QM</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1007/s41745-020-00211-3">10.1007/s41745-020-00211-3 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        City-Scale Agent-Based Simulators for the Study of Non-Pharmaceutical Interventions in the Context of the COVID-19 Epidemic
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Agrawal%2C+S">Shubhada Agrawal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bhandari%2C+S">Siddharth Bhandari</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bhattacharjee%2C+A">Anirban Bhattacharjee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deo%2C+A">Anand Deo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dixit%2C+N+M">Narendra M. Dixit</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Harsha%2C+P">Prahladh Harsha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Juneja%2C+S">Sandeep Juneja</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kesarwani%2C+P">Poonam Kesarwani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Swamy%2C+A+K">Aditya Krishna Swamy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Patil%2C+P">Preetam Patil</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rathod%2C+N">Nihesh Rathod</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Saptharishi%2C+R">Ramprasad Saptharishi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shriram%2C+S">Sharad Shriram</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+P">Piyush Srivastava</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sundaresan%2C+R">Rajesh Sundaresan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vaidhiyan%2C+N+K">Nidhin Koshy Vaidhiyan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yasodharan%2C+S">Sarath Yasodharan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2008.04849v1-abstract-short" style="display: inline;">
        We highlight the usefulness of city-scale agent-based simulators in studying various non-pharmaceutical interventions to manage an evolving pandemic. We ground our studies in the context of the COVID-19 pandemic and demonstrate the power of the simulator via several exploratory case studies in two metropolises, Bengaluru and Mumbai. Such tools become common-place in any city administration&#39;s tool&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.04849v1-abstract-full').style.display = 'inline'; document.getElementById('2008.04849v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2008.04849v1-abstract-full" style="display: none;">
        We highlight the usefulness of city-scale agent-based simulators in studying various non-pharmaceutical interventions to manage an evolving pandemic. We ground our studies in the context of the COVID-19 pandemic and demonstrate the power of the simulator via several exploratory case studies in two metropolises, Bengaluru and Mumbai. Such tools become common-place in any city administration&#39;s tool kit in our march towards digital health.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.04849v1-abstract-full').style.display = 'none'; document.getElementById('2008.04849v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 August, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">56 pages</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Journal of the Indian Institute of Science, volume 100, pages 809-847, 2020
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2008.02731">arXiv:2008.02731</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2008.02731">pdf</a>, <a href="https://arxiv.org/format/2008.02731">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Analysing Risk of Coronary Heart Disease through Discriminative Neural Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Khaneja%2C+A">Ayush Khaneja</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rai%2C+A">Astha Rai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheema%2C+A+S">A S Cheema</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+P+K">P K Srivastava</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2008.02731v1-abstract-short" style="display: inline;">
        The application of data mining, machine learning and artificial intelligence techniques in the field of diagnostics is not a new concept, and these techniques have been very successfully applied in a variety of applications, especially in dermatology and cancer research. But, in the case of medical problems that involve tests resulting in true or false (binary classification), the data generally h&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.02731v1-abstract-full').style.display = 'inline'; document.getElementById('2008.02731v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2008.02731v1-abstract-full" style="display: none;">
        The application of data mining, machine learning and artificial intelligence techniques in the field of diagnostics is not a new concept, and these techniques have been very successfully applied in a variety of applications, especially in dermatology and cancer research. But, in the case of medical problems that involve tests resulting in true or false (binary classification), the data generally has a class imbalance with samples majorly belonging to one class (ex: a patient undergoes a regular test and the results are false). Such disparity in data causes problems when trying to model predictive systems on the data. In critical applications like diagnostics, this class imbalance cannot be overlooked and must be given extra attention. In our research, we depict how we can handle this class imbalance through neural networks using a discriminative model and contrastive loss using a Siamese neural network structure. Such a model does not work on a probability-based approach to classify samples into labels. Instead it uses a distance-based approach to differentiate between samples classified under different labels. The code is available at https://tinyurl.com/DiscriminativeCHD/
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2008.02731v1-abstract-full').style.display = 'none'; document.getElementById('2008.02731v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 June, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2007.06702">arXiv:2007.06702</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2007.06702">pdf</a>, <a href="https://arxiv.org/format/2007.06702">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Generalized Relational Heuristic Networks for Model-Agnostic Planning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Karia%2C+R">Rushang Karia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2007.06702v2-abstract-short" style="display: inline;">
        Computing goal-directed behavior is essential to designing efficient AI systems. Due to the computational complexity of planning, current approaches rely primarily upon hand-coded symbolic action models and hand-coded heuristic-function generators for efficiency. Learned heuristics for such problems have been of limited utility as they are difficult to apply to problems with objects and object qua&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.06702v2-abstract-full').style.display = 'inline'; document.getElementById('2007.06702v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2007.06702v2-abstract-full" style="display: none;">
        Computing goal-directed behavior is essential to designing efficient AI systems. Due to the computational complexity of planning, current approaches rely primarily upon hand-coded symbolic action models and hand-coded heuristic-function generators for efficiency. Learned heuristics for such problems have been of limited utility as they are difficult to apply to problems with objects and object quantities that are significantly different from those in the training data. This paper develops a new approach for learning generalized heuristics in the absence of symbolic action models using deep neural networks that utilize an input predicate vocabulary but are agnostic to object names and quantities. It uses an abstract state representation to facilitate data efficient, generalizable learning. Empirical evaluation on a range of benchmark domains show that in contrast to prior approaches, generalized heuristics computed by this method can be transferred easily to problems with different objects and with object quantities much larger than those in the training data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2007.06702v2-abstract-full').style.display = 'none'; document.getElementById('2007.06702v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 October, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 10 July, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to AAAI-21</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68T20; 68T07
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.6
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2002.01080">arXiv:2002.01080</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2002.01080">pdf</a>, <a href="https://arxiv.org/format/2002.01080">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Bridging the Gap: Providing Post-Hoc Symbolic Explanations for Sequential Decision-Making Problems with Inscrutable Representations
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sreedharan%2C+S">Sarath Sreedharan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Soni%2C+U">Utkarsh Soni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Verma%2C+M">Mudit Verma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kambhampati%2C+S">Subbarao Kambhampati</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2002.01080v4-abstract-short" style="display: inline;">
        As increasingly complex AI systems are introduced into our daily lives, it becomes important for such systems to be capable of explaining the rationale for their decisions and allowing users to contest these decisions. A significant hurdle to allowing for such explanatory dialogue could be the vocabulary mismatch between the user and the AI system. This paper introduces methods for providing contr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2002.01080v4-abstract-full').style.display = 'inline'; document.getElementById('2002.01080v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2002.01080v4-abstract-full" style="display: none;">
        As increasingly complex AI systems are introduced into our daily lives, it becomes important for such systems to be capable of explaining the rationale for their decisions and allowing users to contest these decisions. A significant hurdle to allowing for such explanatory dialogue could be the vocabulary mismatch between the user and the AI system. This paper introduces methods for providing contrastive explanations in terms of user-specified concepts for sequential decision-making settings where the system&#39;s model of the task may be best represented as an inscrutable model. We do this by building partial symbolic models of a local approximation of the task that can be leveraged to answer the user queries. We test these methods on a popular Atari game (Montezuma&#39;s Revenge) and variants of Sokoban (a well-known planning benchmark) and report the results of user studies to evaluate whether people find explanations generated in this form useful.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2002.01080v4-abstract-full').style.display = 'none'; document.getElementById('2002.01080v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 March, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 3 February, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1912.12613">arXiv:1912.12613</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1912.12613">pdf</a>, <a href="https://arxiv.org/format/1912.12613">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Asking the Right Questions: Learning Interpretable Action Models Through Query Answering
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Verma%2C+P">Pulkit Verma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Marpally%2C+S+R">Shashank Rao Marpally</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1912.12613v6-abstract-short" style="display: inline;">
        This paper develops a new approach for estimating an interpretable, relational model of a black-box autonomous agent that can plan and act. Our main contributions are a new paradigm for estimating such models using a minimal query interface with the agent, and a hierarchical querying algorithm that generates an interrogation policy for estimating the agent&#39;s internal model in a vocabulary provided&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1912.12613v6-abstract-full').style.display = 'inline'; document.getElementById('1912.12613v6-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1912.12613v6-abstract-full" style="display: none;">
        This paper develops a new approach for estimating an interpretable, relational model of a black-box autonomous agent that can plan and act. Our main contributions are a new paradigm for estimating such models using a minimal query interface with the agent, and a hierarchical querying algorithm that generates an interrogation policy for estimating the agent&#39;s internal model in a vocabulary provided by the user. Empirical evaluation of our approach shows that despite the intractable search space of possible agent models, our approach allows correct and scalable estimation of interpretable agent models for a wide class of black-box autonomous agents. Our results also show that this approach can use predicate classifiers to learn interpretable models of planning agents that represent states as images.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1912.12613v6-abstract-full').style.display = 'none'; document.getElementById('1912.12613v6-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 April, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 29 December, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">AAAI 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1905.10672">arXiv:1905.10672</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1905.10672">pdf</a>, <a href="https://arxiv.org/format/1905.10672">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Signaling Friends and Head-Faking Enemies Simultaneously: Balancing Goal Obfuscation and Goal Legibility
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kulkarni%2C+A">Anagha Kulkarni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kambhampati%2C+S">Subbarao Kambhampati</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1905.10672v2-abstract-short" style="display: inline;">
        In order to be useful in the real world, AI agents need to plan and act in the presence of others, who may include adversarial and cooperative entities. In this paper, we consider the problem where an autonomous agent needs to act in a manner that clarifies its objectives to cooperative entities while preventing adversarial entities from inferring those objectives. We show that this problem is sol&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1905.10672v2-abstract-full').style.display = 'inline'; document.getElementById('1905.10672v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1905.10672v2-abstract-full" style="display: none;">
        In order to be useful in the real world, AI agents need to plan and act in the presence of others, who may include adversarial and cooperative entities. In this paper, we consider the problem where an autonomous agent needs to act in a manner that clarifies its objectives to cooperative entities while preventing adversarial entities from inferring those objectives. We show that this problem is solvable when cooperative entities and adversarial entities use different types of sensors and/or prior knowledge. We develop two new solution approaches for computing such plans. One approach provides an optimal solution to the problem by using an IP solver to provide maximum obfuscation for adversarial entities while providing maximum legibility for cooperative entities in the environment, whereas the other approach provides a satisficing solution using heuristic-guided forward search to achieve preset levels of obfuscation and legibility for adversarial and cooperative entities respectively. We show the feasibility and utility of our algorithms through extensive empirical evaluation on problems derived from planning benchmarks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1905.10672v2-abstract-full').style.display = 'none'; document.getElementById('1905.10672v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 January, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 25 May, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1904.13006">arXiv:1904.13006</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1904.13006">pdf</a>, <a href="https://arxiv.org/ps/1904.13006">ps</a>, <a href="https://arxiv.org/format/1904.13006">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Anytime Integrated Task and Motion Policies for Stochastic Environments
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shah%2C+N">Naman Shah</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vasudevan%2C+D+K">Deepak Kala Vasudevan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kumar%2C+K">Kislay Kumar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kamojjhala%2C+P">Pranav Kamojjhala</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1904.13006v3-abstract-short" style="display: inline;">
        In order to solve complex, long-horizon tasks, intelligent robots need to carry out high-level, abstract planning and reasoning in conjunction with motion planning. However, abstract models are typically lossy and plans or policies computed using them can be unexecutable. These problems are exacerbated in stochastic situations where the robot needs to reason about, and plan for multiple contingenc&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1904.13006v3-abstract-full').style.display = 'inline'; document.getElementById('1904.13006v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1904.13006v3-abstract-full" style="display: none;">
        In order to solve complex, long-horizon tasks, intelligent robots need to carry out high-level, abstract planning and reasoning in conjunction with motion planning. However, abstract models are typically lossy and plans or policies computed using them can be unexecutable. These problems are exacerbated in stochastic situations where the robot needs to reason about, and plan for multiple contingencies.
  We present a new approach for integrated task and motion planning in stochastic settings. In contrast to prior work in this direction, we show that our approach can effectively compute integrated task and motion policies whose branching structures encoding agent behaviors handling multiple execution-time contingencies. We prove that our algorithm is probabilistically complete and can compute feasible solution policies in an anytime fashion so that the probability of encountering an unresolved contingency decreases over time. Empirical results on a set of challenging problems show the utility and scope of our methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1904.13006v3-abstract-full').style.display = 'none'; document.getElementById('1904.13006v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 May, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 29 April, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1904.08775">arXiv:1904.08775</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1904.08775">pdf</a>, <a href="https://arxiv.org/format/1904.08775">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Few Shot Speaker Recognition using Deep Neural Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Anand%2C+P">Prashant Anand</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Singh%2C+A+K">Ajeet Kumar Singh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lall%2C+B">Brejesh Lall</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1904.08775v1-abstract-short" style="display: inline;">
        The recent advances in deep learning are mostly driven by availability of large amount of training data. However, availability of such data is not always possible for specific tasks such as speaker recognition where collection of large amount of data is not possible in practical scenarios. Therefore, in this paper, we propose to identify speakers by learning from only a few training examples. To a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1904.08775v1-abstract-full').style.display = 'inline'; document.getElementById('1904.08775v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1904.08775v1-abstract-full" style="display: none;">
        The recent advances in deep learning are mostly driven by availability of large amount of training data. However, availability of such data is not always possible for specific tasks such as speaker recognition where collection of large amount of data is not possible in practical scenarios. Therefore, in this paper, we propose to identify speakers by learning from only a few training examples. To achieve this, we use a deep neural network with prototypical loss where the input to the network is a spectrogram. For output, we project the class feature vectors into a common embedding space, followed by classification. Further, we show the effectiveness of capsule net in a few shot learning setting. To this end, we utilize an auto-encoder to learn generalized feature embeddings from class-specific embeddings obtained from capsule network. We provide exhaustive experiments on publicly available datasets and competitive baselines, demonstrating the superiority and generalization ability of the proposed few shot learning pipelines.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1904.08775v1-abstract-full').style.display = 'none'; document.getElementById('1904.08775v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 April, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1904.08494">arXiv:1904.08494</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1904.08494">pdf</a>, <a href="https://arxiv.org/format/1904.08494">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning 2D to 3D Lifting for Object Detection in 3D for Autonomous Vehicles
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Srivastava%2C+S">Siddharth Srivastava</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jurie%2C+F">Frederic Jurie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sharma%2C+G">Gaurav Sharma</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1904.08494v2-abstract-short" style="display: inline;">
        We address the problem of 3D object detection from 2D monocular images in autonomous driving scenarios. We propose to lift the 2D images to 3D representations using learned neural networks and leverage existing networks working directly on 3D data to perform 3D object detection and localization. We show that, with carefully designed training mechanism and automatically selected minimally noisy dat&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1904.08494v2-abstract-full').style.display = 'inline'; document.getElementById('1904.08494v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1904.08494v2-abstract-full" style="display: none;">
        We address the problem of 3D object detection from 2D monocular images in autonomous driving scenarios. We propose to lift the 2D images to 3D representations using learned neural networks and leverage existing networks working directly on 3D data to perform 3D object detection and localization. We show that, with carefully designed training mechanism and automatically selected minimally noisy data, such a method is not only feasible, but gives higher results than many methods working on actual 3D inputs acquired from physical sensors. On the challenging KITTI benchmark, we show that our 2D to 3D lifted method outperforms many recent competitive 3D networks while significantly outperforming previous state-of-the-art for 3D detection from monocular images. We also show that a late fusion of the output of the network trained on generated 3D images, with that trained on real 3D images, improves performance. We find the results very interesting and argue that such a method could serve as a highly reliable backup in case of malfunction of expensive 3D sensors, if not potentially making them redundant, at least in the case of low human injury risk autonomous navigation scenarios like warehouse automation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1904.08494v2-abstract-full').style.display = 'none'; document.getElementById('1904.08494v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 October, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 27 March, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2019.
      
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?query=Siddharth+Srivastava&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?query=Siddharth+Srivastava&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?query=Siddharth+Srivastava&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
      
    </ul>
  </nav>
  

  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>