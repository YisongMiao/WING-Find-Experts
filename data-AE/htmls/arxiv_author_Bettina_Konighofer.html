<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;21 of 21 results for author: <span class="mathjax">Bettina Konighofer</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/"  aria-role="search">
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Bettina Konighofer">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Bettina+Konighofer&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Bettina Konighofer">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      




<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2412.11994">arXiv:2412.11994</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2412.11994">pdf</a>, <a href="https://arxiv.org/format/2412.11994">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Fairness Shields: Safeguarding against Biased Decision Makers
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cano%2C+F">Filip Cano</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Henzinger%2C+T+A">Thomas A. Henzinger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=K%C3%B6nighofer%2C+B">Bettina Könighofer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kueffner%2C+K">Konstantin Kueffner</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mallik%2C+K">Kaushik Mallik</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2412.11994v1-abstract-short" style="display: inline;">
        As AI-based decision-makers increasingly influence human lives, it is a growing concern that their decisions are often unfair or biased with respect to people&#39;s sensitive attributes, such as gender and race. Most existing bias prevention measures provide probabilistic fairness guarantees in the long run, and it is possible that the decisions are biased on specific instances of short decision seque&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.11994v1-abstract-full').style.display = 'inline'; document.getElementById('2412.11994v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2412.11994v1-abstract-full" style="display: none;">
        As AI-based decision-makers increasingly influence human lives, it is a growing concern that their decisions are often unfair or biased with respect to people&#39;s sensitive attributes, such as gender and race. Most existing bias prevention measures provide probabilistic fairness guarantees in the long run, and it is possible that the decisions are biased on specific instances of short decision sequences. We introduce fairness shielding, where a symbolic decision-maker -- the fairness shield -- continuously monitors the sequence of decisions of another deployed black-box decision-maker, and makes interventions so that a given fairness criterion is met while the total intervention costs are minimized. We present four different algorithms for computing fairness shields, among which one guarantees fairness over fixed horizons, and three guarantee fairness periodically after fixed intervals. Given a distribution over future decisions and their intervention costs, our algorithms solve different instances of bounded-horizon optimal control problems with different levels of computational costs and optimality guarantees. Our empirical evaluation demonstrates the effectiveness of these shields in ensuring fairness while maintaining cost efficiency across various scenarios.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.11994v1-abstract-full').style.display = 'none'; document.getElementById('2412.11994v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 December, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To appear in AAAI 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.07700">arXiv:2411.07700</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.07700">pdf</a>, <a href="https://arxiv.org/format/2411.07700">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Test Where Decisions Matter: Importance-driven Testing for Deep Reinforcement Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Pranger%2C+S">Stefan Pranger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chockler%2C+H">Hana Chockler</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tappler%2C+M">Martin Tappler</a>, 
      
      <a href="/search/?searchtype=author&amp;query=K%C3%B6nighofer%2C+B">Bettina Könighofer</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.07700v1-abstract-short" style="display: inline;">
        In many Deep Reinforcement Learning (RL) problems, decisions in a trained policy vary in significance for the expected safety and performance of the policy. Since RL policies are very complex, testing efforts should concentrate on states in which the agent&#39;s decisions have the highest impact on the expected outcome. In this paper, we propose a novel model-based method to rigorously compute a ranki&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.07700v1-abstract-full').style.display = 'inline'; document.getElementById('2411.07700v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.07700v1-abstract-full" style="display: none;">
        In many Deep Reinforcement Learning (RL) problems, decisions in a trained policy vary in significance for the expected safety and performance of the policy. Since RL policies are very complex, testing efforts should concentrate on states in which the agent&#39;s decisions have the highest impact on the expected outcome. In this paper, we propose a novel model-based method to rigorously compute a ranking of state importance across the entire state space. We then focus our testing efforts on the highest-ranked states. In this paper, we focus on testing for safety. However, the proposed methods can be easily adapted to test for performance. In each iteration, our testing framework computes optimistic and pessimistic safety estimates. These estimates provide lower and upper bounds on the expected outcomes of the policy execution across all modeled states in the state space. Our approach divides the state space into safe and unsafe regions upon convergence, providing clear insights into the policy&#39;s weaknesses. Two important properties characterize our approach. (1) Optimal Test-Case Selection: At any time in the testing process, our approach evaluates the policy in the states that are most critical for safety. (2) Guaranteed Safety: Our approach can provide formal verification guarantees over the entire state space by sampling only a fraction of the policy. Any safety properties assured by the pessimistic estimate are formally proven to hold for the policy. We provide a detailed evaluation of our framework on several examples, showing that our method discovers unsafe policy behavior with low testing effort.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.07700v1-abstract-full').style.display = 'none'; document.getElementById('2411.07700v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2405.13583">arXiv:2405.13583</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2405.13583">pdf</a>, <a href="https://arxiv.org/format/2405.13583">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Logic in Computer Science">cs.LO</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Tools at the Frontiers of Quantitative Verification
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Andriushchenko%2C+R">Roman Andriushchenko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bork%2C+A">Alexander Bork</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Budde%2C+C+E">Carlos E. Budde</a>, 
      
      <a href="/search/?searchtype=author&amp;query=%C4%8Ce%C5%A1ka%2C+M">Milan Češka</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Grover%2C+K">Kush Grover</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hahn%2C+E+M">Ernst Moritz Hahn</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hartmanns%2C+A">Arnd Hartmanns</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Israelsen%2C+B">Bryant Israelsen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jansen%2C+N">Nils Jansen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jeppson%2C+J">Joshua Jeppson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Junges%2C+S">Sebastian Junges</a>, 
      
      <a href="/search/?searchtype=author&amp;query=K%C3%B6hl%2C+M+A">Maximilian A. Köhl</a>, 
      
      <a href="/search/?searchtype=author&amp;query=K%C3%B6nighofer%2C+B">Bettina Könighofer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=K%C5%99et%C3%ADnsk%C3%BD%2C+J">Jan Křetínský</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Meggendorfer%2C+T">Tobias Meggendorfer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Parker%2C+D">David Parker</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pranger%2C+S">Stefan Pranger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Quatmann%2C+T">Tim Quatmann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ruijters%2C+E">Enno Ruijters</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Taylor%2C+L">Landon Taylor</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Volk%2C+M">Matthias Volk</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Weininger%2C+M">Maximilian Weininger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Z">Zhen Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2405.13583v1-abstract-short" style="display: inline;">
        The analysis of formal models that include quantitative aspects such as timing or probabilistic choices is performed by quantitative verification tools. Broad and mature tool support is available for computing basic properties such as expected rewards on basic models such as Markov chains. Previous editions of QComp, the comparison of tools for the analysis of quantitative formal models, focused o&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.13583v1-abstract-full').style.display = 'inline'; document.getElementById('2405.13583v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2405.13583v1-abstract-full" style="display: none;">
        The analysis of formal models that include quantitative aspects such as timing or probabilistic choices is performed by quantitative verification tools. Broad and mature tool support is available for computing basic properties such as expected rewards on basic models such as Markov chains. Previous editions of QComp, the comparison of tools for the analysis of quantitative formal models, focused on this setting. Many application scenarios, however, require more advanced property types such as LTL and parameter synthesis queries as well as advanced models like stochastic games and partially observable MDPs. For these, tool support is in its infancy today. This paper presents the outcomes of QComp 2023: a survey of the state of the art in quantitative verification tool support for advanced property types and models. With tools ranging from first research prototypes to well-supported integrations into established toolsets, this report highlights today&#39;s active areas and tomorrow&#39;s challenges in tool-focused research for quantitative verification.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2405.13583v1-abstract-full').style.display = 'none'; document.getElementById('2405.13583v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 May, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2307.02164">arXiv:2307.02164</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2307.02164">pdf</a>, <a href="https://arxiv.org/format/2307.02164">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1609/icaps.v33i1.27181">10.1609/icaps.v33i1.27181 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Safety Shielding under Delayed Observation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=C%C3%B3rdoba%2C+F+C">Filip Cano Córdoba</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Palmisano%2C+A">Alexander Palmisano</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fr%C3%A4nzle%2C+M">Martin Fränzle</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bloem%2C+R">Roderick Bloem</a>, 
      
      <a href="/search/?searchtype=author&amp;query=K%C3%B6nighofer%2C+B">Bettina Könighofer</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2307.02164v1-abstract-short" style="display: inline;">
        Agents operating in physical environments need to be able to handle delays in the input and output signals since neither data transmission nor sensing or actuating the environment are instantaneous. Shields are correct-by-construction runtime enforcers that guarantee safe execution by correcting any action that may cause a violation of a formal safety specification. Besides providing safety guaran&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2307.02164v1-abstract-full').style.display = 'inline'; document.getElementById('2307.02164v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2307.02164v1-abstract-full" style="display: none;">
        Agents operating in physical environments need to be able to handle delays in the input and output signals since neither data transmission nor sensing or actuating the environment are instantaneous. Shields are correct-by-construction runtime enforcers that guarantee safe execution by correcting any action that may cause a violation of a formal safety specification. Besides providing safety guarantees, shields should interfere minimally with the agent. Therefore, shields should pick the safe corrective actions in such a way that future interferences are most likely minimized. Current shielding approaches do not consider possible delays in the input signals in their safety analyses. In this paper, we address this issue. We propose synthesis algorithms to compute \emph{delay-resilient shields} that guarantee safety under worst-case assumptions on the delays of the input signals. We also introduce novel heuristics for deciding between multiple corrective actions, designed to minimize future shield interferences caused by delays. As a further contribution, we present the first integration of shields in a realistic driving simulator. We implemented our delayed shields in the driving simulator \textsc{Carla}. We shield potentially unsafe autonomous driving agents in different safety-critical scenarios and show the effect of delays on the safety analysis.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2307.02164v1-abstract-full').style.display = 'none'; document.getElementById('2307.02164v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 July, 2023; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2023.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">6 pages, Published at ICAPS 2023 (Main Track)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2307.01532">arXiv:2307.01532</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2307.01532">pdf</a>, <a href="https://arxiv.org/format/2307.01532">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Analyzing Intentional Behavior in Autonomous Agents under Uncertainty
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=C%C3%B3rdoba%2C+F+C">Filip Cano Córdoba</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Judson%2C+S">Samuel Judson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Antonopoulos%2C+T">Timos Antonopoulos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bj%C3%B8rner%2C+K">Katrine Bjørner</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shoemaker%2C+N">Nicholas Shoemaker</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shapiro%2C+S+J">Scott J. Shapiro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Piskac%2C+R">Ruzica Piskac</a>, 
      
      <a href="/search/?searchtype=author&amp;query=K%C3%B6nighofer%2C+B">Bettina Könighofer</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2307.01532v1-abstract-short" style="display: inline;">
        Principled accountability for autonomous decision-making in uncertain environments requires distinguishing intentional outcomes from negligent designs from actual accidents. We propose analyzing the behavior of autonomous agents through a quantitative measure of the evidence of intentional behavior. We model an uncertain environment as a Markov Decision Process (MDP). For a given scenario, we rely&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2307.01532v1-abstract-full').style.display = 'inline'; document.getElementById('2307.01532v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2307.01532v1-abstract-full" style="display: none;">
        Principled accountability for autonomous decision-making in uncertain environments requires distinguishing intentional outcomes from negligent designs from actual accidents. We propose analyzing the behavior of autonomous agents through a quantitative measure of the evidence of intentional behavior. We model an uncertain environment as a Markov Decision Process (MDP). For a given scenario, we rely on probabilistic model checking to compute the ability of the agent to influence reaching a certain event. We call this the scope of agency. We say that there is evidence of intentional behavior if the scope of agency is high and the decisions of the agent are close to being optimal for reaching the event. Our method applies counterfactual reasoning to automatically generate relevant scenarios that can be analyzed to increase the confidence of our assessment. In a case study, we show how our method can distinguish between &#39;intentional&#39; and &#39;accidental&#39; traffic collisions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2307.01532v1-abstract-full').style.display = 'none'; document.getElementById('2307.01532v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 July, 2023; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2023.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages. Accepted for publication at IJCAI 2023 (Main Track)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2306.17204">arXiv:2306.17204</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2306.17204">pdf</a>, <a href="https://arxiv.org/format/2306.17204">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Formal Languages and Automata Theory">cs.FL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Environment Models with Continuous Stochastic Dynamics
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tappler%2C+M">Martin Tappler</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mu%C5%A1kardin%2C+E">Edi Muškardin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aichernig%2C+B+K">Bernhard K. Aichernig</a>, 
      
      <a href="/search/?searchtype=author&amp;query=K%C3%B6nighofer%2C+B">Bettina Könighofer</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2306.17204v1-abstract-short" style="display: inline;">
        Solving control tasks in complex environments automatically through learning offers great potential. While contemporary techniques from deep reinforcement learning (DRL) provide effective solutions, their decision-making is not transparent. We aim to provide insights into the decisions faced by the agent by learning an automaton model of environmental behavior under the control of an agent. Howeve&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2306.17204v1-abstract-full').style.display = 'inline'; document.getElementById('2306.17204v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2306.17204v1-abstract-full" style="display: none;">
        Solving control tasks in complex environments automatically through learning offers great potential. While contemporary techniques from deep reinforcement learning (DRL) provide effective solutions, their decision-making is not transparent. We aim to provide insights into the decisions faced by the agent by learning an automaton model of environmental behavior under the control of an agent. However, for most control problems, automata learning is not scalable enough to learn a useful model. In this work, we raise the capabilities of automata learning such that it is possible to learn models for environments that have complex and continuous dynamics.
  The core of the scalability of our method lies in the computation of an abstract state-space representation, by applying dimensionality reduction and clustering on the observed environmental state space. The stochastic transitions are learned via passive automata learning from observed interactions of the agent and the environment. In an iterative model-based RL process, we sample additional trajectories to learn an accurate environment model in the form of a discrete-state Markov decision process (MDP). We apply our automata learning framework on popular RL benchmarking environments in the OpenAI Gym, including LunarLander, CartPole, Mountain Car, and Acrobot. Our results show that the learned models are so precise that they enable the computation of policies solving the respective control tasks. Yet the models are more concise and more general than neural-network-based policies and by using MDPs we benefit from a wealth of tools available for analyzing them. When solving the task of LunarLander, the learned model even achieved similar or higher rewards than deep RL policies learned with stable-baselines3.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2306.17204v1-abstract-full').style.display = 'none'; document.getElementById('2306.17204v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 June, 2023; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2023.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2305.05731">arXiv:2305.05731</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2305.05731">pdf</a>, <a href="https://arxiv.org/format/2305.05731">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Logic in Computer Science">cs.LO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Programming Languages">cs.PL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        &#39;Put the Car on the Stand&#39;: SMT-based Oracles for Investigating Decisions
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Judson%2C+S">Samuel Judson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elacqua%2C+M">Matthew Elacqua</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cano%2C+F">Filip Cano</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Antonopoulos%2C+T">Timos Antonopoulos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=K%C3%B6nighofer%2C+B">Bettina Könighofer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shapiro%2C+S+J">Scott J. Shapiro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Piskac%2C+R">Ruzica Piskac</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2305.05731v2-abstract-short" style="display: inline;">
        Principled accountability in the aftermath of harms is essential to the trustworthy design and governance of algorithmic decision making. Legal theory offers a paramount method for assessing culpability: putting the agent &#39;on the stand&#39; to subject their actions and intentions to cross-examination. We show that under minimal assumptions automated reasoning can rigorously interrogate algorithmic beh&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2305.05731v2-abstract-full').style.display = 'inline'; document.getElementById('2305.05731v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2305.05731v2-abstract-full" style="display: none;">
        Principled accountability in the aftermath of harms is essential to the trustworthy design and governance of algorithmic decision making. Legal theory offers a paramount method for assessing culpability: putting the agent &#39;on the stand&#39; to subject their actions and intentions to cross-examination. We show that under minimal assumptions automated reasoning can rigorously interrogate algorithmic behaviors as in the adversarial process of legal fact finding. We model accountability processes, such as trials or review boards, as Counterfactual-Guided Logic Exploration and Abstraction Refinement (CLEAR) loops. We use the formal methods of symbolic execution and satisfiability modulo theories (SMT) solving to discharge queries about agent behavior in factual and counterfactual scenarios, as adaptively formulated by a human investigator. In order to do so, for a decision algorithm $\mathcal{A}$ we use symbolic execution to represent its logic as a statement $Π$ in the decidable theory $\texttt{QF_FPBV}$. We implement our framework and demonstrate its utility on an illustrative car crash scenario.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2305.05731v2-abstract-full').style.display = 'none'; document.getElementById('2305.05731v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 January, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 9 May, 2023;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2023.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2212.01861">arXiv:2212.01861</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2212.01861">pdf</a>, <a href="https://arxiv.org/format/2212.01861">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Logic in Computer Science">cs.LO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Online Shielding for Reinforcement Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=K%C3%B6nighofer%2C+B">Bettina Könighofer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rudolf%2C+J">Julian Rudolf</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Palmisano%2C+A">Alexander Palmisano</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tappler%2C+M">Martin Tappler</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bloem%2C+R">Roderick Bloem</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2212.01861v1-abstract-short" style="display: inline;">
        Besides the recent impressive results on reinforcement learning (RL), safety is still one of the major research challenges in RL. RL is a machine-learning approach to determine near-optimal policies in Markov decision processes (MDPs). In this paper, we consider the setting where the safety-relevant fragment of the MDP together with a temporal logic safety specification is given and many safety vi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2212.01861v1-abstract-full').style.display = 'inline'; document.getElementById('2212.01861v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2212.01861v1-abstract-full" style="display: none;">
        Besides the recent impressive results on reinforcement learning (RL), safety is still one of the major research challenges in RL. RL is a machine-learning approach to determine near-optimal policies in Markov decision processes (MDPs). In this paper, we consider the setting where the safety-relevant fragment of the MDP together with a temporal logic safety specification is given and many safety violations can be avoided by planning ahead a short time into the future. We propose an approach for online safety shielding of RL agents. During runtime, the shield analyses the safety of each available action. For any action, the shield computes the maximal probability to not violate the safety specification within the next $k$ steps when executing this action. Based on this probability and a given threshold, the shield decides whether to block an action from the agent. Existing offline shielding approaches compute exhaustively the safety of all state-action combinations ahead of time, resulting in huge computation times and large memory consumption. The intuition behind online shielding is to compute at runtime the set of all states that could be reached in the near future. For each of these states, the safety of all available actions is analysed and used for shielding as soon as one of the considered states is reached. Our approach is well suited for high-level planning problems where the time between decisions can be used for safety computations and it is sustainable for the agent to wait until these computations are finished. For our evaluation, we selected a 2-player version of the classical computer game SNAKE. The game represents a high-level planning problem that requires fast decisions and the multiplayer setting induces a large state space, which is computationally expensive to analyse exhaustively.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2212.01861v1-abstract-full').style.display = 'none'; document.getElementById('2212.01861v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 December, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">arXiv admin note: substantial text overlap with arXiv:2012.09539</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2212.01838">arXiv:2212.01838</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2212.01838">pdf</a>, <a href="https://arxiv.org/format/2212.01838">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Logic in Computer Science">cs.LO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Automata Learning meets Shielding
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tappler%2C+M">Martin Tappler</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pranger%2C+S">Stefan Pranger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=K%C3%B6nighofer%2C+B">Bettina Könighofer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mu%C5%A1kardin%2C+E">Edi Muškardin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bloem%2C+R">Roderick Bloem</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Larsen%2C+K">Kim Larsen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2212.01838v1-abstract-short" style="display: inline;">
        Safety is still one of the major research challenges in reinforcement learning (RL). In this paper, we address the problem of how to avoid safety violations of RL agents during exploration in probabilistic and partially unknown environments. Our approach combines automata learning for Markov Decision Processes (MDPs) and shield synthesis in an iterative approach. Initially, the MDP representing th&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2212.01838v1-abstract-full').style.display = 'inline'; document.getElementById('2212.01838v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2212.01838v1-abstract-full" style="display: none;">
        Safety is still one of the major research challenges in reinforcement learning (RL). In this paper, we address the problem of how to avoid safety violations of RL agents during exploration in probabilistic and partially unknown environments. Our approach combines automata learning for Markov Decision Processes (MDPs) and shield synthesis in an iterative approach. Initially, the MDP representing the environment is unknown. The agent starts exploring the environment and collects traces. From the collected traces, we passively learn MDPs that abstractly represent the safety-relevant aspects of the environment. Given a learned MDP and a safety specification, we construct a shield. For each state-action pair within a learned MDP, the shield computes exact probabilities on how likely it is that executing the action results in violating the specification from the current state within the next $k$ steps. After the shield is constructed, the shield is used during runtime and blocks any actions that induce a too large risk from the agent. The shielded agent continues to explore the environment and collects new data on the environment. Iteratively, we use the collected data to learn new MDPs with higher accuracy, resulting in turn in shields able to prevent more safety violations. We implemented our approach and present a detailed case study of a Q-learning agent exploring slippery Gridworlds. In our experiments, we show that as the agent explores more and more of the environment during training, the improved learned models lead to shields that are able to prevent many safety violations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2212.01838v1-abstract-full').style.display = 'none'; document.getElementById('2212.01838v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 December, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2208.14426">arXiv:2208.14426</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2208.14426">pdf</a>, <a href="https://arxiv.org/ps/2208.14426">ps</a>, <a href="https://arxiv.org/format/2208.14426">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Logic in Computer Science">cs.LO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Correct-by-Construction Runtime Enforcement in AI -- A Survey
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=K%C3%B6nighofer%2C+B">Bettina Könighofer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bloem%2C+R">Roderick Bloem</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ehlers%2C+R">Rüdiger Ehlers</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pek%2C+C">Christian Pek</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2208.14426v1-abstract-short" style="display: inline;">
        Runtime enforcement refers to the theories, techniques, and tools for enforcing correct behavior with respect to a formal specification of systems at runtime. In this paper, we are interested in techniques for constructing runtime enforcers for the concrete application domain of enforcing safety in AI. We discuss how safety is traditionally handled in the field of AI and how more formal guarantees&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2208.14426v1-abstract-full').style.display = 'inline'; document.getElementById('2208.14426v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2208.14426v1-abstract-full" style="display: none;">
        Runtime enforcement refers to the theories, techniques, and tools for enforcing correct behavior with respect to a formal specification of systems at runtime. In this paper, we are interested in techniques for constructing runtime enforcers for the concrete application domain of enforcing safety in AI. We discuss how safety is traditionally handled in the field of AI and how more formal guarantees on the safety of a self-learning agent can be given by integrating a runtime enforcer. We survey a selection of work on such enforcers, where we distinguish between approaches for discrete and continuous action spaces. The purpose of this paper is to foster a better understanding of advantages and limitations of different enforcement techniques, focusing on the specific challenges that arise due to their application in AI. Finally, we present some open challenges and avenues for future work.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2208.14426v1-abstract-full').style.display = 'none'; document.getElementById('2208.14426v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 August, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2205.04887">arXiv:2205.04887</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2205.04887">pdf</a>, <a href="https://arxiv.org/format/2205.04887">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Search-Based Testing of Reinforcement Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tappler%2C+M">Martin Tappler</a>, 
      
      <a href="/search/?searchtype=author&amp;query=C%C3%B3rdoba%2C+F+C">Filip Cano Córdoba</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aichernig%2C+B+K">Bernhard K. Aichernig</a>, 
      
      <a href="/search/?searchtype=author&amp;query=K%C3%B6nighofer%2C+B">Bettina Könighofer</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2205.04887v2-abstract-short" style="display: inline;">
        Evaluation of deep reinforcement learning (RL) is inherently challenging. Especially the opaqueness of learned policies and the stochastic nature of both agents and environments make testing the behavior of deep RL agents difficult. We present a search-based testing framework that enables a wide range of novel analysis capabilities for evaluating the safety and performance of deep RL agents. For s&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.04887v2-abstract-full').style.display = 'inline'; document.getElementById('2205.04887v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2205.04887v2-abstract-full" style="display: none;">
        Evaluation of deep reinforcement learning (RL) is inherently challenging. Especially the opaqueness of learned policies and the stochastic nature of both agents and environments make testing the behavior of deep RL agents difficult. We present a search-based testing framework that enables a wide range of novel analysis capabilities for evaluating the safety and performance of deep RL agents. For safety testing, our framework utilizes a search algorithm that searches for a reference trace that solves the RL task. The backtracking states of the search, called boundary states, pose safety-critical situations. We create safety test-suites that evaluate how well the RL agent escapes safety-critical situations near these boundary states. For robust performance testing, we create a diverse set of traces via fuzz testing. These fuzz traces are used to bring the agent into a wide variety of potentially unknown states from which the average performance of the agent is compared to the average performance of the fuzz traces. We apply our search-based testing approach on RL for Nintendo&#39;s Super Mario Bros.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.04887v2-abstract-full').style.display = 'none'; document.getElementById('2205.04887v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 May, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 May, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">11 pages, 15 figures, Accepted at IJCAI-ECAI 2022 (Main Track)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.12588">arXiv:2105.12588</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.12588">pdf</a>, <a href="https://arxiv.org/format/2105.12588">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Logic in Computer Science">cs.LO</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        TEMPEST -- Synthesis Tool for Reactive Systems and Shields in Probabilistic Environments
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Pranger%2C+S">Stefan Pranger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=K%C3%B6nighofer%2C+B">Bettina Könighofer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Posch%2C+L">Lukas Posch</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bloem%2C+R">Roderick Bloem</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.12588v1-abstract-short" style="display: inline;">
        We present Tempest, a synthesis tool to automatically create correct-by-construction reactive systems and shields from qualitative or quantitative specifications in probabilistic environments. A shield is a special type of reactive system used for run-time enforcement; i.e., a shield enforces a given qualitative or quantitative specification of a running system while interfering with its operation&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.12588v1-abstract-full').style.display = 'inline'; document.getElementById('2105.12588v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.12588v1-abstract-full" style="display: none;">
        We present Tempest, a synthesis tool to automatically create correct-by-construction reactive systems and shields from qualitative or quantitative specifications in probabilistic environments. A shield is a special type of reactive system used for run-time enforcement; i.e., a shield enforces a given qualitative or quantitative specification of a running system while interfering with its operation as little as possible. Shields that enforce a qualitative or quantitative specification are called safety-shields or optimal-shields, respectively. Safety-shields can be implemented as pre-shields or as post-shields, optimal-shields are implemented as post-shields. Pre-shields are placed before the system and restrict the choices of the system. Post-shields are implemented after the system and are able to overwrite the system&#39;s output. Tempest is based on the probabilistic model checker Storm, adding model checking algorithms for stochastic games with safety and mean-payoff objectives. To the best of our knowledge, Tempest is the only synthesis tool able to solve 2-1/2-player games with mean-payoff objectives without restrictions on the state space. Furthermore, Tempest adds the functionality to synthesize safe and optimal strategies that implement reactive systems and shields
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.12588v1-abstract-full').style.display = 'none'; document.getElementById('2105.12588v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 May, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2012.09539">arXiv:2012.09539</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2012.09539">pdf</a>, <a href="https://arxiv.org/format/2012.09539">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Logic in Computer Science">cs.LO</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Online Shielding for Stochastic Systems
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=K%C3%B6nighofer%2C+B">Bettina Könighofer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rudolf%2C+J">Julian Rudolf</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Palmisano%2C+A">Alexander Palmisano</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tappler%2C+M">Martin Tappler</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bloem%2C+R">Roderick Bloem</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2012.09539v1-abstract-short" style="display: inline;">
        In this paper, we propose a method to develop trustworthy reinforcement learning systems. To ensure safety especially during exploration, we automatically synthesize a correct-by-construction runtime enforcer, called a shield, that blocks all actions that are unsafe with respect to a temporal logic specification from the agent. Our main contribution is a new synthesis algorithm for computing the s&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.09539v1-abstract-full').style.display = 'inline'; document.getElementById('2012.09539v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2012.09539v1-abstract-full" style="display: none;">
        In this paper, we propose a method to develop trustworthy reinforcement learning systems. To ensure safety especially during exploration, we automatically synthesize a correct-by-construction runtime enforcer, called a shield, that blocks all actions that are unsafe with respect to a temporal logic specification from the agent. Our main contribution is a new synthesis algorithm for computing the shield online. Existing offline shielding approaches compute exhaustively the safety of all states-action combinations ahead-of-time, resulting in huge offline computation times, large memory consumption, and significant delays at run-time due to the look-ups in a huge database. The intuition behind online shielding is to compute during run-time the set of all states that could be reached in the near future. For each of these states, the safety of all available actions is analysed and used for shielding as soon as one of the considered states is reached. Our proposed method is general and can be applied to a wide range of planning problems with stochastic behavior. For our evaluation, we selected a 2-player version of the classical computer game SNAKE. The game requires fast decisions and the multiplayer setting induces a large state space, computationally expensive to analyze exhaustively. The safety objective of collision avoidance is easily transferable to a variety of planning tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2012.09539v1-abstract-full').style.display = 'none'; document.getElementById('2012.09539v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 December, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">18 Pages, 6 Figures, under submission</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2010.03842">arXiv:2010.03842</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2010.03842">pdf</a>, <a href="https://arxiv.org/format/2010.03842">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Logic in Computer Science">cs.LO</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adaptive Shielding under Uncertainty
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Pranger%2C+S">Stefan Pranger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=K%C3%B6nighofer%2C+B">Bettina Könighofer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tappler%2C+M">Martin Tappler</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deixelberger%2C+M">Martin Deixelberger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jansen%2C+N">Nils Jansen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bloem%2C+R">Roderick Bloem</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2010.03842v1-abstract-short" style="display: inline;">
        This paper targets control problems that exhibit specific safety and performance requirements. In particular, the aim is to ensure that an agent, operating under uncertainty, will at runtime strictly adhere to such requirements. Previous works create so-called shields that correct an existing controller for the agent if it is about to take unbearable safety risks. However, so far, shields do not c&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.03842v1-abstract-full').style.display = 'inline'; document.getElementById('2010.03842v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2010.03842v1-abstract-full" style="display: none;">
        This paper targets control problems that exhibit specific safety and performance requirements. In particular, the aim is to ensure that an agent, operating under uncertainty, will at runtime strictly adhere to such requirements. Previous works create so-called shields that correct an existing controller for the agent if it is about to take unbearable safety risks. However, so far, shields do not consider that an environment may not be fully known in advance and may evolve for complex control and learning tasks. We propose a new method for the efficient computation of a shield that is adaptive to a changing environment. In particular, we base our method on problems that are sufficiently captured by potentially infinite Markov decision processes (MDP) and quantitative specifications such as mean payoff objectives. The shield is independent of the controller, which may, for instance, take the form of a high-performing reinforcement learning agent. At runtime, our method builds an internal abstract representation of the MDP and constantly adapts this abstraction and the shield based on observations from the environment. We showcase the applicability of our method via an urban traffic control problem.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.03842v1-abstract-full').style.display = 'none'; document.getElementById('2010.03842v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 October, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages, 6 figures, 1 table</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2006.16688">arXiv:2006.16688</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2006.16688">pdf</a>, <a href="https://arxiv.org/format/2006.16688">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Logic in Computer Science">cs.LO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        It&#39;s Time to Play Safe: Shield Synthesis for Timed Systems
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bloem%2C+R">Roderick Bloem</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jensen%2C+P+G">Peter Gjøl Jensen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=K%C3%B6nighofer%2C+B">Bettina Könighofer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Larsen%2C+K+G">Kim Guldstrand Larsen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lorber%2C+F">Florian Lorber</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Palmisano%2C+A">Alexander Palmisano</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2006.16688v1-abstract-short" style="display: inline;">
        Erroneous behaviour in safety critical real-time systems may inflict serious consequences. In this paper, we show how to synthesize timed shields from timed safety properties given as timed automata. A timed shield enforces the safety of a running system while interfering with the system as little as possible. We present timed post-shields and timed pre-shields. A timed pre-shield is placed before&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.16688v1-abstract-full').style.display = 'inline'; document.getElementById('2006.16688v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2006.16688v1-abstract-full" style="display: none;">
        Erroneous behaviour in safety critical real-time systems may inflict serious consequences. In this paper, we show how to synthesize timed shields from timed safety properties given as timed automata. A timed shield enforces the safety of a running system while interfering with the system as little as possible. We present timed post-shields and timed pre-shields. A timed pre-shield is placed before the system and provides a set of safe outputs. This set restricts the choices of the system. A timed post-shield is implemented after the system. It monitors the system and corrects the system&#39;s output only if necessary. We further extend the timed post-shield construction to provide a guarantee on the recovery phase, i.e., the time between a specification violation and the point at which full control can be handed back to the system. In our experimental results, we use timed post-shields to ensure the safety in a reinforcement learning setting for controlling a platoon of cars, during the learning and execution phase, and study the effect.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.16688v1-abstract-full').style.display = 'none'; document.getElementById('2006.16688v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 June, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted to RV2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1904.06938">arXiv:1904.06938</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1904.06938">pdf</a>, <a href="https://arxiv.org/format/1904.06938">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Logic in Computer Science">cs.LO</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1007/978-3-319-49052-6\_9">10.1007/978-3-319-49052-6\_9 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Synthesis of Admissible Shields
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Humphrey%2C+L">Laura Humphrey</a>, 
      
      <a href="/search/?searchtype=author&amp;query=K%C3%B6nighofer%2C+B">Bettina Könighofer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=K%C3%B6nighofer%2C+R">Robert Könighofer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Topcu%2C+U">Ufuk Topcu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1904.06938v1-abstract-short" style="display: inline;">
        Shield synthesis is an approach to enforce a set of safety-critical properties of a reactive system at runtime. A shield monitors the system and corrects any erroneous output values instantaneously. The shield deviates from the given outputs as little as it can and recovers to hand back control to the system as soon as possible. This paper takes its inspiration from a case study on mission plannin&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1904.06938v1-abstract-full').style.display = 'inline'; document.getElementById('1904.06938v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1904.06938v1-abstract-full" style="display: none;">
        Shield synthesis is an approach to enforce a set of safety-critical properties of a reactive system at runtime. A shield monitors the system and corrects any erroneous output values instantaneously. The shield deviates from the given outputs as little as it can and recovers to hand back control to the system as soon as possible. This paper takes its inspiration from a case study on mission planning for unmanned aerial vehicles (UAVs) in which k-stabilizing shields, which guarantee recovery in a finite time, could not be constructed. We introduce the notion of admissible shields, which improves k-stabilizing shields in two ways: (1) whereas k-stabilizing shields take an adversarial view on the system, admissible shields take a collaborative view. That is, if there is no shield that guarantees recovery within k steps regardless of system behavior, the admissible shield will attempt to work with the system to recover as soon as possible. (2) Admissible shields can handle system failures during the recovery phase. In our experimental results we show that for UAVs, we can generate admissible shields, even when k-stabilizing shields do not exist.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1904.06938v1-abstract-full').style.display = 'none'; document.getElementById('1904.06938v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 April, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2019.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Hardware and Software: Verification and Testing - 12th International Haifa Verification Conference, {HVC} 2016, Haifa, Israel, November 14-17, 2016, Proceedings
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1807.06096">arXiv:1807.06096</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1807.06096">pdf</a>, <a href="https://arxiv.org/format/1807.06096">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Safe Reinforcement Learning via Probabilistic Shields
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Jansen%2C+N">Nils Jansen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=K%C3%B6nighofer%2C+B">Bettina Könighofer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Junges%2C+S">Sebastian Junges</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Serban%2C+A+C">Alexandru C. Serban</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bloem%2C+R">Roderick Bloem</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1807.06096v2-abstract-short" style="display: inline;">
        This paper targets the efficient construction of a safety shield for decision making in scenarios that incorporate uncertainty. Markov decision processes (MDPs) are prominent models to capture such planning problems. Reinforcement learning (RL) is a machine learning technique to determine near-optimal policies in MDPs that may be unknown prior to exploring the model. However, during exploration, R&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1807.06096v2-abstract-full').style.display = 'inline'; document.getElementById('1807.06096v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1807.06096v2-abstract-full" style="display: none;">
        This paper targets the efficient construction of a safety shield for decision making in scenarios that incorporate uncertainty. Markov decision processes (MDPs) are prominent models to capture such planning problems. Reinforcement learning (RL) is a machine learning technique to determine near-optimal policies in MDPs that may be unknown prior to exploring the model. However, during exploration, RL is prone to induce behavior that is undesirable or not allowed in safety- or mission-critical contexts. We introduce the concept of a probabilistic shield that enables decision-making to adhere to safety constraints with high probability. In a separation of concerns, we employ formal verification to efficiently compute the probabilities of critical decisions within a safety-relevant fragment of the MDP. We use these results to realize a shield that is applied to an RL algorithm which then optimizes the actual performance objective. We discuss tradeoffs between sufficient progress in exploration of the environment and ensuring safety. In our experiments, we demonstrate on the arcade game PAC-MAN and on a case study involving service robots that the learning efficiency increases as the learning needs orders of magnitude fewer episodes.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1807.06096v2-abstract-full').style.display = 'none'; document.getElementById('1807.06096v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 November, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 July, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1708.08611">arXiv:1708.08611</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1708.08611">pdf</a>, <a href="https://arxiv.org/format/1708.08611">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Logic in Computer Science">cs.LO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Safe Reinforcement Learning via Shielding
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Alshiekh%2C+M">Mohammed Alshiekh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bloem%2C+R">Roderick Bloem</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ehlers%2C+R">Ruediger Ehlers</a>, 
      
      <a href="/search/?searchtype=author&amp;query=K%C3%B6nighofer%2C+B">Bettina Könighofer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Niekum%2C+S">Scott Niekum</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Topcu%2C+U">Ufuk Topcu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1708.08611v2-abstract-short" style="display: inline;">
        Reinforcement learning algorithms discover policies that maximize reward, but do not necessarily guarantee safety during learning or execution phases. We introduce a new approach to learn optimal policies while enforcing properties expressed in temporal logic. To this end, given the temporal logic specification that is to be obeyed by the learning system, we propose to synthesize a reactive system&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1708.08611v2-abstract-full').style.display = 'inline'; document.getElementById('1708.08611v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1708.08611v2-abstract-full" style="display: none;">
        Reinforcement learning algorithms discover policies that maximize reward, but do not necessarily guarantee safety during learning or execution phases. We introduce a new approach to learn optimal policies while enforcing properties expressed in temporal logic. To this end, given the temporal logic specification that is to be obeyed by the learning system, we propose to synthesize a reactive system called a shield. The shield is introduced in the traditional learning process in two alternative ways, depending on the location at which the shield is implemented. In the first one, the shield acts each time the learning agent is about to make a decision and provides a list of safe actions. In the second way, the shield is introduced after the learning agent. The shield monitors the actions from the learner and corrects them only if the chosen action causes a violation of the specification. We discuss which requirements a shield must meet to preserve the convergence guarantees of the learner. Finally, we demonstrate the versatility of our approach on several challenging reinforcement learning scenarios.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1708.08611v2-abstract-full').style.display = 'none'; document.getElementById('1708.08611v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 September, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 29 August, 2017;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2017.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1501.02573">arXiv:1501.02573</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1501.02573">pdf</a>, <a href="https://arxiv.org/ps/1501.02573">ps</a>, <a href="https://arxiv.org/format/1501.02573">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Logic in Computer Science">cs.LO</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Shield Synthesis: Runtime Enforcement for Reactive Systems
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bloem%2C+R">Roderick Bloem</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Koenighofer%2C+B">Bettina Koenighofer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Koenighofer%2C+R">Robert Koenighofer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+C">Chao Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1501.02573v2-abstract-short" style="display: inline;">
        Scalability issues may prevent users from verifying critical properties of a complex hardware design. In this situation, we propose to synthesize a &#34;safety shield&#34; that is attached to the design to enforce the properties at run time. Shield synthesis can succeed where model checking and reactive synthesis fail, because it only considers a small set of critical properties, as opposed to the complex&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1501.02573v2-abstract-full').style.display = 'inline'; document.getElementById('1501.02573v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1501.02573v2-abstract-full" style="display: none;">
        Scalability issues may prevent users from verifying critical properties of a complex hardware design. In this situation, we propose to synthesize a &#34;safety shield&#34; that is attached to the design to enforce the properties at run time. Shield synthesis can succeed where model checking and reactive synthesis fail, because it only considers a small set of critical properties, as opposed to the complex design, or the complete specification in the case of reactive synthesis. The shield continuously monitors the input/output of the design and corrects its erroneous output only if necessary, and as little as possible, so other non-critical properties are likely to be retained. Although runtime enforcement has been studied in other domains such as action systems, reactive systems pose unique challenges where the shield must act without delay. We thus present the first shield synthesis solution for reactive hardware systems and report our experimental results. This is an extended version of [5], featuring an additional appendix.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1501.02573v2-abstract-full').style.display = 'none'; document.getElementById('1501.02573v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 January, 2015; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 12 January, 2015;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2015.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This is an extended version of [5], featuring an additional appendix</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1308.4767">arXiv:1308.4767</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1308.4767">pdf</a>, <a href="https://arxiv.org/format/1308.4767">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Logic in Computer Science">cs.LO</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Synthesizing Multiple Boolean Functions using Interpolation on a Single Proof
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hofferek%2C+G">Georg Hofferek</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gupta%2C+A">Ashutosh Gupta</a>, 
      
      <a href="/search/?searchtype=author&amp;query=K%C3%B6nighofer%2C+B">Bettina Könighofer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+J+R">Jie-Hong Roland Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bloem%2C+R">Roderick Bloem</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1308.4767v1-abstract-short" style="display: inline;">
        It is often difficult to correctly implement a Boolean controller for a complex system, especially when concurrency is involved. Yet, it may be easy to formally specify a controller. For instance, for a pipelined processor it suffices to state that the visible behavior of the pipelined system should be identical to a non-pipelined reference system (Burch-Dill paradigm). We present a novel procedur&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1308.4767v1-abstract-full').style.display = 'inline'; document.getElementById('1308.4767v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1308.4767v1-abstract-full" style="display: none;">
        It is often difficult to correctly implement a Boolean controller for a complex system, especially when concurrency is involved. Yet, it may be easy to formally specify a controller. For instance, for a pipelined processor it suffices to state that the visible behavior of the pipelined system should be identical to a non-pipelined reference system (Burch-Dill paradigm). We present a novel procedure to efficiently synthesize multiple Boolean control signals from a specification given as a quantified first-order formula (with a specific quantifier structure). Our approach uses uninterpreted functions to abstract details of the design. We construct an unsatisfiable SMT formula from the given specification. Then, from just one proof of unsatisfiability, we use a variant of Craig interpolation to compute multiple coordinated interpolants that implement the Boolean control signals. Our method avoids iterative learning and back-substitution of the control functions. We applied our approach to synthesize a controller for a simple two-stage pipelined processor, and present first experimental results.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1308.4767v1-abstract-full').style.display = 'none'; document.getElementById('1308.4767v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 August, 2013; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2013.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This paper originally appeared in FMCAD 2013, http://www.cs.utexas.edu/users/hunt/FMCAD/FMCAD13/index.shtml. This version includes an appendix that is missing in the conference version</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1207.1268">arXiv:1207.1268</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1207.1268">pdf</a>, <a href="https://arxiv.org/format/1207.1268">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Logic in Computer Science">cs.LO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Science and Game Theory">cs.GT</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.4204/EPTCS.84.4">10.4204/EPTCS.84.4 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Synthesizing Robust Systems with RATSY
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bloem%2C+R">Roderick Bloem</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gamauf%2C+H">Hans-Jürgen Gamauf</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hofferek%2C+G">Georg Hofferek</a>, 
      
      <a href="/search/?searchtype=author&amp;query=K%C3%B6nighofer%2C+B">Bettina Könighofer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=K%C3%B6nighofer%2C+R">Robert Könighofer</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1207.1268v1-abstract-short" style="display: inline;">
        Specifications for reactive systems often consist of environment assumptions and system guarantees. An implementation should not only be correct, but also robust in the sense that it behaves reasonably even when the assumptions are (temporarily) violated. We present an extension of the requirements analysis and synthesis tool RATSY that is able to synthesize robust systems from GR(1) specification&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1207.1268v1-abstract-full').style.display = 'inline'; document.getElementById('1207.1268v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1207.1268v1-abstract-full" style="display: none;">
        Specifications for reactive systems often consist of environment assumptions and system guarantees. An implementation should not only be correct, but also robust in the sense that it behaves reasonably even when the assumptions are (temporarily) violated. We present an extension of the requirements analysis and synthesis tool RATSY that is able to synthesize robust systems from GR(1) specifications, i.e., system in which a finite number of safety assumption violations is guaranteed to induce only a finite number of safety guarantee violations. We show how the specification can be turned into a two-pair Streett game, and how a winning strategy corresponding to a correct and robust implementation can be computed. Finally, we provide some experimental results.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1207.1268v1-abstract-full').style.display = 'none'; document.getElementById('1207.1268v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 July, 2012; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2012.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">In Proceedings SYNT 2012, arXiv:1207.0554</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        EPTCS 84, 2012, pp. 47-53
      </p>
    
  </li>

</ol>


  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>