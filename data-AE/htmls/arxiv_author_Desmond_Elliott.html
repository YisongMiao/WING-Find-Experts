<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;50 of 54 results for author: <span class="mathjax">Desmond Elliott</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/"  aria-role="search">
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Desmond Elliott">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Desmond+Elliott&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Desmond Elliott">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?query=Desmond+Elliott&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?query=Desmond+Elliott&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?query=Desmond+Elliott&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.02271">arXiv:2508.02271</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.02271">pdf</a>, <a href="https://arxiv.org/ps/2508.02271">ps</a>, <a href="https://arxiv.org/format/2508.02271">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Dynaword: From One-shot to Continuously Developed Datasets
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Enevoldsen%2C+K">Kenneth Enevoldsen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jensen%2C+K+N">Kristian Nørgaard Jensen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kostkan%2C+J">Jan Kostkan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Szab%C3%B3%2C+B">Balázs Szabó</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kardos%2C+M">Márton Kardos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vad%2C+K">Kirten Vad</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Heinsen%2C+J">Johan Heinsen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=N%C3%BA%C3%B1ez%2C+A+B">Andrea Blasi Núñez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Barmina%2C+G">Gianluca Barmina</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nielsen%2C+J">Jacob Nielsen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Larsen%2C+R">Rasmus Larsen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vahlstrup%2C+P">Peter Vahlstrup</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dalum%2C+P+M">Per Møldrup Dalum</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Galke%2C+L">Lukas Galke</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schneider-Kamp%2C+P">Peter Schneider-Kamp</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nielbo%2C+K">Kristoffer Nielbo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.02271v2-abstract-short" style="display: inline;">
        Large-scale datasets are foundational for research and development in natural language processing. However, current approaches face three key challenges: (1) reliance on ambiguously licensed sources restricting use, sharing, and derivative works; (2) static dataset releases that prevent community contributions and diminish longevity; and (3) quality assurance processes restricted to publishing tea&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.02271v2-abstract-full').style.display = 'inline'; document.getElementById('2508.02271v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.02271v2-abstract-full" style="display: none;">
        Large-scale datasets are foundational for research and development in natural language processing. However, current approaches face three key challenges: (1) reliance on ambiguously licensed sources restricting use, sharing, and derivative works; (2) static dataset releases that prevent community contributions and diminish longevity; and (3) quality assurance processes restricted to publishing teams rather than leveraging community expertise.
  To address these limitations, we introduce two contributions: the Dynaword approach and Danish Dynaword. The Dynaword approach is a framework for creating large-scale, open datasets that can be continuously updated through community collaboration. Danish Dynaword is a concrete implementation that validates this approach and demonstrates its potential. Danish Dynaword contains over four times as many tokens as comparable releases, is exclusively openly licensed, and has received multiple contributions across industry and research. The repository includes light-weight tests to ensure data formatting, quality, and documentation, establishing a sustainable framework for ongoing community contributions and dataset evolution.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.02271v2-abstract-full').style.display = 'none'; document.getElementById('2508.02271v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 4 August, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.06275">arXiv:2506.06275</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.06275">pdf</a>, <a href="https://arxiv.org/ps/2506.06275">ps</a>, <a href="https://arxiv.org/format/2506.06275">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Movie Facts and Fibs (MF$^2$): A Benchmark for Long Movie Understanding
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zaranis%2C+E">Emmanouil Zaranis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Farinhas%2C+A">António Farinhas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Santos%2C+S">Saul Santos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Canaverde%2C+B">Beatriz Canaverde</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ramos%2C+M+M">Miguel Moura Ramos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Surikuchi%2C+A+K">Aditya K Surikuchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Viveiros%2C+A">André Viveiros</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liao%2C+B">Baohao Liao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bueno-Benito%2C+E">Elena Bueno-Benito</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sivakumaran%2C+N">Nithin Sivakumaran</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vasylenko%2C+P">Pavlo Vasylenko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+S">Shoubin Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sannigrahi%2C+S">Sonal Sannigrahi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mohammed%2C+W">Wafaa Mohammed</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peters%2C+B">Ben Peters</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Villegas%2C+D+S">Danae Sánchez Villegas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Stengel-Eskin%2C+E">Elias Stengel-Eskin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Attanasio%2C+G">Giuseppe Attanasio</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yoon%2C+J">Jaehong Yoon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Frank%2C+S">Stella Frank</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Suglia%2C+A">Alessandro Suglia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zerva%2C+C">Chrysoula Zerva</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dimiccoli%2C+M">Mariella Dimiccoli</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bansal%2C+M">Mohit Bansal</a>
      , et al. (6 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.06275v1-abstract-short" style="display: inline;">
        Despite recent progress in vision-language models (VLMs), holistic understanding of long-form video content remains a significant challenge, partly due to limitations in current benchmarks. Many focus on peripheral, ``needle-in-a-haystack&#39;&#39; details, encouraging context-insensitive retrieval over deep comprehension. Others rely on large-scale, semi-automatically generated questions (often produced&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.06275v1-abstract-full').style.display = 'inline'; document.getElementById('2506.06275v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.06275v1-abstract-full" style="display: none;">
        Despite recent progress in vision-language models (VLMs), holistic understanding of long-form video content remains a significant challenge, partly due to limitations in current benchmarks. Many focus on peripheral, ``needle-in-a-haystack&#39;&#39; details, encouraging context-insensitive retrieval over deep comprehension. Others rely on large-scale, semi-automatically generated questions (often produced by language models themselves) that are easier for models to answer but fail to reflect genuine understanding. In this paper, we introduce MF$^2$, a new benchmark for evaluating whether models can comprehend, consolidate, and recall key narrative information from full-length movies (50-170 minutes long). MF$^2$ includes over 50 full-length, open-licensed movies, each paired with manually constructed sets of claim pairs -- one true (fact) and one plausible but false (fib), totalling over 850 pairs. These claims target core narrative elements such as character motivations and emotions, causal chains, and event order, and refer to memorable moments that humans can recall without rewatching the movie. Instead of multiple-choice formats, we adopt a binary claim evaluation protocol: for each pair, models must correctly identify both the true and false claims. This reduces biases like answer ordering and enables a more precise assessment of reasoning. Our experiments demonstrate that both open-weight and closed state-of-the-art models fall well short of human performance, underscoring the relative ease of the task for humans and their superior ability to retain and reason over critical narrative information -- an ability current VLMs lack.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.06275v1-abstract-full').style.display = 'none'; document.getElementById('2506.06275v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Under Review</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.03994">arXiv:2506.03994</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.03994">pdf</a>, <a href="https://arxiv.org/ps/2506.03994">ps</a>, <a href="https://arxiv.org/format/2506.03994">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Seeing What Tastes Good: Revisiting Multimodal Distributional Semantics in the Billion Parameter Era
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Oneata%2C+D">Dan Oneata</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Frank%2C+S">Stella Frank</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.03994v1-abstract-short" style="display: inline;">
        Human learning and conceptual representation is grounded in sensorimotor experience, in contrast to state-of-the-art foundation models. In this paper, we investigate how well such large-scale models, trained on vast quantities of data, represent the semantic feature norms of concrete object concepts, e.g. a ROSE is red, smells sweet, and is a flower. More specifically, we use probing tasks to test&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.03994v1-abstract-full').style.display = 'inline'; document.getElementById('2506.03994v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.03994v1-abstract-full" style="display: none;">
        Human learning and conceptual representation is grounded in sensorimotor experience, in contrast to state-of-the-art foundation models. In this paper, we investigate how well such large-scale models, trained on vast quantities of data, represent the semantic feature norms of concrete object concepts, e.g. a ROSE is red, smells sweet, and is a flower. More specifically, we use probing tasks to test which properties of objects these models are aware of. We evaluate image encoders trained on image data alone, as well as multimodally-trained image encoders and language-only models, on predicting an extended denser version of the classic McRae norms and the newer Binder dataset of attribute ratings. We find that multimodal image encoders slightly outperform language-only approaches, and that image-only encoders perform comparably to the language models, even on non-visual attributes that are classified as &#34;encyclopedic&#34; or &#34;function&#34;. These results offer new insights into what can be learned from pure unimodal learning, and the complementarity of the modalities.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.03994v1-abstract-full').style.display = 'none'; document.getElementById('2506.03994v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ACL Findings 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2505.21265">arXiv:2505.21265</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2505.21265">pdf</a>, <a href="https://arxiv.org/ps/2505.21265">ps</a>, <a href="https://arxiv.org/format/2505.21265">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multilingual Pretraining for Pixel Language Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kesen%2C+I">Ilker Kesen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lotz%2C+J+F">Jonas F. Lotz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ziegler%2C+I">Ingo Ziegler</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rust%2C+P">Phillip Rust</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2505.21265v1-abstract-short" style="display: inline;">
        Pixel language models operate directly on images of rendered text, eliminating the need for a fixed vocabulary. While these models have demonstrated strong capabilities for downstream cross-lingual transfer, multilingual pretraining remains underexplored. We introduce PIXEL-M4, a model pretrained on four visually and linguistically diverse languages: English, Hindi, Ukrainian, and Simplified Chine&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.21265v1-abstract-full').style.display = 'inline'; document.getElementById('2505.21265v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2505.21265v1-abstract-full" style="display: none;">
        Pixel language models operate directly on images of rendered text, eliminating the need for a fixed vocabulary. While these models have demonstrated strong capabilities for downstream cross-lingual transfer, multilingual pretraining remains underexplored. We introduce PIXEL-M4, a model pretrained on four visually and linguistically diverse languages: English, Hindi, Ukrainian, and Simplified Chinese. Multilingual evaluations on semantic and syntactic tasks show that PIXEL-M4 outperforms an English-only counterpart on non-Latin scripts. Word-level probing analyses confirm that PIXEL-M4 captures rich linguistic features, even in languages not seen during pretraining. Furthermore, an analysis of its hidden representations shows that multilingual pretraining yields a semantic embedding space closely aligned across the languages used for pretraining. This work demonstrates that multilingual pretraining substantially enhances the capability of pixel language models to effectively support a diverse set of languages.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.21265v1-abstract-full').style.display = 'none'; document.getElementById('2505.21265v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 May, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">17 pages, 19 figures, 7 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2505.20133">arXiv:2505.20133</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2505.20133">pdf</a>, <a href="https://arxiv.org/ps/2505.20133">ps</a>, <a href="https://arxiv.org/format/2505.20133">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AweDist: Attention-aware Embedding Distillation for New Input Token Embeddings
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dobler%2C+K">Konstantin Dobler</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>, 
      
      <a href="/search/?searchtype=author&amp;query=de+Melo%2C+G">Gerard de Melo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2505.20133v1-abstract-short" style="display: inline;">
        Current language models rely on static vocabularies determined at pretraining time, which can lead to decreased performance and increased computational cost for domains underrepresented in the original vocabulary. New tokens can be added to solve this problem, when coupled with a good initialization for their new embeddings. However, existing embedding initialization methods either require expensi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.20133v1-abstract-full').style.display = 'inline'; document.getElementById('2505.20133v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2505.20133v1-abstract-full" style="display: none;">
        Current language models rely on static vocabularies determined at pretraining time, which can lead to decreased performance and increased computational cost for domains underrepresented in the original vocabulary. New tokens can be added to solve this problem, when coupled with a good initialization for their new embeddings. However, existing embedding initialization methods either require expensive further training or pretraining of additional modules. In this paper, we propose AweDist and show that by distilling representations obtained using the original tokenization, we can quickly learn high-quality input embeddings for new tokens. Experimental results with a wide range of open-weight models show that AweDist is able to outperform even strong baselines.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.20133v1-abstract-full').style.display = 'none'; document.getElementById('2505.20133v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 May, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2505.14729">arXiv:2505.14729</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2505.14729">pdf</a>, <a href="https://arxiv.org/ps/2505.14729">ps</a>, <a href="https://arxiv.org/format/2505.14729">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Uncovering Cultural Representation Disparities in Vision-Language Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kadiyala%2C+R+M+R">Ram Mohan Rao Kadiyala</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gupta%2C+S">Siddhant Gupta</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Purbey%2C+J">Jebish Purbey</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yadav%2C+S">Srishti Yadav</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Debnath%2C+S">Suman Debnath</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Salamanca%2C+A">Alejandro Salamanca</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2505.14729v3-abstract-short" style="display: inline;">
        Vision-Language Models (VLMs) have demonstrated impressive capabilities across a range of tasks, yet concerns about their potential biases exist. This work investigates the extent to which prominent VLMs exhibit cultural biases by evaluating their performance on an image-based country identification task at a country level. Utilizing the geographically diverse Country211 dataset, we probe several&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.14729v3-abstract-full').style.display = 'inline'; document.getElementById('2505.14729v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2505.14729v3-abstract-full" style="display: none;">
        Vision-Language Models (VLMs) have demonstrated impressive capabilities across a range of tasks, yet concerns about their potential biases exist. This work investigates the extent to which prominent VLMs exhibit cultural biases by evaluating their performance on an image-based country identification task at a country level. Utilizing the geographically diverse Country211 dataset, we probe several large vision language models (VLMs) under various prompting strategies: open-ended questions, multiple-choice questions (MCQs) including challenging setups like multilingual and adversarial settings. Our analysis aims to uncover disparities in model accuracy across different countries and question formats, providing insights into how training data distribution and evaluation methodologies might influence cultural biases in VLMs. The findings highlight significant variations in performance, suggesting that while VLMs possess considerable visual understanding, they inherit biases from their pre-training data and scale that impact their ability to generalize uniformly across diverse global contexts.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.14729v3-abstract-full').style.display = 'none'; document.getElementById('2505.14729v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 July, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 May, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">28 pages, 36 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2504.11169">arXiv:2504.11169</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2504.11169">pdf</a>, <a href="https://arxiv.org/ps/2504.11169">ps</a>, <a href="https://arxiv.org/format/2504.11169">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MuSeD: A Multimodal Spanish Dataset for Sexism Detection in Social Media Videos
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=De+Grazia%2C+L">Laura De Grazia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pastells%2C+P">Pol Pastells</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chas%2C+M+V">Mauro Vázquez Chas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Villegas%2C+D+S">Danae Sánchez Villegas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Farr%C3%BAs%2C+M">Mireia Farrús</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Taul%C3%A9%2C+M">Mariona Taulé</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2504.11169v2-abstract-short" style="display: inline;">
        Sexism is generally defined as prejudice and discrimination based on sex or gender, affecting every sector of society, from social institutions to relationships and individual behavior. Social media platforms amplify the impact of sexism by conveying discriminatory content not only through text but also across multiple modalities, highlighting the critical need for a multimodal approach to the ana&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2504.11169v2-abstract-full').style.display = 'inline'; document.getElementById('2504.11169v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2504.11169v2-abstract-full" style="display: none;">
        Sexism is generally defined as prejudice and discrimination based on sex or gender, affecting every sector of society, from social institutions to relationships and individual behavior. Social media platforms amplify the impact of sexism by conveying discriminatory content not only through text but also across multiple modalities, highlighting the critical need for a multimodal approach to the analysis of sexism online. With the rise of social media platforms where users share short videos, sexism is increasingly spreading through video content. Automatically detecting sexism in videos is a challenging task, as it requires analyzing the combination of verbal, audio, and visual elements to identify sexist content. In this study, (1) we introduce MuSeD, a new Multimodal Spanish dataset for Sexism Detection consisting of $\approx$ 11 hours of videos extracted from TikTok and BitChute; (2) we propose an innovative annotation framework for analyzing the contributions of textual, vocal, and visual modalities to the classification of content as either sexist or non-sexist; and (3) we evaluate a range of large language models (LLMs) and multimodal LLMs on the task of sexism detection. We find that visual information plays a key role in labeling sexist content for both humans and models. Models effectively detect explicit sexism; however, they struggle with implicit cases, such as stereotypes, instances where annotators also show low agreement. This highlights the inherent difficulty of the task, as identifying implicit sexism depends on the social and cultural context.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2504.11169v2-abstract-full').style.display = 'none'; document.getElementById('2504.11169v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 April, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">COLM 2025 camera-ready version: expanded Section 4.3 with an additional experiment using an extended definition-based prompt (including a definition of sexist content), and applied minor corrections</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2504.07072">arXiv:2504.07072</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2504.07072">pdf</a>, <a href="https://arxiv.org/format/2504.07072">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Kaleidoscope: In-language Exams for Massively Multilingual Vision Evaluation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Salazar%2C+I">Israfel Salazar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Burda%2C+M+F">Manuel Fernández Burda</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Islam%2C+S+B">Shayekh Bin Islam</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Moakhar%2C+A+S">Arshia Soltani Moakhar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Singh%2C+S">Shivalika Singh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Farestam%2C+F">Fabian Farestam</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Romanou%2C+A">Angelika Romanou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boiko%2C+D">Danylo Boiko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Khullar%2C+D">Dipika Khullar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+M">Mike Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Krzemi%C5%84ski%2C+D">Dominik Krzemiński</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Novikova%2C+J">Jekaterina Novikova</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shimabucoro%2C+L">Luísa Shimabucoro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Imperial%2C+J+M">Joseph Marvin Imperial</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Maheshwary%2C+R">Rishabh Maheshwary</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Duwal%2C+S">Sharad Duwal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Amayuelas%2C+A">Alfonso Amayuelas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rajwal%2C+S">Swati Rajwal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Purbey%2C+J">Jebish Purbey</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ruby%2C+A">Ahmed Ruby</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Popovi%C4%8D%2C+N">Nicholas Popovič</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Suppa%2C+M">Marek Suppa</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wasi%2C+A+T">Azmine Toushik Wasi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kadiyala%2C+R+M+R">Ram Mohan Rao Kadiyala</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsymboi%2C+O">Olga Tsymboi</a>
      , et al. (20 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2504.07072v2-abstract-short" style="display: inline;">
        The evaluation of vision-language models (VLMs) has mainly relied on English-language benchmarks, leaving significant gaps in both multilingual and multicultural coverage. While multilingual benchmarks have expanded, both in size and languages, many rely on translations of English datasets, failing to capture cultural nuances. In this work, we propose Kaleidoscope, as the most comprehensive exam b&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2504.07072v2-abstract-full').style.display = 'inline'; document.getElementById('2504.07072v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2504.07072v2-abstract-full" style="display: none;">
        The evaluation of vision-language models (VLMs) has mainly relied on English-language benchmarks, leaving significant gaps in both multilingual and multicultural coverage. While multilingual benchmarks have expanded, both in size and languages, many rely on translations of English datasets, failing to capture cultural nuances. In this work, we propose Kaleidoscope, as the most comprehensive exam benchmark to date for the multilingual evaluation of vision-language models. Kaleidoscope is a large-scale, in-language multimodal benchmark designed to evaluate VLMs across diverse languages and visual inputs. Kaleidoscope covers 18 languages and 14 different subjects, amounting to a total of 20,911 multiple-choice questions. Built through an open science collaboration with a diverse group of researchers worldwide, Kaleidoscope ensures linguistic and cultural authenticity. We evaluate top-performing multilingual vision-language models and find that they perform poorly on low-resource languages and in complex multimodal scenarios. Our results highlight the need for progress on culturally inclusive multimodal evaluation frameworks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2504.07072v2-abstract-full').style.display = 'none'; document.getElementById('2504.07072v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 April, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 9 April, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">v2: corrected the author list</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2502.19409">arXiv:2502.19409</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2502.19409">pdf</a>, <a href="https://arxiv.org/ps/2502.19409">ps</a>, <a href="https://arxiv.org/format/2502.19409">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ImageChain: Advancing Sequential Image-to-Text Reasoning in Multimodal Large Language Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Villegas%2C+D+S">Danae Sánchez Villegas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ziegler%2C+I">Ingo Ziegler</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2502.19409v2-abstract-short" style="display: inline;">
        Reasoning over sequences of images remains a challenge for multimodal large language models (MLLMs). While recent models incorporate multi-image data during pre-training, they still struggle to recognize sequential structures, often treating images independently. This work introduces ImageChain, a framework that enhances MLLMs with sequential reasoning capabilities over image data by modeling visu&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.19409v2-abstract-full').style.display = 'inline'; document.getElementById('2502.19409v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2502.19409v2-abstract-full" style="display: none;">
        Reasoning over sequences of images remains a challenge for multimodal large language models (MLLMs). While recent models incorporate multi-image data during pre-training, they still struggle to recognize sequential structures, often treating images independently. This work introduces ImageChain, a framework that enhances MLLMs with sequential reasoning capabilities over image data by modeling visual sequences as a multi-turn conversation. In ImageChain, images are interleaved with corresponding textual descriptions to form a controlled dialogue that explicitly captures temporal dependencies and narrative progression. Our method optimizes for the task of next-scene description, where the model generates a context-aware description of an upcoming scene based on preceding visual and textual cues. We demonstrate that our approach improves performance on the next-scene description task -- achieving an average improvement from 3.7% to 19% in SimRate, a metric that quantifies semantic similarity to human-annotated ground truths. Moreover, ImageChain achieves robust zero-shot out-of-domain performance in applications ranging from comics to robotics. Extensive experiments validate that instruction-tuning in a multimodal, multi-turn conversation design is key to bridging the gap between static image understanding and temporally-aware reasoning.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.19409v2-abstract-full').style.display = 'none'; document.getElementById('2502.19409v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 June, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 26 February, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Code, dataset, and checkpoints are publicly available at https://github.com/danaesavi/ImageChain; v2: added human annotation study to validate SimRate</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2502.14132">arXiv:2502.14132</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2502.14132">pdf</a>, <a href="https://arxiv.org/format/2502.14132">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Can Community Notes Replace Professional Fact-Checkers?
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Borenstein%2C+N">Nadav Borenstein</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Warren%2C+G">Greta Warren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Augenstein%2C+I">Isabelle Augenstein</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2502.14132v2-abstract-short" style="display: inline;">
        Two commonly employed strategies to combat the rise of misinformation on social media are (i) fact-checking by professional organisations and (ii) community moderation by platform users. Policy changes by Twitter/X and, more recently, Meta, signal a shift away from partnerships with fact-checking organisations and towards an increased reliance on crowdsourced community notes. However, the extent a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.14132v2-abstract-full').style.display = 'inline'; document.getElementById('2502.14132v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2502.14132v2-abstract-full" style="display: none;">
        Two commonly employed strategies to combat the rise of misinformation on social media are (i) fact-checking by professional organisations and (ii) community moderation by platform users. Policy changes by Twitter/X and, more recently, Meta, signal a shift away from partnerships with fact-checking organisations and towards an increased reliance on crowdsourced community notes. However, the extent and nature of dependencies between fact-checking and helpful community notes remain unclear. To address these questions, we use language models to annotate a large corpus of Twitter/X community notes with attributes such as topic, cited sources, and whether they refute claims tied to broader misinformation narratives. Our analysis reveals that community notes cite fact-checking sources up to five times more than previously reported. Fact-checking is especially crucial for notes on posts linked to broader narratives, which are twice as likely to reference fact-checking sources compared to other sources. Our results show that successful community moderation relies on professional fact-checking and highlight how citizen and professional fact-checking are deeply intertwined.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.14132v2-abstract-full').style.display = 'none'; document.getElementById('2502.14132v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 May, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 February, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to the main proceedings of ACL 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.14387">arXiv:2410.14387</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.14387">pdf</a>, <a href="https://arxiv.org/ps/2410.14387">ps</a>, <a href="https://arxiv.org/format/2410.14387">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        How Do Multilingual Language Models Remember Facts?
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Fierro%2C+C">Constanza Fierro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Foroutan%2C+N">Negar Foroutan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>, 
      
      <a href="/search/?searchtype=author&amp;query=S%C3%B8gaard%2C+A">Anders Søgaard</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.14387v3-abstract-short" style="display: inline;">
        Large Language Models (LLMs) store and retrieve vast amounts of factual knowledge acquired during pre-training. Prior research has localized and identified mechanisms behind knowledge recall; however, it has only focused on English monolingual models. The question of how these mechanisms generalize to non-English languages and multilingual LLMs remains unexplored. In this paper, we address this ga&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.14387v3-abstract-full').style.display = 'inline'; document.getElementById('2410.14387v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.14387v3-abstract-full" style="display: none;">
        Large Language Models (LLMs) store and retrieve vast amounts of factual knowledge acquired during pre-training. Prior research has localized and identified mechanisms behind knowledge recall; however, it has only focused on English monolingual models. The question of how these mechanisms generalize to non-English languages and multilingual LLMs remains unexplored. In this paper, we address this gap by conducting a comprehensive analysis of three multilingual LLMs. First, we show that previously identified recall mechanisms in English largely apply to multilingual contexts, with nuances based on language and architecture. Next, through patching intermediate representations, we localize the role of language during recall, finding that subject enrichment is language-independent, while object extraction is language-dependent. Additionally, we discover that the last token representation acts as a Function Vector (FV), encoding both the language of the query and the content to be extracted from the subject. Furthermore, in decoder-only LLMs, FVs compose these two pieces of information in two separate stages. These insights reveal unique mechanisms in multilingual LLMs for recalling information, highlighting the need for new methodologies -- such as knowledge evaluation, fact editing, and knowledge acquisition -- that are specifically tailored for multilingual LLMs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.14387v3-abstract-full').style.display = 'none'; document.getElementById('2410.14387v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 June, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 October, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.12391">arXiv:2410.12391</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.12391">pdf</a>, <a href="https://arxiv.org/format/2410.12391">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Tracking Universal Features Through Fine-Tuning and Model Merging
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Horn%2C+N">Niels Horn</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.12391v1-abstract-short" style="display: inline;">
        We study how features emerge, disappear, and persist across models fine-tuned on different domains of text. More specifically, we start from a base one-layer Transformer language model that is trained on a combination of the BabyLM corpus, and a collection of Python code from The Stack. This base model is adapted to two new domains of text: TinyStories, and the Lua programming language, respective&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.12391v1-abstract-full').style.display = 'inline'; document.getElementById('2410.12391v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.12391v1-abstract-full" style="display: none;">
        We study how features emerge, disappear, and persist across models fine-tuned on different domains of text. More specifically, we start from a base one-layer Transformer language model that is trained on a combination of the BabyLM corpus, and a collection of Python code from The Stack. This base model is adapted to two new domains of text: TinyStories, and the Lua programming language, respectively; and then these two models are merged using these two models using spherical linear interpolation. Our exploration aims to provide deeper insights into the stability and transformation of features across typical transfer-learning scenarios using small-scale models and sparse auto-encoders.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.12391v1-abstract-full').style.display = 'none'; document.getElementById('2410.12391v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.20147">arXiv:2409.20147</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.20147">pdf</a>, <a href="https://arxiv.org/format/2409.20147">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Classification of Radiological Text in Small and Imbalanced Datasets in a Non-English Language
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Beliveau%2C+V">Vincent Beliveau</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kaas%2C+H">Helene Kaas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Prener%2C+M">Martin Prener</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ladefoged%2C+C+N">Claes N. Ladefoged</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Knudsen%2C+G+M">Gitte M. Knudsen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pinborg%2C+L+H">Lars H. Pinborg</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ganz%2C+M">Melanie Ganz</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.20147v1-abstract-short" style="display: inline;">
        Natural language processing (NLP) in the medical domain can underperform in real-world applications involving small datasets in a non-English language with few labeled samples and imbalanced classes. There is yet no consensus on how to approach this problem. We evaluated a set of NLP models including BERT-like transformers, few-shot learning with sentence transformers (SetFit), and prompted large&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.20147v1-abstract-full').style.display = 'inline'; document.getElementById('2409.20147v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.20147v1-abstract-full" style="display: none;">
        Natural language processing (NLP) in the medical domain can underperform in real-world applications involving small datasets in a non-English language with few labeled samples and imbalanced classes. There is yet no consensus on how to approach this problem. We evaluated a set of NLP models including BERT-like transformers, few-shot learning with sentence transformers (SetFit), and prompted large language models (LLM), using three datasets of radiology reports on magnetic resonance images of epilepsy patients in Danish, a low-resource language. Our results indicate that BERT-like models pretrained in the target domain of radiology reports currently offer the optimal performances for this scenario. Notably, the SetFit and LLM models underperformed compared to BERT-like models, with LLM performing the worst. Importantly, none of the models investigated was sufficiently accurate to allow for text classification without any supervision. However, they show potential for data filtering, which could reduce the amount of manual labeling required.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.20147v1-abstract-full').style.display = 'none'; document.getElementById('2409.20147v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.02098">arXiv:2409.02098</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.02098">pdf</a>, <a href="https://arxiv.org/ps/2409.02098">ps</a>, <a href="https://arxiv.org/format/2409.02098">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CRAFT Your Dataset: Task-Specific Synthetic Dataset Generation Through Corpus Retrieval and Augmentation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ziegler%2C+I">Ingo Ziegler</a>, 
      
      <a href="/search/?searchtype=author&amp;query=K%C3%B6ksal%2C+A">Abdullatif Köksal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sch%C3%BCtze%2C+H">Hinrich Schütze</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.02098v2-abstract-short" style="display: inline;">
        Building high-quality datasets for specialized tasks is a time-consuming and resource-intensive process that often requires specialized domain knowledge. We propose Corpus Retrieval and Augmentation for Fine-Tuning (CRAFT), a method for generating synthetic datasets, given a small number of user-written few-shots that demonstrate the task to be performed. Given these examples, CRAFT uses large-sca&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.02098v2-abstract-full').style.display = 'inline'; document.getElementById('2409.02098v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.02098v2-abstract-full" style="display: none;">
        Building high-quality datasets for specialized tasks is a time-consuming and resource-intensive process that often requires specialized domain knowledge. We propose Corpus Retrieval and Augmentation for Fine-Tuning (CRAFT), a method for generating synthetic datasets, given a small number of user-written few-shots that demonstrate the task to be performed. Given these examples, CRAFT uses large-scale public web-crawled corpora and similarity-based document retrieval to find other relevant human-written documents. Lastly, instruction-tuned large language models (LLMs) augment the retrieved documents into custom-formatted task samples, which then can be used for fine-tuning. We demonstrate that CRAFT can efficiently generate large-scale task-specific training datasets for four diverse tasks: biology, medicine, and commonsense question-answering (QA), as well as summarization. Our experiments show that CRAFT-based models outperform or match general LLMs on QA tasks, while exceeding models trained on human-curated summarization data by 46 preference points. CRAFT outperforms other synthetic dataset generation methods such as Self- and Evol-Instruct, and remains robust even when the quality of the initial few-shots varies.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.02098v2-abstract-full').style.display = 'none'; document.getElementById('2409.02098v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 3 September, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at TACL; Pre-MIT Press publication version. Code and dataset available at: https://github.com/ziegler-ingo/CRAFT</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.18403">arXiv:2406.18403</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.18403">pdf</a>, <a href="https://arxiv.org/ps/2406.18403">ps</a>, <a href="https://arxiv.org/format/2406.18403">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LLMs instead of Human Judges? A Large Scale Empirical Study across 20 NLP Evaluation Tasks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bavaresco%2C+A">Anna Bavaresco</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bernardi%2C+R">Raffaella Bernardi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bertolazzi%2C+L">Leonardo Bertolazzi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fern%C3%A1ndez%2C+R">Raquel Fernández</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gatt%2C+A">Albert Gatt</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ghaleb%2C+E">Esam Ghaleb</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Giulianelli%2C+M">Mario Giulianelli</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hanna%2C+M">Michael Hanna</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Koller%2C+A">Alexander Koller</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Martins%2C+A+F+T">André F. T. Martins</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mondorf%2C+P">Philipp Mondorf</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Neplenbroek%2C+V">Vera Neplenbroek</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pezzelle%2C+S">Sandro Pezzelle</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Plank%2C+B">Barbara Plank</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schlangen%2C+D">David Schlangen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Suglia%2C+A">Alessandro Suglia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Surikuchi%2C+A+K">Aditya K Surikuchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Takmaz%2C+E">Ece Takmaz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Testoni%2C+A">Alberto Testoni</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.18403v3-abstract-short" style="display: inline;">
        There is an increasing trend towards evaluating NLP models with LLMs instead of human judgments, raising questions about the validity of these evaluations, as well as their reproducibility in the case of proprietary models. We provide JUDGE-BENCH, an extensible collection of 20 NLP datasets with human annotations covering a broad range of evaluated properties and types of data, and comprehensively&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.18403v3-abstract-full').style.display = 'inline'; document.getElementById('2406.18403v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.18403v3-abstract-full" style="display: none;">
        There is an increasing trend towards evaluating NLP models with LLMs instead of human judgments, raising questions about the validity of these evaluations, as well as their reproducibility in the case of proprietary models. We provide JUDGE-BENCH, an extensible collection of 20 NLP datasets with human annotations covering a broad range of evaluated properties and types of data, and comprehensively evaluate 11 current LLMs, covering both open-weight and proprietary models, for their ability to replicate the annotations. Our evaluations show substantial variance across models and datasets. Models are reliable evaluators on some tasks, but overall display substantial variability depending on the property being evaluated, the expertise level of the human judges, and whether the language is human or model-generated. We conclude that LLMs should be carefully validated against human judgments before being used as evaluators.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.18403v3-abstract-full').style.display = 'none'; document.getElementById('2406.18403v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 June, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 26 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to the main conference of ACL 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.11030">arXiv:2406.11030</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.11030">pdf</a>, <a href="https://arxiv.org/format/2406.11030">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FoodieQA: A Multimodal Dataset for Fine-Grained Understanding of Chinese Food Culture
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+W">Wenyan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+X">Xinyu Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Jiaang Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+Q">Qiwei Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+R">Raphael Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+L">Li Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+W">Weijia Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+G">Guimin Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yuan%2C+Y">Yifei Yuan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=S%C3%B8gaard%2C+A">Anders Søgaard</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hershcovich%2C+D">Daniel Hershcovich</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.11030v2-abstract-short" style="display: inline;">
        Food is a rich and varied dimension of cultural heritage, crucial to both individuals and social groups. To bridge the gap in the literature on the often-overlooked regional diversity in this domain, we introduce FoodieQA, a manually curated, fine-grained image-text dataset capturing the intricate features of food cultures across various regions in China. We evaluate vision-language Models (VLMs)&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.11030v2-abstract-full').style.display = 'inline'; document.getElementById('2406.11030v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.11030v2-abstract-full" style="display: none;">
        Food is a rich and varied dimension of cultural heritage, crucial to both individuals and social groups. To bridge the gap in the literature on the often-overlooked regional diversity in this domain, we introduce FoodieQA, a manually curated, fine-grained image-text dataset capturing the intricate features of food cultures across various regions in China. We evaluate vision-language Models (VLMs) and large language models (LLMs) on newly collected, unseen food images and corresponding questions. FoodieQA comprises three multiple-choice question-answering tasks where models need to answer questions based on multiple images, a single image, and text-only descriptions, respectively. While LLMs excel at text-based question answering, surpassing human accuracy, the open-sourced VLMs still fall short by 41% on multi-image and 21% on single-image VQA tasks, although closed-weights models perform closer to human levels (within 10%). Our findings highlight that understanding food and its cultural implications remains a challenging and under-explored direction.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.11030v2-abstract-full').style.display = 'none'; document.getElementById('2406.11030v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.02265">arXiv:2406.02265</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.02265">pdf</a>, <a href="https://arxiv.org/format/2406.02265">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Understanding Retrieval Robustness for Retrieval-Augmented Image Captioning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+W">Wenyan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Jiaang Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ramos%2C+R">Rita Ramos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+R">Raphael Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.02265v3-abstract-short" style="display: inline;">
        Recent advances in retrieval-augmented models for image captioning highlight the benefit of retrieving related captions for efficient, lightweight models with strong domain-transfer capabilities. While these models demonstrate the success of retrieval augmentation, retrieval models are still far from perfect in practice: the retrieved information can sometimes mislead the model, resulting in incor&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.02265v3-abstract-full').style.display = 'inline'; document.getElementById('2406.02265v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.02265v3-abstract-full" style="display: none;">
        Recent advances in retrieval-augmented models for image captioning highlight the benefit of retrieving related captions for efficient, lightweight models with strong domain-transfer capabilities. While these models demonstrate the success of retrieval augmentation, retrieval models are still far from perfect in practice: the retrieved information can sometimes mislead the model, resulting in incorrect generation and worse performance. In this paper, we analyze the robustness of a retrieval-augmented captioning model SmallCap. Our analysis shows that the model is sensitive to tokens that appear in the majority of the retrieved captions, and the input attribution shows that those tokens are likely copied into the generated output. Given these findings, we propose to train the model by sampling retrieved captions from more diverse sets. This decreases the chance that the model learns to copy majority tokens, and improves both in-domain and cross-domain performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.02265v3-abstract-full').style.display = 'none'; document.getElementById('2406.02265v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 August, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 4 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, long paper at ACL 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2404.12013">arXiv:2404.12013</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2404.12013">pdf</a>, <a href="https://arxiv.org/format/2404.12013">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Sequential Compositional Generalization in Multimodal Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yagcioglu%2C+S">Semih Yagcioglu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=%C4%B0nce%2C+O+B">Osman Batur İnce</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Erdem%2C+A">Aykut Erdem</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Erdem%2C+E">Erkut Erdem</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yuret%2C+D">Deniz Yuret</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2404.12013v1-abstract-short" style="display: inline;">
        The rise of large-scale multimodal models has paved the pathway for groundbreaking advances in generative modeling and reasoning, unlocking transformative applications in a variety of complex tasks. However, a pressing question that remains is their genuine capability for stronger forms of generalization, which has been largely underexplored in the multimodal setting. Our study aims to address thi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.12013v1-abstract-full').style.display = 'inline'; document.getElementById('2404.12013v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2404.12013v1-abstract-full" style="display: none;">
        The rise of large-scale multimodal models has paved the pathway for groundbreaking advances in generative modeling and reasoning, unlocking transformative applications in a variety of complex tasks. However, a pressing question that remains is their genuine capability for stronger forms of generalization, which has been largely underexplored in the multimodal setting. Our study aims to address this by examining sequential compositional generalization using \textsc{CompAct} (\underline{Comp}ositional \underline{Act}ivities)\footnote{Project Page: \url{http://cyberiada.github.io/CompAct}}, a carefully constructed, perceptually grounded dataset set within a rich backdrop of egocentric kitchen activity videos. Each instance in our dataset is represented with a combination of raw video footage, naturally occurring sound, and crowd-sourced step-by-step descriptions. More importantly, our setup ensures that the individual concepts are consistently distributed across training and evaluation sets, while their compositions are novel in the evaluation set. We conduct a comprehensive assessment of several unimodal and multimodal models. Our findings reveal that bi-modal and tri-modal models exhibit a clear edge over their text-only counterparts. This highlights the importance of multimodality while charting a trajectory for future research in this domain.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2404.12013v1-abstract-full').style.display = 'none'; document.getElementById('2404.12013v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 April, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to the main conference of NAACL (2024) as a long paper</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2311.00522">arXiv:2311.00522</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2311.00522">pdf</a>, <a href="https://arxiv.org/format/2311.00522">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Text Rendering Strategies for Pixel Language Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lotz%2C+J+F">Jonas F. Lotz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Salesky%2C+E">Elizabeth Salesky</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rust%2C+P">Phillip Rust</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2311.00522v1-abstract-short" style="display: inline;">
        Pixel-based language models process text rendered as images, which allows them to handle any script, making them a promising approach to open vocabulary language modelling. However, recent approaches use text renderers that produce a large set of almost-equivalent input patches, which may prove sub-optimal for downstream tasks, due to redundancy in the input representations. In this paper, we inve&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2311.00522v1-abstract-full').style.display = 'inline'; document.getElementById('2311.00522v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2311.00522v1-abstract-full" style="display: none;">
        Pixel-based language models process text rendered as images, which allows them to handle any script, making them a promising approach to open vocabulary language modelling. However, recent approaches use text renderers that produce a large set of almost-equivalent input patches, which may prove sub-optimal for downstream tasks, due to redundancy in the input representations. In this paper, we investigate four approaches to rendering text in the PIXEL model (Rust et al., 2023), and find that simple character bigram rendering brings improved performance on sentence-level tasks without compromising performance on token-level or multilingual tasks. This new rendering strategy also makes it possible to train a more compact model with only 22M parameters that performs on par with the original 86M parameter model. Our analyses show that character bigram rendering leads to a consistently better model but with an anisotropic patch embedding space, driven by a patch frequency bias, highlighting the connections between image patch- and tokenization-based language models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2311.00522v1-abstract-full').style.display = 'none'; document.getElementById('2311.00522v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 November, 2023; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2023.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">EMNLP 2023</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2310.18343">arXiv:2310.18343</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2310.18343">pdf</a>, <a href="https://arxiv.org/format/2310.18343">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PHD: Pixel-Based Language Modeling of Historical Documents
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Borenstein%2C+N">Nadav Borenstein</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rust%2C+P">Phillip Rust</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Augenstein%2C+I">Isabelle Augenstein</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2310.18343v2-abstract-short" style="display: inline;">
        The digitisation of historical documents has provided historians with unprecedented research opportunities. Yet, the conventional approach to analysing historical documents involves converting them from images to text using OCR, a process that overlooks the potential benefits of treating them as images and introduces high levels of noise. To bridge this gap, we take advantage of recent advancement&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2310.18343v2-abstract-full').style.display = 'inline'; document.getElementById('2310.18343v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2310.18343v2-abstract-full" style="display: none;">
        The digitisation of historical documents has provided historians with unprecedented research opportunities. Yet, the conventional approach to analysing historical documents involves converting them from images to text using OCR, a process that overlooks the potential benefits of treating them as images and introduces high levels of noise. To bridge this gap, we take advantage of recent advancements in pixel-based language models trained to reconstruct masked patches of pixels instead of predicting token distributions. Due to the scarcity of real historical scans, we propose a novel method for generating synthetic scans to resemble real historical documents. We then pre-train our model, PHD, on a combination of synthetic scans and real historical newspapers from the 1700-1900 period. Through our experiments, we demonstrate that PHD exhibits high proficiency in reconstructing masked image patches and provide evidence of our model&#39;s noteworthy language understanding capabilities. Notably, we successfully apply our model to a historical QA task, highlighting its usefulness in this domain.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2310.18343v2-abstract-full').style.display = 'none'; document.getElementById('2310.18343v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 November, 2023; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 22 October, 2023;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2023.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to the main conference of EMNLP 2023</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2310.17530">arXiv:2310.17530</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2310.17530">pdf</a>, <a href="https://arxiv.org/format/2310.17530">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Evaluating Bias and Fairness in Gender-Neutral Pretrained Vision-and-Language Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cabello%2C+L">Laura Cabello</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bugliarello%2C+E">Emanuele Bugliarello</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brandl%2C+S">Stephanie Brandl</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2310.17530v1-abstract-short" style="display: inline;">
        Pretrained machine learning models are known to perpetuate and even amplify existing biases in data, which can result in unfair outcomes that ultimately impact user experience. Therefore, it is crucial to understand the mechanisms behind those prejudicial biases to ensure that model performance does not result in discriminatory behaviour toward certain groups or populations. In this work, we defin&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2310.17530v1-abstract-full').style.display = 'inline'; document.getElementById('2310.17530v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2310.17530v1-abstract-full" style="display: none;">
        Pretrained machine learning models are known to perpetuate and even amplify existing biases in data, which can result in unfair outcomes that ultimately impact user experience. Therefore, it is crucial to understand the mechanisms behind those prejudicial biases to ensure that model performance does not result in discriminatory behaviour toward certain groups or populations. In this work, we define gender bias as our case study. We quantify bias amplification in pretraining and after fine-tuning on three families of vision-and-language models. We investigate the connection, if any, between the two learning stages, and evaluate how bias amplification reflects on model performance. Overall, we find that bias amplification in pretraining and after fine-tuning are independent. We then examine the effect of continued pretraining on gender-neutral data, finding that this reduces group disparities, i.e., promotes fairness, on VQAv2 and retrieval tasks without significantly compromising task performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2310.17530v1-abstract-full').style.display = 'none'; document.getElementById('2310.17530v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 October, 2023; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2023.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To appear in EMNLP 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2305.19821">arXiv:2305.19821</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2305.19821">pdf</a>, <a href="https://arxiv.org/format/2305.19821">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LMCap: Few-shot Multilingual Image Captioning by Retrieval Augmented Language Model Prompting
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ramos%2C+R">Rita Ramos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Martins%2C+B">Bruno Martins</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2305.19821v1-abstract-short" style="display: inline;">
        Multilingual image captioning has recently been tackled by training with large-scale machine translated data, which is an expensive, noisy, and time-consuming process. Without requiring any multilingual caption data, we propose LMCap, an image-blind few-shot multilingual captioning model that works by prompting a language model with retrieved captions. Specifically, instead of following the standa&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2305.19821v1-abstract-full').style.display = 'inline'; document.getElementById('2305.19821v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2305.19821v1-abstract-full" style="display: none;">
        Multilingual image captioning has recently been tackled by training with large-scale machine translated data, which is an expensive, noisy, and time-consuming process. Without requiring any multilingual caption data, we propose LMCap, an image-blind few-shot multilingual captioning model that works by prompting a language model with retrieved captions. Specifically, instead of following the standard encoder-decoder paradigm, given an image, LMCap first retrieves the captions of similar images using a multilingual CLIP encoder. These captions are then combined into a prompt for an XGLM decoder, in order to generate captions in the desired language. In other words, the generation model does not directly process the image, instead processing retrieved captions. Experiments on the XM3600 dataset of geographically diverse images show that our model is competitive with fully-supervised multilingual captioning models, without requiring any supervised training on any captioning data.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2305.19821v1-abstract-full').style.display = 'none'; document.getElementById('2305.19821v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 May, 2023; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2023.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To appear in the Findings of ACL 2023</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2305.03610">arXiv:2305.03610</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2305.03610">pdf</a>, <a href="https://arxiv.org/format/2305.03610">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Role of Data Curation in Image Captioning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+W">Wenyan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lotz%2C+J+F">Jonas F. Lotz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qiu%2C+C">Chen Qiu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2305.03610v2-abstract-short" style="display: inline;">
        Image captioning models are typically trained by treating all samples equally, neglecting to account for mismatched or otherwise difficult data points. In contrast, recent work has shown the effectiveness of training models by scheduling the data using curriculum learning strategies. This paper contributes to this direction by actively curating difficult samples in datasets without increasing the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2305.03610v2-abstract-full').style.display = 'inline'; document.getElementById('2305.03610v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2305.03610v2-abstract-full" style="display: none;">
        Image captioning models are typically trained by treating all samples equally, neglecting to account for mismatched or otherwise difficult data points. In contrast, recent work has shown the effectiveness of training models by scheduling the data using curriculum learning strategies. This paper contributes to this direction by actively curating difficult samples in datasets without increasing the total number of samples. We explore the effect of using three data curation methods within the training process: complete removal of an sample, caption replacement, or image replacement via a text-to-image generation model. Experiments on the Flickr30K and COCO datasets with the BLIP and BEiT-3 models demonstrate that these curation methods do indeed yield improved image captioning models, underscoring their efficacy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2305.03610v2-abstract-full').style.display = 'none'; document.getElementById('2305.03610v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 February, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 May, 2023;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2023.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2302.08268">arXiv:2302.08268</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2302.08268">pdf</a>, <a href="https://arxiv.org/format/2302.08268">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Retrieval-augmented Image Captioning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ramos%2C+R">Rita Ramos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Martins%2C+B">Bruno Martins</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2302.08268v1-abstract-short" style="display: inline;">
        Inspired by retrieval-augmented language generation and pretrained Vision and Language (V&amp;L) encoders, we present a new approach to image captioning that generates sentences given the input image and a set of captions retrieved from a datastore, as opposed to the image alone. The encoder in our model jointly processes the image and retrieved captions using a pretrained V&amp;L BERT, while the decoder&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2302.08268v1-abstract-full').style.display = 'inline'; document.getElementById('2302.08268v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2302.08268v1-abstract-full" style="display: none;">
        Inspired by retrieval-augmented language generation and pretrained Vision and Language (V&amp;L) encoders, we present a new approach to image captioning that generates sentences given the input image and a set of captions retrieved from a datastore, as opposed to the image alone. The encoder in our model jointly processes the image and retrieved captions using a pretrained V&amp;L BERT, while the decoder attends to the multimodal encoder representations, benefiting from the extra textual evidence from the retrieved captions. Experimental results on the COCO dataset show that image captioning can be effectively formulated from this new perspective. Our model, named EXTRA, benefits from using captions retrieved from the training dataset, and it can also benefit from using an external dataset without the need for retraining. Ablation studies show that retrieving a sufficient number of captions (e.g., k=5) can improve captioning quality. Our work contributes towards using pretrained V&amp;L encoders for generative tasks, instead of standard classification tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2302.08268v1-abstract-full').style.display = 'none'; document.getElementById('2302.08268v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 February, 2023; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2023.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        EACL 2023
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2210.13134">arXiv:2210.13134</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2210.13134">pdf</a>, <a href="https://arxiv.org/format/2210.13134">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multilingual Multimodal Learning with Machine Translated Text
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Qiu%2C+C">Chen Qiu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Oneata%2C+D">Dan Oneata</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bugliarello%2C+E">Emanuele Bugliarello</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Frank%2C+S">Stella Frank</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2210.13134v1-abstract-short" style="display: inline;">
        Most vision-and-language pretraining research focuses on English tasks. However, the creation of multilingual multimodal evaluation datasets (e.g. Multi30K, xGQA, XVNLI, and MaRVL) poses a new challenge in finding high-quality training data that is both multilingual and multimodal. In this paper, we investigate whether machine translating English multimodal data can be an effective proxy for the l&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2210.13134v1-abstract-full').style.display = 'inline'; document.getElementById('2210.13134v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2210.13134v1-abstract-full" style="display: none;">
        Most vision-and-language pretraining research focuses on English tasks. However, the creation of multilingual multimodal evaluation datasets (e.g. Multi30K, xGQA, XVNLI, and MaRVL) poses a new challenge in finding high-quality training data that is both multilingual and multimodal. In this paper, we investigate whether machine translating English multimodal data can be an effective proxy for the lack of readily available multilingual data. We call this framework TD-MML: Translated Data for Multilingual Multimodal Learning, and it can be applied to any multimodal dataset and model. We apply it to both pretraining and fine-tuning data with a state-of-the-art model. In order to prevent models from learning from low-quality translated text, we propose two metrics for automatically removing such translations from the resulting datasets. In experiments on five tasks across 20 languages in the IGLUE benchmark, we show that translated data can provide a useful signal for multilingual multimodal learning, both at pretraining and fine-tuning.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2210.13134v1-abstract-full').style.display = 'none'; document.getElementById('2210.13134v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 October, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">EMNLP 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2210.05529">arXiv:2210.05529</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2210.05529">pdf</a>, <a href="https://arxiv.org/format/2210.05529">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An Exploration of Hierarchical Attention Transformers for Efficient Long Document Classification
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chalkidis%2C+I">Ilias Chalkidis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+X">Xiang Dai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fergadiotis%2C+M">Manos Fergadiotis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Malakasiotis%2C+P">Prodromos Malakasiotis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2210.05529v1-abstract-short" style="display: inline;">
        Non-hierarchical sparse attention Transformer-based models, such as Longformer and Big Bird, are popular approaches to working with long documents. There are clear benefits to these approaches compared to the original Transformer in terms of efficiency, but Hierarchical Attention Transformer (HAT) models are a vastly understudied alternative. We develop and release fully pre-trained HAT models tha&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2210.05529v1-abstract-full').style.display = 'inline'; document.getElementById('2210.05529v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2210.05529v1-abstract-full" style="display: none;">
        Non-hierarchical sparse attention Transformer-based models, such as Longformer and Big Bird, are popular approaches to working with long documents. There are clear benefits to these approaches compared to the original Transformer in terms of efficiency, but Hierarchical Attention Transformer (HAT) models are a vastly understudied alternative. We develop and release fully pre-trained HAT models that use segment-wise followed by cross-segment encoders and compare them with Longformer models and partially pre-trained HATs. In several long document downstream classification tasks, our best HAT model outperforms equally-sized Longformer models while using 10-20% less GPU memory and processing documents 40-45% faster. In a series of ablation studies, we find that HATs perform best with cross-segment contextualization throughout the model than alternative configurations that implement either early or late cross-segment contextualization. Our code is on GitHub: https://github.com/coastalcph/hierarchical-transformers.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2210.05529v1-abstract-full').style.display = 'none'; document.getElementById('2210.05529v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 October, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2209.15323">arXiv:2209.15323</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2209.15323">pdf</a>, <a href="https://arxiv.org/format/2209.15323">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SmallCap: Lightweight Image Captioning Prompted with Retrieval Augmentation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ramos%2C+R">Rita Ramos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Martins%2C+B">Bruno Martins</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kementchedjhieva%2C+Y">Yova Kementchedjhieva</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2209.15323v2-abstract-short" style="display: inline;">
        Recent advances in image captioning have focused on scaling the data and model size, substantially increasing the cost of pre-training and finetuning. As an alternative to large models, we present SmallCap, which generates a caption conditioned on an input image and related captions retrieved from a datastore. Our model is lightweight and fast to train, as the only learned parameters are in newly&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2209.15323v2-abstract-full').style.display = 'inline'; document.getElementById('2209.15323v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2209.15323v2-abstract-full" style="display: none;">
        Recent advances in image captioning have focused on scaling the data and model size, substantially increasing the cost of pre-training and finetuning. As an alternative to large models, we present SmallCap, which generates a caption conditioned on an input image and related captions retrieved from a datastore. Our model is lightweight and fast to train, as the only learned parameters are in newly introduced cross-attention layers between a pre-trained CLIP encoder and GPT-2 decoder. SmallCap can transfer to new domains without additional finetuning and can exploit large-scale data in a training-free fashion since the contents of the datastore can be readily replaced. Our experiments show that SmallCap, trained only on COCO, has competitive performance on this benchmark, and also transfers to other domains without retraining, solely through retrieval from target-domain data. Further improvement is achieved through the training-free exploitation of diverse human-labeled and web data, which proves to be effective for a range of domains, including the nocaps benchmark, designed to test generalization to unseen visual concepts.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2209.15323v2-abstract-full').style.display = 'none'; document.getElementById('2209.15323v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 March, 2023; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 30 September, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to CVPR 2023</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2207.06991">arXiv:2207.06991</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2207.06991">pdf</a>, <a href="https://arxiv.org/format/2207.06991">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Language Modelling with Pixels
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Rust%2C+P">Phillip Rust</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lotz%2C+J+F">Jonas F. Lotz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bugliarello%2C+E">Emanuele Bugliarello</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Salesky%2C+E">Elizabeth Salesky</a>, 
      
      <a href="/search/?searchtype=author&amp;query=de+Lhoneux%2C+M">Miryam de Lhoneux</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2207.06991v2-abstract-short" style="display: inline;">
        Language models are defined over a finite set of inputs, which creates a vocabulary bottleneck when we attempt to scale the number of supported languages. Tackling this bottleneck results in a trade-off between what can be represented in the embedding matrix and computational issues in the output layer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which suffers from neither of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2207.06991v2-abstract-full').style.display = 'inline'; document.getElementById('2207.06991v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2207.06991v2-abstract-full" style="display: none;">
        Language models are defined over a finite set of inputs, which creates a vocabulary bottleneck when we attempt to scale the number of supported languages. Tackling this bottleneck results in a trade-off between what can be represented in the embedding matrix and computational issues in the output layer. This paper introduces PIXEL, the Pixel-based Encoder of Language, which suffers from neither of these issues. PIXEL is a pretrained language model that renders text as images, making it possible to transfer representations across languages based on orthographic similarity or the co-activation of pixels. PIXEL is trained to reconstruct the pixels of masked patches instead of predicting a distribution over tokens. We pretrain the 86M parameter PIXEL model on the same English data as BERT and evaluate on syntactic and semantic tasks in typologically diverse languages, including various non-Latin scripts. We find that PIXEL substantially outperforms BERT on syntactic and semantic processing tasks on scripts that are not found in the pretraining data, but PIXEL is slightly weaker than BERT when working with Latin scripts. Furthermore, we find that PIXEL is more robust than BERT to orthographic attacks and linguistic code-switching, further confirming the benefits of modelling language with pixels.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2207.06991v2-abstract-full').style.display = 'none'; document.getElementById('2207.06991v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 April, 2023; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 14 July, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ICLR 2023</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2204.06683">arXiv:2204.06683</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2204.06683">pdf</a>, <a href="https://arxiv.org/format/2204.06683">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Revisiting Transformer-based Models for Long Document Classification
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+X">Xiang Dai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chalkidis%2C+I">Ilias Chalkidis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Darkner%2C+S">Sune Darkner</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2204.06683v2-abstract-short" style="display: inline;">
        The recent literature in text classification is biased towards short text sequences (e.g., sentences or paragraphs). In real-world applications, multi-page multi-paragraph documents are common and they cannot be efficiently encoded by vanilla Transformer-based models. We compare different Transformer-based Long Document Classification (TrLDC) approaches that aim to mitigate the computational overh&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.06683v2-abstract-full').style.display = 'inline'; document.getElementById('2204.06683v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2204.06683v2-abstract-full" style="display: none;">
        The recent literature in text classification is biased towards short text sequences (e.g., sentences or paragraphs). In real-world applications, multi-page multi-paragraph documents are common and they cannot be efficiently encoded by vanilla Transformer-based models. We compare different Transformer-based Long Document Classification (TrLDC) approaches that aim to mitigate the computational overhead of vanilla transformers to encode much longer text, namely sparse attention and hierarchical encoding methods. We examine several aspects of sparse attention (e.g., size of local attention window, use of global attention) and hierarchical (e.g., document splitting strategy) transformers on four document classification datasets covering different domains. We observe a clear benefit from being able to process longer text, and, based on our results, we derive practical advice of applying Transformer-based models on long document classification tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2204.06683v2-abstract-full').style.display = 'none'; document.getElementById('2204.06683v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 October, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 April, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Findings of EMNLP 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2201.11732">arXiv:2201.11732</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2201.11732">pdf</a>, <a href="https://arxiv.org/format/2201.11732">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        IGLUE: A Benchmark for Transfer Learning across Modalities, Tasks, and Languages
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bugliarello%2C+E">Emanuele Bugliarello</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+F">Fangyu Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pfeiffer%2C+J">Jonas Pfeiffer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Reddy%2C+S">Siva Reddy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ponti%2C+E+M">Edoardo Maria Ponti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vuli%C4%87%2C+I">Ivan Vulić</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2201.11732v2-abstract-short" style="display: inline;">
        Reliable evaluation benchmarks designed for replicability and comprehensiveness have driven progress in machine learning. Due to the lack of a multilingual benchmark, however, vision-and-language research has mostly focused on English language tasks. To fill this gap, we introduce the Image-Grounded Language Understanding Evaluation benchmark. IGLUE brings together - by both aggregating pre-existi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.11732v2-abstract-full').style.display = 'inline'; document.getElementById('2201.11732v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2201.11732v2-abstract-full" style="display: none;">
        Reliable evaluation benchmarks designed for replicability and comprehensiveness have driven progress in machine learning. Due to the lack of a multilingual benchmark, however, vision-and-language research has mostly focused on English language tasks. To fill this gap, we introduce the Image-Grounded Language Understanding Evaluation benchmark. IGLUE brings together - by both aggregating pre-existing datasets and creating new ones - visual question answering, cross-modal retrieval, grounded reasoning, and grounded entailment tasks across 20 diverse languages. Our benchmark enables the evaluation of multilingual multimodal models for transfer learning, not only in a zero-shot setting, but also in newly defined few-shot learning setups. Based on the evaluation of the available state-of-the-art models, we find that translate-test transfer is superior to zero-shot transfer and that few-shot learning is hard to harness for many tasks. Moreover, downstream performance is partially explained by the amount of available unlabelled textual data for pretraining, and only weakly by the typological distance of target-source languages. We hope to encourage future research efforts in this area by releasing the benchmark to the community.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.11732v2-abstract-full').style.display = 'none'; document.getElementById('2201.11732v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 July, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 27 January, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ICML 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2109.13238">arXiv:2109.13238</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2109.13238">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Visually Grounded Reasoning across Languages and Cultures
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+F">Fangyu Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bugliarello%2C+E">Emanuele Bugliarello</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ponti%2C+E+M">Edoardo Maria Ponti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Reddy%2C+S">Siva Reddy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Collier%2C+N">Nigel Collier</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2109.13238v2-abstract-short" style="display: inline;">
        The design of widespread vision-and-language datasets and pre-trained encoders directly adopts, or draws inspiration from, the concepts and images of ImageNet. While one can hardly overestimate how much this benchmark contributed to progress in computer vision, it is mostly derived from lexical databases and image queries in English, resulting in source material with a North American or Western Eu&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.13238v2-abstract-full').style.display = 'inline'; document.getElementById('2109.13238v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2109.13238v2-abstract-full" style="display: none;">
        The design of widespread vision-and-language datasets and pre-trained encoders directly adopts, or draws inspiration from, the concepts and images of ImageNet. While one can hardly overestimate how much this benchmark contributed to progress in computer vision, it is mostly derived from lexical databases and image queries in English, resulting in source material with a North American or Western European bias. Therefore, we devise a new protocol to construct an ImageNet-style hierarchy representative of more languages and cultures. In particular, we let the selection of both concepts and images be entirely driven by native speakers, rather than scraping them automatically. Specifically, we focus on a typologically diverse set of languages, namely, Indonesian, Mandarin Chinese, Swahili, Tamil, and Turkish. On top of the concepts and images obtained through this new protocol, we create a multilingual dataset for {M}ulticultur{a}l {R}easoning over {V}ision and {L}anguage (MaRVL) by eliciting statements from native speaker annotators about pairs of images. The task consists of discriminating whether each grounded statement is true or false. We establish a series of baselines using state-of-the-art models and find that their cross-lingual transfer performance lags dramatically behind supervised performance in English. These results invite us to reassess the robustness and accuracy of current state-of-the-art models beyond a narrow domain, but also open up new exciting challenges for the development of truly multilingual and multicultural systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.13238v2-abstract-full').style.display = 'none'; document.getElementById('2109.13238v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 October, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 28 September, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">EMNLP 2021; Fangyu and Emanuele contributed equally; MaRVL website: https://marvl-challenge.github.io</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2109.06605">arXiv:2109.06605</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2109.06605">pdf</a>, <a href="https://arxiv.org/format/2109.06605">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MDAPT: Multilingual Domain Adaptive Pretraining in a Single Model
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=J%C3%B8rgensen%2C+R+K">Rasmus Kær Jørgensen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hartmann%2C+M">Mareike Hartmann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+X">Xiang Dai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2109.06605v1-abstract-short" style="display: inline;">
        Domain adaptive pretraining, i.e. the continued unsupervised pretraining of a language model on domain-specific text, improves the modelling of text for downstream tasks within the domain. Numerous real-world applications are based on domain-specific text, e.g. working with financial or biomedical documents, and these applications often need to support multiple languages. However, large-scale doma&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.06605v1-abstract-full').style.display = 'inline'; document.getElementById('2109.06605v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2109.06605v1-abstract-full" style="display: none;">
        Domain adaptive pretraining, i.e. the continued unsupervised pretraining of a language model on domain-specific text, improves the modelling of text for downstream tasks within the domain. Numerous real-world applications are based on domain-specific text, e.g. working with financial or biomedical documents, and these applications often need to support multiple languages. However, large-scale domain-specific multilingual pretraining data for such scenarios can be difficult to obtain, due to regulations, legislation, or simply a lack of language- and domain-specific text. One solution is to train a single multilingual model, taking advantage of the data available in as many languages as possible. In this work, we explore the benefits of domain adaptive pretraining with a focus on adapting to multiple languages within a specific domain. We propose different techniques to compose pretraining corpora that enable a language model to both become domain-specific and multilingual. Evaluation on nine domain-specific datasets-for biomedical named entity recognition and financial sentence classification-covering seven different languages show that a single multilingual domain-specific model can outperform the general multilingual model, and performs close to its monolingual counterpart. This finding holds across two different pretraining methods, adapter-based pretraining and full model pretraining.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.06605v1-abstract-full').style.display = 'none'; document.getElementById('2109.06605v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 September, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Findings of EMNLP 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2109.04448">arXiv:2109.04448</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2109.04448">pdf</a>, <a href="https://arxiv.org/format/2109.04448">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Vision-and-Language or Vision-for-Language? On Cross-Modal Influence in Multimodal Transformers
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Frank%2C+S">Stella Frank</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bugliarello%2C+E">Emanuele Bugliarello</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2109.04448v1-abstract-short" style="display: inline;">
        Pretrained vision-and-language BERTs aim to learn representations that combine information from both modalities. We propose a diagnostic method based on cross-modal input ablation to assess the extent to which these models actually integrate cross-modal information. This method involves ablating inputs from one modality, either entirely or selectively based on cross-modal grounding alignments, and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.04448v1-abstract-full').style.display = 'inline'; document.getElementById('2109.04448v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2109.04448v1-abstract-full" style="display: none;">
        Pretrained vision-and-language BERTs aim to learn representations that combine information from both modalities. We propose a diagnostic method based on cross-modal input ablation to assess the extent to which these models actually integrate cross-modal information. This method involves ablating inputs from one modality, either entirely or selectively based on cross-modal grounding alignments, and evaluating the model prediction performance on the other modality. Model performance is measured by modality-specific tasks that mirror the model pretraining objectives (e.g. masked language modelling for text). Models that have learned to construct cross-modal representations using both modalities are expected to perform worse when inputs are missing from a modality. We find that recently proposed models have much greater relative difficulty predicting text when visual information is ablated, compared to predicting visual object categories when text is ablated, indicating that these models are not symmetrically cross-modal.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.04448v1-abstract-full').style.display = 'none'; document.getElementById('2109.04448v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 September, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">EMNLP 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.11911">arXiv:2101.11911</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.11911">pdf</a>, <a href="https://arxiv.org/format/2101.11911">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Role of Syntactic Planning in Compositional Image Captioning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bugliarello%2C+E">Emanuele Bugliarello</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.11911v1-abstract-short" style="display: inline;">
        Image captioning has focused on generalizing to images drawn from the same distribution as the training set, and not to the more challenging problem of generalizing to different distributions of images. Recently, Nikolaus et al. (2019) introduced a dataset to assess compositional generalization in image captioning, where models are evaluated on their ability to describe images with unseen adjectiv&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.11911v1-abstract-full').style.display = 'inline'; document.getElementById('2101.11911v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.11911v1-abstract-full" style="display: none;">
        Image captioning has focused on generalizing to images drawn from the same distribution as the training set, and not to the more challenging problem of generalizing to different distributions of images. Recently, Nikolaus et al. (2019) introduced a dataset to assess compositional generalization in image captioning, where models are evaluated on their ability to describe images with unseen adjective-noun and noun-verb compositions. In this work, we investigate different methods to improve compositional generalization by planning the syntactic structure of a caption. Our experiments show that jointly modeling tokens and syntactic tags enhances generalization in both RNN- and Transformer-based models, while also improving performance on standard metrics.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.11911v1-abstract-full').style.display = 'none'; document.getElementById('2101.11911v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 January, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at EACL 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2011.15124">arXiv:2011.15124</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2011.15124">pdf</a>, <a href="https://arxiv.org/format/2011.15124">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multimodal Pretraining Unmasked: A Meta-Analysis and a Unified Framework of Vision-and-Language BERTs
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bugliarello%2C+E">Emanuele Bugliarello</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cotterell%2C+R">Ryan Cotterell</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Okazaki%2C+N">Naoaki Okazaki</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2011.15124v2-abstract-short" style="display: inline;">
        Large-scale pretraining and task-specific fine-tuning is now the standard methodology for many tasks in computer vision and natural language processing. Recently, a multitude of methods have been proposed for pretraining vision and language BERTs to tackle challenges at the intersection of these two key areas of AI. These models can be categorised into either single-stream or dual-stream encoders.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.15124v2-abstract-full').style.display = 'inline'; document.getElementById('2011.15124v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2011.15124v2-abstract-full" style="display: none;">
        Large-scale pretraining and task-specific fine-tuning is now the standard methodology for many tasks in computer vision and natural language processing. Recently, a multitude of methods have been proposed for pretraining vision and language BERTs to tackle challenges at the intersection of these two key areas of AI. These models can be categorised into either single-stream or dual-stream encoders. We study the differences between these two categories, and show how they can be unified under a single theoretical framework. We then conduct controlled experiments to discern the empirical differences between five V&amp;L BERTs. Our experiments show that training data and hyperparameters are responsible for most of the differences between the reported results, but they also reveal that the embedding layer plays a crucial role in these massive models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.15124v2-abstract-full').style.display = 'none'; document.getElementById('2011.15124v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 May, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 30 November, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To appear in TACL 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2010.08642">arXiv:2010.08642</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2010.08642">pdf</a>, <a href="https://arxiv.org/format/2010.08642">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multimodal Speech Recognition with Unstructured Audio Masking
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Srinivasan%2C+T">Tejas Srinivasan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sanabria%2C+R">Ramon Sanabria</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Metze%2C+F">Florian Metze</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2010.08642v1-abstract-short" style="display: inline;">
        Visual context has been shown to be useful for automatic speech recognition (ASR) systems when the speech signal is noisy or corrupted. Previous work, however, has only demonstrated the utility of visual context in an unrealistic setting, where a fixed set of words are systematically masked in the audio. In this paper, we simulate a more realistic masking scenario during model training, called Ran&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.08642v1-abstract-full').style.display = 'inline'; document.getElementById('2010.08642v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2010.08642v1-abstract-full" style="display: none;">
        Visual context has been shown to be useful for automatic speech recognition (ASR) systems when the speech signal is noisy or corrupted. Previous work, however, has only demonstrated the utility of visual context in an unrealistic setting, where a fixed set of words are systematically masked in the audio. In this paper, we simulate a more realistic masking scenario during model training, called RandWordMask, where the masking can occur for any word segment. Our experiments on the Flickr 8K Audio Captions Corpus show that multimodal ASR can generalize to recover different types of masked words in this unstructured masking setting. Moreover, our analysis shows that our models are capable of attending to the visual signal when the audio signal is corrupted. These results show that multimodal ASR systems can leverage the visual signal in more generalized noisy scenarios.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.08642v1-abstract-full').style.display = 'none'; document.getElementById('2010.08642v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 October, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to NLP Beyond Text workshop, EMNLP 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2010.02806">arXiv:2010.02806</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2010.02806">pdf</a>, <a href="https://arxiv.org/format/2010.02806">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Textual Supervision for Visually Grounded Spoken Language Understanding
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Higy%2C+B">Bertrand Higy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chrupa%C5%82a%2C+G">Grzegorz Chrupała</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2010.02806v2-abstract-short" style="display: inline;">
        Visually-grounded models of spoken language understanding extract semantic information directly from speech, without relying on transcriptions. This is useful for low-resource languages, where transcriptions can be expensive or impossible to obtain. Recent work showed that these models can be improved if transcriptions are available at training time. However, it is not clear how an end-to-end appr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.02806v2-abstract-full').style.display = 'inline'; document.getElementById('2010.02806v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2010.02806v2-abstract-full" style="display: none;">
        Visually-grounded models of spoken language understanding extract semantic information directly from speech, without relying on transcriptions. This is useful for low-resource languages, where transcriptions can be expensive or impossible to obtain. Recent work showed that these models can be improved if transcriptions are available at training time. However, it is not clear how an end-to-end approach compares to a traditional pipeline-based approach when one has access to transcriptions. Comparing different strategies, we find that the pipeline approach works better when enough text is available. With low-resource languages in mind, we also show that translations can be effectively used in place of transcriptions but more data is needed to obtain similar results.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.02806v2-abstract-full').style.display = 'none'; document.getElementById('2010.02806v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 October, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 October, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Findings of EMNLP 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2010.02384">arXiv:2010.02384</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2010.02384">pdf</a>, <a href="https://arxiv.org/format/2010.02384">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Fine-Grained Grounding for Multimodal Speech Recognition
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Srinivasan%2C+T">Tejas Srinivasan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sanabria%2C+R">Ramon Sanabria</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Metze%2C+F">Florian Metze</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2010.02384v1-abstract-short" style="display: inline;">
        Multimodal automatic speech recognition systems integrate information from images to improve speech recognition quality, by grounding the speech in the visual context. While visual signals have been shown to be useful for recovering entities that have been masked in the audio, these models should be capable of recovering a broader range of word types. Existing systems rely on global visual feature&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.02384v1-abstract-full').style.display = 'inline'; document.getElementById('2010.02384v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2010.02384v1-abstract-full" style="display: none;">
        Multimodal automatic speech recognition systems integrate information from images to improve speech recognition quality, by grounding the speech in the visual context. While visual signals have been shown to be useful for recovering entities that have been masked in the audio, these models should be capable of recovering a broader range of word types. Existing systems rely on global visual features that represent the entire image, but localizing the relevant regions of the image will make it possible to recover a larger set of words, such as adjectives and verbs. In this paper, we propose a model that uses finer-grained visual information from different parts of the image, using automatic object proposals. In experiments on the Flickr8K Audio Captions Corpus, we find that our model improves over approaches that use global visual features, that the proposals enable the model to recover entities and other related words, such as adjectives, and that improvements are due to the model&#39;s ability to localize the correct proposals.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.02384v1-abstract-full').style.display = 'none'; document.getElementById('2010.02384v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 October, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to Findings of EMNLP 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2006.02174">arXiv:2006.02174</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2006.02174">pdf</a>, <a href="https://arxiv.org/format/2006.02174">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CompGuessWhat?!: A Multi-task Evaluation Framework for Grounded Language Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Suglia%2C+A">Alessandro Suglia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Konstas%2C+I">Ioannis Konstas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vanzo%2C+A">Andrea Vanzo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bastianelli%2C+E">Emanuele Bastianelli</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Frank%2C+S">Stella Frank</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lemon%2C+O">Oliver Lemon</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2006.02174v1-abstract-short" style="display: inline;">
        Approaches to Grounded Language Learning typically focus on a single task-based final performance measure that may not depend on desirable properties of the learned hidden representations, such as their ability to predict salient attributes or to generalise to unseen situations. To remedy this, we present GROLLA, an evaluation framework for Grounded Language Learning with Attributes with three sub&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.02174v1-abstract-full').style.display = 'inline'; document.getElementById('2006.02174v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2006.02174v1-abstract-full" style="display: none;">
        Approaches to Grounded Language Learning typically focus on a single task-based final performance measure that may not depend on desirable properties of the learned hidden representations, such as their ability to predict salient attributes or to generalise to unseen situations. To remedy this, we present GROLLA, an evaluation framework for Grounded Language Learning with Attributes with three sub-tasks: 1) Goal-oriented evaluation; 2) Object attribute prediction evaluation; and 3) Zero-shot evaluation. We also propose a new dataset CompGuessWhat?! as an instance of this framework for evaluating the quality of learned neural representations, in particular concerning attribute grounding. To this end, we extend the original GuessWhat?! dataset by including a semantic layer on top of the perceptual one. Specifically, we enrich the VisualGenome scene graphs associated with the GuessWhat?! images with abstract and situated attributes. By using diagnostic classifiers, we show that current models learn representations that are not expressive enough to encode object attributes (average F1 of 44.27). In addition, they do not learn strategies nor representations that are robust enough to perform well when novel scenes or objects are involved in gameplay (zero-shot best accuracy 50.06%).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.02174v1-abstract-full').style.display = 'none'; document.getElementById('2006.02174v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 June, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to the Annual Conference of the Association for Computational Linguistics (ACL) 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2005.01348">arXiv:2005.01348</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2005.01348">pdf</a>, <a href="https://arxiv.org/format/2005.01348">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Sensitivity of Language Models and Humans to Winograd Schema Perturbations
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Abdou%2C+M">Mostafa Abdou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ravishankar%2C+V">Vinit Ravishankar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Barrett%2C+M">Maria Barrett</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Belinkov%2C+Y">Yonatan Belinkov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>, 
      
      <a href="/search/?searchtype=author&amp;query=S%C3%B8gaard%2C+A">Anders Søgaard</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2005.01348v2-abstract-short" style="display: inline;">
        Large-scale pretrained language models are the major driving force behind recent improvements in performance on the Winograd Schema Challenge, a widely employed test of common sense reasoning ability. We show, however, with a new diagnostic dataset, that these models are sensitive to linguistic perturbations of the Winograd examples that minimally affect human understanding. Our results highlight&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.01348v2-abstract-full').style.display = 'inline'; document.getElementById('2005.01348v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2005.01348v2-abstract-full" style="display: none;">
        Large-scale pretrained language models are the major driving force behind recent improvements in performance on the Winograd Schema Challenge, a widely employed test of common sense reasoning ability. We show, however, with a new diagnostic dataset, that these models are sensitive to linguistic perturbations of the Winograd examples that minimally affect human understanding. Our results highlight interesting differences between humans and language models: language models are more sensitive to number or gender alternations and synonym replacements than humans, and humans are more stable and consistent in their predictions, maintain a much higher absolute performance, and perform better on non-associative instances than associative ones. Overall, humans are correct more often than out-of-the-box models, and the models are sometimes right for the wrong reasons. Finally, we show that fine-tuning on a large, task-specific dataset can offer a solution to these issues.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2005.01348v2-abstract-full').style.display = 'none'; document.getElementById('2005.01348v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 May, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 4 May, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ACL 2020</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1911.12798">arXiv:1911.12798</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1911.12798">pdf</a>, <a href="https://arxiv.org/format/1911.12798">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multimodal Machine Translation through Visuals and Speech
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sulubacak%2C+U">Umut Sulubacak</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Caglayan%2C+O">Ozan Caglayan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gr%C3%B6nroos%2C+S">Stig-Arne Grönroos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rouhe%2C+A">Aku Rouhe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Specia%2C+L">Lucia Specia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tiedemann%2C+J">Jörg Tiedemann</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1911.12798v1-abstract-short" style="display: inline;">
        Multimodal machine translation involves drawing information from more than one modality, based on the assumption that the additional modalities will contain useful alternative views of the input data. The most prominent tasks in this area are spoken language translation, image-guided translation, and video-guided translation, which exploit audio and visual modalities, respectively. These tasks are&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.12798v1-abstract-full').style.display = 'inline'; document.getElementById('1911.12798v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1911.12798v1-abstract-full" style="display: none;">
        Multimodal machine translation involves drawing information from more than one modality, based on the assumption that the additional modalities will contain useful alternative views of the input data. The most prominent tasks in this area are spoken language translation, image-guided translation, and video-guided translation, which exploit audio and visual modalities, respectively. These tasks are distinguished from their monolingual counterparts of speech recognition, image captioning, and video captioning by the requirement of models to generate outputs in a different language. This survey reviews the major data resources for these tasks, the evaluation campaigns concentrated around them, the state of the art in end-to-end and pipeline approaches, and also the challenges in performance evaluation. The paper concludes with a discussion of directions for future research in these areas: the need for more expansive and challenging datasets, for targeted evaluations of model performance, and for multimodality in both the input and output space.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.12798v1-abstract-full').style.display = 'none'; document.getElementById('1911.12798v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 November, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">34 pages, 4 tables, 8 figures. Submitted (Nov 2019) to the Machine Translation journal (Springer)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1911.03678">arXiv:1911.03678</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1911.03678">pdf</a>, <a href="https://arxiv.org/format/1911.03678">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Bootstrapping Disjoint Datasets for Multilingual Multimodal Representation Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=K%C3%A1d%C3%A1r%2C+%C3%81">Ákos Kádár</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chrupa%C5%82a%2C+G">Grzegorz Chrupała</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Alishahi%2C+A">Afra Alishahi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1911.03678v1-abstract-short" style="display: inline;">
        Recent work has highlighted the advantage of jointly learning grounded sentence representations from multiple languages. However, the data used in these studies has been limited to an aligned scenario: the same images annotated with sentences in multiple languages. We focus on the more realistic disjoint scenario in which there is no overlap between the images in multilingual image--caption datase&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.03678v1-abstract-full').style.display = 'inline'; document.getElementById('1911.03678v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1911.03678v1-abstract-full" style="display: none;">
        Recent work has highlighted the advantage of jointly learning grounded sentence representations from multiple languages. However, the data used in these studies has been limited to an aligned scenario: the same images annotated with sentences in multiple languages. We focus on the more realistic disjoint scenario in which there is no overlap between the images in multilingual image--caption datasets. We confirm that training with aligned data results in better grounded sentence representations than training with disjoint data, as measured by image--sentence retrieval performance. In order to close this gap in performance, we propose a pseudopairing method to generate synthetically aligned English--German--image triplets from the disjoint sets. The method works by first training a model on the disjoint data, and then creating new triples across datasets using sentence similarity under the learned model. Experiments show that pseudopairs improve image--sentence retrieval performance compared to disjoint training, despite requiring no external data or models. However, we do find that using an external machine translation model to generate the synthetic data sets results in better performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.03678v1-abstract-full').style.display = 'none'; document.getElementById('1911.03678v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 November, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1909.04402">arXiv:1909.04402</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1909.04402">pdf</a>, <a href="https://arxiv.org/format/1909.04402">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.18653/v1/K19-1009">10.18653/v1/K19-1009 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Compositional Generalization in Image Captioning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Nikolaus%2C+M">Mitja Nikolaus</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Abdou%2C+M">Mostafa Abdou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lamm%2C+M">Matthew Lamm</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aralikatte%2C+R">Rahul Aralikatte</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1909.04402v2-abstract-short" style="display: inline;">
        Image captioning models are usually evaluated on their ability to describe a held-out set of images, not on their ability to generalize to unseen concepts. We study the problem of compositional generalization, which measures how well a model composes unseen combinations of concepts when describing images. State-of-the-art image captioning models show poor generalization performance on this task. W&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1909.04402v2-abstract-full').style.display = 'inline'; document.getElementById('1909.04402v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1909.04402v2-abstract-full" style="display: none;">
        Image captioning models are usually evaluated on their ability to describe a held-out set of images, not on their ability to generalize to unseen concepts. We study the problem of compositional generalization, which measures how well a model composes unseen combinations of concepts when describing images. State-of-the-art image captioning models show poor generalization performance on this task. We propose a multi-task model to address the poor performance, that combines caption generation and image--sentence ranking, and uses a decoding mechanism that re-ranks the captions according their similarity to the image. This model is substantially better at generalizing to unseen combinations of concepts compared to state-of-the-art captioning models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1909.04402v2-abstract-full').style.display = 'none'; document.getElementById('1909.04402v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 September, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 10 September, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To appear at CoNLL 2019, EMNLP</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Proceedings of the 23rd Conference on Computational Natural Language Learning (CoNLL), pp. 87--98, ACL, 2019
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1904.05092">arXiv:1904.05092</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1904.05092">pdf</a>, <a href="https://arxiv.org/format/1904.05092">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Cross-lingual Visual Verb Sense Disambiguation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gella%2C+S">Spandana Gella</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Keller%2C+F">Frank Keller</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1904.05092v2-abstract-short" style="display: inline;">
        Recent work has shown that visual context improves cross-lingual sense disambiguation for nouns. We extend this line of work to the more challenging task of cross-lingual verb sense disambiguation, introducing the MultiSense dataset of 9,504 images annotated with English, German, and Spanish verbs. Each image in MultiSense is annotated with an English verb and its translation in German or Spanish.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1904.05092v2-abstract-full').style.display = 'inline'; document.getElementById('1904.05092v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1904.05092v2-abstract-full" style="display: none;">
        Recent work has shown that visual context improves cross-lingual sense disambiguation for nouns. We extend this line of work to the more challenging task of cross-lingual verb sense disambiguation, introducing the MultiSense dataset of 9,504 images annotated with English, German, and Spanish verbs. Each image in MultiSense is annotated with an English verb and its translation in German or Spanish. We show that cross-lingual verb sense disambiguation models benefit from visual context, compared to unimodal baselines. We also show that the verb sense predicted by our best disambiguation model can improve the results of a text-only machine translation system when used for a multimodal translation task.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1904.05092v2-abstract-full').style.display = 'none'; document.getElementById('1904.05092v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 April, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 10 April, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">NAACL 2019; fix typo in author name</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1811.00347">arXiv:1811.00347</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1811.00347">pdf</a>, <a href="https://arxiv.org/format/1811.00347">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        How2: A Large-scale Dataset for Multimodal Language Understanding
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sanabria%2C+R">Ramon Sanabria</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Caglayan%2C+O">Ozan Caglayan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Palaskar%2C+S">Shruti Palaskar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Barrault%2C+L">Loïc Barrault</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Specia%2C+L">Lucia Specia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Metze%2C+F">Florian Metze</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1811.00347v2-abstract-short" style="display: inline;">
        In this paper, we introduce How2, a multimodal collection of instructional videos with English subtitles and crowdsourced Portuguese translations. We also present integrated sequence-to-sequence baselines for machine translation, automatic speech recognition, spoken language translation, and multimodal summarization. By making available data and code for several multimodal natural language tasks,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.00347v2-abstract-full').style.display = 'inline'; document.getElementById('1811.00347v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1811.00347v2-abstract-full" style="display: none;">
        In this paper, we introduce How2, a multimodal collection of instructional videos with English subtitles and crowdsourced Portuguese translations. We also present integrated sequence-to-sequence baselines for machine translation, automatic speech recognition, spoken language translation, and multimodal summarization. By making available data and code for several multimodal natural language tasks, we hope to stimulate more research on these and similar challenges, to obtain a deeper understanding of multimodality in language processing.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1811.00347v2-abstract-full').style.display = 'none'; document.getElementById('1811.00347v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 December, 2018; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 1 November, 2018;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2018.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1809.07615">arXiv:1809.07615</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1809.07615">pdf</a>, <a href="https://arxiv.org/format/1809.07615">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Lessons learned in multilingual grounded language learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=K%C3%A1d%C3%A1r%2C+%C3%81">Ákos Kádár</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>, 
      
      <a href="/search/?searchtype=author&amp;query=C%C3%B4t%C3%A9%2C+M">Marc-Alexandre Côté</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chrupa%C5%82a%2C+G">Grzegorz Chrupała</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Alishahi%2C+A">Afra Alishahi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1809.07615v1-abstract-short" style="display: inline;">
        Recent work has shown how to learn better visual-semantic embeddings by leveraging image descriptions in more than one language. Here, we investigate in detail which conditions affect the performance of this type of grounded language learning model. We show that multilingual training improves over bilingual training, and that low-resource languages benefit from training with higher-resource langua&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.07615v1-abstract-full').style.display = 'inline'; document.getElementById('1809.07615v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1809.07615v1-abstract-full" style="display: none;">
        Recent work has shown how to learn better visual-semantic embeddings by leveraging image descriptions in more than one language. Here, we investigate in detail which conditions affect the performance of this type of grounded language learning model. We show that multilingual training improves over bilingual training, and that low-resource languages benefit from training with higher-resource languages. We demonstrate that a multilingual model can be trained equally well on either translations or comparable sentence pairs, and that annotating the same set of images in multiple language enables further improvements via an additional caption-caption ranking objective.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1809.07615v1-abstract-full').style.display = 'none'; document.getElementById('1809.07615v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 September, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">CoNLL 2018</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1710.07177">arXiv:1710.07177</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1710.07177">pdf</a>, <a href="https://arxiv.org/format/1710.07177">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Findings of the Second Shared Task on Multimodal Machine Translation and Multilingual Image Description
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Frank%2C+S">Stella Frank</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Barrault%2C+L">Loïc Barrault</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bougares%2C+F">Fethi Bougares</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Specia%2C+L">Lucia Specia</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1710.07177v1-abstract-short" style="display: inline;">
        We present the results from the second shared task on multimodal machine translation and multilingual image description. Nine teams submitted 19 systems to two tasks. The multimodal translation task, in which the source sentence is supplemented by an image, was extended with a new language (French) and two new test sets. The multilingual image description task was changed such that at test time, o&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1710.07177v1-abstract-full').style.display = 'inline'; document.getElementById('1710.07177v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1710.07177v1-abstract-full" style="display: none;">
        We present the results from the second shared task on multimodal machine translation and multilingual image description. Nine teams submitted 19 systems to two tasks. The multimodal translation task, in which the source sentence is supplemented by an image, was extended with a new language (French) and two new test sets. The multilingual image description task was changed such that at test time, only the image is given. Compared to last year, multimodal systems improved, but text-only systems remain competitive.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1710.07177v1-abstract-full').style.display = 'none'; document.getElementById('1710.07177v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 October, 2017; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2017.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Proceedings of the Second Conference on Machine Translation, 2017, pp. 215--233
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1707.01736">arXiv:1707.01736</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1707.01736">pdf</a>, <a href="https://arxiv.org/format/1707.01736">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Cross-linguistic differences and similarities in image descriptions
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=van+Miltenburg%2C+E">Emiel van Miltenburg</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vossen%2C+P">Piek Vossen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1707.01736v2-abstract-short" style="display: inline;">
        Automatic image description systems are commonly trained and evaluated on large image description datasets. Recently, researchers have started to collect such datasets for languages other than English. An unexplored question is how different these datasets are from English and, if there are any differences, what causes them to differ. This paper provides a cross-linguistic comparison of Dutch, Eng&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1707.01736v2-abstract-full').style.display = 'inline'; document.getElementById('1707.01736v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1707.01736v2-abstract-full" style="display: none;">
        Automatic image description systems are commonly trained and evaluated on large image description datasets. Recently, researchers have started to collect such datasets for languages other than English. An unexplored question is how different these datasets are from English and, if there are any differences, what causes them to differ. This paper provides a cross-linguistic comparison of Dutch, English, and German image descriptions. We find that these descriptions are similar in many respects, but the familiarity of crowd workers with the subjects of the images has a noticeable influence on description specificity.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1707.01736v2-abstract-full').style.display = 'none'; document.getElementById('1707.01736v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 August, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 July, 2017;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2017.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted for INLG 2017, Santiago de Compostela, Spain, 4-7 September, 2017. Camera-ready version. See the ACL anthology for full bibliographic information</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1705.04350">arXiv:1705.04350</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1705.04350">pdf</a>, <a href="https://arxiv.org/format/1705.04350">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Imagination improves Multimodal Translation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>, 
      
      <a href="/search/?searchtype=author&amp;query=K%C3%A1d%C3%A1r%2C+%C3%81">Ákos Kádár</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1705.04350v2-abstract-short" style="display: inline;">
        We decompose multimodal translation into two sub-tasks: learning to translate and learning visually grounded representations. In a multitask learning framework, translations are learned in an attention-based encoder-decoder, and grounded representations are learned through image representation prediction. Our approach improves translation performance compared to the state of the art on the Multi30&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1705.04350v2-abstract-full').style.display = 'inline'; document.getElementById('1705.04350v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1705.04350v2-abstract-full" style="display: none;">
        We decompose multimodal translation into two sub-tasks: learning to translate and learning visually grounded representations. In a multitask learning framework, translations are learned in an attention-based encoder-decoder, and grounded representations are learned through image representation prediction. Our approach improves translation performance compared to the state of the art on the Multi30K dataset. Furthermore, it is equally effective if we train the image prediction task on the external MS COCO dataset, and we find improvements if we train the translation model on the external News Commentary parallel text.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1705.04350v2-abstract-full').style.display = 'none'; document.getElementById('1705.04350v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 July, 2017; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 May, 2017;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2017.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Clarified main contributions, minor correction to Equation 8, additional comparisons in Table 2, added more related work</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1704.04198">arXiv:1704.04198</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1704.04198">pdf</a>, <a href="https://arxiv.org/format/1704.04198">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Room for improvement in automatic image description: an error analysis
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=van+Miltenburg%2C+E">Emiel van Miltenburg</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elliott%2C+D">Desmond Elliott</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1704.04198v1-abstract-short" style="display: inline;">
        In recent years we have seen rapid and significant progress in automatic image description but what are the open problems in this area? Most work has been evaluated using text-based similarity metrics, which only indicate that there have been improvements, without explaining what has improved. In this paper, we present a detailed error analysis of the descriptions generated by a state-of-the-art a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1704.04198v1-abstract-full').style.display = 'inline'; document.getElementById('1704.04198v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1704.04198v1-abstract-full" style="display: none;">
        In recent years we have seen rapid and significant progress in automatic image description but what are the open problems in this area? Most work has been evaluated using text-based similarity metrics, which only indicate that there have been improvements, without explaining what has improved. In this paper, we present a detailed error analysis of the descriptions generated by a state-of-the-art attention-based model. Our analysis operates on two levels: first we check the descriptions for accuracy, and then we categorize the types of errors we observe in the inaccurate descriptions. We find only 20% of the descriptions are free from errors, and surprisingly that 26% are unrelated to the image. Finally, we manually correct the most frequently occurring error types (e.g. gender identification) to estimate the performance reward for addressing these errors, observing gains of 0.2--1 BLEU point per type.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1704.04198v1-abstract-full').style.display = 'none'; document.getElementById('1704.04198v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 April, 2017; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2017.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submitted</span>
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?query=Desmond+Elliott&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?query=Desmond+Elliott&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?query=Desmond+Elliott&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
      
    </ul>
  </nav>
  

  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>