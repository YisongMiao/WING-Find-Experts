<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;50 of 57 results for author: <span class="mathjax">Pablo Samuel Castro</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/"  aria-role="search">
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Pablo Samuel Castro">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Pablo+Samuel+Castro&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Pablo Samuel Castro">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?query=Pablo+Samuel+Castro&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?query=Pablo+Samuel+Castro&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?query=Pablo+Samuel+Castro&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.06261">arXiv:2507.06261</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.06261">pdf</a>, <a href="https://arxiv.org/ps/2507.06261">ps</a>, <a href="https://arxiv.org/format/2507.06261">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Comanici%2C+G">Gheorghe Comanici</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bieber%2C+E">Eric Bieber</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schaekermann%2C+M">Mike Schaekermann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pasupat%2C+I">Ice Pasupat</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sachdeva%2C+N">Noveen Sachdeva</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dhillon%2C+I">Inderjit Dhillon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Blistein%2C+M">Marcel Blistein</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ram%2C+O">Ori Ram</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+D">Dan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rosen%2C+E">Evan Rosen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Marris%2C+L">Luke Marris</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Petulla%2C+S">Sam Petulla</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gaffney%2C+C">Colin Gaffney</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aharoni%2C+A">Asaf Aharoni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lintz%2C+N">Nathan Lintz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pais%2C+T+C">Tiago Cardal Pais</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jacobsson%2C+H">Henrik Jacobsson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Szpektor%2C+I">Idan Szpektor</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+N">Nan-Jiang Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Haridasan%2C+K">Krishna Haridasan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Omran%2C+A">Ahmed Omran</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Saunshi%2C+N">Nikunj Saunshi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bahri%2C+D">Dara Bahri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mishra%2C+G">Gaurav Mishra</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chu%2C+E">Eric Chu</a>
      , et al. (3284 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.06261v4-abstract-short" style="display: inline;">
        In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. In addition to its incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that excels at multimodal unde&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.06261v4-abstract-full').style.display = 'inline'; document.getElementById('2507.06261v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.06261v4-abstract-full" style="display: none;">
        In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. In addition to its incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that excels at multimodal understanding and it is now able to process up to 3 hours of video content. Its unique combination of long context, multimodal and reasoning capabilities can be combined to unlock new agentic workflows. Gemini 2.5 Flash provides excellent reasoning abilities at a fraction of the compute and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high performance at low latency and cost. Taken together, the Gemini 2.X model generation spans the full Pareto frontier of model capability vs cost, allowing users to explore the boundaries of what is possible with complex agentic problem solving.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.06261v4-abstract-full').style.display = 'none'; document.getElementById('2507.06261v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 July, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 July, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">72 pages, 17 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.17518">arXiv:2506.17518</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.17518">pdf</a>, <a href="https://arxiv.org/ps/2506.17518">ps</a>, <a href="https://arxiv.org/format/2506.17518">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Survey of State Representation Learning for Deep Reinforcement Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Echchahed%2C+A">Ayoub Echchahed</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.17518v1-abstract-short" style="display: inline;">
        Representation learning methods are an important tool for addressing the challenges posed by complex observations spaces in sequential decision making problems. Recently, many methods have used a wide variety of types of approaches for learning meaningful state representations in reinforcement learning, allowing better sample efficiency, generalization, and performance. This survey aims to provide&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.17518v1-abstract-full').style.display = 'inline'; document.getElementById('2506.17518v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.17518v1-abstract-full" style="display: none;">
        Representation learning methods are an important tool for addressing the challenges posed by complex observations spaces in sequential decision making problems. Recently, many methods have used a wide variety of types of approaches for learning meaningful state representations in reinforcement learning, allowing better sample efficiency, generalization, and performance. This survey aims to provide a broad categorization of these methods within a model-free online setting, exploring how they tackle the learning of state representations differently. We categorize the methods into six main classes, detailing their mechanisms, benefits, and limitations. Through this taxonomy, our aim is to enhance the understanding of this field and provide a guide for new researchers. We also discuss techniques for assessing the quality of representations, and detail relevant future directions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.17518v1-abstract-full').style.display = 'none'; document.getElementById('2506.17518v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.15544">arXiv:2506.15544</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.15544">pdf</a>, <a href="https://arxiv.org/ps/2506.15544">ps</a>, <a href="https://arxiv.org/format/2506.15544">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Stable Gradients for Stable Learning at Scale in Deep Reinforcement Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Castanyer%2C+R+C">Roger Creus Castanyer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Obando-Ceron%2C+J">Johan Obando-Ceron</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+L">Lu Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bacon%2C+P">Pierre-Luc Bacon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berseth%2C+G">Glen Berseth</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Courville%2C+A">Aaron Courville</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.15544v1-abstract-short" style="display: inline;">
        Scaling deep reinforcement learning networks is challenging and often results in degraded performance, yet the root causes of this failure mode remain poorly understood. Several recent works have proposed mechanisms to address this, but they are often complex and fail to highlight the causes underlying this difficulty. In this work, we conduct a series of empirical analyses which suggest that the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.15544v1-abstract-full').style.display = 'inline'; document.getElementById('2506.15544v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.15544v1-abstract-full" style="display: none;">
        Scaling deep reinforcement learning networks is challenging and often results in degraded performance, yet the root causes of this failure mode remain poorly understood. Several recent works have proposed mechanisms to address this, but they are often complex and fail to highlight the causes underlying this difficulty. In this work, we conduct a series of empirical analyses which suggest that the combination of non-stationarity with gradient pathologies, due to suboptimal architectural choices, underlie the challenges of scale. We propose a series of direct interventions that stabilize gradient flow, enabling robust performance across a range of network depths and widths. Our interventions are simple to implement and compatible with well-established algorithms, and result in an effective mechanism that enables strong performance even at large scales. We validate our findings on a variety of agents and suites of environments.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.15544v1-abstract-full').style.display = 'none'; document.getElementById('2506.15544v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.14723">arXiv:2506.14723</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.14723">pdf</a>, <a href="https://arxiv.org/ps/2506.14723">ps</a>, <a href="https://arxiv.org/format/2506.14723">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Adaptive Accompaniment with ReaLchords
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+Y">Yusong Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cooijmans%2C+T">Tim Cooijmans</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kastner%2C+K">Kyle Kastner</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Roberts%2C+A">Adam Roberts</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Simon%2C+I">Ian Simon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Scarlatos%2C+A">Alexander Scarlatos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Donahue%2C+C">Chris Donahue</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tarakajian%2C+C">Cassie Tarakajian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Omidshafiei%2C+S">Shayegan Omidshafiei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Courville%2C+A">Aaron Courville</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jaques%2C+N">Natasha Jaques</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+C+A">Cheng-Zhi Anna Huang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.14723v1-abstract-short" style="display: inline;">
        Jamming requires coordination, anticipation, and collaborative creativity between musicians. Current generative models of music produce expressive output but are not able to generate in an \emph{online} manner, meaning simultaneously with other musicians (human or otherwise). We propose ReaLchords, an online generative model for improvising chord accompaniment to user melody. We start with an onli&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.14723v1-abstract-full').style.display = 'inline'; document.getElementById('2506.14723v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.14723v1-abstract-full" style="display: none;">
        Jamming requires coordination, anticipation, and collaborative creativity between musicians. Current generative models of music produce expressive output but are not able to generate in an \emph{online} manner, meaning simultaneously with other musicians (human or otherwise). We propose ReaLchords, an online generative model for improvising chord accompaniment to user melody. We start with an online model pretrained by maximum likelihood, and use reinforcement learning to finetune the model for online use. The finetuning objective leverages both a novel reward model that provides feedback on both harmonic and temporal coherency between melody and chord, and a divergence term that implements a novel type of distillation from a teacher model that can see the future melody. Through quantitative experiments and listening tests, we demonstrate that the resulting model adapts well to unfamiliar input and produce fitting accompaniment. ReaLchords opens the door to live jamming, as well as simultaneous co-creation in other modalities.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.14723v1-abstract-full').style.display = 'none'; document.getElementById('2506.14723v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by ICML 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.13672">arXiv:2506.13672</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.13672">pdf</a>, <a href="https://arxiv.org/ps/2506.13672">ps</a>, <a href="https://arxiv.org/format/2506.13672">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Courage to Stop: Overcoming Sunk Cost Fallacy in Deep Reinforcement Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Jiashun Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Obando-Ceron%2C+J">Johan Obando-Ceron</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Courville%2C+A">Aaron Courville</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pan%2C+L">Ling Pan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.13672v1-abstract-short" style="display: inline;">
        Off-policy deep reinforcement learning (RL) typically leverages replay buffers for reusing past experiences during learning. This can help improve sample efficiency when the collected data is informative and aligned with the learning objectives; when that is not the case, it can have the effect of &#34;polluting&#34; the replay buffer with data which can exacerbate optimization challenges in addition to w&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.13672v1-abstract-full').style.display = 'inline'; document.getElementById('2506.13672v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.13672v1-abstract-full" style="display: none;">
        Off-policy deep reinforcement learning (RL) typically leverages replay buffers for reusing past experiences during learning. This can help improve sample efficiency when the collected data is informative and aligned with the learning objectives; when that is not the case, it can have the effect of &#34;polluting&#34; the replay buffer with data which can exacerbate optimization challenges in addition to wasting environment interactions due to wasteful sampling. We argue that sampling these uninformative and wasteful transitions can be avoided by addressing the sunk cost fallacy, which, in the context of deep RL, is the tendency towards continuing an episode until termination. To address this, we propose learn to stop (LEAST), a lightweight mechanism that enables strategic early episode termination based on Q-value and gradient statistics, which helps agents recognize when to terminate unproductive episodes early. We demonstrate that our method improves learning efficiency on a variety of RL algorithms, evaluated on both the MuJoCo and DeepMind Control Suite benchmarks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.13672v1-abstract-full').style.display = 'none'; document.getElementById('2506.13672v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Proceedings of the 42nd International Conference on Machine Learning (ICML 2025)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.03404">arXiv:2506.03404</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.03404">pdf</a>, <a href="https://arxiv.org/ps/2506.03404">ps</a>, <a href="https://arxiv.org/format/2506.03404">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Impact of On-Policy Parallelized Data Collection on Deep Reinforcement Learning Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mayor%2C+W">Walter Mayor</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Obando-Ceron%2C+J">Johan Obando-Ceron</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Courville%2C+A">Aaron Courville</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.03404v1-abstract-short" style="display: inline;">
        The use of parallel actors for data collection has been an effective technique used in reinforcement learning (RL) algorithms. The manner in which data is collected in these algorithms, controlled via the number of parallel environments and the rollout length, induces a form of bias-variance trade-off; the number of training passes over the collected data, on the other hand, must strike a balance&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.03404v1-abstract-full').style.display = 'inline'; document.getElementById('2506.03404v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.03404v1-abstract-full" style="display: none;">
        The use of parallel actors for data collection has been an effective technique used in reinforcement learning (RL) algorithms. The manner in which data is collected in these algorithms, controlled via the number of parallel environments and the rollout length, induces a form of bias-variance trade-off; the number of training passes over the collected data, on the other hand, must strike a balance between sample efficiency and overfitting. We conduct an empirical analysis of these trade-offs on PPO, one of the most popular RL algorithms that uses parallel actors, and establish connections to network plasticity and, more generally, optimization stability. We examine its impact on network architectures, as well as the hyper-parameter sensitivity when scaling data. Our analyses indicate that larger dataset sizes can increase final performance across a variety of settings, and that scaling parallel environments is more effective than increasing rollout lengths. These findings highlight the critical role of data collection strategies in improving agent performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.03404v1-abstract-full').style.display = 'none'; document.getElementById('2506.03404v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Proceedings of the 42nd International Conference on Machine Learning (ICML 2025)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.03189">arXiv:2506.03189</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.03189">pdf</a>, <a href="https://arxiv.org/ps/2506.03189">ps</a>, <a href="https://arxiv.org/format/2506.03189">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Continual Learning in Vision-Language Models via Aligned Model Merging
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sokar%2C+G">Ghada Sokar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dziugaite%2C+G+K">Gintare Karolina Dziugaite</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Arnab%2C+A">Anurag Arnab</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Iscen%2C+A">Ahmet Iscen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schmid%2C+C">Cordelia Schmid</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.03189v1-abstract-short" style="display: inline;">
        Continual learning is conventionally tackled through sequential fine-tuning, a process that, while enabling adaptation, inherently favors plasticity over the stability needed to retain prior knowledge. While existing approaches attempt to mitigate catastrophic forgetting, a bias towards recent tasks persists as they build upon this sequential nature. In this work we present a new perspective based&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.03189v1-abstract-full').style.display = 'inline'; document.getElementById('2506.03189v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.03189v1-abstract-full" style="display: none;">
        Continual learning is conventionally tackled through sequential fine-tuning, a process that, while enabling adaptation, inherently favors plasticity over the stability needed to retain prior knowledge. While existing approaches attempt to mitigate catastrophic forgetting, a bias towards recent tasks persists as they build upon this sequential nature. In this work we present a new perspective based on model merging to maintain stability while still retaining plasticity. Rather than just sequentially updating the model weights, we propose merging newly trained task parameters with previously learned ones, promoting a better balance. To maximize the effectiveness of the merging process, we propose a simple mechanism that promotes learning aligned weights with previous ones, thereby avoiding interference when merging. We evaluate this approach on large Vision-Language Models (VLMs), and demonstrate its effectiveness in reducing forgetting, increasing robustness to various task orders and similarities, and improving generalization.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.03189v1-abstract-full').style.display = 'none'; document.getElementById('2506.03189v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 May, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.01016">arXiv:2506.01016</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.01016">pdf</a>, <a href="https://arxiv.org/ps/2506.01016">ps</a>, <a href="https://arxiv.org/format/2506.01016">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Optimistic critics can empower small actors
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mastikhina%2C+O">Olya Mastikhina</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sreenivas%2C+D">Dhruv Sreenivas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.01016v3-abstract-short" style="display: inline;">
        Actor-critic methods have been central to many of the recent advances in deep reinforcement learning. The most common approach is to use symmetric architectures, whereby both actor and critic have the same network topology and number of parameters. However, recent works have argued for the advantages of asymmetric setups, specifically with the use of smaller actors. We perform broad empirical inve&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.01016v3-abstract-full').style.display = 'inline'; document.getElementById('2506.01016v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.01016v3-abstract-full" style="display: none;">
        Actor-critic methods have been central to many of the recent advances in deep reinforcement learning. The most common approach is to use symmetric architectures, whereby both actor and critic have the same network topology and number of parameters. However, recent works have argued for the advantages of asymmetric setups, specifically with the use of smaller actors. We perform broad empirical investigations and analyses to better understand the implications of this and find that, in general, smaller actors result in performance degradation and overfit critics. Our analyses suggest poor data collection, due to value underestimation, as one of the main causes for this behavior, and further highlight the crucial role the critic can play in alleviating this pathology. We explore techniques to mitigate the observed value underestimation, which enables further research in asymmetric actor-critic methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.01016v3-abstract-full').style.display = 'none'; document.getElementById('2506.01016v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 1 June, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">RLC 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.00592">arXiv:2506.00592</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.00592">pdf</a>, <a href="https://arxiv.org/ps/2506.00592">ps</a>, <a href="https://arxiv.org/format/2506.00592">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Mitigating Plasticity Loss in Continual Reinforcement Learning by Reducing Churn
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+H">Hongyao Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Obando-Ceron%2C+J">Johan Obando-Ceron</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Courville%2C+A">Aaron Courville</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berseth%2C+G">Glen Berseth</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.00592v1-abstract-short" style="display: inline;">
        Plasticity, or the ability of an agent to adapt to new tasks, environments, or distributions, is crucial for continual learning. In this paper, we study the loss of plasticity in deep continual RL from the lens of churn: network output variability for out-of-batch data induced by mini-batch training. We demonstrate that (1) the loss of plasticity is accompanied by the exacerbation of churn due to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.00592v1-abstract-full').style.display = 'inline'; document.getElementById('2506.00592v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.00592v1-abstract-full" style="display: none;">
        Plasticity, or the ability of an agent to adapt to new tasks, environments, or distributions, is crucial for continual learning. In this paper, we study the loss of plasticity in deep continual RL from the lens of churn: network output variability for out-of-batch data induced by mini-batch training. We demonstrate that (1) the loss of plasticity is accompanied by the exacerbation of churn due to the gradual rank decrease of the Neural Tangent Kernel (NTK) matrix; (2) reducing churn helps prevent rank collapse and adjusts the step size of regular RL gradients adaptively. Moreover, we introduce Continual Churn Approximated Reduction (C-CHAIN) and demonstrate it improves learning performance and outperforms baselines in a diverse range of continual learning environments on OpenAI Gym Control, ProcGen, DeepMind Control Suite, and MinAtar benchmarks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.00592v1-abstract-full').style.display = 'none'; document.getElementById('2506.00592v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 May, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to ICML 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2505.24061">arXiv:2505.24061</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2505.24061">pdf</a>, <a href="https://arxiv.org/ps/2505.24061">ps</a>, <a href="https://arxiv.org/format/2505.24061">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Measure gradients, not activations! Enhancing neuronal activity in deep reinforcement learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Jiashun Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+Z">Zihao Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Obando-Ceron%2C+J">Johan Obando-Ceron</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Courville%2C+A">Aaron Courville</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pan%2C+L">Ling Pan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2505.24061v1-abstract-short" style="display: inline;">
        Deep reinforcement learning (RL) agents frequently suffer from neuronal activity loss, which impairs their ability to adapt to new data and learn continually. A common method to quantify and address this issue is the tau-dormant neuron ratio, which uses activation statistics to measure the expressive ability of neurons. While effective for simple MLP-based agents, this approach loses statistical p&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.24061v1-abstract-full').style.display = 'inline'; document.getElementById('2505.24061v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2505.24061v1-abstract-full" style="display: none;">
        Deep reinforcement learning (RL) agents frequently suffer from neuronal activity loss, which impairs their ability to adapt to new data and learn continually. A common method to quantify and address this issue is the tau-dormant neuron ratio, which uses activation statistics to measure the expressive ability of neurons. While effective for simple MLP-based agents, this approach loses statistical power in more complex architectures. To address this, we argue that in advanced RL agents, maintaining a neuron&#39;s learning capacity, its ability to adapt via gradient updates, is more critical than preserving its expressive ability. Based on this insight, we shift the statistical objective from activations to gradients, and introduce GraMa (Gradient Magnitude Neural Activity Metric), a lightweight, architecture-agnostic metric for quantifying neuron-level learning capacity. We show that GraMa effectively reveals persistent neuron inactivity across diverse architectures, including residual networks, diffusion models, and agents with varied activation functions. Moreover, resetting neurons guided by GraMa (ReGraMa) consistently improves learning performance across multiple deep RL algorithms and benchmarks, such as MuJoCo and the DeepMind Control Suite.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.24061v1-abstract-full').style.display = 'none'; document.getElementById('2505.24061v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 May, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2505.17749">arXiv:2505.17749</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2505.17749">pdf</a>, <a href="https://arxiv.org/ps/2505.17749">ps</a>, <a href="https://arxiv.org/format/2505.17749">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Mind the GAP! The Challenges of Scale in Pixel-based Deep Reinforcement Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sokar%2C+G">Ghada Sokar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2505.17749v1-abstract-short" style="display: inline;">
        Scaling deep reinforcement learning in pixel-based environments presents a significant challenge, often resulting in diminished performance. While recent works have proposed algorithmic and architectural approaches to address this, the underlying cause of the performance drop remains unclear. In this paper, we identify the connection between the output of the encoder (a stack of convolutional laye&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.17749v1-abstract-full').style.display = 'inline'; document.getElementById('2505.17749v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2505.17749v1-abstract-full" style="display: none;">
        Scaling deep reinforcement learning in pixel-based environments presents a significant challenge, often resulting in diminished performance. While recent works have proposed algorithmic and architectural approaches to address this, the underlying cause of the performance drop remains unclear. In this paper, we identify the connection between the output of the encoder (a stack of convolutional layers) and the ensuing dense layers as the main underlying factor limiting scaling capabilities; we denote this connection as the bottleneck, and we demonstrate that previous approaches implicitly target this bottleneck. As a result of our analyses, we present global average pooling as a simple yet effective way of targeting the bottleneck, thereby avoiding the complexity of earlier approaches.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.17749v1-abstract-full').style.display = 'none'; document.getElementById('2505.17749v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 May, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2505.11289">arXiv:2505.11289</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2505.11289">pdf</a>, <a href="https://arxiv.org/format/2505.11289">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Meta-World+: An Improved, Standardized, RL Benchmark
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=McLean%2C+R">Reginald McLean</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chatzaroulas%2C+E">Evangelos Chatzaroulas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McCutcheon%2C+L">Luc McCutcheon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=R%C3%B6der%2C+F">Frank Röder</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+T">Tianhe Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+Z">Zhanpeng He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zentner%2C+K+R">K. R. Zentner</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Julian%2C+R">Ryan Julian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Terry%2C+J+K">J K Terry</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Woungang%2C+I">Isaac Woungang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Farsad%2C+N">Nariman Farsad</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2505.11289v1-abstract-short" style="display: inline;">
        Meta-World is widely used for evaluating multi-task and meta-reinforcement learning agents, which are challenged to master diverse skills simultaneously. Since its introduction however, there have been numerous undocumented changes which inhibit a fair comparison of algorithms. This work strives to disambiguate these results from the literature, while also leveraging the past versions of Meta-Worl&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.11289v1-abstract-full').style.display = 'inline'; document.getElementById('2505.11289v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2505.11289v1-abstract-full" style="display: none;">
        Meta-World is widely used for evaluating multi-task and meta-reinforcement learning agents, which are challenged to master diverse skills simultaneously. Since its introduction however, there have been numerous undocumented changes which inhibit a fair comparison of algorithms. This work strives to disambiguate these results from the literature, while also leveraging the past versions of Meta-World to provide insights into multi-task and meta-reinforcement learning benchmark design. Through this process we release a new open-source version of Meta-World (https://github.com/Farama-Foundation/Metaworld/) that has full reproducibility of past results, is more technically ergonomic, and gives users more control over the tasks that are included in a task set.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.11289v1-abstract-full').style.display = 'none'; document.getElementById('2505.11289v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 May, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2503.06343">arXiv:2503.06343</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2503.06343">pdf</a>, <a href="https://arxiv.org/format/2503.06343">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Studying the Interplay Between the Actor and Critic Representations in Reinforcement Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Garcin%2C+S">Samuel Garcin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=McInroe%2C+T">Trevor McInroe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Panangaden%2C+P">Prakash Panangaden</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lucas%2C+C+G">Christopher G. Lucas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Abel%2C+D">David Abel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Albrecht%2C+S+V">Stefano V. Albrecht</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2503.06343v2-abstract-short" style="display: inline;">
        Extracting relevant information from a stream of high-dimensional observations is a central challenge for deep reinforcement learning agents. Actor-critic algorithms add further complexity to this challenge, as it is often unclear whether the same information will be relevant to both the actor and the critic. To this end, we here explore the principles that underlie effective representations for t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.06343v2-abstract-full').style.display = 'inline'; document.getElementById('2503.06343v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2503.06343v2-abstract-full" style="display: none;">
        Extracting relevant information from a stream of high-dimensional observations is a central challenge for deep reinforcement learning agents. Actor-critic algorithms add further complexity to this challenge, as it is often unclear whether the same information will be relevant to both the actor and the critic. To this end, we here explore the principles that underlie effective representations for the actor and for the critic in on-policy algorithms. We focus our study on understanding whether the actor and critic will benefit from separate, rather than shared, representations. Our primary finding is that when separated, the representations for the actor and critic systematically specialise in extracting different types of information from the environment -- the actor&#39;s representation tends to focus on action-relevant information, while the critic&#39;s representation specialises in encoding value and dynamics information. We conduct a rigourous empirical study to understand how different representation learning approaches affect the actor and critic&#39;s specialisations and their downstream performance, in terms of sample efficiency and generation capabilities. Finally, we discover that a separated critic plays an important role in exploration and data collection during training. Our code, trained models and data are accessible at https://github.com/francelico/deac-rep.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.06343v2-abstract-full').style.display = 'none'; document.getElementById('2503.06343v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 March, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 March, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published as a conference paper at ICLR 2025. 10 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2503.05126">arXiv:2503.05126</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2503.05126">pdf</a>, <a href="https://arxiv.org/format/2503.05126">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multi-Task Reinforcement Learning Enables Parameter Scaling
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=McLean%2C+R">Reginald McLean</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chatzaroulas%2C+E">Evangelos Chatzaroulas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Terry%2C+J">Jordan Terry</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Woungang%2C+I">Isaac Woungang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Farsad%2C+N">Nariman Farsad</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2503.05126v3-abstract-short" style="display: inline;">
        Multi-task reinforcement learning (MTRL) aims to endow a single agent with the ability to perform well on multiple tasks. Recent works have focused on developing novel sophisticated architectures to improve performance, often resulting in larger models; it is unclear, however, whether the performance gains are a consequence of the architecture design itself or the extra parameters. We argue that g&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.05126v3-abstract-full').style.display = 'inline'; document.getElementById('2503.05126v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2503.05126v3-abstract-full" style="display: none;">
        Multi-task reinforcement learning (MTRL) aims to endow a single agent with the ability to perform well on multiple tasks. Recent works have focused on developing novel sophisticated architectures to improve performance, often resulting in larger models; it is unclear, however, whether the performance gains are a consequence of the architecture design itself or the extra parameters. We argue that gains are mostly due to scale by demonstrating that naively scaling up a simple MTRL baseline to match parameter counts outperforms the more sophisticated architectures, and these gains benefit most from scaling the critic over the actor. Additionally, we explore the training stability advantages that come with task diversity, demonstrating that increasing the number of tasks can help mitigate plasticity loss. Our findings suggest that MTRL&#39;s simultaneous training across multiple tasks provides a natural framework for beneficial parameter scaling in reinforcement learning, challenging the need for complex architectural innovations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.05126v3-abstract-full').style.display = 'none'; document.getElementById('2503.05126v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 March, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 March, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.23810">arXiv:2410.23810</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.23810">pdf</a>, <a href="https://arxiv.org/format/2410.23810">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CALE: Continuous Arcade Learning Environment
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Farebrother%2C+J">Jesse Farebrother</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.23810v1-abstract-short" style="display: inline;">
        We introduce the Continuous Arcade Learning Environment (CALE), an extension of the well-known Arcade Learning Environment (ALE) [Bellemare et al., 2013]. The CALE uses the same underlying emulator of the Atari 2600 gaming system (Stella), but adds support for continuous actions. This enables the benchmarking and evaluation of continuous-control agents (such as PPO [Schulman et al., 2017] and SAC&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.23810v1-abstract-full').style.display = 'inline'; document.getElementById('2410.23810v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.23810v1-abstract-full" style="display: none;">
        We introduce the Continuous Arcade Learning Environment (CALE), an extension of the well-known Arcade Learning Environment (ALE) [Bellemare et al., 2013]. The CALE uses the same underlying emulator of the Atari 2600 gaming system (Stella), but adds support for continuous actions. This enables the benchmarking and evaluation of continuous-control agents (such as PPO [Schulman et al., 2017] and SAC [Haarnoja et al., 2018]) and value-based agents (such as DQN [Mnih et al., 2015] and Rainbow [Hessel et al., 2018]) on the same environment suite. We provide a series of open questions and research directions that CALE enables, as well as initial baseline results using Soft Actor-Critic. CALE is available as part of the ALE athttps://github.com/Farama-Foundation/Arcade-Learning-Environment.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.23810v1-abstract-full').style.display = 'none'; document.getElementById('2410.23810v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 October, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.01930">arXiv:2410.01930</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.01930">pdf</a>, <a href="https://arxiv.org/format/2410.01930">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Don&#39;t flatten, tokenize! Unlocking the key to SoftMoE&#39;s efficacy in deep RL
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sokar%2C+G">Ghada Sokar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Obando-Ceron%2C+J">Johan Obando-Ceron</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Courville%2C+A">Aaron Courville</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Larochelle%2C+H">Hugo Larochelle</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.01930v2-abstract-short" style="display: inline;">
        The use of deep neural networks in reinforcement learning (RL) often suffers from performance degradation as model size increases. While soft mixtures of experts (SoftMoEs) have recently shown promise in mitigating this issue for online RL, the reasons behind their effectiveness remain largely unknown. In this work we provide an in-depth analysis identifying the key factors driving this performanc&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.01930v2-abstract-full').style.display = 'inline'; document.getElementById('2410.01930v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.01930v2-abstract-full" style="display: none;">
        The use of deep neural networks in reinforcement learning (RL) often suffers from performance degradation as model size increases. While soft mixtures of experts (SoftMoEs) have recently shown promise in mitigating this issue for online RL, the reasons behind their effectiveness remain largely unknown. In this work we provide an in-depth analysis identifying the key factors driving this performance gain. We discover the surprising result that tokenizing the encoder output, rather than the use of multiple experts, is what is behind the efficacy of SoftMoEs. Indeed, we demonstrate that even with an appropriately scaled single expert, we are able to maintain the performance gains, largely thanks to tokenization.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.01930v2-abstract-full').style.display = 'none'; document.getElementById('2410.01930v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 February, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 2 October, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2407.19396">arXiv:2407.19396</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2407.19396">pdf</a>, <a href="https://arxiv.org/format/2407.19396">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        NAVIX: Scaling MiniGrid Environments with JAX
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Pignatelli%2C+E">Eduardo Pignatelli</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liesen%2C+J">Jarek Liesen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lange%2C+R+T">Robert Tjarko Lange</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+C">Chris Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Toni%2C+L">Laura Toni</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2407.19396v1-abstract-short" style="display: inline;">
        As Deep Reinforcement Learning (Deep RL) research moves towards solving large-scale worlds, efficient environment simulations become crucial for rapid experimentation. However, most existing environments struggle to scale to high throughput, setting back meaningful progress. Interactions are typically computed on the CPU, limiting training speed and throughput, due to slower computation and commun&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.19396v1-abstract-full').style.display = 'inline'; document.getElementById('2407.19396v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2407.19396v1-abstract-full" style="display: none;">
        As Deep Reinforcement Learning (Deep RL) research moves towards solving large-scale worlds, efficient environment simulations become crucial for rapid experimentation. However, most existing environments struggle to scale to high throughput, setting back meaningful progress. Interactions are typically computed on the CPU, limiting training speed and throughput, due to slower computation and communication overhead when distributing the task across multiple machines. Ultimately, Deep RL training is CPU-bound, and developing batched, fast, and scalable environments has become a frontier for progress. Among the most used Reinforcement Learning (RL) environments, MiniGrid is at the foundation of several studies on exploration, curriculum learning, representation learning, diversity, meta-learning, credit assignment, and language-conditioned RL, and still suffers from the limitations described above. In this work, we introduce NAVIX, a re-implementation of MiniGrid in JAX. NAVIX achieves over 200 000x speed improvements in batch mode, supporting up to 2048 agents in parallel on a single Nvidia A100 80 GB. This reduces experiment times from one week to 15 minutes, promoting faster design iterations and more scalable RL model development.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2407.19396v1-abstract-full').style.display = 'none'; document.getElementById('2407.19396v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 July, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.18420">arXiv:2406.18420</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.18420">pdf</a>, <a href="https://arxiv.org/format/2406.18420">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Mixture of Experts in a Mixture of RL settings
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Willi%2C+T">Timon Willi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Obando-Ceron%2C+J">Johan Obando-Ceron</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Foerster%2C+J">Jakob Foerster</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dziugaite%2C+K">Karolina Dziugaite</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.18420v1-abstract-short" style="display: inline;">
        Mixtures of Experts (MoEs) have gained prominence in (self-)supervised learning due to their enhanced inference efficiency, adaptability to distributed training, and modularity. Previous research has illustrated that MoEs can significantly boost Deep Reinforcement Learning (DRL) performance by expanding the network&#39;s parameter count while reducing dormant neurons, thereby enhancing the model&#39;s lea&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.18420v1-abstract-full').style.display = 'inline'; document.getElementById('2406.18420v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.18420v1-abstract-full" style="display: none;">
        Mixtures of Experts (MoEs) have gained prominence in (self-)supervised learning due to their enhanced inference efficiency, adaptability to distributed training, and modularity. Previous research has illustrated that MoEs can significantly boost Deep Reinforcement Learning (DRL) performance by expanding the network&#39;s parameter count while reducing dormant neurons, thereby enhancing the model&#39;s learning capacity and ability to deal with non-stationarity. In this work, we shed more light on MoEs&#39; ability to deal with non-stationarity and investigate MoEs in DRL settings with &#34;amplified&#34; non-stationarity via multi-task training, providing further evidence that MoEs improve learning capacity. In contrast to previous work, our multi-task results allow us to better understand the underlying causes for the beneficial effect of MoE in DRL training, the impact of the various MoE components, and insights into how best to incorporate them in actor-critic-based DRL networks. Finally, we also confirm results from previous work.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.18420v1-abstract-full').style.display = 'none'; document.getElementById('2406.18420v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 June, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.17523">arXiv:2406.17523</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.17523">pdf</a>, <a href="https://arxiv.org/format/2406.17523">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On the consistency of hyper-parameter selection in value-based deep reinforcement learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Obando-Ceron%2C+J">Johan Obando-Ceron</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ara%C3%BAjo%2C+J+G+M">João G. M. Araújo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Courville%2C+A">Aaron Courville</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.17523v3-abstract-short" style="display: inline;">
        Deep reinforcement learning (deep RL) has achieved tremendous success on various domains through a combination of algorithmic design and careful selection of hyper-parameters. Algorithmic improvements are often the result of iterative enhancements built upon prior approaches, while hyper-parameter choices are typically inherited from previous methods or fine-tuned specifically for the proposed tec&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.17523v3-abstract-full').style.display = 'inline'; document.getElementById('2406.17523v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.17523v3-abstract-full" style="display: none;">
        Deep reinforcement learning (deep RL) has achieved tremendous success on various domains through a combination of algorithmic design and careful selection of hyper-parameters. Algorithmic improvements are often the result of iterative enhancements built upon prior approaches, while hyper-parameter choices are typically inherited from previous methods or fine-tuned specifically for the proposed technique. Despite their crucial impact on performance, hyper-parameter choices are frequently overshadowed by algorithmic advancements. This paper conducts an extensive empirical study focusing on the reliability of hyper-parameter selection for value-based deep reinforcement learning agents, including the introduction of a new score to quantify the consistency and reliability of various hyper-parameters. Our findings not only help establish which hyper-parameters are most critical to tune, but also help clarify which tunings remain consistent across different training regimes.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.17523v3-abstract-full').style.display = 'none'; document.getElementById('2406.17523v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 November, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 25 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2406.05447">arXiv:2406.05447</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2406.05447">pdf</a>, <a href="https://arxiv.org/format/2406.05447">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Instrumentation and Methods for Astrophysics">astro-ph.IM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Earth and Planetary Astrophysics">astro-ph.EP</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Solar and Stellar Astrophysics">astro-ph.SR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The PLATO Mission
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Rauer%2C+H">Heike Rauer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aerts%2C+C">Conny Aerts</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cabrera%2C+J">Juan Cabrera</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deleuil%2C+M">Magali Deleuil</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Erikson%2C+A">Anders Erikson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gizon%2C+L">Laurent Gizon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goupil%2C+M">Mariejo Goupil</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Heras%2C+A">Ana Heras</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lorenzo-Alvarez%2C+J">Jose Lorenzo-Alvarez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Marliani%2C+F">Filippo Marliani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Martin-Garcia%2C+C">César Martin-Garcia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mas-Hesse%2C+J+M">J. Miguel Mas-Hesse</a>, 
      
      <a href="/search/?searchtype=author&amp;query=O%27Rourke%2C+L">Laurence O&#39;Rourke</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Osborn%2C+H">Hugh Osborn</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pagano%2C+I">Isabella Pagano</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Piotto%2C+G">Giampaolo Piotto</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pollacco%2C+D">Don Pollacco</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ragazzoni%2C+R">Roberto Ragazzoni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ramsay%2C+G">Gavin Ramsay</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Udry%2C+S">Stéphane Udry</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Appourchaux%2C+T">Thierry Appourchaux</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Benz%2C+W">Willy Benz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brandeker%2C+A">Alexis Brandeker</a>, 
      
      <a href="/search/?searchtype=author&amp;query=G%C3%BCdel%2C+M">Manuel Güdel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Janot-Pacheco%2C+E">Eduardo Janot-Pacheco</a>
      , et al. (820 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2406.05447v2-abstract-short" style="display: inline;">
        PLATO (PLAnetary Transits and Oscillations of stars) is ESA&#39;s M3 mission designed to detect and characterise extrasolar planets and perform asteroseismic monitoring of a large number of stars. PLATO will detect small planets (down to &lt;2 R_(Earth)) around bright stars (&lt;11 mag), including terrestrial planets in the habitable zone of solar-like stars. With the complement of radial velocity observati&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.05447v2-abstract-full').style.display = 'inline'; document.getElementById('2406.05447v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2406.05447v2-abstract-full" style="display: none;">
        PLATO (PLAnetary Transits and Oscillations of stars) is ESA&#39;s M3 mission designed to detect and characterise extrasolar planets and perform asteroseismic monitoring of a large number of stars. PLATO will detect small planets (down to &lt;2 R_(Earth)) around bright stars (&lt;11 mag), including terrestrial planets in the habitable zone of solar-like stars. With the complement of radial velocity observations from the ground, planets will be characterised for their radius, mass, and age with high accuracy (5 %, 10 %, 10 % for an Earth-Sun combination respectively). PLATO will provide us with a large-scale catalogue of well-characterised small planets up to intermediate orbital periods, relevant for a meaningful comparison to planet formation theories and to better understand planet evolution. It will make possible comparative exoplanetology to place our Solar System planets in a broader context. In parallel, PLATO will study (host) stars using asteroseismology, allowing us to determine the stellar properties with high accuracy, substantially enhancing our knowledge of stellar structure and evolution.
  The payload instrument consists of 26 cameras with 12cm aperture each. For at least four years, the mission will perform high-precision photometric measurements. Here we review the science objectives, present PLATO&#39;s target samples and fields, provide an overview of expected core science performance as well as a description of the instrument and the mission profile at the beginning of the serial production of the flight cameras. PLATO is scheduled for a launch date end 2026. This overview therefore provides a summary of the mission to the community in preparation of the upcoming operational phases.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2406.05447v2-abstract-full').style.display = 'none'; document.getElementById('2406.05447v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 November, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 June, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2403.03950">arXiv:2403.03950</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2403.03950">pdf</a>, <a href="https://arxiv.org/format/2403.03950">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Stop Regressing: Training Value Functions via Classification for Scalable Deep RL
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Farebrother%2C+J">Jesse Farebrother</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Orbay%2C+J">Jordi Orbay</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vuong%2C+Q">Quan Vuong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ta%C3%AFga%2C+A+A">Adrien Ali Taïga</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chebotar%2C+Y">Yevgen Chebotar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiao%2C+T">Ted Xiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Irpan%2C+A">Alex Irpan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Levine%2C+S">Sergey Levine</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Faust%2C+A">Aleksandra Faust</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kumar%2C+A">Aviral Kumar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agarwal%2C+R">Rishabh Agarwal</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2403.03950v1-abstract-short" style="display: inline;">
        Value functions are a central component of deep reinforcement learning (RL). These functions, parameterized by neural networks, are trained using a mean squared error regression objective to match bootstrapped target values. However, scaling value-based RL methods that use regression to large networks, such as high-capacity Transformers, has proven challenging. This difficulty is in stark contrast&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.03950v1-abstract-full').style.display = 'inline'; document.getElementById('2403.03950v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2403.03950v1-abstract-full" style="display: none;">
        Value functions are a central component of deep reinforcement learning (RL). These functions, parameterized by neural networks, are trained using a mean squared error regression objective to match bootstrapped target values. However, scaling value-based RL methods that use regression to large networks, such as high-capacity Transformers, has proven challenging. This difficulty is in stark contrast to supervised learning: by leveraging a cross-entropy classification loss, supervised methods have scaled reliably to massive networks. Observing this discrepancy, in this paper, we investigate whether the scalability of deep RL can also be improved simply by using classification in place of regression for training value functions. We demonstrate that value functions trained with categorical cross-entropy significantly improves performance and scalability in a variety of domains. These include: single-task RL on Atari 2600 games with SoftMoEs, multi-task RL on Atari with large-scale ResNets, robotic manipulation with Q-transformers, playing Chess without search, and a language-agent Wordle task with high-capacity Transformers, achieving state-of-the-art results on these domains. Through careful analysis, we show that the benefits of categorical cross-entropy primarily stem from its ability to mitigate issues inherent to value-based RL, such as noisy targets and non-stationarity. Overall, we argue that a simple shift to training value functions with categorical cross-entropy can yield substantial improvements in the scalability of deep RL at little-to-no cost.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2403.03950v1-abstract-full').style.display = 'none'; document.getElementById('2403.03950v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 March, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.12479">arXiv:2402.12479</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.12479">pdf</a>, <a href="https://arxiv.org/format/2402.12479">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        In value-based deep reinforcement learning, a pruned network is a good network
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Obando-Ceron%2C+J">Johan Obando-Ceron</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Courville%2C+A">Aaron Courville</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.12479v3-abstract-short" style="display: inline;">
        Recent work has shown that deep reinforcement learning agents have difficulty in effectively using their network parameters. We leverage prior insights into the advantages of sparse training techniques and demonstrate that gradual magnitude pruning enables value-based agents to maximize parameter effectiveness. This results in networks that yield dramatic performance improvements over traditional&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.12479v3-abstract-full').style.display = 'inline'; document.getElementById('2402.12479v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.12479v3-abstract-full" style="display: none;">
        Recent work has shown that deep reinforcement learning agents have difficulty in effectively using their network parameters. We leverage prior insights into the advantages of sparse training techniques and demonstrate that gradual magnitude pruning enables value-based agents to maximize parameter effectiveness. This results in networks that yield dramatic performance improvements over traditional networks, using only a small fraction of the full network parameters.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.12479v3-abstract-full').style.display = 'none'; document.getElementById('2402.12479v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 June, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 February, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2402.08609">arXiv:2402.08609</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2402.08609">pdf</a>, <a href="https://arxiv.org/format/2402.08609">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Mixtures of Experts Unlock Parameter Scaling for Deep RL
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Obando-Ceron%2C+J">Johan Obando-Ceron</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sokar%2C+G">Ghada Sokar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Willi%2C+T">Timon Willi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lyle%2C+C">Clare Lyle</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Farebrother%2C+J">Jesse Farebrother</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Foerster%2C+J">Jakob Foerster</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dziugaite%2C+G+K">Gintare Karolina Dziugaite</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Precup%2C+D">Doina Precup</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2402.08609v3-abstract-short" style="display: inline;">
        The recent rapid progress in (self) supervised learning models is in large part predicted by empirical scaling laws: a model&#39;s performance scales proportionally to its size. Analogous scaling laws remain elusive for reinforcement learning domains, however, where increasing the parameter count of a model often hurts its final performance. In this paper, we demonstrate that incorporating Mixture-of-&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.08609v3-abstract-full').style.display = 'inline'; document.getElementById('2402.08609v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2402.08609v3-abstract-full" style="display: none;">
        The recent rapid progress in (self) supervised learning models is in large part predicted by empirical scaling laws: a model&#39;s performance scales proportionally to its size. Analogous scaling laws remain elusive for reinforcement learning domains, however, where increasing the parameter count of a model often hurts its final performance. In this paper, we demonstrate that incorporating Mixture-of-Expert (MoE) modules, and in particular Soft MoEs (Puigcerver et al., 2023), into value-based networks results in more parameter-scalable models, evidenced by substantial performance increases across a variety of training regimes and model sizes. This work thus provides strong empirical evidence towards developing scaling laws for reinforcement learning.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2402.08609v3-abstract-full').style.display = 'none'; document.getElementById('2402.08609v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 June, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 February, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2311.17894">arXiv:2311.17894</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2311.17894">pdf</a>, <a href="https://arxiv.org/format/2311.17894">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Mesoscale and Nanoscale Physics">cond-mat.mes-hall</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Materials Science">cond-mat.mtrl-sci</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning and Controlling Silicon Dopant Transitions in Graphene using Scanning Transmission Electron Microscopy
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Schwarzer%2C+M">Max Schwarzer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Farebrother%2C+J">Jesse Farebrother</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Greaves%2C+J">Joshua Greaves</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cubuk%2C+E+D">Ekin Dogus Cubuk</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agarwal%2C+R">Rishabh Agarwal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Courville%2C+A">Aaron Courville</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bellemare%2C+M+G">Marc G. Bellemare</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kalinin%2C+S">Sergei Kalinin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mordatch%2C+I">Igor Mordatch</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Roccapriore%2C+K+M">Kevin M. Roccapriore</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2311.17894v1-abstract-short" style="display: inline;">
        We introduce a machine learning approach to determine the transition dynamics of silicon atoms on a single layer of carbon atoms, when stimulated by the electron beam of a scanning transmission electron microscope (STEM). Our method is data-centric, leveraging data collected on a STEM. The data samples are processed and filtered to produce symbolic representations, which we use to train a neural n&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2311.17894v1-abstract-full').style.display = 'inline'; document.getElementById('2311.17894v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2311.17894v1-abstract-full" style="display: none;">
        We introduce a machine learning approach to determine the transition dynamics of silicon atoms on a single layer of carbon atoms, when stimulated by the electron beam of a scanning transmission electron microscope (STEM). Our method is data-centric, leveraging data collected on a STEM. The data samples are processed and filtered to produce symbolic representations, which we use to train a neural network to predict transition probabilities. These learned transition dynamics are then leveraged to guide a single silicon atom throughout the lattice to pre-determined target destinations. We present empirical analyses that demonstrate the efficacy and generality of our approach.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2311.17894v1-abstract-full').style.display = 'none'; document.getElementById('2311.17894v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 November, 2023; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2023.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2311.14115">arXiv:2311.14115</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2311.14115">pdf</a>, <a href="https://arxiv.org/format/2311.14115">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A density estimation perspective on learning from pairwise human preferences
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dumoulin%2C+V">Vincent Dumoulin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Johnson%2C+D+D">Daniel D. Johnson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Larochelle%2C+H">Hugo Larochelle</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dauphin%2C+Y">Yann Dauphin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2311.14115v3-abstract-short" style="display: inline;">
        Learning from human feedback (LHF) -- and in particular learning from pairwise preferences -- has recently become a crucial ingredient in training large language models (LLMs), and has been the subject of much research. Most recent works frame it as a reinforcement learning problem, where a reward function is learned from pairwise preference data and the LLM is treated as a policy which is adapted&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2311.14115v3-abstract-full').style.display = 'inline'; document.getElementById('2311.14115v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2311.14115v3-abstract-full" style="display: none;">
        Learning from human feedback (LHF) -- and in particular learning from pairwise preferences -- has recently become a crucial ingredient in training large language models (LLMs), and has been the subject of much research. Most recent works frame it as a reinforcement learning problem, where a reward function is learned from pairwise preference data and the LLM is treated as a policy which is adapted to maximize the rewards, often under additional regularization constraints. We propose an alternative interpretation which centers on the generative process for pairwise preferences and treats LHF as a density estimation problem. We provide theoretical and empirical results showing that for a family of generative processes defined via preference behavior distribution equations, training a reward function on pairwise preferences effectively models an annotator&#39;s implicit preference distribution. Finally, we discuss and present findings on &#34;annotator misspecification&#34; -- failure cases where wrong modeling assumptions are made about annotator behavior, resulting in poorly-adapted models -- suggesting that approaches that learn from pairwise human preferences could have trouble learning from a population of annotators with diverse viewpoints.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2311.14115v3-abstract-full').style.display = 'none'; document.getElementById('2311.14115v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 January, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 23 November, 2023;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2023.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2310.19804">arXiv:2310.19804</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2310.19804">pdf</a>, <a href="https://arxiv.org/format/2310.19804">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Kernel Perspective on Behavioural Metrics for Markov Decision Processes
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kastner%2C+T">Tyler Kastner</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Panangaden%2C+P">Prakash Panangaden</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rowland%2C+M">Mark Rowland</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2310.19804v1-abstract-short" style="display: inline;">
        Behavioural metrics have been shown to be an effective mechanism for constructing representations in reinforcement learning. We present a novel perspective on behavioural metrics for Markov decision processes via the use of positive definite kernels. We leverage this new perspective to define a new metric that is provably equivalent to the recently introduced MICo distance (Castro et al., 2021). T&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2310.19804v1-abstract-full').style.display = 'inline'; document.getElementById('2310.19804v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2310.19804v1-abstract-full" style="display: none;">
        Behavioural metrics have been shown to be an effective mechanism for constructing representations in reinforcement learning. We present a novel perspective on behavioural metrics for Markov decision processes via the use of positive definite kernels. We leverage this new perspective to define a new metric that is provably equivalent to the recently introduced MICo distance (Castro et al., 2021). The kernel perspective further enables us to provide new theoretical results, which has so far eluded prior work. These include bounding value function differences by means of our metric, and the demonstration that our metric can be provably embedded into a finite-dimensional Euclidean space with low distortion error. These are two crucial properties when using behavioural metrics for reinforcement learning representations. We complement our theory with strong empirical results that demonstrate the effectiveness of these methods in practice.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2310.19804v1-abstract-full').style.display = 'none'; document.getElementById('2310.19804v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 October, 2023; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2023.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published in TMLR</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2310.19430">arXiv:2310.19430</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2310.19430">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Applied Physics">physics.app-ph</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Materials Science">cond-mat.mtrl-sci</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1088/2515-7655/ad7404">10.1088/2515-7655/ad7404 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Roadmap on Photovoltaic Absorber Materials for Sustainable Energy Conversion
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Blakesley%2C+J+C">James C. Blakesley</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bonilla%2C+R+S">Ruy S. Bonilla</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Freitag%2C+M">Marina Freitag</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ganose%2C+A+M">Alex M. Ganose</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gasparini%2C+N">Nicola Gasparini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kaienburg%2C+P">Pascal Kaienburg</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Koutsourakis%2C+G">George Koutsourakis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Major%2C+J+D">Jonathan D. Major</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nelson%2C+J">Jenny Nelson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Noel%2C+N+K">Nakita K. Noel</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Roose%2C+B">Bart Roose</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yun%2C+J+S">Jae Sung Yun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aliwell%2C+S">Simon Aliwell</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Altermatt%2C+P+P">Pietro P. Altermatt</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ameri%2C+T">Tayebeh Ameri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Andrei%2C+V">Virgil Andrei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Armin%2C+A">Ardalan Armin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bagnis%2C+D">Diego Bagnis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Baker%2C+J">Jenny Baker</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Beath%2C+H">Hamish Beath</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bellanger%2C+M">Mathieu Bellanger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berrouard%2C+P">Philippe Berrouard</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Blumberger%2C+J">Jochen Blumberger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boden%2C+S+A">Stuart A. Boden</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bronstein%2C+H">Hugo Bronstein</a>
      , et al. (61 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2310.19430v1-abstract-short" style="display: inline;">
        Photovoltaics (PVs) are a critical technology for curbing growing levels of anthropogenic greenhouse gas emissions, and meeting increases in future demand for low-carbon electricity. In order to fulfil ambitions for net-zero carbon dioxide equivalent (CO&lt;sub&gt;2&lt;/sub&gt;eq) emissions worldwide, the global cumulative capacity of solar PVs must increase by an order of magnitude from 0.9 TWp in 2021 to 8.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2310.19430v1-abstract-full').style.display = 'inline'; document.getElementById('2310.19430v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2310.19430v1-abstract-full" style="display: none;">
        Photovoltaics (PVs) are a critical technology for curbing growing levels of anthropogenic greenhouse gas emissions, and meeting increases in future demand for low-carbon electricity. In order to fulfil ambitions for net-zero carbon dioxide equivalent (CO&lt;sub&gt;2&lt;/sub&gt;eq) emissions worldwide, the global cumulative capacity of solar PVs must increase by an order of magnitude from 0.9 TWp in 2021 to 8.5 TWp by 2050 according to the International Renewable Energy Agency, which is considered to be a highly conservative estimate. In 2020, the Henry Royce Institute brought together the UK PV community to discuss the critical technological and infrastructure challenges that need to be overcome to address the vast challenges in accelerating PV deployment. Herein, we examine the key developments in the global community, especially the progress made in the field since this earlier roadmap, bringing together experts primarily from the UK across the breadth of the photovoltaics community. The focus is both on the challenges in improving the efficiency, stability and levelized cost of electricity of current technologies for utility-scale PVs, as well as the fundamental questions in novel technologies that can have a significant impact on emerging markets, such as indoor PVs, space PVs, and agrivoltaics. We discuss challenges in advanced metrology and computational tools, as well as the growing synergies between PVs and solar fuels, and offer a perspective on the environmental sustainability of the PV industry. Through this roadmap, we emphasize promising pathways forward in both the short- and long-term, and for communities working on technologies across a range of maturity levels to learn from each other.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2310.19430v1-abstract-full').style.display = 'none'; document.getElementById('2310.19430v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 October, 2023; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2023.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">160 pages, 21 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2310.03882">arXiv:2310.03882</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2310.03882">pdf</a>, <a href="https://arxiv.org/format/2310.03882">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Small batch deep reinforcement learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Obando-Ceron%2C+J">Johan Obando-Ceron</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bellemare%2C+M+G">Marc G. Bellemare</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2310.03882v1-abstract-short" style="display: inline;">
        In value-based deep reinforcement learning with replay memories, the batch size parameter specifies how many transitions to sample for each gradient update. Although critical to the learning process, this value is typically not adjusted when proposing new algorithms. In this work we present a broad empirical study that suggests {\em reducing} the batch size can result in a number of significant pe&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2310.03882v1-abstract-full').style.display = 'inline'; document.getElementById('2310.03882v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2310.03882v1-abstract-full" style="display: none;">
        In value-based deep reinforcement learning with replay memories, the batch size parameter specifies how many transitions to sample for each gradient update. Although critical to the learning process, this value is typically not adjusted when proposing new algorithms. In this work we present a broad empirical study that suggests {\em reducing} the batch size can result in a number of significant performance gains; this is surprising, as the general tendency when training neural networks is towards larger batch sizes for improved performance. We complement our experimental findings with a set of empirical analyses towards better understanding this phenomenon.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2310.03882v1-abstract-full').style.display = 'none'; document.getElementById('2310.03882v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 October, 2023; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2023.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published at NeurIPS 2023</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2307.13824">arXiv:2307.13824</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2307.13824">pdf</a>, <a href="https://arxiv.org/format/2307.13824">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Offline Reinforcement Learning with On-Policy Q-Function Regularization
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+L">Laixi Shi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dadashi%2C+R">Robert Dadashi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chi%2C+Y">Yuejie Chi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Geist%2C+M">Matthieu Geist</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2307.13824v1-abstract-short" style="display: inline;">
        The core challenge of offline reinforcement learning (RL) is dealing with the (potentially catastrophic) extrapolation error induced by the distribution shift between the history dataset and the desired policy. A large portion of prior work tackles this challenge by implicitly/explicitly regularizing the learning policy towards the behavior policy, which is hard to estimate reliably in practice. I&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2307.13824v1-abstract-full').style.display = 'inline'; document.getElementById('2307.13824v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2307.13824v1-abstract-full" style="display: none;">
        The core challenge of offline reinforcement learning (RL) is dealing with the (potentially catastrophic) extrapolation error induced by the distribution shift between the history dataset and the desired policy. A large portion of prior work tackles this challenge by implicitly/explicitly regularizing the learning policy towards the behavior policy, which is hard to estimate reliably in practice. In this work, we propose to regularize towards the Q-function of the behavior policy instead of the behavior policy itself, under the premise that the Q-function can be estimated more reliably and easily by a SARSA-style estimate and handles the extrapolation error more straightforwardly. We propose two algorithms taking advantage of the estimated Q-function through regularizations, and demonstrate they exhibit strong performance on the D4RL benchmarks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2307.13824v1-abstract-full').style.display = 'none'; document.getElementById('2307.13824v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 July, 2023; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2023.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published at European Conference on Machine Learning (ECML), 2023</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2306.13831">arXiv:2306.13831</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2306.13831">pdf</a>, <a href="https://arxiv.org/format/2306.13831">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Minigrid &amp; Miniworld: Modular &amp; Customizable Reinforcement Learning Environments for Goal-Oriented Tasks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chevalier-Boisvert%2C+M">Maxime Chevalier-Boisvert</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+B">Bolun Dai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Towers%2C+M">Mark Towers</a>, 
      
      <a href="/search/?searchtype=author&amp;query=de+Lazcano%2C+R">Rodrigo de Lazcano</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Willems%2C+L">Lucas Willems</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lahlou%2C+S">Salem Lahlou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pal%2C+S">Suman Pal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Terry%2C+J">Jordan Terry</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2306.13831v1-abstract-short" style="display: inline;">
        We present the Minigrid and Miniworld libraries which provide a suite of goal-oriented 2D and 3D environments. The libraries were explicitly created with a minimalistic design paradigm to allow users to rapidly develop new environments for a wide range of research-specific needs. As a result, both have received widescale adoption by the RL community, facilitating research in a wide range of areas.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2306.13831v1-abstract-full').style.display = 'inline'; document.getElementById('2306.13831v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2306.13831v1-abstract-full" style="display: none;">
        We present the Minigrid and Miniworld libraries which provide a suite of goal-oriented 2D and 3D environments. The libraries were explicitly created with a minimalistic design paradigm to allow users to rapidly develop new environments for a wide range of research-specific needs. As a result, both have received widescale adoption by the RL community, facilitating research in a wide range of areas. In this paper, we outline the design philosophy, environment details, and their world generation API. We also showcase the additional capabilities brought by the unified API between Minigrid and Miniworld through case studies on transfer learning (for both RL agents and humans) between the different observation spaces. The source code of Minigrid and Miniworld can be found at https://github.com/Farama-Foundation/{Minigrid, Miniworld} along with their documentation at https://{minigrid, miniworld}.farama.org/.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2306.13831v1-abstract-full').style.display = 'none'; document.getElementById('2306.13831v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 June, 2023; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2023.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2305.19452">arXiv:2305.19452</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2305.19452">pdf</a>, <a href="https://arxiv.org/format/2305.19452">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Bigger, Better, Faster: Human-level Atari with human-level efficiency
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Schwarzer%2C+M">Max Schwarzer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Obando-Ceron%2C+J">Johan Obando-Ceron</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Courville%2C+A">Aaron Courville</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bellemare%2C+M">Marc Bellemare</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agarwal%2C+R">Rishabh Agarwal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2305.19452v3-abstract-short" style="display: inline;">
        We introduce a value-based RL agent, which we call BBF, that achieves super-human performance in the Atari 100K benchmark. BBF relies on scaling the neural networks used for value estimation, as well as a number of other design choices that enable this scaling in a sample-efficient manner. We conduct extensive analyses of these design choices and provide insights for future work. We end with a dis&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2305.19452v3-abstract-full').style.display = 'inline'; document.getElementById('2305.19452v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2305.19452v3-abstract-full" style="display: none;">
        We introduce a value-based RL agent, which we call BBF, that achieves super-human performance in the Atari 100K benchmark. BBF relies on scaling the neural networks used for value estimation, as well as a number of other design choices that enable this scaling in a sample-efficient manner. We conduct extensive analyses of these design choices and provide insights for future work. We end with a discussion about updating the goalposts for sample-efficient RL research on the ALE. We make our code and data publicly available at https://github.com/google-research/google-research/tree/master/bigger_better_faster.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2305.19452v3-abstract-full').style.display = 'none'; document.getElementById('2305.19452v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 November, 2023; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 30 May, 2023;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2023.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ICML 2023, revised version</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2304.14082">arXiv:2304.14082</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2304.14082">pdf</a>, <a href="https://arxiv.org/format/2304.14082">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        JaxPruner: A concise library for sparsity research
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+J+H">Joo Hyung Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Park%2C+W">Wonpyo Park</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mitchell%2C+N">Nicole Mitchell</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pilault%2C+J">Jonathan Pilault</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Obando-Ceron%2C+J">Johan Obando-Ceron</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+H">Han-Byul Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lee%2C+N">Namhoon Lee</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Frantar%2C+E">Elias Frantar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Long%2C+Y">Yun Long</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yazdanbakhsh%2C+A">Amir Yazdanbakhsh</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agrawal%2C+S">Shivani Agrawal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Subramanian%2C+S">Suvinay Subramanian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xin Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kao%2C+S">Sheng-Chun Kao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+X">Xingyao Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gale%2C+T">Trevor Gale</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bik%2C+A">Aart Bik</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+W">Woohyun Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ferev%2C+M">Milen Ferev</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+Z">Zhonglin Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+H">Hong-Seok Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dauphin%2C+Y">Yann Dauphin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dziugaite%2C+G+K">Gintare Karolina Dziugaite</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Evci%2C+U">Utku Evci</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2304.14082v3-abstract-short" style="display: inline;">
        This paper introduces JaxPruner, an open-source JAX-based pruning and sparse training library for machine learning research. JaxPruner aims to accelerate research on sparse neural networks by providing concise implementations of popular pruning and sparse training algorithms with minimal memory and latency overhead. Algorithms implemented in JaxPruner use a common API and work seamlessly with the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2304.14082v3-abstract-full').style.display = 'inline'; document.getElementById('2304.14082v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2304.14082v3-abstract-full" style="display: none;">
        This paper introduces JaxPruner, an open-source JAX-based pruning and sparse training library for machine learning research. JaxPruner aims to accelerate research on sparse neural networks by providing concise implementations of popular pruning and sparse training algorithms with minimal memory and latency overhead. Algorithms implemented in JaxPruner use a common API and work seamlessly with the popular optimization library Optax, which, in turn, enables easy integration with existing JAX based libraries. We demonstrate this ease of integration by providing examples in four different codebases: Scenic, t5x, Dopamine and FedJAX and provide baseline experiments on popular benchmarks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2304.14082v3-abstract-full').style.display = 'none'; document.getElementById('2304.14082v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 December, 2023; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 27 April, 2023;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2023.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Jaxpruner is hosted at http://github.com/google-research/jaxpruner</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2304.12567">arXiv:2304.12567</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2304.12567">pdf</a>, <a href="https://arxiv.org/format/2304.12567">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Proto-Value Networks: Scaling Representation Learning with Auxiliary Tasks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Farebrother%2C+J">Jesse Farebrother</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Greaves%2C+J">Joshua Greaves</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agarwal%2C+R">Rishabh Agarwal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lan%2C+C+L">Charline Le Lan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Goroshin%2C+R">Ross Goroshin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bellemare%2C+M+G">Marc G. Bellemare</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2304.12567v1-abstract-short" style="display: inline;">
        Auxiliary tasks improve the representations learned by deep reinforcement learning agents. Analytically, their effect is reasonably well understood; in practice, however, their primary use remains in support of a main learning objective, rather than as a method for learning representations. This is perhaps surprising given that many auxiliary tasks are defined procedurally, and hence can be treate&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2304.12567v1-abstract-full').style.display = 'inline'; document.getElementById('2304.12567v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2304.12567v1-abstract-full" style="display: none;">
        Auxiliary tasks improve the representations learned by deep reinforcement learning agents. Analytically, their effect is reasonably well understood; in practice, however, their primary use remains in support of a main learning objective, rather than as a method for learning representations. This is perhaps surprising given that many auxiliary tasks are defined procedurally, and hence can be treated as an essentially infinite source of information about the environment. Based on this observation, we study the effectiveness of auxiliary tasks for learning rich representations, focusing on the setting where the number of tasks and the size of the agent&#39;s network are simultaneously increased. For this purpose, we derive a new family of auxiliary tasks based on the successor measure. These tasks are easy to implement and have appealing theoretical properties. Combined with a suitable off-policy learning rule, the result is a representation learning algorithm that can be understood as extending Mahadevan &amp; Maggioni (2007)&#39;s proto-value functions to deep reinforcement learning -- accordingly, we call the resulting object proto-value networks. Through a series of experiments on the Arcade Learning Environment, we demonstrate that proto-value networks produce rich features that may be used to obtain performance comparable to established algorithms, using only linear approximation and a small number (~4M) of interactions with the environment&#39;s reward function.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2304.12567v1-abstract-full').style.display = 'none'; document.getElementById('2304.12567v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 April, 2023; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2023.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ICLR 2023. Code and models are available at https://github.com/google-research/google-research/tree/master/pvn 22 pages, 8 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2302.12902">arXiv:2302.12902</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2302.12902">pdf</a>, <a href="https://arxiv.org/format/2302.12902">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Dormant Neuron Phenomenon in Deep Reinforcement Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sokar%2C+G">Ghada Sokar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Agarwal%2C+R">Rishabh Agarwal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Evci%2C+U">Utku Evci</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2302.12902v2-abstract-short" style="display: inline;">
        In this work we identify the dormant neuron phenomenon in deep reinforcement learning, where an agent&#39;s network suffers from an increasing number of inactive neurons, thereby affecting network expressivity. We demonstrate the presence of this phenomenon across a variety of algorithms and environments, and highlight its effect on learning. To address this issue, we propose a simple and effective me&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2302.12902v2-abstract-full').style.display = 'inline'; document.getElementById('2302.12902v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2302.12902v2-abstract-full" style="display: none;">
        In this work we identify the dormant neuron phenomenon in deep reinforcement learning, where an agent&#39;s network suffers from an increasing number of inactive neurons, thereby affecting network expressivity. We demonstrate the presence of this phenomenon across a variety of algorithms and environments, and highlight its effect on learning. To address this issue, we propose a simple and effective method (ReDo) that Recycles Dormant neurons throughout training. Our experiments demonstrate that ReDo maintains the expressive power of networks by reducing the number of dormant neurons and results in improved performance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2302.12902v2-abstract-full').style.display = 'none'; document.getElementById('2302.12902v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 June, 2023; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 24 February, 2023;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2023.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Oral at ICML 2023</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2206.10369">arXiv:2206.10369</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2206.10369">pdf</a>, <a href="https://arxiv.org/format/2206.10369">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The State of Sparse Training in Deep Reinforcement Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Graesser%2C+L">Laura Graesser</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Evci%2C+U">Utku Evci</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elsen%2C+E">Erich Elsen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2206.10369v1-abstract-short" style="display: inline;">
        The use of sparse neural networks has seen rapid growth in recent years, particularly in computer vision. Their appeal stems largely from the reduced number of parameters required to train and store, as well as in an increase in learning efficiency. Somewhat surprisingly, there have been very few efforts exploring their use in Deep Reinforcement Learning (DRL). In this work we perform a systematic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2206.10369v1-abstract-full').style.display = 'inline'; document.getElementById('2206.10369v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2206.10369v1-abstract-full" style="display: none;">
        The use of sparse neural networks has seen rapid growth in recent years, particularly in computer vision. Their appeal stems largely from the reduced number of parameters required to train and store, as well as in an increase in learning efficiency. Somewhat surprisingly, there have been very few efforts exploring their use in Deep Reinforcement Learning (DRL). In this work we perform a systematic investigation into applying a number of existing sparse training techniques on a variety of DRL agents and environments. Our results corroborate the findings from sparse training in the computer vision domain - sparse networks perform better than dense networks for the same parameter count - in the DRL domain. We provide detailed analyses on how the various components in DRL are affected by the use of sparse networks and conclude by suggesting promising avenues for improving the effectiveness of sparse training methods, as well as for advancing their use in DRL.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2206.10369v1-abstract-full').style.display = 'none'; document.getElementById('2206.10369v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 June, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Proceedings of the 39th International Conference on Machine Learning (ICML&#39;22)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2206.01626">arXiv:2206.01626</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2206.01626">pdf</a>, <a href="https://arxiv.org/format/2206.01626">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Reincarnating Reinforcement Learning: Reusing Prior Computation to Accelerate Progress
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Agarwal%2C+R">Rishabh Agarwal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schwarzer%2C+M">Max Schwarzer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Courville%2C+A">Aaron Courville</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bellemare%2C+M+G">Marc G. Bellemare</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2206.01626v2-abstract-short" style="display: inline;">
        Learning tabula rasa, that is without any prior knowledge, is the prevalent workflow in reinforcement learning (RL) research. However, RL systems, when applied to large-scale settings, rarely operate tabula rasa. Such large-scale systems undergo multiple design or algorithmic changes during their development cycle and use ad hoc approaches for incorporating these changes without re-training from s&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2206.01626v2-abstract-full').style.display = 'inline'; document.getElementById('2206.01626v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2206.01626v2-abstract-full" style="display: none;">
        Learning tabula rasa, that is without any prior knowledge, is the prevalent workflow in reinforcement learning (RL) research. However, RL systems, when applied to large-scale settings, rarely operate tabula rasa. Such large-scale systems undergo multiple design or algorithmic changes during their development cycle and use ad hoc approaches for incorporating these changes without re-training from scratch, which would have been prohibitively expensive. Additionally, the inefficiency of deep RL typically excludes researchers without access to industrial-scale resources from tackling computationally-demanding problems. To address these issues, we present reincarnating RL as an alternative workflow or class of problem settings, where prior computational work (e.g., learned policies) is reused or transferred between design iterations of an RL agent, or from one RL agent to another. As a step towards enabling reincarnating RL from any agent to any other agent, we focus on the specific setting of efficiently transferring an existing sub-optimal policy to a standalone value-based RL agent. We find that existing approaches fail in this setting and propose a simple algorithm to address their limitations. Equipped with this algorithm, we demonstrate reincarnating RL&#39;s gains over tabula rasa RL on Atari 2600 games, a challenging locomotion task, and the real-world problem of navigating stratospheric balloons. Overall, this work argues for an alternative approach to RL research, which we believe could significantly improve real-world RL adoption and help democratize it further. Open-sourced code and trained agents at https://agarwl.github.io/reincarnating_rl.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2206.01626v2-abstract-full').style.display = 'none'; document.getElementById('2206.01626v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 October, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 3 June, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">NeurIPS 2022. Code and agents at https://agarwl.github.io/reincarnating_rl</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2111.05128">arXiv:2111.05128</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2111.05128">pdf</a>, <a href="https://arxiv.org/format/2111.05128">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Losses, Dissonances, and Distortions
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2111.05128v1-abstract-short" style="display: inline;">
        In this paper I present a study in using the losses and gradients obtained during the training of a simple function approximator as a mechanism for creating musical dissonance and visual distortion in a solo piano performance setting. These dissonances and distortions become part of an artistic performance not just by affecting the visualizations, but also by affecting the artistic musical perform&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.05128v1-abstract-full').style.display = 'inline'; document.getElementById('2111.05128v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2111.05128v1-abstract-full" style="display: none;">
        In this paper I present a study in using the losses and gradients obtained during the training of a simple function approximator as a mechanism for creating musical dissonance and visual distortion in a solo piano performance setting. These dissonances and distortions become part of an artistic performance not just by affecting the visualizations, but also by affecting the artistic musical performance. The system is designed such that the performer can in turn affect the training process itself, thereby creating a closed feedback loop between two processes: the training of a machine learning model and the performance of an improvised piano piece.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2111.05128v1-abstract-full').style.display = 'none'; document.getElementById('2111.05128v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 November, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">In the 5th Machine Learning for Creativity and Design Workshop at NeurIPS 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.14020">arXiv:2110.14020</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.14020">pdf</a>, <a href="https://arxiv.org/format/2110.14020">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Difficulty of Passive Learning in Deep Reinforcement Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ostrovski%2C+G">Georg Ostrovski</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dabney%2C+W">Will Dabney</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.14020v1-abstract-short" style="display: inline;">
        Learning to act from observational data without active environmental interaction is a well-known challenge in Reinforcement Learning (RL). Recent approaches involve constraints on the learned policy or conservative updates, preventing strong deviations from the state-action distribution of the dataset. Although these methods are evaluated using non-linear function approximation, theoretical justif&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.14020v1-abstract-full').style.display = 'inline'; document.getElementById('2110.14020v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.14020v1-abstract-full" style="display: none;">
        Learning to act from observational data without active environmental interaction is a well-known challenge in Reinforcement Learning (RL). Recent approaches involve constraints on the learned policy or conservative updates, preventing strong deviations from the state-action distribution of the dataset. Although these methods are evaluated using non-linear function approximation, theoretical justifications are mostly limited to the tabular or linear cases. Given the impressive results of deep reinforcement learning, we argue for a need to more clearly understand the challenges in this setting.
  In the vein of Held &amp; Hein&#39;s classic 1963 experiment, we propose the &#34;tandem learning&#34; experimental paradigm which facilitates our empirical analysis of the difficulties in offline reinforcement learning. We identify function approximation in conjunction with fixed data distributions as the strongest factors, thereby extending but also challenging hypotheses stated in past work. Our results provide relevant insights for offline deep reinforcement learning, while also shedding new light on phenomena observed in the online case of learning control.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.14020v1-abstract-full').style.display = 'none'; document.getElementById('2110.14020v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 October, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted paper at NeurIPS 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2108.13264">arXiv:2108.13264</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2108.13264">pdf</a>, <a href="https://arxiv.org/format/2108.13264">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Methodology">stat.ME</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Deep Reinforcement Learning at the Edge of the Statistical Precipice
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Agarwal%2C+R">Rishabh Agarwal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schwarzer%2C+M">Max Schwarzer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Courville%2C+A">Aaron Courville</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bellemare%2C+M+G">Marc G. Bellemare</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2108.13264v4-abstract-short" style="display: inline;">
        Deep reinforcement learning (RL) algorithms are predominantly evaluated by comparing their relative performance on a large suite of tasks. Most published results on deep RL benchmarks compare point estimates of aggregate performance such as mean and median scores across tasks, ignoring the statistical uncertainty implied by the use of a finite number of training runs. Beginning with the Arcade Lea&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.13264v4-abstract-full').style.display = 'inline'; document.getElementById('2108.13264v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2108.13264v4-abstract-full" style="display: none;">
        Deep reinforcement learning (RL) algorithms are predominantly evaluated by comparing their relative performance on a large suite of tasks. Most published results on deep RL benchmarks compare point estimates of aggregate performance such as mean and median scores across tasks, ignoring the statistical uncertainty implied by the use of a finite number of training runs. Beginning with the Arcade Learning Environment (ALE), the shift towards computationally-demanding benchmarks has led to the practice of evaluating only a small number of runs per task, exacerbating the statistical uncertainty in point estimates. In this paper, we argue that reliable evaluation in the few run deep RL regime cannot ignore the uncertainty in results without running the risk of slowing down progress in the field. We illustrate this point using a case study on the Atari 100k benchmark, where we find substantial discrepancies between conclusions drawn from point estimates alone versus a more thorough statistical analysis. With the aim of increasing the field&#39;s confidence in reported results with a handful of runs, we advocate for reporting interval estimates of aggregate performance and propose performance profiles to account for the variability in results, as well as present more robust and efficient aggregate metrics, such as interquartile mean scores, to achieve small uncertainty in results. Using such statistical tools, we scrutinize performance evaluations of existing algorithms on other widely used RL benchmarks including the ALE, Procgen, and the DeepMind Control Suite, again revealing discrepancies in prior comparisons. Our findings call for a change in how we evaluate performance in deep RL, for which we present a more rigorous evaluation methodology, accompanied with an open-source library rliable, to prevent unreliable results from stagnating the field.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.13264v4-abstract-full').style.display = 'none'; document.getElementById('2108.13264v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 January, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 30 August, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Outstanding Paper Award at NeurIPS 2021. Website: https://agarwl.github.io/rliable. 28 Pages, 33 Figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2108.05828">arXiv:2108.05828</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2108.05828">pdf</a>, <a href="https://arxiv.org/format/2108.05828">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A general class of surrogate functions for stable and efficient reinforcement learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Vaswani%2C+S">Sharan Vaswani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bachem%2C+O">Olivier Bachem</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Totaro%2C+S">Simone Totaro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mueller%2C+R">Robert Mueller</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Garg%2C+S">Shivam Garg</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Geist%2C+M">Matthieu Geist</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Machado%2C+M+C">Marlos C. Machado</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Roux%2C+N+L">Nicolas Le Roux</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2108.05828v5-abstract-short" style="display: inline;">
        Common policy gradient methods rely on the maximization of a sequence of surrogate functions. In recent years, many such surrogate functions have been proposed, most without strong theoretical guarantees, leading to algorithms such as TRPO, PPO or MPO. Rather than design yet another surrogate function, we instead propose a general framework (FMA-PG) based on functional mirror ascent that gives ris&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.05828v5-abstract-full').style.display = 'inline'; document.getElementById('2108.05828v5-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2108.05828v5-abstract-full" style="display: none;">
        Common policy gradient methods rely on the maximization of a sequence of surrogate functions. In recent years, many such surrogate functions have been proposed, most without strong theoretical guarantees, leading to algorithms such as TRPO, PPO or MPO. Rather than design yet another surrogate function, we instead propose a general framework (FMA-PG) based on functional mirror ascent that gives rise to an entire family of surrogate functions. We construct surrogate functions that enable policy improvement guarantees, a property not shared by most existing surrogate functions. Crucially, these guarantees hold regardless of the choice of policy parameterization. Moreover, a particular instantiation of FMA-PG recovers important implementation heuristics (e.g., using forward vs reverse KL divergence) resulting in a variant of TRPO with additional desirable properties. Via experiments on simple bandit problems, we evaluate the algorithms instantiated by FMA-PG. The proposed framework also suggests an improved variant of PPO, whose robustness and efficiency we empirically demonstrate on the MuJoCo suite.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.05828v5-abstract-full').style.display = 'none'; document.getElementById('2108.05828v5-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 October, 2023; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 12 August, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Fixed minor typos</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2106.08229">arXiv:2106.08229</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2106.08229">pdf</a>, <a href="https://arxiv.org/format/2106.08229">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MICo: Improved representations via sampling-based state similarity for Markov decision processes
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kastner%2C+T">Tyler Kastner</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Panangaden%2C+P">Prakash Panangaden</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rowland%2C+M">Mark Rowland</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2106.08229v4-abstract-short" style="display: inline;">
        We present a new behavioural distance over the state space of a Markov decision process, and demonstrate the use of this distance as an effective means of shaping the learnt representations of deep reinforcement learning agents. While existing notions of state similarity are typically difficult to learn at scale due to high computational cost and lack of sample-based algorithms, our newly-proposed&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.08229v4-abstract-full').style.display = 'inline'; document.getElementById('2106.08229v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2106.08229v4-abstract-full" style="display: none;">
        We present a new behavioural distance over the state space of a Markov decision process, and demonstrate the use of this distance as an effective means of shaping the learnt representations of deep reinforcement learning agents. While existing notions of state similarity are typically difficult to learn at scale due to high computational cost and lack of sample-based algorithms, our newly-proposed distance addresses both of these issues. In addition to providing detailed theoretical analysis, we provide empirical evidence that learning this distance alongside the value function yields structured and informative representations, including strong results on the Arcade Learning Environment benchmark.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2106.08229v4-abstract-full').style.display = 'none'; document.getElementById('2106.08229v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 January, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 3 June, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published at NeurIPS 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2102.01514">arXiv:2102.01514</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2102.01514">pdf</a>, <a href="https://arxiv.org/format/2102.01514">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Metrics and continuity in reinforcement learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lan%2C+C+L">Charline Le Lan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bellemare%2C+M+G">Marc G. Bellemare</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2102.01514v1-abstract-short" style="display: inline;">
        In most practical applications of reinforcement learning, it is untenable to maintain direct estimates for individual states; in continuous-state systems, it is impossible. Instead, researchers often leverage state similarity (whether explicitly or implicitly) to build models that can generalize well from a limited set of samples. The notion of state similarity used, and the neighbourhoods and top&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.01514v1-abstract-full').style.display = 'inline'; document.getElementById('2102.01514v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2102.01514v1-abstract-full" style="display: none;">
        In most practical applications of reinforcement learning, it is untenable to maintain direct estimates for individual states; in continuous-state systems, it is impossible. Instead, researchers often leverage state similarity (whether explicitly or implicitly) to build models that can generalize well from a limited set of samples. The notion of state similarity used, and the neighbourhoods and topologies they induce, is thus of crucial importance, as it will directly affect the performance of the algorithms. Indeed, a number of recent works introduce algorithms assuming the existence of &#34;well-behaved&#34; neighbourhoods, but leave the full specification of such topologies for future work. In this paper we introduce a unified formalism for defining these topologies through the lens of metrics. We establish a hierarchy amongst these metrics and demonstrate their theoretical implications on the Markov Decision Process specifying the reinforcement learning problem. We complement our theoretical results with empirical evaluations showcasing the differences between the metrics considered.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2102.01514v1-abstract-full').style.display = 'none'; document.getElementById('2102.01514v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 February, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at AAAI 2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.05265">arXiv:2101.05265</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.05265">pdf</a>, <a href="https://arxiv.org/format/2101.05265">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Contrastive Behavioral Similarity Embeddings for Generalization in Reinforcement Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Agarwal%2C+R">Rishabh Agarwal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Machado%2C+M+C">Marlos C. Machado</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bellemare%2C+M+G">Marc G. Bellemare</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.05265v2-abstract-short" style="display: inline;">
        Reinforcement learning methods trained on few environments rarely learn policies that generalize to unseen environments. To improve generalization, we incorporate the inherent sequential structure in reinforcement learning into the representation learning process. This approach is orthogonal to recent approaches, which rarely exploit this structure explicitly. Specifically, we introduce a theoreti&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.05265v2-abstract-full').style.display = 'inline'; document.getElementById('2101.05265v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.05265v2-abstract-full" style="display: none;">
        Reinforcement learning methods trained on few environments rarely learn policies that generalize to unseen environments. To improve generalization, we incorporate the inherent sequential structure in reinforcement learning into the representation learning process. This approach is orthogonal to recent approaches, which rarely exploit this structure explicitly. Specifically, we introduce a theoretically motivated policy similarity metric (PSM) for measuring behavioral similarity between states. PSM assigns high similarity to states for which the optimal policies in those states as well as in future states are similar. We also present a contrastive representation learning procedure to embed any state similarity metric, which we instantiate with PSM to obtain policy similarity embeddings (PSEs). We demonstrate that PSEs improve generalization on diverse benchmarks, including LQR with spurious correlations, a jumping task from pixels, and Distracting DM Control Suite.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.05265v2-abstract-full').style.display = 'none'; document.getElementById('2101.05265v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 March, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 January, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ICLR 2021 (Spotlight). Website: https://agarwl.github.io/pse</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2011.14826">arXiv:2011.14826</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2011.14826">pdf</a>, <a href="https://arxiv.org/format/2011.14826">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Revisiting Rainbow: Promoting more Insightful and Inclusive Deep Reinforcement Learning Research
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Obando-Ceron%2C+J+S">Johan S. Obando-Ceron</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2011.14826v2-abstract-short" style="display: inline;">
        Since the introduction of DQN, a vast majority of reinforcement learning research has focused on reinforcement learning with deep neural networks as function approximators. New methods are typically evaluated on a set of environments that have now become standard, such as Atari 2600 games. While these benchmarks help standardize evaluation, their computational cost has the unfortunate side effect&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.14826v2-abstract-full').style.display = 'inline'; document.getElementById('2011.14826v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2011.14826v2-abstract-full" style="display: none;">
        Since the introduction of DQN, a vast majority of reinforcement learning research has focused on reinforcement learning with deep neural networks as function approximators. New methods are typically evaluated on a set of environments that have now become standard, such as Atari 2600 games. While these benchmarks help standardize evaluation, their computational cost has the unfortunate side effect of widening the gap between those with ample access to computational resources, and those without. In this work we argue that, despite the community&#39;s emphasis on large-scale environments, the traditional small-scale environments can still yield valuable scientific insights and can help reduce the barriers to entry for underprivileged communities. To substantiate our claims, we empirically revisit the paper which introduced the Rainbow algorithm [Hessel et al., 2018] and present some new insights into the algorithms used by Rainbow.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.14826v2-abstract-full').style.display = 'none'; document.getElementById('2011.14826v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 May, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 November, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Proceedings of the 38th International Conference on Machine Learning (ICML 2021)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2011.05158">arXiv:2011.05158</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2011.05158">pdf</a>, <a href="https://arxiv.org/format/2011.05158">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        GANterpretations
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2011.05158v1-abstract-short" style="display: inline;">
        Since the introduction of Generative Adversarial Networks (GANs) [Goodfellow et al., 2014] there has been a regular stream of both technical advances (e.g., Arjovsky et al. [2017]) and creative uses of these generative models (e.g., [Karras et al., 2019, Zhu et al., 2017, Jin et al., 2017]). In this work we propose an approach for using the power of GANs to automatically generate videos to accompa&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.05158v1-abstract-full').style.display = 'inline'; document.getElementById('2011.05158v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2011.05158v1-abstract-full" style="display: none;">
        Since the introduction of Generative Adversarial Networks (GANs) [Goodfellow et al., 2014] there has been a regular stream of both technical advances (e.g., Arjovsky et al. [2017]) and creative uses of these generative models (e.g., [Karras et al., 2019, Zhu et al., 2017, Jin et al., 2017]). In this work we propose an approach for using the power of GANs to automatically generate videos to accompany audio recordings by aligning to spectral properties of the recording. This allows musicians to explore new forms of multi-modal creative expression, where musical performance can induce an AI-generated musical video that is guided by said performance, as well as a medium for creating a visual narrative to follow a storyline (similar to what was proposed by Frosst and Kereliuk [2019]).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.05158v1-abstract-full').style.display = 'none'; document.getElementById('2011.05158v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 November, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">In 4th Workshop on Machine Learning for Creativity and Design at NeurIPS 2020, Vancouver, Canada</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1911.11134">arXiv:1911.11134</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1911.11134">pdf</a>, <a href="https://arxiv.org/format/1911.11134">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Rigging the Lottery: Making All Tickets Winners
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Evci%2C+U">Utku Evci</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gale%2C+T">Trevor Gale</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Menick%2C+J">Jacob Menick</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Elsen%2C+E">Erich Elsen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1911.11134v3-abstract-short" style="display: inline;">
        Many applications require sparse neural networks due to space or inference time restrictions. There is a large body of work on training dense networks to yield sparse networks for inference, but this limits the size of the largest trainable sparse model to that of the largest trainable dense model. In this paper we introduce a method to train sparse neural networks with a fixed parameter count and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.11134v3-abstract-full').style.display = 'inline'; document.getElementById('1911.11134v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1911.11134v3-abstract-full" style="display: none;">
        Many applications require sparse neural networks due to space or inference time restrictions. There is a large body of work on training dense networks to yield sparse networks for inference, but this limits the size of the largest trainable sparse model to that of the largest trainable dense model. In this paper we introduce a method to train sparse neural networks with a fixed parameter count and a fixed computational cost throughout training, without sacrificing accuracy relative to existing dense-to-sparse training methods. Our method updates the topology of the sparse network during training by using parameter magnitudes and infrequent gradient calculations. We show that this approach requires fewer floating-point operations (FLOPs) to achieve a given level of accuracy compared to prior techniques. We demonstrate state-of-the-art sparse training results on a variety of networks and datasets, including ResNet-50, MobileNets on Imagenet-2012, and RNNs on WikiText-103. Finally, we provide some insights into why allowing the topology to change during the optimization can overcome local minima encountered when the topology remains static. Code used in our work can be found in github.com/google-research/rigl.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.11134v3-abstract-full').style.display = 'none'; document.getElementById('1911.11134v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 July, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 25 November, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Published in Proceedings of the 37th International Conference on Machine Learning. Code can be found in github.com/google-research/rigl</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Proceedings of the 37th International Conference on Machine Learning (2020) 471-481
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1911.09291">arXiv:1911.09291</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1911.09291">pdf</a>, <a href="https://arxiv.org/format/1911.09291">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Scalable methods for computing state similarity in deterministic Markov Decision Processes
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1911.09291v1-abstract-short" style="display: inline;">
        We present new algorithms for computing and approximating bisimulation metrics in Markov Decision Processes (MDPs). Bisimulation metrics are an elegant formalism that capture behavioral equivalence between states and provide strong theoretical guarantees on differences in optimal behaviour. Unfortunately, their computation is expensive and requires a tabular representation of the states, which has&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.09291v1-abstract-full').style.display = 'inline'; document.getElementById('1911.09291v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1911.09291v1-abstract-full" style="display: none;">
        We present new algorithms for computing and approximating bisimulation metrics in Markov Decision Processes (MDPs). Bisimulation metrics are an elegant formalism that capture behavioral equivalence between states and provide strong theoretical guarantees on differences in optimal behaviour. Unfortunately, their computation is expensive and requires a tabular representation of the states, which has thus far rendered them impractical for large problems. In this paper we present a new version of the metric that is tied to a behavior policy in an MDP, along with an analysis of its theoretical properties. We then present two new algorithms for approximating bisimulation metrics in large, deterministic MDPs. The first does so via sampling and is guaranteed to converge to the true metric. The second is a differentiable loss which allows us to learn an approximation even for continuous state MDPs, which prior to this work had not been possible.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.09291v1-abstract-full').style.display = 'none'; document.getElementById('1911.09291v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 November, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To appear in Proceedings of the Thirty-Fourth AAAI Conference on Artificial Intelligence (AAAI-20)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1907.13411">arXiv:1907.13411</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1907.13411">pdf</a>, <a href="https://arxiv.org/format/1907.13411">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Inverse Reinforcement Learning with Multiple Ranked Experts
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+S">Shijian Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+D">Daqing Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1907.13411v1-abstract-short" style="display: inline;">
        We consider the problem of learning to behave optimally in a Markov Decision Process when a reward function is not specified, but instead we have access to a set of demonstrators of varying performance. We assume the demonstrators are classified into one of k ranks, and use ideas from ordinal regression to find a reward function that maximizes the margin between the different ranks. This approach&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1907.13411v1-abstract-full').style.display = 'inline'; document.getElementById('1907.13411v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1907.13411v1-abstract-full" style="display: none;">
        We consider the problem of learning to behave optimally in a Markov Decision Process when a reward function is not specified, but instead we have access to a set of demonstrators of varying performance. We assume the demonstrators are classified into one of k ranks, and use ideas from ordinal regression to find a reward function that maximizes the margin between the different ranks. This approach is based on the idea that agents should not only learn how to behave from experts, but also how not to behave from non-experts. We show there are MDPs where important differences in the reward function would be hidden from existing algorithms by the behaviour of the expert. Our method is particularly useful for problems where we have access to a large set of agent behaviours with varying degrees of expertise (such as through GPS or cellphones). We highlight the differences between our approach and existing methods using a simple grid domain and demonstrate its efficacy on determining passenger-finding strategies for taxi drivers, using a large dataset of GPS trajectories.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1907.13411v1-abstract-full').style.display = 'none'; document.getElementById('1907.13411v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 July, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1904.13285">arXiv:1904.13285</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1904.13285">pdf</a>, <a href="https://arxiv.org/format/1904.13285">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Performing Structured Improvisations with pre-trained Deep Learning Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1904.13285v1-abstract-short" style="display: inline;">
        The quality of outputs produced by deep generative models for music have seen a dramatic improvement in the last few years. However, most deep learning models perform in &#34;offline&#34; mode, with few restrictions on the processing time. Integrating these types of models into a live structured performance poses a challenge because of the necessity to respect the beat and harmony. Further, these deep mod&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1904.13285v1-abstract-full').style.display = 'inline'; document.getElementById('1904.13285v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1904.13285v1-abstract-full" style="display: none;">
        The quality of outputs produced by deep generative models for music have seen a dramatic improvement in the last few years. However, most deep learning models perform in &#34;offline&#34; mode, with few restrictions on the processing time. Integrating these types of models into a live structured performance poses a challenge because of the necessity to respect the beat and harmony. Further, these deep models tend to be agnostic to the style of a performer, which often renders them impractical for live performance. In this paper we propose a system which enables the integration of out-of-the-box generative models by leveraging the musician&#39;s creativity and expertise.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1904.13285v1-abstract-full').style.display = 'none'; document.getElementById('1904.13285v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 April, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1902.03149">arXiv:1902.03149</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1902.03149">pdf</a>, <a href="https://arxiv.org/format/1902.03149">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Distributional reinforcement learning with linear function approximation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bellemare%2C+M+G">Marc G. Bellemare</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Roux%2C+N+L">Nicolas Le Roux</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castro%2C+P+S">Pablo Samuel Castro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Moitra%2C+S">Subhodeep Moitra</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1902.03149v1-abstract-short" style="display: inline;">
        Despite many algorithmic advances, our theoretical understanding of practical distributional reinforcement learning methods remains limited. One exception is Rowland et al. (2018)&#39;s analysis of the C51 algorithm in terms of the Cramér distance, but their results only apply to the tabular setting and ignore C51&#39;s use of a softmax to produce normalized distributions. In this paper we adapt the Cramé&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1902.03149v1-abstract-full').style.display = 'inline'; document.getElementById('1902.03149v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1902.03149v1-abstract-full" style="display: none;">
        Despite many algorithmic advances, our theoretical understanding of practical distributional reinforcement learning methods remains limited. One exception is Rowland et al. (2018)&#39;s analysis of the C51 algorithm in terms of the Cramér distance, but their results only apply to the tabular setting and ignore C51&#39;s use of a softmax to produce normalized distributions. In this paper we adapt the Cramér distance to deal with arbitrary vectors. From it we derive a new distributional algorithm which is fully Cramér-based and can be combined to linear function approximation, with formal guarantees in the context of policy evaluation. In allowing the model&#39;s prediction to be any real vector, we lose the probabilistic interpretation behind the method, but otherwise maintain the appealing properties of distributional approaches. To the best of our knowledge, ours is the first proof of convergence of a distributional algorithm combined with function approximation. Perhaps surprisingly, our results provide evidence that Cramér-based distributional methods may perform worse than directly approximating the value function.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1902.03149v1-abstract-full').style.display = 'none'; document.getElementById('1902.03149v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 February, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">To appear</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Proceedings of AISTATS 2019
      </p>
    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?query=Pablo+Samuel+Castro&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?query=Pablo+Samuel+Castro&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?query=Pablo+Samuel+Castro&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
      
    </ul>
  </nav>
  

  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>