<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;50 of 262 results for author: <span class="mathjax">Tongliang Liu</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/"  aria-role="search">
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Tongliang Liu">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Tongliang+Liu&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Tongliang Liu">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?query=Tongliang+Liu&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?query=Tongliang+Liu&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?query=Tongliang+Liu&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
        <li>
          <a href="/search/?query=Tongliang+Liu&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=100"
            class="pagination-link "
            aria-label="Page 3"
            aria-current="page">3
          </a>
        </li>
        
        <li>
          <a href="/search/?query=Tongliang+Liu&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=150"
            class="pagination-link "
            aria-label="Page 4"
            aria-current="page">4
          </a>
        </li>
        
        <li>
          <a href="/search/?query=Tongliang+Liu&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=200"
            class="pagination-link "
            aria-label="Page 5"
            aria-current="page">5
          </a>
        </li>
        
        <li>
          <a href="/search/?query=Tongliang+Liu&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=250"
            class="pagination-link "
            aria-label="Page 6"
            aria-current="page">6
          </a>
        </li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.04329">arXiv:2508.04329</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.04329">pdf</a>, <a href="https://arxiv.org/ps/2508.04329">ps</a>, <a href="https://arxiv.org/format/2508.04329">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Forgetting: A New Mechanism Towards Better Large Language Model Fine-tuning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ghahrizjani%2C+A+T">Ali Taheri Ghahrizjani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Taban%2C+A">Alireza Taban</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ye%2C+S">Shanshan Ye</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mirzaei%2C+A">Abdolreza Mirzaei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+B">Bo Han</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.04329v3-abstract-short" style="display: inline;">
        Supervised fine-tuning (SFT) plays a critical role for pretrained large language models (LLMs), notably enhancing their capacity to acquire domain-specific knowledge while preserving or potentially augmenting their general-purpose capabilities. However, the efficacy of SFT hinges on data quality as well as data volume, otherwise it may result in limited performance gains or even degradation relati&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.04329v3-abstract-full').style.display = 'inline'; document.getElementById('2508.04329v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.04329v3-abstract-full" style="display: none;">
        Supervised fine-tuning (SFT) plays a critical role for pretrained large language models (LLMs), notably enhancing their capacity to acquire domain-specific knowledge while preserving or potentially augmenting their general-purpose capabilities. However, the efficacy of SFT hinges on data quality as well as data volume, otherwise it may result in limited performance gains or even degradation relative to the associated baselines. To mitigate such reliance, we suggest categorizing tokens within each corpus into two parts -- positive and negative tokens -- based on whether they are useful to improve model performance. Positive tokens can be trained in common ways, whereas negative tokens, which may lack essential semantics or be misleading, should be explicitly forgotten. Overall, the token categorization facilitate the model to learn less informative message, and the forgetting process shapes a knowledge boundary to guide the model on what information to learn more precisely. We conduct experiments on well-established benchmarks, finding that this forgetting mechanism not only improves overall model performance and also facilitate more diverse model responses.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.04329v3-abstract-full').style.display = 'none'; document.getElementById('2508.04329v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 August, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.03691">arXiv:2508.03691</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.03691">pdf</a>, <a href="https://arxiv.org/ps/2508.03691">ps</a>, <a href="https://arxiv.org/format/2508.03691">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        La La LiDAR: Large-Scale Layout Generation from LiDAR Data
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Youquan Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kong%2C+L">Lingdong Kong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+W">Weidong Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xin Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liang%2C+A">Ao Liang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+R">Runnan Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fei%2C+B">Ben Fei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.03691v1-abstract-short" style="display: inline;">
        Controllable generation of realistic LiDAR scenes is crucial for applications such as autonomous driving and robotics. While recent diffusion-based models achieve high-fidelity LiDAR generation, they lack explicit control over foreground objects and spatial relationships, limiting their usefulness for scenario simulation and safety validation. To address these limitations, we propose Large-scale L&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.03691v1-abstract-full').style.display = 'inline'; document.getElementById('2508.03691v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.03691v1-abstract-full" style="display: none;">
        Controllable generation of realistic LiDAR scenes is crucial for applications such as autonomous driving and robotics. While recent diffusion-based models achieve high-fidelity LiDAR generation, they lack explicit control over foreground objects and spatial relationships, limiting their usefulness for scenario simulation and safety validation. To address these limitations, we propose Large-scale Layout-guided LiDAR generation model (&#34;La La LiDAR&#34;), a novel layout-guided generative framework that introduces semantic-enhanced scene graph diffusion with relation-aware contextual conditioning for structured LiDAR layout generation, followed by foreground-aware control injection for complete scene generation. This enables customizable control over object placement while ensuring spatial and semantic consistency. To support our structured LiDAR generation, we introduce Waymo-SG and nuScenes-SG, two large-scale LiDAR scene graph datasets, along with new evaluation metrics for layout synthesis. Extensive experiments demonstrate that La La LiDAR achieves state-of-the-art performance in both LiDAR generation and downstream perception tasks, establishing a new benchmark for controllable 3D scene generation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.03691v1-abstract-full').style.display = 'none'; document.getElementById('2508.03691v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Preprint; 10 pages, 6 figures, 7 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.07781">arXiv:2507.07781</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.07781">pdf</a>, <a href="https://arxiv.org/ps/2507.07781">ps</a>, <a href="https://arxiv.org/format/2507.07781">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SURPRISE3D: A Dataset for Spatial Understanding and Reasoning in Complex 3D Scenes
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+J">Jiaxin Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Ziwen Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+H">Hanlve Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+R">Runnan Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+X">Xiao He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Y">Yandong Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+W">Wenping Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gong%2C+M">Mingming Gong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.07781v1-abstract-short" style="display: inline;">
        The integration of language and 3D perception is critical for embodied AI and robotic systems to perceive, understand, and interact with the physical world. Spatial reasoning, a key capability for understanding spatial relationships between objects, remains underexplored in current 3D vision-language research. Existing datasets often mix semantic cues (e.g., object name) with spatial context, lead&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.07781v1-abstract-full').style.display = 'inline'; document.getElementById('2507.07781v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.07781v1-abstract-full" style="display: none;">
        The integration of language and 3D perception is critical for embodied AI and robotic systems to perceive, understand, and interact with the physical world. Spatial reasoning, a key capability for understanding spatial relationships between objects, remains underexplored in current 3D vision-language research. Existing datasets often mix semantic cues (e.g., object name) with spatial context, leading models to rely on superficial shortcuts rather than genuinely interpreting spatial relationships. To address this gap, we introduce S\textsc{urprise}3D, a novel dataset designed to evaluate language-guided spatial reasoning segmentation in complex 3D scenes. S\textsc{urprise}3D consists of more than 200k vision language pairs across 900+ detailed indoor scenes from ScanNet++ v2, including more than 2.8k unique object classes. The dataset contains 89k+ human-annotated spatial queries deliberately crafted without object name, thereby mitigating shortcut biases in spatial understanding. These queries comprehensively cover various spatial reasoning skills, such as relative position, narrative perspective, parametric perspective, and absolute distance reasoning. Initial benchmarks demonstrate significant challenges for current state-of-the-art expert 3D visual grounding methods and 3D-LLMs, underscoring the necessity of our dataset and the accompanying 3D Spatial Reasoning Segmentation (3D-SRS) benchmark suite. S\textsc{urprise}3D and 3D-SRS aim to facilitate advancements in spatially aware AI, paving the way for effective embodied interaction and robotic planning. The code and datasets can be found in https://github.com/liziwennba/SUPRISE.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.07781v1-abstract-full').style.display = 'none'; document.getElementById('2507.07781v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 July, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.04119">arXiv:2507.04119</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.04119">pdf</a>, <a href="https://arxiv.org/ps/2507.04119">ps</a>, <a href="https://arxiv.org/format/2507.04119">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        When Data-Free Knowledge Distillation Meets Non-Transferable Teacher: Escaping Out-of-Distribution Trap is All You Need
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hong%2C+Z">Ziming Hong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+R">Runnan Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Z">Zengmao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+B">Bo Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+B">Bo Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.04119v1-abstract-short" style="display: inline;">
        Data-free knowledge distillation (DFKD) transfers knowledge from a teacher to a student without access the real in-distribution (ID) data. Its common solution is to use a generator to synthesize fake data and use them as a substitute for real ID data. However, existing works typically assume teachers are trustworthy, leaving the robustness and security of DFKD from untrusted teachers largely unexp&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.04119v1-abstract-full').style.display = 'inline'; document.getElementById('2507.04119v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.04119v1-abstract-full" style="display: none;">
        Data-free knowledge distillation (DFKD) transfers knowledge from a teacher to a student without access the real in-distribution (ID) data. Its common solution is to use a generator to synthesize fake data and use them as a substitute for real ID data. However, existing works typically assume teachers are trustworthy, leaving the robustness and security of DFKD from untrusted teachers largely unexplored. In this work, we conduct the first investigation into distilling non-transferable learning (NTL) teachers using DFKD, where the transferability from an ID domain to an out-of-distribution (OOD) domain is prohibited. We find that NTL teachers fool DFKD through divert the generator&#39;s attention from the useful ID knowledge to the misleading OOD knowledge. This hinders ID knowledge transfer but prioritizes OOD knowledge transfer. To mitigate this issue, we propose Adversarial Trap Escaping (ATEsc) to benefit DFKD by identifying and filtering out OOD-like synthetic samples. Specifically, inspired by the evidence that NTL teachers show stronger adversarial robustness on OOD samples than ID samples, we split synthetic samples into two groups according to their robustness. The fragile group is treated as ID-like data and used for normal knowledge distillation, while the robust group is seen as OOD-like data and utilized for forgetting OOD knowledge. Extensive experiments demonstrate the effectiveness of ATEsc for improving DFKD against NTL teachers. Code is released at https://github.com/tmllab/2025_ICML_ATEsc.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.04119v1-abstract-full').style.display = 'none'; document.getElementById('2507.04119v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 July, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by ICML 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.21215">arXiv:2506.21215</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.21215">pdf</a>, <a href="https://arxiv.org/ps/2506.21215">ps</a>, <a href="https://arxiv.org/format/2506.21215">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chi%2C+H">Haoang Chi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+H">He Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+W">Wenjing Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+F">Feng Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lan%2C+L">Long Lan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ren%2C+X">Xiaoguang Ren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+B">Bo Han</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.21215v1-abstract-short" style="display: inline;">
        Causal reasoning capability is critical in advancing large language models (LLMs) toward strong artificial intelligence. While versatile LLMs appear to have demonstrated capabilities in understanding contextual causality and providing responses that obey the laws of causality, it remains unclear whether they perform genuine causal reasoning akin to humans. However, current evidence indicates the c&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.21215v1-abstract-full').style.display = 'inline'; document.getElementById('2506.21215v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.21215v1-abstract-full" style="display: none;">
        Causal reasoning capability is critical in advancing large language models (LLMs) toward strong artificial intelligence. While versatile LLMs appear to have demonstrated capabilities in understanding contextual causality and providing responses that obey the laws of causality, it remains unclear whether they perform genuine causal reasoning akin to humans. However, current evidence indicates the contrary. Specifically, LLMs are only capable of performing shallow (level-1) causal reasoning, primarily attributed to the causal knowledge embedded in their parameters, but they lack the capacity for genuine human-like (level-2) causal reasoning. To support this hypothesis, methodologically, we delve into the autoregression mechanism of transformer-based LLMs, revealing that it is not inherently causal. Empirically, we introduce a new causal Q&amp;A benchmark called CausalProbe-2024, whose corpora are fresh and nearly unseen for the studied LLMs. The LLMs exhibit a significant performance drop on CausalProbe-2024 compared to earlier benchmarks, indicating the fact that they primarily engage in level-1 causal reasoning. To bridge the gap towards level-2 causal reasoning, we draw inspiration from the fact that human reasoning is usually facilitated by general knowledge and intended goals. We propose G^2-Reasoner, a method that incorporates general knowledge and goal-oriented prompts into LLMs&#39; causal reasoning processes. Experiments demonstrate that G^2-Reasoner significantly enhances LLMs&#39; causal reasoning capability, particularly in fresh and counterfactual contexts. This work sheds light on a new path for LLMs to advance towards genuine causal reasoning, going beyond level-1 and making strides towards level-2.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.21215v1-abstract-full').style.display = 'none'; document.getElementById('2506.21215v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">24 pages, accepted at NeurIPS 2024</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Advances in Neural Information Processing Systems, 2024, 37: 96640-96670
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.09645">arXiv:2506.09645</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.09645">pdf</a>, <a href="https://arxiv.org/ps/2506.09645">ps</a>, <a href="https://arxiv.org/format/2506.09645">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Efficient and Generalizable Graph Retriever for Knowledge-Graph Question Answering
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yao%2C+T">Tianjun Yao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+H">Haoxuan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shen%2C+Z">Zhiqiang Shen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+P">Pan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+K">Kun Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.09645v1-abstract-short" style="display: inline;">
        Large Language Models (LLMs) have shown strong inductive reasoning ability across various domains, but their reliability is hindered by the outdated knowledge and hallucinations. Retrieval-Augmented Generation mitigates these issues by grounding LLMs with external knowledge; however, most existing RAG pipelines rely on unstructured text, limiting interpretability and structured reasoning. Knowledg&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.09645v1-abstract-full').style.display = 'inline'; document.getElementById('2506.09645v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.09645v1-abstract-full" style="display: none;">
        Large Language Models (LLMs) have shown strong inductive reasoning ability across various domains, but their reliability is hindered by the outdated knowledge and hallucinations. Retrieval-Augmented Generation mitigates these issues by grounding LLMs with external knowledge; however, most existing RAG pipelines rely on unstructured text, limiting interpretability and structured reasoning. Knowledge graphs, which represent facts as relational triples, offer a more structured and compact alternative. Recent studies have explored integrating knowledge graphs with LLMs for knowledge graph question answering (KGQA), with a significant proportion adopting the retrieve-then-reasoning paradigm. In this framework, graph-based retrievers have demonstrated strong empirical performance, yet they still face challenges in generalization ability. In this work, we propose RAPL, a novel framework for efficient and effective graph retrieval in KGQA. RAPL addresses these limitations through three aspects: (1) a two-stage labeling strategy that combines heuristic signals with parametric models to provide causally grounded supervision; (2) a model-agnostic graph transformation approach to capture both intra- and inter-triple interactions, thereby enhancing representational capacity; and (3) a path-based reasoning strategy that facilitates learning from the injected rational knowledge, and supports downstream reasoner through structured inputs. Empirically, RAPL outperforms state-of-the-art methods by $2.66\%-20.34\%$, and significantly reduces the performance gap between smaller and more powerful LLM-based reasoners, as well as the gap under cross-dataset settings, highlighting its superior retrieval capability and generalizability. Codes are available at: https://github.com/tianyao-aka/RAPL.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.09645v1-abstract-full').style.display = 'none'; document.getElementById('2506.09645v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">32 pages, 28 figures</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.6
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.08747">arXiv:2506.08747</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.08747">pdf</a>, <a href="https://arxiv.org/ps/2506.08747">ps</a>, <a href="https://arxiv.org/format/2506.08747">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Sample Efficient Conditional Independence Test in the Presence of Discretization
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+B">Boyang Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yao%2C+Y">Yu Yao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dong%2C+X">Xinshuai Dong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Z">Zongfang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qiu%2C+Y">Yumou Qiu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+K">Kun Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.08747v1-abstract-short" style="display: inline;">
        In many real-world scenarios, interested variables are often represented as discretized values due to measurement limitations. Applying Conditional Independence (CI) tests directly to such discretized data, however, can lead to incorrect conclusions. To address this, recent advancements have sought to infer the correct CI relationship between the latent variables through binarizing observed data.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.08747v1-abstract-full').style.display = 'inline'; document.getElementById('2506.08747v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.08747v1-abstract-full" style="display: none;">
        In many real-world scenarios, interested variables are often represented as discretized values due to measurement limitations. Applying Conditional Independence (CI) tests directly to such discretized data, however, can lead to incorrect conclusions. To address this, recent advancements have sought to infer the correct CI relationship between the latent variables through binarizing observed data. However, this process inevitably results in a loss of information, which degrades the test&#39;s performance. Motivated by this, this paper introduces a sample-efficient CI test that does not rely on the binarization process. We find that the independence relationships of latent continuous variables can be established by addressing an over-identifying restriction problem with Generalized Method of Moments (GMM). Based on this insight, we derive an appropriate test statistic and establish its asymptotic distribution correctly reflecting CI by leveraging nodewise regression. Theoretical findings and Empirical results across various datasets demonstrate that the superiority and effectiveness of our proposed test. Our code implementation is provided in https://github.com/boyangaaaaa/DCT
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.08747v1-abstract-full').style.display = 'none'; document.getElementById('2506.08747v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.08292">arXiv:2506.08292</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.08292">pdf</a>, <a href="https://arxiv.org/ps/2506.08292">ps</a>, <a href="https://arxiv.org/format/2506.08292">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        From Debate to Equilibrium: Belief-Driven Multi-Agent LLM Reasoning via Bayesian Nash Equilibrium
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yi%2C+X">Xie Yi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+Z">Zhanke Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+C">Chentao Cao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Niu%2C+Q">Qiyu Niu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+B">Bo Han</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.08292v1-abstract-short" style="display: inline;">
        Multi-agent frameworks can substantially boost the reasoning power of large language models (LLMs), but they typically incur heavy computational costs and lack convergence guarantees. To overcome these challenges, we recast multi-LLM coordination as an incomplete-information game and seek a Bayesian Nash equilibrium (BNE), in which each agent optimally responds to its probabilistic beliefs about t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.08292v1-abstract-full').style.display = 'inline'; document.getElementById('2506.08292v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.08292v1-abstract-full" style="display: none;">
        Multi-agent frameworks can substantially boost the reasoning power of large language models (LLMs), but they typically incur heavy computational costs and lack convergence guarantees. To overcome these challenges, we recast multi-LLM coordination as an incomplete-information game and seek a Bayesian Nash equilibrium (BNE), in which each agent optimally responds to its probabilistic beliefs about the strategies of others. We introduce Efficient Coordination via Nash Equilibrium (ECON), a hierarchical reinforcement-learning paradigm that marries distributed reasoning with centralized final output. Under ECON, each LLM independently selects responses that maximize its expected reward, conditioned on its beliefs about co-agents, without requiring costly inter-agent exchanges. We mathematically prove that ECON attains a markedly tighter regret bound than non-equilibrium multi-agent schemes. Empirically, ECON outperforms existing multi-LLM approaches by 11.2% on average across six benchmarks spanning complex reasoning and planning tasks. Further experiments demonstrate ECON&#39;s ability to flexibly incorporate additional models, confirming its scalability and paving the way toward larger, more powerful multi-LLM ensembles. The code is publicly available at: https://github.com/tmlr-group/ECON.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.08292v1-abstract-full').style.display = 'none'; document.getElementById('2506.08292v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by ICML 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.05957">arXiv:2506.05957</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.05957">pdf</a>, <a href="https://arxiv.org/ps/2506.05957">ps</a>, <a href="https://arxiv.org/format/2506.05957">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Pruning Spurious Subgraphs for Graph Out-of-Distribtuion Generalization
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yao%2C+T">Tianjun Yao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+H">Haoxuan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yongqiang Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+L">Le Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xing%2C+E">Eric Xing</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shen%2C+Z">Zhiqiang Shen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.05957v3-abstract-short" style="display: inline;">
        Graph Neural Networks (GNNs) often encounter significant performance degradation under distribution shifts between training and test data, hindering their applicability in real-world scenarios. Recent studies have proposed various methods to address the out-of-distribution generalization challenge, with many methods in the graph domain focusing on directly identifying an invariant subgraph that is&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.05957v3-abstract-full').style.display = 'inline'; document.getElementById('2506.05957v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.05957v3-abstract-full" style="display: none;">
        Graph Neural Networks (GNNs) often encounter significant performance degradation under distribution shifts between training and test data, hindering their applicability in real-world scenarios. Recent studies have proposed various methods to address the out-of-distribution generalization challenge, with many methods in the graph domain focusing on directly identifying an invariant subgraph that is predictive of the target label. However, we argue that identifying the edges from the invariant subgraph directly is challenging and error-prone, especially when some spurious edges exhibit strong correlations with the targets. In this paper, we propose PrunE, the first pruning-based graph OOD method that eliminates spurious edges to improve OOD generalizability. By pruning spurious edges, PrunE retains the invariant subgraph more comprehensively, which is critical for OOD generalization. Specifically, PrunE employs two regularization terms to prune spurious edges: 1) graph size constraint to exclude uninformative spurious edges, and 2) $$-probability alignment to further suppress the occurrence of spurious edges. Through theoretical analysis and extensive experiments, we show that PrunE achieves superior OOD performance and outperforms previous state-of-the-art methods significantly. Codes are available at: \href{https://github.com/tianyao-aka/PrunE-GraphOOD}{https://github.com/tianyao-aka/PrunE-GraphOOD}.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.05957v3-abstract-full').style.display = 'none'; document.getElementById('2506.05957v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 June, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 June, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Submission of ICML2025, with score 4/4/3/3</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.6
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.01657">arXiv:2506.01657</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.01657">pdf</a>, <a href="https://arxiv.org/ps/2506.01657">ps</a>, <a href="https://arxiv.org/format/2506.01657">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Quantum Physics">quant-ph</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        State Similarity in Modular Superconducting Quantum Processors with Classical Communications
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+B">Bujiao Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+C">Changrong Xie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mi%2C+P">Peng Mi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+Z">Zhiyi Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Z">Zechen Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+P">Peisheng Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+W">Wenhui Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+X">Xuandong Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+J">Jiawei Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+L">Libo Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qiu%2C+J">Jiawei Qiu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Linpeng%2C+X">Xiayu Linpeng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tao%2C+Z">Ziyu Tao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chu%2C+J">Ji Chu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+J">Ji Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+S">Song Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Niu%2C+J">Jingjing Niu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+Y">Yuxuan Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+Y">Yuxuan Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ren%2C+W">Wenhui Ren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhong%2C+Y">Youpeng Zhong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+D">Dapeng Yu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.01657v3-abstract-short" style="display: inline;">
        As quantum devices continue to scale, distributed quantum computing emerges as a promising strategy for executing large-scale tasks across modular quantum processors. A central challenge in this paradigm is verifying the correctness of computational outcomes when subcircuits are executed independently following circuit cutting. Here we propose a cross-platform fidelity estimation algorithm tailore&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.01657v3-abstract-full').style.display = 'inline'; document.getElementById('2506.01657v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.01657v3-abstract-full" style="display: none;">
        As quantum devices continue to scale, distributed quantum computing emerges as a promising strategy for executing large-scale tasks across modular quantum processors. A central challenge in this paradigm is verifying the correctness of computational outcomes when subcircuits are executed independently following circuit cutting. Here we propose a cross-platform fidelity estimation algorithm tailored for modular architectures. Our method achieves substantial reductions in sample complexity compared to previous approaches designed for single-processor systems. We experimentally implement the protocol on modular superconducting quantum processors with up to 6 qubits to verify the similarity of two 11-qubit GHZ states. Beyond verification, we show that our algorithm enables a federated quantum kernel method that preserves data privacy. As a proof of concept, we apply it to a 5-qubit quantum phase learning task using six 3-qubit modules, successfully extracting phase information with just eight training samples. These results establish a practical path for scalable verification and trustworthy quantum machine learning of modular quantum processors.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.01657v3-abstract-full').style.display = 'none'; document.getElementById('2506.01657v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 June, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 2 June, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 3 figures, 27-page appendix, reference citation typos corrected</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.00641">arXiv:2506.00641</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.00641">pdf</a>, <a href="https://arxiv.org/ps/2506.00641">ps</a>, <a href="https://arxiv.org/format/2506.00641">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AgentAuditor: Human-Level Safety and Security Evaluation for LLM Agents
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+H">Hanjun Luo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+S">Shenyu Dai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ni%2C+C">Chiming Ni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xinfeng Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+G">Guibin Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+K">Kun Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Salam%2C+H">Hanan Salam</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.00641v1-abstract-short" style="display: inline;">
        Despite the rapid advancement of LLM-based agents, the reliable evaluation of their safety and security remains a significant challenge. Existing rule-based or LLM-based evaluators often miss dangers in agents&#39; step-by-step actions, overlook subtle meanings, fail to see how small issues compound, and get confused by unclear safety or security rules. To overcome this evaluation crisis, we introduce&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.00641v1-abstract-full').style.display = 'inline'; document.getElementById('2506.00641v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.00641v1-abstract-full" style="display: none;">
        Despite the rapid advancement of LLM-based agents, the reliable evaluation of their safety and security remains a significant challenge. Existing rule-based or LLM-based evaluators often miss dangers in agents&#39; step-by-step actions, overlook subtle meanings, fail to see how small issues compound, and get confused by unclear safety or security rules. To overcome this evaluation crisis, we introduce \sys, a universal, training-free, memory-augmented reasoning framework that empowers LLM evaluators to emulate human expert evaluators. \sys constructs an experiential memory by having an LLM adaptively extract structured semantic features (e.g., scenario, risk, behavior) and generate associated chain-of-thought reasoning traces for past interactions. A multi-stage, context-aware retrieval-augmented generation process then dynamically retrieves the most relevant reasoning experiences to guide the LLM evaluator&#39;s assessment of new cases. Moreover, we developed \data, the first benchmark designed to check how well LLM-based evaluators can spot both safety risks and security threats. \data comprises \textbf{2293} meticulously annotated interaction records, covering \textbf{15} risk types across \textbf{29} application scenarios. A key feature of \data is its nuanced approach to ambiguous risk situations, employing ``Strict&#39;&#39; and ``Lenient&#39;&#39; judgment standards. Experiments demonstrate that \sys not only consistently improves the evaluation performance of LLMs across all benchmarks but also sets a new state-of-the-art in LLM-as-a-judge for agent safety and security, achieving human-level accuracy. Our work is openly openly accessible.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.00641v1-abstract-full').style.display = 'none'; document.getElementById('2506.00641v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 May, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2505.23346">arXiv:2505.23346</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2505.23346">pdf</a>, <a href="https://arxiv.org/format/2505.23346">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Beyond Optimal Transport: Model-Aligned Coupling for Flow Matching
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+Y">Yexiong Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yao%2C+Y">Yu Yao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2505.23346v1-abstract-short" style="display: inline;">
        Flow Matching (FM) is an effective framework for training a model to learn a vector field that transports samples from a source distribution to a target distribution. To train the model, early FM methods use random couplings, which often result in crossing paths and lead the model to learn non-straight trajectories that require many integration steps to generate high-quality samples. To address th&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.23346v1-abstract-full').style.display = 'inline'; document.getElementById('2505.23346v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2505.23346v1-abstract-full" style="display: none;">
        Flow Matching (FM) is an effective framework for training a model to learn a vector field that transports samples from a source distribution to a target distribution. To train the model, early FM methods use random couplings, which often result in crossing paths and lead the model to learn non-straight trajectories that require many integration steps to generate high-quality samples. To address this, recent methods adopt Optimal Transport (OT) to construct couplings by minimizing geometric distances, which helps reduce path crossings. However, we observe that such geometry-based couplings do not necessarily align with the model&#39;s preferred trajectories, making it difficult to learn the vector field induced by these couplings, which prevents the model from learning straight trajectories. Motivated by this, we propose Model-Aligned Coupling (MAC), an effective method that matches training couplings based not only on geometric distance but also on alignment with the model&#39;s preferred transport directions based on its prediction error. To avoid the time-costly match process, MAC proposes to select the top-$k$ fraction of couplings with the lowest error for training. Extensive experiments show that MAC significantly improves generation quality and efficiency in few-step settings compared to existing methods. Project page: https://yexionglin.github.io/mac
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.23346v1-abstract-full').style.display = 'none'; document.getElementById('2505.23346v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 May, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2505.20729">arXiv:2505.20729</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2505.20729">pdf</a>, <a href="https://arxiv.org/ps/2505.20729">ps</a>, <a href="https://arxiv.org/format/2505.20729">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Intern-GS: Vision Model Guided Sparse-View 3D Gaussian Splatting
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+X">Xiangyu Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+R">Runnan Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gong%2C+M">Mingming Gong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+D">Dong Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2505.20729v1-abstract-short" style="display: inline;">
        Sparse-view scene reconstruction often faces significant challenges due to the constraints imposed by limited observational data. These limitations result in incomplete information, leading to suboptimal reconstructions using existing methodologies. To address this, we present Intern-GS, a novel approach that effectively leverages rich prior knowledge from vision foundation models to enhance the p&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.20729v1-abstract-full').style.display = 'inline'; document.getElementById('2505.20729v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2505.20729v1-abstract-full" style="display: none;">
        Sparse-view scene reconstruction often faces significant challenges due to the constraints imposed by limited observational data. These limitations result in incomplete information, leading to suboptimal reconstructions using existing methodologies. To address this, we present Intern-GS, a novel approach that effectively leverages rich prior knowledge from vision foundation models to enhance the process of sparse-view Gaussian Splatting, thereby enabling high-quality scene reconstruction. Specifically, Intern-GS utilizes vision foundation models to guide both the initialization and the optimization process of 3D Gaussian splatting, effectively addressing the limitations of sparse inputs. In the initialization process, our method employs DUSt3R to generate a dense and non-redundant gaussian point cloud. This approach significantly alleviates the limitations encountered by traditional structure-from-motion (SfM) methods, which often struggle under sparse-view constraints. During the optimization process, vision foundation models predict depth and appearance for unobserved views, refining the 3D Gaussians to compensate for missing information in unseen regions. Extensive experiments demonstrate that Intern-GS achieves state-of-the-art rendering quality across diverse datasets, including both forward-facing and large-scale scenes, such as LLFF, DTU, and Tanks and Temples.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.20729v1-abstract-full').style.display = 'none'; document.getElementById('2505.20729v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 May, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2505.18672">arXiv:2505.18672</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2505.18672">pdf</a>, <a href="https://arxiv.org/ps/2505.18672">ps</a>, <a href="https://arxiv.org/format/2505.18672">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Does Representation Intervention Really Identify Desired Concepts and Elicit Alignment?
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+H">Hongzheng Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yongqiang Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qin%2C+Z">Zeyu Qin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiao%2C+C">Chaowei Xiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+K">Kun Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+B">Bo Han</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2505.18672v1-abstract-short" style="display: inline;">
        Representation intervention aims to locate and modify the representations that encode the underlying concepts in Large Language Models (LLMs) to elicit the aligned and expected behaviors. Despite the empirical success, it has never been examined whether one could locate the faithful concepts for intervention. In this work, we explore the question in safety alignment. If the interventions are faith&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.18672v1-abstract-full').style.display = 'inline'; document.getElementById('2505.18672v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2505.18672v1-abstract-full" style="display: none;">
        Representation intervention aims to locate and modify the representations that encode the underlying concepts in Large Language Models (LLMs) to elicit the aligned and expected behaviors. Despite the empirical success, it has never been examined whether one could locate the faithful concepts for intervention. In this work, we explore the question in safety alignment. If the interventions are faithful, the intervened LLMs should erase the harmful concepts and be robust to both in-distribution adversarial prompts and the out-of-distribution (OOD) jailbreaks. While it is feasible to erase harmful concepts without degrading the benign functionalities of LLMs in linear settings, we show that it is infeasible in the general non-linear setting. To tackle the issue, we propose Concept Concentration (COCA). Instead of identifying the faithful locations to intervene, COCA refractors the training data with an explicit reasoning process, which firstly identifies the potential unsafe concepts and then decides the responses. Essentially, COCA simplifies the decision boundary between harmful and benign representations, enabling more effective linear erasure. Extensive experiments with multiple representation intervention methods and model architectures demonstrate that COCA significantly reduces both in-distribution and OOD jailbreak success rates, and meanwhile maintaining strong performance on regular tasks such as math and code generation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.18672v1-abstract-full').style.display = 'none'; document.getElementById('2505.18672v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 May, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Hongzheng and Yongqiang contributed equally; project page: https://causalcoat.github.io/coca</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2505.12896">arXiv:2505.12896</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2505.12896">pdf</a>, <a href="https://arxiv.org/format/2505.12896">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        On the Thinking-Language Modeling Gap in Large Language Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+C">Chenxi Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yongqiang Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+J">James Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+B">Bo Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+K">Kun Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2505.12896v1-abstract-short" style="display: inline;">
        System 2 reasoning is one of the defining characteristics of intelligence, which requires slow and logical thinking. Human conducts System 2 reasoning via the language of thoughts that organizes the reasoning process as a causal sequence of mental language, or thoughts. Recently, it has been observed that System 2 reasoning can be elicited from Large Language Models (LLMs) pre-trained on large-sca&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.12896v1-abstract-full').style.display = 'inline'; document.getElementById('2505.12896v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2505.12896v1-abstract-full" style="display: none;">
        System 2 reasoning is one of the defining characteristics of intelligence, which requires slow and logical thinking. Human conducts System 2 reasoning via the language of thoughts that organizes the reasoning process as a causal sequence of mental language, or thoughts. Recently, it has been observed that System 2 reasoning can be elicited from Large Language Models (LLMs) pre-trained on large-scale natural languages. However, in this work, we show that there is a significant gap between the modeling of languages and thoughts. As language is primarily a tool for humans to share knowledge and thinking, modeling human language can easily absorb language biases into LLMs deviated from the chain of thoughts in minds. Furthermore, we show that the biases will mislead the eliciting of &#34;thoughts&#34; in LLMs to focus only on a biased part of the premise. To this end, we propose a new prompt technique termed Language-of-Thoughts (LoT) to demonstrate and alleviate this gap. Instead of directly eliciting the chain of thoughts from partial information, LoT instructs LLMs to adjust the order and token used for the expressions of all the relevant information. We show that the simple strategy significantly reduces the language modeling biases in LLMs and improves the performance of LLMs across a variety of reasoning tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.12896v1-abstract-full').style.display = 'none'; document.getElementById('2505.12896v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 May, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Chenxi and Yongqiang contributed equally; project page: https://causalcoat.github.io/lot.html</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2505.11953">arXiv:2505.11953</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2505.11953">pdf</a>, <a href="https://arxiv.org/ps/2505.11953">ps</a>, <a href="https://arxiv.org/format/2505.11953">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Exploring Criteria of Loss Reweighting to Enhance LLM Unlearning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+P">Puning Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Q">Qizhou Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+Z">Zhuo Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+C">Chengqi Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+B">Bo Han</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2505.11953v2-abstract-short" style="display: inline;">
        Loss reweighting has shown significant benefits for machine unlearning with large language models (LLMs). However, their exact functionalities are left unclear and the optimal strategy remains an open question, thus impeding the understanding and improvement of existing methodologies. In this paper, we identify two distinct goals of loss reweighting, namely, Saturation and Importance -- the former&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.11953v2-abstract-full').style.display = 'inline'; document.getElementById('2505.11953v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2505.11953v2-abstract-full" style="display: none;">
        Loss reweighting has shown significant benefits for machine unlearning with large language models (LLMs). However, their exact functionalities are left unclear and the optimal strategy remains an open question, thus impeding the understanding and improvement of existing methodologies. In this paper, we identify two distinct goals of loss reweighting, namely, Saturation and Importance -- the former indicates that those insufficiently optimized data should be emphasized, while the latter stresses some critical data that are most influential for loss minimization. To study their usefulness, we design specific reweighting strategies for each goal and evaluate their respective effects on unlearning. We conduct extensive empirical analyses on well-established benchmarks, and summarize some important observations as follows: (i) Saturation enhances efficacy more than importance-based reweighting, and their combination can yield additional improvements. (ii) Saturation typically allocates lower weights to data with lower likelihoods, whereas importance-based reweighting does the opposite. (iii) The efficacy of unlearning is also largely influenced by the smoothness and granularity of the weight distributions. Based on these findings, we propose SatImp, a simple reweighting method that combines the advantages of both saturation and importance. Empirical results on extensive datasets validate the efficacy of our method, potentially bridging existing research gaps and indicating directions for future research. Our code is available at https://github.com/tmlr-group/SatImp.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.11953v2-abstract-full').style.display = 'none'; document.getElementById('2505.11953v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 May, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 17 May, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2504.14177">arXiv:2504.14177</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2504.14177">pdf</a>, <a href="https://arxiv.org/format/2504.14177">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Direct Advantage Regression: Aligning LLMs with Online AI Reward
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=He%2C+L">Li He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+H">He Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wan%2C+S">Stephen Wan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+D">Dadong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yao%2C+L">Lina Yao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2504.14177v1-abstract-short" style="display: inline;">
        Online AI Feedback (OAIF) presents a promising alternative to Reinforcement Learning from Human Feedback (RLHF) by utilizing online AI preference in aligning language models (LLMs). However, the straightforward replacement of humans with AI deprives LLMs from learning more fine-grained AI supervision beyond binary signals. In this paper, we propose Direct Advantage Regression (DAR), a simple align&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2504.14177v1-abstract-full').style.display = 'inline'; document.getElementById('2504.14177v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2504.14177v1-abstract-full" style="display: none;">
        Online AI Feedback (OAIF) presents a promising alternative to Reinforcement Learning from Human Feedback (RLHF) by utilizing online AI preference in aligning language models (LLMs). However, the straightforward replacement of humans with AI deprives LLMs from learning more fine-grained AI supervision beyond binary signals. In this paper, we propose Direct Advantage Regression (DAR), a simple alignment algorithm using online AI reward to optimize policy improvement through weighted supervised fine-tuning. As an RL-free approach, DAR maintains theoretical consistency with online RLHF pipelines while significantly reducing implementation complexity and improving learning efficiency. Our empirical results underscore that AI reward is a better form of AI supervision consistently achieving higher human-AI agreement as opposed to AI preference. Additionally, evaluations using GPT-4-Turbo and MT-bench show that DAR outperforms both OAIF and online RLHF baselines.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2504.14177v1-abstract-full').style.display = 'none'; document.getElementById('2504.14177v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 April, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2504.08411">arXiv:2504.08411</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2504.08411">pdf</a>, <a href="https://arxiv.org/format/2504.08411">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Knowledge-guided Adversarial Defense for Resisting Malicious Visual Manipulation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+D">Dawei Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gang%2C+S">Suzhi Gang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+D">Decheng Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+N">Nannan Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+X">Xinbo Gao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2504.08411v2-abstract-short" style="display: inline;">
        Malicious applications of visual manipulation have raised serious threats to the security and reputation of users in many fields. To alleviate these issues, adversarial noise-based defenses have been enthusiastically studied in recent years. However, ``data-only&#34; methods tend to distort fake samples in the low-level feature space rather than the high-level semantic space, leading to limitations in&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2504.08411v2-abstract-full').style.display = 'inline'; document.getElementById('2504.08411v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2504.08411v2-abstract-full" style="display: none;">
        Malicious applications of visual manipulation have raised serious threats to the security and reputation of users in many fields. To alleviate these issues, adversarial noise-based defenses have been enthusiastically studied in recent years. However, ``data-only&#34; methods tend to distort fake samples in the low-level feature space rather than the high-level semantic space, leading to limitations in resisting malicious manipulation. Frontier research has shown that integrating knowledge in deep learning can produce reliable and generalizable solutions. Inspired by these, we propose a knowledge-guided adversarial defense (KGAD) to actively force malicious manipulation models to output semantically confusing samples. Specifically, in the process of generating adversarial noise, we focus on constructing significant semantic confusions at the domain-specific knowledge level, and exploit a metric closely related to visual perception to replace the general pixel-wise metrics. The generated adversarial noise can actively interfere with the malicious manipulation model by triggering knowledge-guided and perception-related disruptions in the fake samples. To validate the effectiveness of the proposed method, we conduct qualitative and quantitative experiments on human perception and visual quality assessment. The results on two different tasks both show that our defense provides better protection compared to state-of-the-art methods and achieves great generalizability.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2504.08411v2-abstract-full').style.display = 'none'; document.getElementById('2504.08411v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 May, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 April, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2504.01990">arXiv:2504.01990</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2504.01990">pdf</a>, <a href="https://arxiv.org/ps/2504.01990">ps</a>, <a href="https://arxiv.org/format/2504.01990">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Advances and Challenges in Foundation Agents: From Brain-Inspired Intelligence to Evolutionary, Collaborative, and Safe Systems
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+B">Bang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xinfeng Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+J">Jiayi Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jinlin Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+T">Tanjin He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hong%2C+S">Sirui Hong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+H">Hongzhang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+S">Shaokun Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+K">Kaitao Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+K">Kunlun Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+Y">Yuheng Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+S">Suyuchen Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xiaoqiang Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+Y">Yuyu Luo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jin%2C+H">Haibo Jin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+P">Peiyan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+O">Ollie Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+J">Jiaqi Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+H">Huan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+Z">Zhaoyang Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+H">Haochen Shi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+B">Boyan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+D">Dekun Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Teng%2C+F">Fengwei Teng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jia%2C+X">Xiaojun Jia</a>
      , et al. (23 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2504.01990v2-abstract-short" style="display: inline;">
        The advent of large language models (LLMs) has catalyzed a transformative shift in artificial intelligence, paving the way for advanced intelligent agents capable of sophisticated reasoning, robust perception, and versatile action across diverse domains. As these agents increasingly drive AI research and practical applications, their design, evaluation, and continuous improvement present intricate&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2504.01990v2-abstract-full').style.display = 'inline'; document.getElementById('2504.01990v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2504.01990v2-abstract-full" style="display: none;">
        The advent of large language models (LLMs) has catalyzed a transformative shift in artificial intelligence, paving the way for advanced intelligent agents capable of sophisticated reasoning, robust perception, and versatile action across diverse domains. As these agents increasingly drive AI research and practical applications, their design, evaluation, and continuous improvement present intricate, multifaceted challenges. This book provides a comprehensive overview, framing intelligent agents within modular, brain-inspired architectures that integrate principles from cognitive science, neuroscience, and computational research. We structure our exploration into four interconnected parts. First, we systematically investigate the modular foundation of intelligent agents, systematically mapping their cognitive, perceptual, and operational modules onto analogous human brain functionalities and elucidating core components such as memory, world modeling, reward processing, goal, and emotion. Second, we discuss self-enhancement and adaptive evolution mechanisms, exploring how agents autonomously refine their capabilities, adapt to dynamic environments, and achieve continual learning through automated optimization paradigms. Third, we examine multi-agent systems, investigating the collective intelligence emerging from agent interactions, cooperation, and societal structures. Finally, we address the critical imperative of building safe and beneficial AI systems, emphasizing intrinsic and extrinsic security threats, ethical alignment, robustness, and practical mitigation strategies necessary for trustworthy real-world deployment. By synthesizing modular AI architectures with insights from different disciplines, this survey identifies key research challenges and opportunities, encouraging innovations that harmonize technological advancement with meaningful societal benefit.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2504.01990v2-abstract-full').style.display = 'none'; document.getElementById('2504.01990v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 31 March, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2503.18552">arXiv:2503.18552</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2503.18552">pdf</a>, <a href="https://arxiv.org/format/2503.18552">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        EvAnimate: Event-conditioned Image-to-Video Generation for Human Animation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Qu%2C+Q">Qiang Qu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+M">Ming Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xiaoming Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2503.18552v2-abstract-short" style="display: inline;">
        Conditional human animation traditionally animates static reference images using pose-based motion cues extracted from video data. However, these video-derived cues often suffer from low temporal resolution, motion blur, and unreliable performance under challenging lighting conditions. In contrast, event cameras inherently provide robust and high temporal-resolution motion information, offering re&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.18552v2-abstract-full').style.display = 'inline'; document.getElementById('2503.18552v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2503.18552v2-abstract-full" style="display: none;">
        Conditional human animation traditionally animates static reference images using pose-based motion cues extracted from video data. However, these video-derived cues often suffer from low temporal resolution, motion blur, and unreliable performance under challenging lighting conditions. In contrast, event cameras inherently provide robust and high temporal-resolution motion information, offering resilience to motion blur, low-light environments, and exposure variations. In this paper, we propose EvAnimate, the first method leveraging event streams as robust and precise motion cues for conditional human image animation. Our approach is fully compatible with diffusion-based generative models, enabled by encoding asynchronous event data into a specialized three-channel representation with adaptive slicing rates and densities. High-quality and temporally coherent animations are achieved through a dual-branch architecture explicitly designed to exploit event-driven dynamics, significantly enhancing performance under challenging real-world conditions. Enhanced cross-subject generalization is further achieved using specialized augmentation strategies. To facilitate future research, we establish a new benchmarking, including simulated event data for training and validation, and a real-world event dataset capturing human actions under normal and challenging scenarios. The experiment results demonstrate that EvAnimate achieves high temporal fidelity and robust performance in scenarios where traditional video-derived cues fall short.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.18552v2-abstract-full').style.display = 'none'; document.getElementById('2503.18552v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 May, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 24 March, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2503.18135">arXiv:2503.18135</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2503.18135">pdf</a>, <a href="https://arxiv.org/format/2503.18135">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MLLM-For3D: Adapting Multimodal Large Language Model for 3D Reasoning Segmentation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+J">Jiaxin Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+R">Runnan Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Ziwen Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+Z">Zhengqing Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+X">Xiao He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Y">Yandong Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gong%2C+M">Mingming Gong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2503.18135v1-abstract-short" style="display: inline;">
        Reasoning segmentation aims to segment target objects in complex scenes based on human intent and spatial reasoning. While recent multimodal large language models (MLLMs) have demonstrated impressive 2D image reasoning segmentation, adapting these capabilities to 3D scenes remains underexplored. In this paper, we introduce MLLM-For3D, a simple yet effective framework that transfers knowledge from&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.18135v1-abstract-full').style.display = 'inline'; document.getElementById('2503.18135v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2503.18135v1-abstract-full" style="display: none;">
        Reasoning segmentation aims to segment target objects in complex scenes based on human intent and spatial reasoning. While recent multimodal large language models (MLLMs) have demonstrated impressive 2D image reasoning segmentation, adapting these capabilities to 3D scenes remains underexplored. In this paper, we introduce MLLM-For3D, a simple yet effective framework that transfers knowledge from 2D MLLMs to 3D scene understanding. Specifically, we utilize MLLMs to generate multi-view pseudo segmentation masks and corresponding text embeddings, then unproject 2D masks into 3D space and align them with the text embeddings. The primary challenge lies in the absence of 3D context and spatial consistency across multiple views, causing the model to hallucinate objects that do not exist and fail to target objects consistently. Training the 3D model with such irrelevant objects leads to performance degradation. To address this, we introduce a spatial consistency strategy to enforce that segmentation masks remain coherent in the 3D space, effectively capturing the geometry of the scene. Moreover, we develop a Token-for-Query approach for multimodal semantic alignment, enabling consistent identification of the same object across different views. Extensive evaluations on various challenging indoor scene benchmarks demonstrate that, even without any labeled 3D training data, MLLM-For3D outperforms existing 3D reasoning segmentation methods, effectively interpreting user intent, understanding 3D scenes, and reasoning about spatial relationships.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.18135v1-abstract-full').style.display = 'none'; document.getElementById('2503.18135v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 March, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2503.17788">arXiv:2503.17788</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2503.17788">pdf</a>, <a href="https://arxiv.org/ps/2503.17788">ps</a>, <a href="https://arxiv.org/format/2503.17788">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning to Align and Refine: A Foundation-to-Diffusion Framework for Occlusion-Robust Two-Hand Reconstruction
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+G">Gaoge Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+Y">Yongkang Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Z">Zhe Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+S">Shaoli Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2503.17788v2-abstract-short" style="display: inline;">
        Two-hand reconstruction from monocular images faces persistent challenges due to complex and dynamic hand postures and occlusions, causing significant difficulty in achieving plausible interaction alignment. Existing approaches struggle with such alignment issues, often resulting in misalignment and penetration artifacts. To tackle this, we propose a dual-stage Foundation-to-Diffusion framework th&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.17788v2-abstract-full').style.display = 'inline'; document.getElementById('2503.17788v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2503.17788v2-abstract-full" style="display: none;">
        Two-hand reconstruction from monocular images faces persistent challenges due to complex and dynamic hand postures and occlusions, causing significant difficulty in achieving plausible interaction alignment. Existing approaches struggle with such alignment issues, often resulting in misalignment and penetration artifacts. To tackle this, we propose a dual-stage Foundation-to-Diffusion framework that precisely align 2D prior guidance from vision foundation models and diffusion-based generative 3D interaction refinement to achieve occlusion-robust two-hand reconstruction. First, we introduce a lightweight fusion alignment encoder that aligns fused multimodal 2D priors like key points, segmentation maps, and depth cues from vision foundation models during training. This provides robust structured guidance, further enabling efficient inference without heavy foundation model encoders at test time while maintaining high reconstruction accuracy. Second, we implement a two-hand diffusion model explicitly trained to convert interpenetrated 3D poses into plausible, penetration-free counterparts. Through collision gradient-guided denoising, the model rectifies artifacts while preserving natural spatial relationships between hands. Extensive evaluations demonstrate that our method achieves state-of-the-art performance on InterHand2.6M, HIC, and FreiHAND datasets, significantly advancing occlusion handling and interaction robustness. Our code will be publicly released.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.17788v2-abstract-full').style.display = 'none'; document.getElementById('2503.17788v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 July, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 22 March, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2503.17486">arXiv:2503.17486</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2503.17486">pdf</a>, <a href="https://arxiv.org/format/2503.17486">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ProtoGS: Efficient and High-Quality Rendering with 3D Gaussian Prototypes
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+Z">Zhengqing Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+D">Dongting Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bian%2C+J">Jia-Wang Bian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fu%2C+H">Huan Fu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gong%2C+M">Mingming Gong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+K">Kun Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2503.17486v3-abstract-short" style="display: inline;">
        3D Gaussian Splatting (3DGS) has made significant strides in novel view synthesis but is limited by the substantial number of Gaussian primitives required, posing challenges for deployment on lightweight devices. Recent methods address this issue by compressing the storage size of densified Gaussians, yet fail to preserve rendering quality and efficiency. To overcome these limitations, we propose&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.17486v3-abstract-full').style.display = 'inline'; document.getElementById('2503.17486v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2503.17486v3-abstract-full" style="display: none;">
        3D Gaussian Splatting (3DGS) has made significant strides in novel view synthesis but is limited by the substantial number of Gaussian primitives required, posing challenges for deployment on lightweight devices. Recent methods address this issue by compressing the storage size of densified Gaussians, yet fail to preserve rendering quality and efficiency. To overcome these limitations, we propose ProtoGS to learn Gaussian prototypes to represent Gaussian primitives, significantly reducing the total Gaussian amount without sacrificing visual quality. Our method directly uses Gaussian prototypes to enable efficient rendering and leverage the resulting reconstruction loss to guide prototype learning. To further optimize memory efficiency during training, we incorporate structure-from-motion (SfM) points as anchor points to group Gaussian primitives. Gaussian prototypes are derived within each group by clustering of K-means, and both the anchor points and the prototypes are optimized jointly. Our experiments on real-world and synthetic datasets prove that we outperform existing methods, achieving a substantial reduction in the number of Gaussians, and enabling high rendering speed while maintaining or even enhancing rendering fidelity.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.17486v3-abstract-full').style.display = 'none'; document.getElementById('2503.17486v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 April, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 March, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2503.17198">arXiv:2503.17198</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2503.17198">pdf</a>, <a href="https://arxiv.org/format/2503.17198">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Jailbreaking the Non-Transferable Barrier via Test-Time Data Disguising
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xiang%2C+Y">Yongli Xiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hong%2C+Z">Ziming Hong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yao%2C+L">Lina Yao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+D">Dadong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2503.17198v1-abstract-short" style="display: inline;">
        Non-transferable learning (NTL) has been proposed to protect model intellectual property (IP) by creating a &#34;non-transferable barrier&#34; to restrict generalization from authorized to unauthorized domains. Recently, well-designed attack, which restores the unauthorized-domain performance by fine-tuning NTL models on few authorized samples, highlights the security risks of NTL-based applications. Howe&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.17198v1-abstract-full').style.display = 'inline'; document.getElementById('2503.17198v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2503.17198v1-abstract-full" style="display: none;">
        Non-transferable learning (NTL) has been proposed to protect model intellectual property (IP) by creating a &#34;non-transferable barrier&#34; to restrict generalization from authorized to unauthorized domains. Recently, well-designed attack, which restores the unauthorized-domain performance by fine-tuning NTL models on few authorized samples, highlights the security risks of NTL-based applications. However, such attack requires modifying model weights, thus being invalid in the black-box scenario. This raises a critical question: can we trust the security of NTL models deployed as black-box systems? In this work, we reveal the first loophole of black-box NTL models by proposing a novel attack method (dubbed as JailNTL) to jailbreak the non-transferable barrier through test-time data disguising. The main idea of JailNTL is to disguise unauthorized data so it can be identified as authorized by the NTL model, thereby bypassing the non-transferable barrier without modifying the NTL model weights. Specifically, JailNTL encourages unauthorized-domain disguising in two levels, including: (i) data-intrinsic disguising (DID) for eliminating domain discrepancy and preserving class-related content at the input-level, and (ii) model-guided disguising (MGD) for mitigating output-level statistics difference of the NTL model. Empirically, when attacking state-of-the-art (SOTA) NTL models in the black-box scenario, JailNTL achieves an accuracy increase of up to 55.7% in the unauthorized domain by using only 1% authorized samples, largely exceeding existing SOTA white-box attacks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.17198v1-abstract-full').style.display = 'none'; document.getElementById('2503.17198v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 March, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Code is released at https://github.com/tmllab/2025_CVPR_JailNTL</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2503.09947">arXiv:2503.09947</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2503.09947">pdf</a>, <a href="https://arxiv.org/ps/2503.09947">ps</a>, <a href="https://arxiv.org/format/2503.09947">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Identifying Trustworthiness Challenges in Deep Learning Models for Continental-Scale Water Quality Prediction
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xia%2C+X">Xiaobo Xia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+X">Xiaofeng Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Jiale Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fang%2C+K">Kuai Fang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+L">Lu Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Oymak%2C+S">Samet Oymak</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Currie%2C+W+S">William S. Currie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2503.09947v2-abstract-short" style="display: inline;">
        Water quality is foundational to environmental sustainability, ecosystem resilience, and public health. Deep learning models, particularly Long Short-Term Memory (LSTM) networks, offer transformative potential for large-scale water quality prediction and scientific insights generation. However, their widespread adoption in high-stakes decision-making, such as pollution mitigation and equitable res&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.09947v2-abstract-full').style.display = 'inline'; document.getElementById('2503.09947v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2503.09947v2-abstract-full" style="display: none;">
        Water quality is foundational to environmental sustainability, ecosystem resilience, and public health. Deep learning models, particularly Long Short-Term Memory (LSTM) networks, offer transformative potential for large-scale water quality prediction and scientific insights generation. However, their widespread adoption in high-stakes decision-making, such as pollution mitigation and equitable resource allocation, is prevented by unresolved trustworthiness challenges including fairness, uncertainty, interpretability, robustness, generalizability, and reproducibility. In this work, we present the first comprehensive evaluation of trustworthiness in a continental-scale multi-task LSTM model predicting 20 water quality variables (encompassing physical/chemical processes, geochemical weathering, and nutrient cycling) across 482 U.S. basins. Our investigation uncovers systematic patterns of model performance disparities linked to basin characteristics, the inherent complexity of biogeochemical processes, and variable predictability, emphasizing critical performance fairness concerns. We further propose methodological frameworks for quantitatively evaluating critical aspects of trustworthiness, including uncertainty, interpretability, and robustness, identifying key limitations that could challenge reliable real-world deployment. This work serves as a timely call to action for advancing trustworthy data-driven methods for water resources management and provides a pathway to offering critical insights for researchers, decision-makers, and practitioners seeking to leverage artificial intelligence (AI) responsibly in environmental management.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.09947v2-abstract-full').style.display = 'none'; document.getElementById('2503.09947v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 June, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 12 March, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2503.08650">arXiv:2503.08650</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2503.08650">pdf</a>, <a href="https://arxiv.org/format/2503.08650">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MF-VITON: High-Fidelity Mask-Free Virtual Try-On with Minimal Input
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wan%2C+Z">Zhenchen Wan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=xu%2C+Y">Yanwu xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+D">Dongting Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+W">Weilun Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+T">Tianxi Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Z">Zhaoqing Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+F">Feng Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gong%2C+M">Mingming Gong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2503.08650v1-abstract-short" style="display: inline;">
        Recent advancements in Virtual Try-On (VITON) have significantly improved image realism and garment detail preservation, driven by powerful text-to-image (T2I) diffusion models. However, existing methods often rely on user-provided masks, introducing complexity and performance degradation due to imperfect inputs, as shown in Fig.1(a). To address this, we propose a Mask-Free VITON (MF-VITON) framew&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.08650v1-abstract-full').style.display = 'inline'; document.getElementById('2503.08650v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2503.08650v1-abstract-full" style="display: none;">
        Recent advancements in Virtual Try-On (VITON) have significantly improved image realism and garment detail preservation, driven by powerful text-to-image (T2I) diffusion models. However, existing methods often rely on user-provided masks, introducing complexity and performance degradation due to imperfect inputs, as shown in Fig.1(a). To address this, we propose a Mask-Free VITON (MF-VITON) framework that achieves realistic VITON using only a single person image and a target garment, eliminating the requirement for auxiliary masks. Our approach introduces a novel two-stage pipeline: (1) We leverage existing Mask-based VITON models to synthesize a high-quality dataset. This dataset contains diverse, realistic pairs of person images and corresponding garments, augmented with varied backgrounds to mimic real-world scenarios. (2) The pre-trained Mask-based model is fine-tuned on the generated dataset, enabling garment transfer without mask dependencies. This stage simplifies the input requirements while preserving garment texture and shape fidelity. Our framework achieves state-of-the-art (SOTA) performance regarding garment transfer accuracy and visual realism. Notably, the proposed Mask-Free model significantly outperforms existing Mask-based approaches, setting a new benchmark and demonstrating a substantial lead over previous approaches. For more details, visit our project page: https://zhenchenwan.github.io/MF-VITON/.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.08650v1-abstract-full').style.display = 'none'; document.getElementById('2503.08650v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 March, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">The project page is available at: https://zhenchenwan.github.io/MF-VITON/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2503.01139">arXiv:2503.01139</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2503.01139">pdf</a>, <a href="https://arxiv.org/format/2503.01139">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Methodology">stat.ME</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Can Large Language Models Help Experimental Design for Causal Discovery?
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Junyi Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yongqiang Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+C">Chenxi Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+Q">Qianyi Cai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+B">Bo Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+K">Kun Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiong%2C+H">Hui Xiong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2503.01139v2-abstract-short" style="display: inline;">
        Designing proper experiments and selecting optimal intervention targets is a longstanding problem in scientific or causal discovery. Identifying the underlying causal structure from observational data alone is inherently difficult. Obtaining interventional data, on the other hand, is crucial to causal discovery, yet it is usually expensive and time-consuming to gather sufficient interventional dat&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.01139v2-abstract-full').style.display = 'inline'; document.getElementById('2503.01139v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2503.01139v2-abstract-full" style="display: none;">
        Designing proper experiments and selecting optimal intervention targets is a longstanding problem in scientific or causal discovery. Identifying the underlying causal structure from observational data alone is inherently difficult. Obtaining interventional data, on the other hand, is crucial to causal discovery, yet it is usually expensive and time-consuming to gather sufficient interventional data to facilitate causal discovery. Previous approaches commonly utilize uncertainty or gradient signals to determine the intervention targets. However, numerical-based approaches may yield suboptimal results due to the inaccurate estimation of the guiding signals at the beginning when with limited interventional data. In this work, we investigate a different approach, whether we can leverage Large Language Models (LLMs) to assist with the intervention targeting in causal discovery by making use of the rich world knowledge about the experimental design in LLMs. Specifically, we present Large Language Model Guided Intervention Targeting (LeGIT) -- a robust framework that effectively incorporates LLMs to augment existing numerical approaches for the intervention targeting in causal discovery. Across 4 realistic benchmark scales, LeGIT demonstrates significant improvements and robustness over existing methods and even surpasses humans, which demonstrates the usefulness of LLMs in assisting with experimental design for scientific discovery.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.01139v2-abstract-full').style.display = 'none'; document.getElementById('2503.01139v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 March, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 2 March, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2502.14604">arXiv:2502.14604</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2502.14604">pdf</a>, <a href="https://arxiv.org/format/2502.14604">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Noisy Test-Time Adaptation in Vision-Language Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+C">Chentao Cao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhong%2C+Z">Zhun Zhong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+Z">Zhanke Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+K">Kun Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+B">Bo Han</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2502.14604v2-abstract-short" style="display: inline;">
        Test-time adaptation (TTA) aims to address distribution shifts between source and target data by relying solely on target data during testing. In open-world scenarios, models often encounter noisy samples, i.e., samples outside the in-distribution (ID) label space. Leveraging the zero-shot capability of pre-trained vision-language models (VLMs), this paper introduces Zero-Shot Noisy TTA (ZS-NTTA),&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.14604v2-abstract-full').style.display = 'inline'; document.getElementById('2502.14604v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2502.14604v2-abstract-full" style="display: none;">
        Test-time adaptation (TTA) aims to address distribution shifts between source and target data by relying solely on target data during testing. In open-world scenarios, models often encounter noisy samples, i.e., samples outside the in-distribution (ID) label space. Leveraging the zero-shot capability of pre-trained vision-language models (VLMs), this paper introduces Zero-Shot Noisy TTA (ZS-NTTA), focusing on adapting the model to target data with noisy samples during test-time in a zero-shot manner. We find existing TTA methods underperform under ZS-NTTA, often lagging behind even the frozen model. We conduct comprehensive experiments to analyze this phenomenon, revealing that the negative impact of unfiltered noisy data outweighs the benefits of clean data during model updating. Also, adapting a classifier for ID classification and noise detection hampers both sub-tasks. Built on this, we propose a framework that decouples the classifier and detector, focusing on developing an individual detector while keeping the classifier frozen. Technically, we introduce the Adaptive Noise Detector (AdaND), which utilizes the frozen model&#39;s outputs as pseudo-labels to train a noise detector. To handle clean data streams, we further inject Gaussian noise during adaptation, preventing the detector from misclassifying clean samples as noisy. Beyond the ZS-NTTA, AdaND can also improve the zero-shot out-of-distribution (ZS-OOD) detection ability of VLMs. Experiments show that AdaND outperforms in both ZS-NTTA and ZS-OOD detection. On ImageNet, AdaND achieves a notable improvement of $8.32\%$ in harmonic mean accuracy ($\text{Acc}_\text{H}$) for ZS-NTTA and $9.40\%$ in FPR95 for ZS-OOD detection, compared to SOTA methods. Importantly, AdaND is computationally efficient and comparable to the model-frozen method. The code is publicly available at: https://github.com/tmlr-group/ZS-NTTA.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.14604v2-abstract-full').style.display = 'none'; document.getElementById('2502.14604v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 April, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 February, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ICLR 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2502.13593">arXiv:2502.13593</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2502.13593">pdf</a>, <a href="https://arxiv.org/format/2502.13593">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Toward Robust Non-Transferable Learning: A Survey and Benchmark
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hong%2C+Z">Ziming Hong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiang%2C+Y">Yongli Xiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2502.13593v2-abstract-short" style="display: inline;">
        Over the past decades, researchers have primarily focused on improving the generalization abilities of models, with limited attention given to regulating such generalization. However, the ability of models to generalize to unintended data (e.g., harmful or unauthorized data) can be exploited by malicious adversaries in unforeseen ways, potentially resulting in violations of model ethics. Non-trans&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.13593v2-abstract-full').style.display = 'inline'; document.getElementById('2502.13593v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2502.13593v2-abstract-full" style="display: none;">
        Over the past decades, researchers have primarily focused on improving the generalization abilities of models, with limited attention given to regulating such generalization. However, the ability of models to generalize to unintended data (e.g., harmful or unauthorized data) can be exploited by malicious adversaries in unforeseen ways, potentially resulting in violations of model ethics. Non-transferable learning (NTL), a task aimed at reshaping the generalization abilities of deep learning models, was proposed to address these challenges. While numerous methods have been proposed in this field, a comprehensive review of existing progress and a thorough analysis of current limitations remain lacking. In this paper, we bridge this gap by presenting the first comprehensive survey on NTL and introducing NTLBench, the first benchmark to evaluate NTL performance and robustness within a unified framework. Specifically, we first introduce the task settings, general framework, and criteria of NTL, followed by a summary of NTL approaches. Furthermore, we emphasize the often-overlooked issue of robustness against various attacks that can destroy the non-transferable mechanism established by NTL. Experiments conducted via NTLBench verify the limitations of existing NTL methods in robustness. Finally, we discuss the practical applications of NTL, along with its future directions and associated challenges.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.13593v2-abstract-full').style.display = 'none'; document.getElementById('2502.13593v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 March, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 February, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Code is available at https://github.com/tmllab/NTLBench</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2502.13059">arXiv:2502.13059</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2502.13059">pdf</a>, <a href="https://arxiv.org/format/2502.13059">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SimpleVQA: Multimodal Factuality Evaluation for Multimodal Large Language Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+X">Xianfu Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+W">Wei Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+S">Shiwei Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+J">Jian Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guan%2C+X">Xiangyuan Guan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+X">Xianjie Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xiang Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+G">Ge Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Jiaheng Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mai%2C+Y">Yuying Mai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zeng%2C+Y">Yutao Zeng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wen%2C+Z">Zhoufutu Wen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jin%2C+K">Ke Jin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+B">Baorui Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+W">Weixiao Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+Y">Yunhong Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+T">Tongliang Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+W">Wenhao Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zhoujun Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2502.13059v1-abstract-short" style="display: inline;">
        The increasing application of multi-modal large language models (MLLMs) across various sectors have spotlighted the essence of their output reliability and accuracy, particularly their ability to produce content grounded in factual information (e.g. common and domain-specific knowledge). In this work, we introduce SimpleVQA, the first comprehensive multi-modal benchmark to evaluate the factuality&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.13059v1-abstract-full').style.display = 'inline'; document.getElementById('2502.13059v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2502.13059v1-abstract-full" style="display: none;">
        The increasing application of multi-modal large language models (MLLMs) across various sectors have spotlighted the essence of their output reliability and accuracy, particularly their ability to produce content grounded in factual information (e.g. common and domain-specific knowledge). In this work, we introduce SimpleVQA, the first comprehensive multi-modal benchmark to evaluate the factuality ability of MLLMs to answer natural language short questions. SimpleVQA is characterized by six key features: it covers multiple tasks and multiple scenarios, ensures high quality and challenging queries, maintains static and timeless reference answers, and is straightforward to evaluate. Our approach involves categorizing visual question-answering items into 9 different tasks around objective events or common knowledge and situating these within 9 topics. Rigorous quality control processes are implemented to guarantee high-quality, concise, and clear answers, facilitating evaluation with minimal variance via an LLM-as-a-judge scoring system. Using SimpleVQA, we perform a comprehensive assessment of leading 18 MLLMs and 8 text-only LLMs, delving into their image comprehension and text generation abilities by identifying and analyzing error cases.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.13059v1-abstract-full').style.display = 'none'; document.getElementById('2502.13059v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 February, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2502.08227">arXiv:2502.08227</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2502.08227">pdf</a>, <a href="https://arxiv.org/format/2502.08227">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Enhancing Sample Selection Against Label Noise by Cutting Mislabeled Easy Examples
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yuan%2C+S">Suqin Yuan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feng%2C+L">Lei Feng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+B">Bo Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2502.08227v2-abstract-short" style="display: inline;">
        Sample selection is a prevalent approach in learning with noisy labels, aiming to identify confident samples for training. Although existing sample selection methods have achieved decent results by reducing the noise rate of the selected subset, they often overlook that not all mislabeled examples harm the model&#39;s performance equally. In this paper, we demonstrate that mislabeled examples correctl&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.08227v2-abstract-full').style.display = 'inline'; document.getElementById('2502.08227v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2502.08227v2-abstract-full" style="display: none;">
        Sample selection is a prevalent approach in learning with noisy labels, aiming to identify confident samples for training. Although existing sample selection methods have achieved decent results by reducing the noise rate of the selected subset, they often overlook that not all mislabeled examples harm the model&#39;s performance equally. In this paper, we demonstrate that mislabeled examples correctly predicted by the model early in the training process are particularly harmful to model performance. We refer to these examples as Mislabeled Easy Examples (MEEs). To address this, we propose Early Cutting, which introduces a recalibration step that employs the model&#39;s later training state to re-select the confident subset identified early in training, thereby avoiding misleading confidence from early learning and effectively filtering out MEEs. Experiments on the CIFAR, WebVision, and full ImageNet-1k datasets demonstrate that our method effectively improves sample selection and model performance by reducing MEEs.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.08227v2-abstract-full').style.display = 'none'; document.getElementById('2502.08227v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 May, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 12 February, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2502.07551">arXiv:2502.07551</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2502.07551">pdf</a>, <a href="https://arxiv.org/format/2502.07551">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Early Stopping Against Label Noise Without Validation Data
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yuan%2C+S">Suqin Yuan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feng%2C+L">Lei Feng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2502.07551v1-abstract-short" style="display: inline;">
        Early stopping methods in deep learning face the challenge of balancing the volume of training and validation data, especially in the presence of label noise. Concretely, sparing more data for validation from training data would limit the performance of the learned model, yet insufficient validation data could result in a sub-optimal selection of the desired model. In this paper, we propose a nove&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.07551v1-abstract-full').style.display = 'inline'; document.getElementById('2502.07551v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2502.07551v1-abstract-full" style="display: none;">
        Early stopping methods in deep learning face the challenge of balancing the volume of training and validation data, especially in the presence of label noise. Concretely, sparing more data for validation from training data would limit the performance of the learned model, yet insufficient validation data could result in a sub-optimal selection of the desired model. In this paper, we propose a novel early stopping method called Label Wave, which does not require validation data for selecting the desired model in the presence of label noise. It works by tracking the changes in the model&#39;s predictions on the training set during the training process, aiming to halt training before the model unduly fits mislabeled data. This method is empirically supported by our observation that minimum fluctuations in predictions typically occur at the training epoch before the model excessively fits mislabeled data. Through extensive experiments, we show both the effectiveness of the Label Wave method across various settings and its capability to enhance the performance of existing methods for learning with noisy labels.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.07551v1-abstract-full').style.display = 'none'; document.getElementById('2502.07551v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 February, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by ICLR 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2502.07547">arXiv:2502.07547</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2502.07547">pdf</a>, <a href="https://arxiv.org/format/2502.07547">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Instance-dependent Early Stopping
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yuan%2C+S">Suqin Yuan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+R">Runqi Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feng%2C+L">Lei Feng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+B">Bo Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2502.07547v1-abstract-short" style="display: inline;">
        In machine learning practice, early stopping has been widely used to regularize models and can save computational costs by halting the training process when the model&#39;s performance on a validation set stops improving. However, conventional early stopping applies the same stopping criterion to all instances without considering their individual learning statuses, which leads to redundant computation&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.07547v1-abstract-full').style.display = 'inline'; document.getElementById('2502.07547v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2502.07547v1-abstract-full" style="display: none;">
        In machine learning practice, early stopping has been widely used to regularize models and can save computational costs by halting the training process when the model&#39;s performance on a validation set stops improving. However, conventional early stopping applies the same stopping criterion to all instances without considering their individual learning statuses, which leads to redundant computations on instances that are already well-learned. To further improve the efficiency, we propose an Instance-dependent Early Stopping (IES) method that adapts the early stopping mechanism from the entire training set to the instance level, based on the core principle that once the model has mastered an instance, the training on it should stop. IES considers an instance as mastered if the second-order differences of its loss value remain within a small range around zero. This offers a more consistent measure of an instance&#39;s learning status compared with directly using the loss value, and thus allows for a unified threshold to determine when an instance can be excluded from further backpropagation. We show that excluding mastered instances from backpropagation can increase the gradient norms, thereby accelerating the decrease of the training loss and speeding up the training process. Extensive experiments on benchmarks demonstrate that IES method can reduce backpropagation instances by 10%-50% while maintaining or even slightly improving the test accuracy and transfer learning performance of a model.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.07547v1-abstract-full').style.display = 'none'; document.getElementById('2502.07547v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 February, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by ICLR 2025 (Spotlight)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2502.05206">arXiv:2502.05206</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2502.05206">pdf</a>, <a href="https://arxiv.org/ps/2502.05206">ps</a>, <a href="https://arxiv.org/format/2502.05206">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Safety at Scale: A Comprehensive Survey of Large Model and Agent Safety
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+X">Xingjun Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+Y">Yifeng Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yixu Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+R">Ruofan Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xin Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+Y">Ye Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ding%2C+Y">Yifan Ding</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+H">Hengyuan Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yunhao Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+Y">Yunhan Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+H">Hanxun Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yige Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+Y">Yutao Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+J">Jiaming Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+X">Xiang Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+Y">Yang Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+Z">Zuxuan Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qiu%2C+X">Xipeng Qiu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+J">Jingfeng Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yiming Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+X">Xudong Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+H">Haonan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+J">Jun Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+C">Cong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gu%2C+J">Jindong Gu</a>
      , et al. (23 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2502.05206v5-abstract-short" style="display: inline;">
        The rapid advancement of large models, driven by their exceptional abilities in learning and generalization through large-scale pre-training, has reshaped the landscape of Artificial Intelligence (AI). These models are now foundational to a wide range of applications, including conversational AI, recommendation systems, autonomous driving, content generation, medical diagnostics, and scientific di&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.05206v5-abstract-full').style.display = 'inline'; document.getElementById('2502.05206v5-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2502.05206v5-abstract-full" style="display: none;">
        The rapid advancement of large models, driven by their exceptional abilities in learning and generalization through large-scale pre-training, has reshaped the landscape of Artificial Intelligence (AI). These models are now foundational to a wide range of applications, including conversational AI, recommendation systems, autonomous driving, content generation, medical diagnostics, and scientific discovery. However, their widespread deployment also exposes them to significant safety risks, raising concerns about robustness, reliability, and ethical implications. This survey provides a systematic review of current safety research on large models, covering Vision Foundation Models (VFMs), Large Language Models (LLMs), Vision-Language Pre-training (VLP) models, Vision-Language Models (VLMs), Diffusion Models (DMs), and large-model-powered Agents. Our contributions are summarized as follows: (1) We present a comprehensive taxonomy of safety threats to these models, including adversarial attacks, data poisoning, backdoor attacks, jailbreak and prompt injection attacks, energy-latency attacks, data and model extraction attacks, and emerging agent-specific threats. (2) We review defense strategies proposed for each type of attacks if available and summarize the commonly used datasets and benchmarks for safety research. (3) Building on this, we identify and discuss the open challenges in large model safety, emphasizing the need for comprehensive safety evaluations, scalable and effective defense mechanisms, and sustainable data practices. More importantly, we highlight the necessity of collective efforts from the research community and international collaboration. Our work can serve as a useful reference for researchers and practitioners, fostering the ongoing development of comprehensive defense systems and platforms to safeguard AI models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.05206v5-abstract-full').style.display = 'none'; document.getElementById('2502.05206v5-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 2 February, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">706 papers, 60 pages, 3 figures, 14 tables; GitHub: https://github.com/xingjunm/Awesome-Large-Model-Safety</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2502.01170">arXiv:2502.01170</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2502.01170">pdf</a>, <a href="https://arxiv.org/format/2502.01170">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Label Distribution Learning with Biased Annotations by Learning Multi-Label Representation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Kou%2C+Z">Zhiqiang Kou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qin%2C+S">Si Qin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Hailin Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+M">Mingkun Xie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+S">Shuo Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jia%2C+Y">Yuheng Jia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sugiyama%2C+M">Masashi Sugiyama</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Geng%2C+X">Xin Geng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2502.01170v1-abstract-short" style="display: inline;">
        Multi-label learning (MLL) has gained attention for its ability to represent real-world data. Label Distribution Learning (LDL), an extension of MLL to learning from label distributions, faces challenges in collecting accurate label distributions. To address the issue of biased annotations, based on the low-rank assumption, existing works recover true distributions from biased observations by expl&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.01170v1-abstract-full').style.display = 'inline'; document.getElementById('2502.01170v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2502.01170v1-abstract-full" style="display: none;">
        Multi-label learning (MLL) has gained attention for its ability to represent real-world data. Label Distribution Learning (LDL), an extension of MLL to learning from label distributions, faces challenges in collecting accurate label distributions. To address the issue of biased annotations, based on the low-rank assumption, existing works recover true distributions from biased observations by exploring the label correlations. However, recent evidence shows that the label distribution tends to be full-rank, and naive apply of low-rank approximation on biased observation leads to inaccurate recovery and performance degradation. In this paper, we address the LDL with biased annotations problem from a novel perspective, where we first degenerate the soft label distribution into a hard multi-hot label and then recover the true label information for each instance. This idea stems from an insight that assigning hard multi-hot labels is often easier than assigning a soft label distribution, and it shows stronger immunity to noise disturbances, leading to smaller label bias. Moreover, assuming that the multi-label space for predicting label distributions is low-rank offers a more reasonable approach to capturing label correlations. Theoretical analysis and experiments confirm the effectiveness and robustness of our method on real-world datasets.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2502.01170v1-abstract-full').style.display = 'none'; document.getElementById('2502.01170v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 February, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2501.07834">arXiv:2501.07834</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2501.07834">pdf</a>, <a href="https://arxiv.org/format/2501.07834">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multiagent Systems">cs.MA</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Flow: Modularized Agentic Workflow Automation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Niu%2C+B">Boye Niu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+Y">Yiliao Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lian%2C+K">Kai Lian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shen%2C+Y">Yifan Shen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yao%2C+Y">Yu Yao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+K">Kun Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2501.07834v2-abstract-short" style="display: inline;">
        Multi-agent frameworks powered by large language models (LLMs) have demonstrated great success in automated planning and task execution. However, the effective adjustment of agentic workflows during execution has not been well studied. An effective workflow adjustment is crucial in real-world scenarios, as the initial plan must adjust to unforeseen challenges and changing conditions in real time t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2501.07834v2-abstract-full').style.display = 'inline'; document.getElementById('2501.07834v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2501.07834v2-abstract-full" style="display: none;">
        Multi-agent frameworks powered by large language models (LLMs) have demonstrated great success in automated planning and task execution. However, the effective adjustment of agentic workflows during execution has not been well studied. An effective workflow adjustment is crucial in real-world scenarios, as the initial plan must adjust to unforeseen challenges and changing conditions in real time to ensure the efficient execution of complex tasks. In this paper, we define workflows as an activity-on-vertex (AOV) graph, which allows continuous workflow refinement by LLM agents through dynamic subtask allocation adjustment based on historical performance and previous AOVs. To further enhance framework performance, we emphasize modularity in workflow design based on evaluating parallelism and dependency complexity. With this design, our proposed multi-agent framework achieves efficient concurrent execution of subtasks, effective goal achievement, and enhanced error tolerance. Empirical results across various practical tasks demonstrate significant improvements in the efficiency of multi-agent frameworks through dynamic workflow refinement and modularization. The code is available at: https://github.com/tmllab/2025_ICLR_FLOW.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2501.07834v2-abstract-full').style.display = 'none'; document.getElementById('2501.07834v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 February, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 January, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2501.06488">arXiv:2501.06488</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2501.06488">pdf</a>, <a href="https://arxiv.org/ps/2501.06488">ps</a>, <a href="https://arxiv.org/format/2501.06488">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        NVS-SQA: Exploring Self-Supervised Quality Representation Learning for Neurally Synthesized Scenes without References
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Qu%2C+Q">Qiang Qu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shen%2C+Y">Yiran Shen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xiaoming Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chung%2C+Y+Y">Yuk Ying Chung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+W">Weidong Cai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2501.06488v2-abstract-short" style="display: inline;">
        Neural View Synthesis (NVS), such as NeRF and 3D Gaussian Splatting, effectively creates photorealistic scenes from sparse viewpoints, typically evaluated by quality assessment methods like PSNR, SSIM, and LPIPS. However, these full-reference methods, which compare synthesized views to reference views, may not fully capture the perceptual quality of neurally synthesized scenes (NSS), particularly&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2501.06488v2-abstract-full').style.display = 'inline'; document.getElementById('2501.06488v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2501.06488v2-abstract-full" style="display: none;">
        Neural View Synthesis (NVS), such as NeRF and 3D Gaussian Splatting, effectively creates photorealistic scenes from sparse viewpoints, typically evaluated by quality assessment methods like PSNR, SSIM, and LPIPS. However, these full-reference methods, which compare synthesized views to reference views, may not fully capture the perceptual quality of neurally synthesized scenes (NSS), particularly due to the limited availability of dense reference views. Furthermore, the challenges in acquiring human perceptual labels hinder the creation of extensive labeled datasets, risking model overfitting and reduced generalizability. To address these issues, we propose NVS-SQA, a NSS quality assessment method to learn no-reference quality representations through self-supervision without reliance on human labels. Traditional self-supervised learning predominantly relies on the &#34;same instance, similar representation&#34; assumption and extensive datasets. However, given that these conditions do not apply in NSS quality assessment, we employ heuristic cues and quality scores as learning objectives, along with a specialized contrastive pair preparation process to improve the effectiveness and efficiency of learning. The results show that NVS-SQA outperforms 17 no-reference methods by a large margin (i.e., on average 109.5% in SRCC, 98.6% in PLCC, and 91.5% in KRCC over the second best) and even exceeds 16 full-reference methods across all evaluation metrics (i.e., 22.9% in SRCC, 19.1% in PLCC, and 18.6% in KRCC over the second best).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2501.06488v2-abstract-full').style.display = 'none'; document.getElementById('2501.06488v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 July, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 January, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2501.00352">arXiv:2501.00352</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2501.00352">pdf</a>, <a href="https://arxiv.org/format/2501.00352">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PanoSLAM: Panoptic 3D Scene Reconstruction via Gaussian SLAM
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+R">Runnan Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Z">Zhaoqing Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jiepeng Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+Y">Yuexin Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gong%2C+M">Mingming Gong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+W">Wenping Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2501.00352v1-abstract-short" style="display: inline;">
        Understanding geometric, semantic, and instance information in 3D scenes from sequential video data is essential for applications in robotics and augmented reality. However, existing Simultaneous Localization and Mapping (SLAM) methods generally focus on either geometric or semantic reconstruction. In this paper, we introduce PanoSLAM, the first SLAM system to integrate geometric reconstruction, 3&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2501.00352v1-abstract-full').style.display = 'inline'; document.getElementById('2501.00352v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2501.00352v1-abstract-full" style="display: none;">
        Understanding geometric, semantic, and instance information in 3D scenes from sequential video data is essential for applications in robotics and augmented reality. However, existing Simultaneous Localization and Mapping (SLAM) methods generally focus on either geometric or semantic reconstruction. In this paper, we introduce PanoSLAM, the first SLAM system to integrate geometric reconstruction, 3D semantic segmentation, and 3D instance segmentation within a unified framework. Our approach builds upon 3D Gaussian Splatting, modified with several critical components to enable efficient rendering of depth, color, semantic, and instance information from arbitrary viewpoints. To achieve panoptic 3D scene reconstruction from sequential RGB-D videos, we propose an online Spatial-Temporal Lifting (STL) module that transfers 2D panoptic predictions from vision models into 3D Gaussian representations. This STL module addresses the challenges of label noise and inconsistencies in 2D predictions by refining the pseudo labels across multi-view inputs, creating a coherent 3D representation that enhances segmentation accuracy. Our experiments show that PanoSLAM outperforms recent semantic SLAM methods in both mapping and tracking accuracy. For the first time, it achieves panoptic 3D reconstruction of open-world environments directly from the RGB-D video. (https://github.com/runnanchen/PanoSLAM)
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2501.00352v1-abstract-full').style.display = 'none'; document.getElementById('2501.00352v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 December, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2501.00326">arXiv:2501.00326</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2501.00326">pdf</a>, <a href="https://arxiv.org/format/2501.00326">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        OVGaussian: Generalizable 3D Gaussian Segmentation with Open Vocabularies
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+R">Runnan Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+X">Xiangyu Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Z">Zhaoqing Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Youquan Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jiepeng Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kong%2C+L">Lingdong Kong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+J">Jiankang Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gong%2C+M">Mingming Gong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pan%2C+L">Liang Pan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+W">Wenping Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2501.00326v1-abstract-short" style="display: inline;">
        Open-vocabulary scene understanding using 3D Gaussian (3DGS) representations has garnered considerable attention. However, existing methods mostly lift knowledge from large 2D vision models into 3DGS on a scene-by-scene basis, restricting the capabilities of open-vocabulary querying within their training scenes so that lacking the generalizability to novel scenes. In this work, we propose \textbf{&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2501.00326v1-abstract-full').style.display = 'inline'; document.getElementById('2501.00326v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2501.00326v1-abstract-full" style="display: none;">
        Open-vocabulary scene understanding using 3D Gaussian (3DGS) representations has garnered considerable attention. However, existing methods mostly lift knowledge from large 2D vision models into 3DGS on a scene-by-scene basis, restricting the capabilities of open-vocabulary querying within their training scenes so that lacking the generalizability to novel scenes. In this work, we propose \textbf{OVGaussian}, a generalizable \textbf{O}pen-\textbf{V}ocabulary 3D semantic segmentation framework based on the 3D \textbf{Gaussian} representation. We first construct a large-scale 3D scene dataset based on 3DGS, dubbed \textbf{SegGaussian}, which provides detailed semantic and instance annotations for both Gaussian points and multi-view images. To promote semantic generalization across scenes, we introduce Generalizable Semantic Rasterization (GSR), which leverages a 3D neural network to learn and predict the semantic property for each 3D Gaussian point, where the semantic property can be rendered as multi-view consistent 2D semantic maps. In the next, we propose a Cross-modal Consistency Learning (CCL) framework that utilizes open-vocabulary annotations of 2D images and 3D Gaussians within SegGaussian to train the 3D neural network capable of open-vocabulary semantic segmentation across Gaussian-based 3D scenes. Experimental results demonstrate that OVGaussian significantly outperforms baseline methods, exhibiting robust cross-scene, cross-domain, and novel-view generalization capabilities. Code and the SegGaussian dataset will be released. (https://github.com/runnanchen/OVGaussian).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2501.00326v1-abstract-full').style.display = 'none'; document.getElementById('2501.00326v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 December, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2412.06461">arXiv:2412.06461</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2412.06461">pdf</a>, <a href="https://arxiv.org/format/2412.06461">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Ranked from Within: Ranking Large Multimodal Models for Visual Question Answering Without Labels
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tu%2C+W">Weijie Tu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+W">Weijian Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Campbell%2C+D">Dylan Campbell</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yao%2C+Y">Yu Yao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+J">Jiyang Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gedeon%2C+T">Tom Gedeon</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2412.06461v1-abstract-short" style="display: inline;">
        As large multimodal models (LMMs) are increasingly deployed across diverse applications, the need for adaptable, real-world model ranking has become paramount. Traditional evaluation methods are largely dataset-centric, relying on fixed, labeled datasets and supervised metrics, which are resource-intensive and may lack generalizability to novel scenarios, highlighting the importance of unsupervise&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.06461v1-abstract-full').style.display = 'inline'; document.getElementById('2412.06461v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2412.06461v1-abstract-full" style="display: none;">
        As large multimodal models (LMMs) are increasingly deployed across diverse applications, the need for adaptable, real-world model ranking has become paramount. Traditional evaluation methods are largely dataset-centric, relying on fixed, labeled datasets and supervised metrics, which are resource-intensive and may lack generalizability to novel scenarios, highlighting the importance of unsupervised ranking. In this work, we explore unsupervised model ranking for LMMs by leveraging their uncertainty signals, such as softmax probabilities. We evaluate state-of-the-art LMMs (e.g., LLaVA) across visual question answering benchmarks, analyzing how uncertainty-based metrics can reflect model performance. Our findings show that uncertainty scores derived from softmax distributions provide a robust, consistent basis for ranking models across varied tasks. This finding enables the ranking of LMMs on real-world, unlabeled data for visual question answering, providing a practical approach for selecting models across diverse domains without requiring manual annotation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.06461v1-abstract-full').style.display = 'none'; document.getElementById('2412.06461v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 9 December, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2412.05897">arXiv:2412.05897</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2412.05897">pdf</a>, <a href="https://arxiv.org/format/2412.05897">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Detecting Discrepancies Between AI-Generated and Natural Images Using Uncertainty
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Nie%2C+J">Jun Nie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yonggang Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheung%2C+Y">Yiu-ming Cheung</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+B">Bo Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tian%2C+X">Xinmei Tian</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2412.05897v1-abstract-short" style="display: inline;">
        In this work, we propose a novel approach for detecting AI-generated images by leveraging predictive uncertainty to mitigate misuse and associated risks. The motivation arises from the fundamental assumption regarding the distributional discrepancy between natural and AI-generated images. The feasibility of distinguishing natural images from AI-generated ones is grounded in the distribution discre&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.05897v1-abstract-full').style.display = 'inline'; document.getElementById('2412.05897v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2412.05897v1-abstract-full" style="display: none;">
        In this work, we propose a novel approach for detecting AI-generated images by leveraging predictive uncertainty to mitigate misuse and associated risks. The motivation arises from the fundamental assumption regarding the distributional discrepancy between natural and AI-generated images. The feasibility of distinguishing natural images from AI-generated ones is grounded in the distribution discrepancy between them. Predictive uncertainty offers an effective approach for capturing distribution shifts, thereby providing insights into detecting AI-generated images. Namely, as the distribution shift between training and testing data increases, model performance typically degrades, often accompanied by increased predictive uncertainty. Therefore, we propose to employ predictive uncertainty to reflect the discrepancies between AI-generated and natural images. In this context, the challenge lies in ensuring that the model has been trained over sufficient natural images to avoid the risk of determining the distribution of natural images as that of generated images. We propose to leverage large-scale pre-trained models to calculate the uncertainty as the score for detecting AI-generated images. This leads to a simple yet effective method for detecting AI-generated images using large-scale vision models: images that induce high uncertainty are identified as AI-generated. Comprehensive experiments across multiple benchmarks demonstrate the effectiveness of our method.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.05897v1-abstract-full').style.display = 'none'; document.getElementById('2412.05897v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 December, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2412.03473">arXiv:2412.03473</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2412.03473">pdf</a>, <a href="https://arxiv.org/format/2412.03473">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        UrbanGS: Semantic-Guided Gaussian Splatting for Urban Scene Reconstruction
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Ziwen Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+J">Jiaxin Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+R">Runnan Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Che%2C+Y">Yunlong Che</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Y">Yandong Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Karray%2C+F">Fakhri Karray</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gong%2C+M">Mingming Gong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2412.03473v2-abstract-short" style="display: inline;">
        Reconstructing urban scenes is challenging due to their complex geometries and the presence of potentially dynamic objects. 3D Gaussian Splatting (3DGS)-based methods have shown strong performance, but existing approaches often incorporate manual 3D annotations to improve dynamic object modeling, which is impractical due to high labeling costs. Some methods leverage 4D Gaussian Splatting (4DGS) to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.03473v2-abstract-full').style.display = 'inline'; document.getElementById('2412.03473v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2412.03473v2-abstract-full" style="display: none;">
        Reconstructing urban scenes is challenging due to their complex geometries and the presence of potentially dynamic objects. 3D Gaussian Splatting (3DGS)-based methods have shown strong performance, but existing approaches often incorporate manual 3D annotations to improve dynamic object modeling, which is impractical due to high labeling costs. Some methods leverage 4D Gaussian Splatting (4DGS) to represent the entire scene, but they treat static and dynamic objects uniformly, leading to unnecessary updates for static elements and ultimately degrading reconstruction quality. To address these issues, we propose UrbanGS, which leverages 2D semantic maps and an existing dynamic Gaussian approach to distinguish static objects from the scene, enabling separate processing of definite static and potentially dynamic elements. Specifically, for definite static regions, we enforce global consistency to prevent unintended changes in dynamic Gaussian and introduce a K-nearest neighbor (KNN)-based regularization to improve local coherence on low-textured ground surfaces. Notably, for potentially dynamic objects, we aggregate temporal information using learnable time embeddings, allowing each Gaussian to model deformations over time. Extensive experiments on real-world datasets demonstrate that our approach outperforms state-of-the-art methods in reconstruction quality and efficiency, accurately preserving static content while capturing dynamic elements.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.03473v2-abstract-full').style.display = 'none'; document.getElementById('2412.03473v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 March, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 4 December, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2412.00452">arXiv:2412.00452</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2412.00452">pdf</a>, <a href="https://arxiv.org/format/2412.00452">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Locally, Revising Globally: Global Reviser for Federated Learning with Noisy Labels
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tian%2C+Y">Yuxin Tian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+M">Mouxing Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+Y">Yuhao Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jian Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ye%2C+Q">Qing Ye</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Niu%2C+G">Gang Niu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lv%2C+J">Jiancheng Lv</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2412.00452v1-abstract-short" style="display: inline;">
        The success of most federated learning (FL) methods heavily depends on label quality, which is often inaccessible in real-world scenarios, such as medicine, leading to the federated label-noise (F-LN) problem. In this study, we observe that the global model of FL memorizes the noisy labels slowly. Based on the observations, we propose a novel approach dubbed Global Reviser for Federated Learning w&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.00452v1-abstract-full').style.display = 'inline'; document.getElementById('2412.00452v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2412.00452v1-abstract-full" style="display: none;">
        The success of most federated learning (FL) methods heavily depends on label quality, which is often inaccessible in real-world scenarios, such as medicine, leading to the federated label-noise (F-LN) problem. In this study, we observe that the global model of FL memorizes the noisy labels slowly. Based on the observations, we propose a novel approach dubbed Global Reviser for Federated Learning with Noisy Labels (FedGR) to enhance the label-noise robustness of FL. In brief, FedGR employs three novel modules to achieve noisy label sniffing and refining, local knowledge revising, and local model regularization. Specifically, the global model is adopted to infer local data proxies for global sample selection and refine incorrect labels. To maximize the utilization of local knowledge, we leverage the global model to revise the local exponential moving average (EMA) model of each client and distill it into the clients&#39; models. Additionally, we introduce a global-to-local representation regularization to mitigate the overfitting of noisy labels. Extensive experiments on three F-LNL benchmarks against seven baseline methods demonstrate the effectiveness of the proposed FedGR.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.00452v1-abstract-full').style.display = 'none'; document.getElementById('2412.00452v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">19 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.17017">arXiv:2411.17017</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.17017">pdf</a>, <a href="https://arxiv.org/format/2411.17017">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        TED-VITON: Transformer-Empowered Diffusion Models for Virtual Try-On
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wan%2C+Z">Zhenchen Wan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+Y">Yanwu Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Z">Zhaoqing Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+F">Feng Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gong%2C+M">Mingming Gong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.17017v3-abstract-short" style="display: inline;">
        Recent advancements in Virtual Try-On (VTO) have demonstrated exceptional efficacy in generating realistic images and preserving garment details, largely attributed to the robust generative capabilities of text-to-image (T2I) diffusion backbones. However, the T2I models that underpin these methods have become outdated, thereby limiting the potential for further improvement in VTO. Additionally, cu&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.17017v3-abstract-full').style.display = 'inline'; document.getElementById('2411.17017v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.17017v3-abstract-full" style="display: none;">
        Recent advancements in Virtual Try-On (VTO) have demonstrated exceptional efficacy in generating realistic images and preserving garment details, largely attributed to the robust generative capabilities of text-to-image (T2I) diffusion backbones. However, the T2I models that underpin these methods have become outdated, thereby limiting the potential for further improvement in VTO. Additionally, current methods face notable challenges in accurately rendering text on garments without distortion and preserving fine-grained details, such as textures and material fidelity. The emergence of Diffusion Transformer (DiT) based T2I models has showcased impressive performance and offers a promising opportunity for advancing VTO. Directly applying existing VTO techniques to transformer-based T2I models is ineffective due to substantial architectural differences, which hinder their ability to fully leverage the models&#39; advanced capabilities for improved text generation. To address these challenges and unlock the full potential of DiT-based T2I models for VTO, we propose TED-VITON, a novel framework that integrates a Garment Semantic (GS) Adapter for enhancing garment-specific features, a Text Preservation Loss to ensure accurate and distortion-free text rendering, and a constraint mechanism to generate prompts by optimizing Large Language Model (LLM). These innovations enable state-of-the-art (SOTA) performance in visual quality and text fidelity, establishing a new benchmark for VTO task. Project page: https://zhenchenwan.github.io/TED-VITON/
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.17017v3-abstract-full').style.display = 'none'; document.getElementById('2411.17017v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 March, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 25 November, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project page: https://github.com/ZhenchenWan/TED-VITON</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.12440">arXiv:2411.12440</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.12440">pdf</a>, <a href="https://arxiv.org/format/2411.12440">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Beyond Gaussians: Fast and High-Fidelity 3D Splatting with Linear Kernels
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+H">Haodong Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+R">Runnan Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qu%2C+Q">Qiang Qu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Z">Zhaoqing Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xiaoming Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chung%2C+Y+Y">Yuk Ying Chung</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.12440v3-abstract-short" style="display: inline;">
        Recent advancements in 3D Gaussian Splatting (3DGS) have substantially improved novel view synthesis, enabling high-quality reconstruction and real-time rendering. However, blurring artifacts, such as floating primitives and over-reconstruction, remain challenging. Current methods address these issues by refining scene structure, enhancing geometric representations, addressing blur in training ima&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.12440v3-abstract-full').style.display = 'inline'; document.getElementById('2411.12440v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.12440v3-abstract-full" style="display: none;">
        Recent advancements in 3D Gaussian Splatting (3DGS) have substantially improved novel view synthesis, enabling high-quality reconstruction and real-time rendering. However, blurring artifacts, such as floating primitives and over-reconstruction, remain challenging. Current methods address these issues by refining scene structure, enhancing geometric representations, addressing blur in training images, improving rendering consistency, and optimizing density control, yet the role of kernel design remains underexplored. We identify the soft boundaries of Gaussian ellipsoids as one of the causes of these artifacts, limiting detail capture in high-frequency regions. To bridge this gap, we introduce 3D Linear Splatting (3DLS), which replaces Gaussian kernels with linear kernels to achieve sharper and more precise results, particularly in high-frequency regions. Through evaluations on three datasets, 3DLS demonstrates state-of-the-art fidelity and accuracy, along with a 30% FPS improvement over baseline 3DGS. The implementation will be made publicly available upon acceptance.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.12440v3-abstract-full').style.display = 'none'; document.getElementById('2411.12440v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 December, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 November, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.11505">arXiv:2411.11505</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.11505">pdf</a>, <a href="https://arxiv.org/format/2411.11505">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LaVin-DiT: Large Vision Diffusion Transformer
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Z">Zhaoqing Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xia%2C+X">Xiaobo Xia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+R">Runnan Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+D">Dongdong Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+C">Changhu Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gong%2C+M">Mingming Gong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.11505v4-abstract-short" style="display: inline;">
        This paper presents the Large Vision Diffusion Transformer (LaVin-DiT), a scalable and unified foundation model designed to tackle over 20 computer vision tasks in a generative framework. Unlike existing large vision models directly adapted from natural language processing architectures, which rely on less efficient autoregressive techniques and disrupt spatial relationships essential for vision d&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.11505v4-abstract-full').style.display = 'inline'; document.getElementById('2411.11505v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.11505v4-abstract-full" style="display: none;">
        This paper presents the Large Vision Diffusion Transformer (LaVin-DiT), a scalable and unified foundation model designed to tackle over 20 computer vision tasks in a generative framework. Unlike existing large vision models directly adapted from natural language processing architectures, which rely on less efficient autoregressive techniques and disrupt spatial relationships essential for vision data, LaVin-DiT introduces key innovations to optimize generative performance for vision tasks. First, to address the high dimensionality of visual data, we incorporate a spatial-temporal variational autoencoder that encodes data into a continuous latent space. Second, for generative modeling, we develop a joint diffusion transformer that progressively produces vision outputs. Third, for unified multi-task training, in-context learning is implemented. Input-target pairs serve as task context, which guides the diffusion transformer to align outputs with specific tasks within the latent space. During inference, a task-specific context set and test data as queries allow LaVin-DiT to generalize across tasks without fine-tuning. Trained on extensive vision datasets, the model is scaled from 0.1B to 3.4B parameters, demonstrating substantial scalability and state-of-the-art performance across diverse vision tasks. This work introduces a novel pathway for large vision foundation models, underscoring the promising potential of diffusion transformers. The code and models are available.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.11505v4-abstract-full').style.display = 'none'; document.getElementById('2411.11505v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 March, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 November, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">37 pages, 30 figures, 4 tables. Accepted by CVPR 2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2411.10023">arXiv:2411.10023</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2411.10023">pdf</a>, <a href="https://arxiv.org/format/2411.10023">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Model Inversion Attacks: A Survey of Approaches and Countermeasures
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+Z">Zhanke Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+J">Jianing Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+F">Fengfei Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xuan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+X">Xiong Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+B">Bo Han</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2411.10023v1-abstract-short" style="display: inline;">
        The success of deep neural networks has driven numerous research studies and applications from Euclidean to non-Euclidean data. However, there are increasing concerns about privacy leakage, as these networks rely on processing private data. Recently, a new type of privacy attack, the model inversion attacks (MIAs), aims to extract sensitive features of private data for training by abusing access t&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.10023v1-abstract-full').style.display = 'inline'; document.getElementById('2411.10023v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2411.10023v1-abstract-full" style="display: none;">
        The success of deep neural networks has driven numerous research studies and applications from Euclidean to non-Euclidean data. However, there are increasing concerns about privacy leakage, as these networks rely on processing private data. Recently, a new type of privacy attack, the model inversion attacks (MIAs), aims to extract sensitive features of private data for training by abusing access to a well-trained model. The effectiveness of MIAs has been demonstrated in various domains, including images, texts, and graphs. These attacks highlight the vulnerability of neural networks and raise awareness about the risk of privacy leakage within the research community. Despite the significance, there is a lack of systematic studies that provide a comprehensive overview and deeper insights into MIAs across different domains. This survey aims to summarize up-to-date MIA methods in both attacks and defenses, highlighting their contributions and limitations, underlying modeling principles, optimization challenges, and future directions. We hope this survey bridges the gap in the literature and facilitates future research in this critical area. Besides, we are maintaining a repository to keep track of relevant research at https://github.com/AndrewZhou924/Awesome-model-inversion-attack.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2411.10023v1-abstract-full').style.display = 'none'; document.getElementById('2411.10023v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 November, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">40 pages, 17 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.18472">arXiv:2410.18472</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.18472">pdf</a>, <a href="https://arxiv.org/format/2410.18472">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        What If the Input is Expanded in OOD Detection?
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+B">Boxuan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+J">Jianing Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Z">Zengmao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+B">Bo Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+B">Bo Han</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.18472v2-abstract-short" style="display: inline;">
        Out-of-distribution (OOD) detection aims to identify OOD inputs from unknown classes, which is important for the reliable deployment of machine learning models in the open world. Various scoring functions are proposed to distinguish it from in-distribution (ID) data. However, existing methods generally focus on excavating the discriminative information from a single input, which implicitly limits&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.18472v2-abstract-full').style.display = 'inline'; document.getElementById('2410.18472v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.18472v2-abstract-full" style="display: none;">
        Out-of-distribution (OOD) detection aims to identify OOD inputs from unknown classes, which is important for the reliable deployment of machine learning models in the open world. Various scoring functions are proposed to distinguish it from in-distribution (ID) data. However, existing methods generally focus on excavating the discriminative information from a single input, which implicitly limits its representation dimension. In this work, we introduce a novel perspective, i.e., employing different common corruptions on the input space, to expand that. We reveal an interesting phenomenon termed confidence mutation, where the confidence of OOD data can decrease significantly under the corruptions, while the ID data shows a higher confidence expectation considering the resistance of semantic features. Based on that, we formalize a new scoring method, namely, Confidence aVerage (CoVer), which can capture the dynamic differences by simply averaging the scores obtained from different corrupted inputs and the original ones, making the OOD and ID distributions more separable in detection tasks. Extensive experiments and analyses have been conducted to understand and verify the effectiveness of CoVer. The code is publicly available at: https://github.com/tmlr-group/CoVer.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.18472v2-abstract-full').style.display = 'none'; document.getElementById('2410.18472v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 October, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 24 October, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">accepted by NeurIPS 2024</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2410.12474">arXiv:2410.12474</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2410.12474">pdf</a>, <a href="https://arxiv.org/format/2410.12474">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Mind the Gap Between Prototypes and Images in Cross-domain Finetuning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tian%2C+H">Hongduan Tian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+F">Feng Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+Z">Zhanke Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+C">Chengqi Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+B">Bo Han</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2410.12474v2-abstract-short" style="display: inline;">
        In cross-domain few-shot classification (CFC), recent works mainly focus on adapting a simple transformation head on top of a frozen pre-trained backbone with few labeled data to project embeddings into a task-specific metric space where classification can be performed by measuring similarities between image instance and prototype representations. Technically, an assumption implicitly adopted in s&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.12474v2-abstract-full').style.display = 'inline'; document.getElementById('2410.12474v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2410.12474v2-abstract-full" style="display: none;">
        In cross-domain few-shot classification (CFC), recent works mainly focus on adapting a simple transformation head on top of a frozen pre-trained backbone with few labeled data to project embeddings into a task-specific metric space where classification can be performed by measuring similarities between image instance and prototype representations. Technically, an assumption implicitly adopted in such a framework is that the prototype and image instance embeddings share the same representation transformation. However, in this paper, we find that there naturally exists a gap, which resembles the modality gap, between the prototype and image instance embeddings extracted from the frozen pre-trained backbone, and simply applying the same transformation during the adaptation phase constrains exploring the optimal representations and shrinks the gap between prototype and image representations. To solve this problem, we propose a simple yet effective method, contrastive prototype-image adaptation (CoPA), to adapt different transformations respectively for prototypes and images similarly to CLIP by treating prototypes as text prompts. Extensive experiments on Meta-Dataset demonstrate that CoPA achieves the state-of-the-art performance more efficiently. Meanwhile, further analyses also indicate that CoPA can learn better representation clusters, enlarge the gap, and achieve minimal validation loss at the enlarged gap.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2410.12474v2-abstract-full').style.display = 'none'; document.getElementById('2410.12474v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 October, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 16 October, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2409.03801">arXiv:2409.03801</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2409.03801">pdf</a>, <a href="https://arxiv.org/format/2409.03801">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Resultant: Incremental Effectiveness on Likelihood for Unsupervised Out-of-Distribution Detection
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yewen Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+C">Chaojie Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xia%2C+X">Xiaobo Xia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+X">Xu He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=An%2C+R">Ruyi An</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+D">Dong Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=An%2C+B">Bo An</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xinrun Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2409.03801v1-abstract-short" style="display: inline;">
        Unsupervised out-of-distribution (U-OOD) detection is to identify OOD data samples with a detector trained solely on unlabeled in-distribution (ID) data. The likelihood function estimated by a deep generative model (DGM) could be a natural detector, but its performance is limited in some popular &#34;hard&#34; benchmarks, such as FashionMNIST (ID) vs. MNIST (OOD). Recent studies have developed various det&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.03801v1-abstract-full').style.display = 'inline'; document.getElementById('2409.03801v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2409.03801v1-abstract-full" style="display: none;">
        Unsupervised out-of-distribution (U-OOD) detection is to identify OOD data samples with a detector trained solely on unlabeled in-distribution (ID) data. The likelihood function estimated by a deep generative model (DGM) could be a natural detector, but its performance is limited in some popular &#34;hard&#34; benchmarks, such as FashionMNIST (ID) vs. MNIST (OOD). Recent studies have developed various detectors based on DGMs to move beyond likelihood. However, despite their success on &#34;hard&#34; benchmarks, most of them struggle to consistently surpass or match the performance of likelihood on some &#34;non-hard&#34; cases, such as SVHN (ID) vs. CIFAR10 (OOD) where likelihood could be a nearly perfect detector. Therefore, we appeal for more attention to incremental effectiveness on likelihood, i.e., whether a method could always surpass or at least match the performance of likelihood in U-OOD detection. We first investigate the likelihood of variational DGMs and find its detection performance could be improved in two directions: i) alleviating latent distribution mismatch, and ii) calibrating the dataset entropy-mutual integration. Then, we apply two techniques for each direction, specifically post-hoc prior and dataset entropy-mutual calibration. The final method, named Resultant, combines these two directions for better incremental effectiveness compared to either technique alone. Experimental results demonstrate that the Resultant could be a new state-of-the-art U-OOD detector while maintaining incremental effectiveness on likelihood in a wide range of tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2409.03801v1-abstract-full').style.display = 'none'; document.getElementById('2409.03801v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 September, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2024.
      
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?query=Tongliang+Liu&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?query=Tongliang+Liu&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
        
        <li>
          <a href="/search/?query=Tongliang+Liu&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
            class="pagination-link "
            aria-label="Page 2"
            aria-current="page">2
          </a>
        </li>
        
        <li>
          <a href="/search/?query=Tongliang+Liu&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=100"
            class="pagination-link "
            aria-label="Page 3"
            aria-current="page">3
          </a>
        </li>
        
        <li>
          <a href="/search/?query=Tongliang+Liu&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=150"
            class="pagination-link "
            aria-label="Page 4"
            aria-current="page">4
          </a>
        </li>
        
        <li>
          <a href="/search/?query=Tongliang+Liu&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=200"
            class="pagination-link "
            aria-label="Page 5"
            aria-current="page">5
          </a>
        </li>
        
        <li>
          <a href="/search/?query=Tongliang+Liu&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=250"
            class="pagination-link "
            aria-label="Page 6"
            aria-current="page">6
          </a>
        </li>
        
      
    </ul>
  </nav>
  

  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>