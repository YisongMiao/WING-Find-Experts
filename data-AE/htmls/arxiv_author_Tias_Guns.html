<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;36 of 36 results for author: <span class="mathjax">Tias Guns</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/"  aria-role="search">
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Tias Guns">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Tias+Guns&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Tias Guns">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      




<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.11365">arXiv:2508.11365</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.11365">pdf</a>, <a href="https://arxiv.org/ps/2508.11365">ps</a>, <a href="https://arxiv.org/format/2508.11365">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Minimizing Surrogate Losses for Decision-Focused Learning using Differentiable Optimization
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mandi%2C+J">Jayanta Mandi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mahmuto%C4%9Fullar%C4%B1%2C+A+%C4%B0">Ali İrfan Mahmutoğulları</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berden%2C+S">Senne Berden</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.11365v2-abstract-short" style="display: inline;">
        Decision-focused learning (DFL) trains a machine learning (ML) model to predict parameters of an optimization problem, to directly minimize decision regret, i.e., maximize decision quality. Gradient-based DFL requires computing the derivative of the solution to the optimization problem with respect to the predicted parameters. However, for many optimization problems, such as linear programs (LPs),&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.11365v2-abstract-full').style.display = 'inline'; document.getElementById('2508.11365v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.11365v2-abstract-full" style="display: none;">
        Decision-focused learning (DFL) trains a machine learning (ML) model to predict parameters of an optimization problem, to directly minimize decision regret, i.e., maximize decision quality. Gradient-based DFL requires computing the derivative of the solution to the optimization problem with respect to the predicted parameters. However, for many optimization problems, such as linear programs (LPs), the gradient of the regret with respect to the predicted parameters is zero almost everywhere. Existing gradient-based DFL approaches for LPs try to circumvent this issue in one of two ways: (a) smoothing the LP into a differentiable optimization problem by adding a quadratic regularizer and then minimizing the regret directly or (b) minimizing surrogate losses that have informative (sub)gradients. In this paper, we show that the former approach still results in zero gradients, because even after smoothing the regret remains constant across large regions of the parameter space. To address this, we propose minimizing surrogate losses -- even when a differentiable optimization layer is used and regret can be minimized directly. Our experiments demonstrate that minimizing surrogate losses allows differentiable optimization layers to achieve regret comparable to or better than surrogate-loss based DFL methods. Further, we demonstrate that this also holds for DYS-Net, a recently proposed differentiable optimization technique for LPs, that computes approximate solutions and gradients through operations that can be performed using feedforward neural network layers. Because DYS-Net executes the forward and the backward pass very efficiently, by minimizing surrogate losses using DYS-Net, we are able to attain regret on par with the state-of-the-art while reducing training time by a significant margin.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.11365v2-abstract-full').style.display = 'none'; document.getElementById('2508.11365v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 August, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.06052">arXiv:2506.06052</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.06052">pdf</a>, <a href="https://arxiv.org/format/2506.06052">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CP-Bench: Evaluating Large Language Models for Constraint Modelling
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Michailidis%2C+K">Kostis Michailidis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsouros%2C+D">Dimos Tsouros</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.06052v1-abstract-short" style="display: inline;">
        Combinatorial problems are present in a wide range of industries. Constraint Programming (CP) is a well-suited problem-solving paradigm, but its core process, namely constraint modelling, is a bottleneck for wider adoption. Aiming to alleviate this bottleneck, recent studies have explored using Large Language Models (LLMs) as modelling assistants, transforming combinatorial problem descriptions to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.06052v1-abstract-full').style.display = 'inline'; document.getElementById('2506.06052v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.06052v1-abstract-full" style="display: none;">
        Combinatorial problems are present in a wide range of industries. Constraint Programming (CP) is a well-suited problem-solving paradigm, but its core process, namely constraint modelling, is a bottleneck for wider adoption. Aiming to alleviate this bottleneck, recent studies have explored using Large Language Models (LLMs) as modelling assistants, transforming combinatorial problem descriptions to executable constraint models, similar to coding assistants. However, the existing evaluation datasets for constraint modelling are often limited to small, homogeneous, or domain-specific instances, which do not capture the diversity of real-world scenarios. This work addresses this gap by introducing CP-Bench, a novel benchmark dataset that includes a diverse set of well-known combinatorial problem classes sourced from the CP community, structured explicitly for evaluating LLM-driven CP modelling. With this dataset, and given the variety of constraint modelling frameworks, we compare and evaluate the modelling capabilities of LLMs for three distinct constraint modelling systems, which vary in abstraction level and underlying syntax: the high-level MiniZinc language and Python-based CPMpy library, and the lower-level Python interface of the OR-Tools CP-SAT solver. In order to enhance the ability of LLMs to produce valid constraint models, we systematically evaluate the use of prompt-based and inference-time compute methods adapted from existing LLM-based code generation research. Our results underscore the modelling convenience provided by Python-based frameworks, as well as the effectiveness of documentation-rich system prompts, which, augmented with repeated sampling and self-verification, achieve further improvements, reaching up to 70\% accuracy on this new, highly challenging benchmark.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.06052v1-abstract-full').style.display = 'none'; document.getElementById('2506.06052v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2505.22224">arXiv:2505.22224</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2505.22224">pdf</a>, <a href="https://arxiv.org/ps/2505.22224">ps</a>, <a href="https://arxiv.org/format/2505.22224">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Solver-Free Decision-Focused Learning for Linear Optimization Problems
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Berden%2C+S">Senne Berden</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mahmuto%C4%9Fullar%C4%B1%2C+A+%C4%B0">Ali İrfan Mahmutoğulları</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsouros%2C+D">Dimos Tsouros</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2505.22224v1-abstract-short" style="display: inline;">
        Mathematical optimization is a fundamental tool for decision-making in a wide range of applications. However, in many real-world scenarios, the parameters of the optimization problem are not known a priori and must be predicted from contextual features. This gives rise to predict-then-optimize problems, where a machine learning model predicts problem parameters that are then used to make decisions&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.22224v1-abstract-full').style.display = 'inline'; document.getElementById('2505.22224v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2505.22224v1-abstract-full" style="display: none;">
        Mathematical optimization is a fundamental tool for decision-making in a wide range of applications. However, in many real-world scenarios, the parameters of the optimization problem are not known a priori and must be predicted from contextual features. This gives rise to predict-then-optimize problems, where a machine learning model predicts problem parameters that are then used to make decisions via optimization. A growing body of work on decision-focused learning (DFL) addresses this setting by training models specifically to produce predictions that maximize downstream decision quality, rather than accuracy. While effective, DFL is computationally expensive, because it requires solving the optimization problem with the predicted parameters at each loss evaluation. In this work, we address this computational bottleneck for linear optimization problems, a common class of problems in both DFL literature and real-world applications. We propose a solver-free training method that exploits the geometric structure of linear optimization to enable efficient training with minimal degradation in solution quality. Our method is based on the insight that a solution is optimal if and only if it achieves an objective value that is at least as good as that of its adjacent vertices on the feasible polytope. Building on this, our method compares the estimated quality of the ground-truth optimal solution with that of its precomputed adjacent vertices, and uses this as loss function. Experiments demonstrate that our method significantly reduces computational cost while maintaining high decision quality.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2505.22224v1-abstract-full').style.display = 'none'; document.getElementById('2505.22224v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 May, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2503.11435">arXiv:2503.11435</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2503.11435">pdf</a>, <a href="https://arxiv.org/ps/2503.11435">ps</a>, <a href="https://arxiv.org/format/2503.11435">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Preference Elicitation for Multi-objective Combinatorial Optimization with Active Learning and Maximum Likelihood Estimation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Defresne%2C+M">Marianne Defresne</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mandi%2C+J">Jayanta Mandi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2503.11435v2-abstract-short" style="display: inline;">
        Real-life combinatorial optimization problems often involve several conflicting objectives, such as price, product quality and sustainability. A computationally-efficient way to tackle multiple objectives is to aggregate them into a single-objective function, such as a linear combination. However, defining the weights of the linear combination upfront is hard; alternatively, the use of interactive&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.11435v2-abstract-full').style.display = 'inline'; document.getElementById('2503.11435v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2503.11435v2-abstract-full" style="display: none;">
        Real-life combinatorial optimization problems often involve several conflicting objectives, such as price, product quality and sustainability. A computationally-efficient way to tackle multiple objectives is to aggregate them into a single-objective function, such as a linear combination. However, defining the weights of the linear combination upfront is hard; alternatively, the use of interactive learning methods that ask users to compare candidate solutions is highly promising. The key challenges are to generate candidates quickly, to learn an objective function that leads to high-quality solutions and to do so with few user interactions. We build upon the Constructive Preference Elicitation framework and show how each of the three properties can be improved: to increase the interaction speed we investigate using pools of (relaxed) solutions, to improve the learning we adopt Maximum Likelihood Estimation of a Bradley-Terry preference model; and to reduce the number of user interactions, we select the pair of candidates to compare with an ensemble-based acquisition function inspired from Active Learning. Our careful experimentation demonstrates each of these improvements: on a PC configuration task and a realistic multi-instance routing problem, our method selects queries faster, needs fewer queries and synthesizes higher-quality combinatorial solutions than previous CPE methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2503.11435v2-abstract-full').style.display = 'none'; document.getElementById('2503.11435v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 14 March, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 2 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2412.14950">arXiv:2412.14950</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2412.14950">pdf</a>, <a href="https://arxiv.org/format/2412.14950">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Generalizing Constraint Models in Constraint Acquisition
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tsouros%2C+D">Dimos Tsouros</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berden%2C+S">Senne Berden</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Prestwich%2C+S">Steven Prestwich</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2412.14950v1-abstract-short" style="display: inline;">
        Constraint Acquisition (CA) aims to widen the use of constraint programming by assisting users in the modeling process. However, most CA methods suffer from a significant drawback: they learn a single set of individual constraints for a specific problem instance, but cannot generalize these constraints to the parameterized constraint specifications of the problem. In this paper, we address this li&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.14950v1-abstract-full').style.display = 'inline'; document.getElementById('2412.14950v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2412.14950v1-abstract-full" style="display: none;">
        Constraint Acquisition (CA) aims to widen the use of constraint programming by assisting users in the modeling process. However, most CA methods suffer from a significant drawback: they learn a single set of individual constraints for a specific problem instance, but cannot generalize these constraints to the parameterized constraint specifications of the problem. In this paper, we address this limitation by proposing GenCon, a novel approach to learn parameterized constraint models capable of modeling varying instances of the same problem. To achieve this generalization, we make use of statistical learning techniques at the level of individual constraints. Specifically, we propose to train a classifier to predict, for any possible constraint and parameterization, whether the constraint belongs to the problem. We then show how, for some classes of classifiers, we can extract decision rules to construct interpretable constraint specifications. This enables the generation of ground constraints for any parameter instantiation. Additionally, we present a generate-and-test approach that can be used with any classifier, to generate the ground constraints on the fly. Our empirical results demonstrate that our approach achieves high accuracy and is robust to noise in the input instances.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.14950v1-abstract-full').style.display = 'none'; document.getElementById('2412.14950v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 December, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2412.13606">arXiv:2412.13606</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2412.13606">pdf</a>, <a href="https://arxiv.org/format/2412.13606">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Exploiting Symmetries in MUS Computation (Extended version)
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bleukx%2C+I">Ignace Bleukx</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Verhaeghe%2C+H">Hélène Verhaeghe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bogaerts%2C+B">Bart Bogaerts</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2412.13606v1-abstract-short" style="display: inline;">
        In eXplainable Constraint Solving (XCS), it is common to extract a Minimal Unsatisfiable Subset (MUS) from a set of unsatisfiable constraints. This helps explain to a user why a constraint specification does not admit a solution. Finding MUSes can be computationally expensive for highly symmetric problems, as many combinations of constraints need to be considered. In the traditional context of sol&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.13606v1-abstract-full').style.display = 'inline'; document.getElementById('2412.13606v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2412.13606v1-abstract-full" style="display: none;">
        In eXplainable Constraint Solving (XCS), it is common to extract a Minimal Unsatisfiable Subset (MUS) from a set of unsatisfiable constraints. This helps explain to a user why a constraint specification does not admit a solution. Finding MUSes can be computationally expensive for highly symmetric problems, as many combinations of constraints need to be considered. In the traditional context of solving satisfaction problems, symmetry has been well studied, and effective ways to detect and exploit symmetries during the search exist. However, in the setting of finding MUSes of unsatisfiable constraint programs, symmetries are understudied. In this paper, we take inspiration from existing symmetry-handling techniques and adapt well-known MUS-computation methods to exploit symmetries in the specification, speeding-up overall computation time. Our results display a significant reduction of runtime for our adapted algorithms compared to the baseline on symmetric problems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.13606v1-abstract-full').style.display = 'none'; document.getElementById('2412.13606v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 December, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at AAAI25 conference</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2412.10272">arXiv:2412.10272</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2412.10272">pdf</a>, <a href="https://arxiv.org/format/2412.10272">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Trustworthy and Explainable Decision-Making for Workforce allocation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Pov%C3%A9da%2C+G">Guillaume Povéda</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boumazouza%2C+R">Ryma Boumazouza</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Strahl%2C+A">Andreas Strahl</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hall%2C+M">Mark Hall</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Quintana-Amate%2C+S">Santiago Quintana-Amate</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Alvarez%2C+N">Nahum Alvarez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bleukx%2C+I">Ignace Bleukx</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tsouros%2C+D">Dimos Tsouros</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Verhaeghe%2C+H">Hélène Verhaeghe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2412.10272v1-abstract-short" style="display: inline;">
        In industrial contexts, effective workforce allocation is crucial for operational efficiency. This paper presents an ongoing project focused on developing a decision-making tool designed for workforce allocation, emphasising the explainability to enhance its trustworthiness. Our objective is to create a system that not only optimises the allocation of teams to scheduled tasks but also provides cle&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.10272v1-abstract-full').style.display = 'inline'; document.getElementById('2412.10272v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2412.10272v1-abstract-full" style="display: none;">
        In industrial contexts, effective workforce allocation is crucial for operational efficiency. This paper presents an ongoing project focused on developing a decision-making tool designed for workforce allocation, emphasising the explainability to enhance its trustworthiness. Our objective is to create a system that not only optimises the allocation of teams to scheduled tasks but also provides clear, understandable explanations for its decisions, particularly in cases where the problem is infeasible. By incorporating human-in-the-loop mechanisms, the tool aims to enhance user trust and facilitate interactive conflict resolution. We implemented our approach on a prototype tool/digital demonstrator intended to be evaluated on a real industrial scenario both in terms of performance and user acceptability.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2412.10272v1-abstract-full').style.display = 'none'; document.getElementById('2412.10272v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 13 December, 2024; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2024.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted for presentation at PTHG-24: The Seventh Workshop on Progress Towards the Holy Grail, part of the 30th International Conference on Principles and Practice of Constraint Programming. For more details, visit the workshop webpage: https://freuder.wordpress.com/progress-towards-the-holy-grail-workshops/pthg-24-the-seventh-workshop-on-progress-towards-the-holy-grail/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2408.06876">arXiv:2408.06876</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2408.06876">pdf</a>, <a href="https://arxiv.org/format/2408.06876">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Decision-Focused Learning to Predict Action Costs for Planning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mandi%2C+J">Jayanta Mandi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Foschini%2C+M">Marco Foschini</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Holler%2C+D">Daniel Holler</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Thiebaux%2C+S">Sylvie Thiebaux</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hoffmann%2C+J">Jorg Hoffmann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2408.06876v2-abstract-short" style="display: inline;">
        In many automated planning applications, action costs can be hard to specify. An example is the time needed to travel through a certain road segment, which depends on many factors, such as the current weather conditions. A natural way to address this issue is to learn to predict these parameters based on input features (e.g., weather forecasts) and use the predicted action costs in automated plann&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.06876v2-abstract-full').style.display = 'inline'; document.getElementById('2408.06876v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2408.06876v2-abstract-full" style="display: none;">
        In many automated planning applications, action costs can be hard to specify. An example is the time needed to travel through a certain road segment, which depends on many factors, such as the current weather conditions. A natural way to address this issue is to learn to predict these parameters based on input features (e.g., weather forecasts) and use the predicted action costs in automated planning afterward. Decision-Focused Learning (DFL) has been successful in learning to predict the parameters of combinatorial optimization problems in a way that optimizes solution quality rather than prediction quality. This approach yields better results than treating prediction and optimization as separate tasks. In this paper, we investigate for the first time the challenges of implementing DFL for automated planning in order to learn to predict the action costs. There are two main challenges to overcome: (1) planning systems are called during gradient descent learning, to solve planning problems with negative action costs, which are not supported in planning. We propose novel methods for gradient computation to avoid this issue. (2) DFL requires repeated planner calls during training, which can limit the scalability of the method. We experiment with different methods approximating the optimal plan as well as an easy-to-implement caching mechanism to speed up the learning process. As the first work that addresses DFL for automated planning, we demonstrate that the proposed gradient computation consistently yields significantly better plans than predictions aimed at minimizing prediction error; and that caching can temper the computation requirements.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2408.06876v2-abstract-full').style.display = 'none'; document.getElementById('2408.06876v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 August, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 13 August, 2024;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2024.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2312.10795">arXiv:2312.10795</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2312.10795">pdf</a>, <a href="https://arxiv.org/format/2312.10795">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning to Learn in Interactive Constraint Acquisition
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tsouros%2C+D">Dimos Tsouros</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berden%2C+S">Senne Berden</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2312.10795v1-abstract-short" style="display: inline;">
        Constraint Programming (CP) has been successfully used to model and solve complex combinatorial problems. However, modeling is often not trivial and requires expertise, which is a bottleneck to wider adoption. In Constraint Acquisition (CA), the goal is to assist the user by automatically learning the model. In (inter)active CA, this is done by interactively posting queries to the user, e.g., aski&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2312.10795v1-abstract-full').style.display = 'inline'; document.getElementById('2312.10795v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2312.10795v1-abstract-full" style="display: none;">
        Constraint Programming (CP) has been successfully used to model and solve complex combinatorial problems. However, modeling is often not trivial and requires expertise, which is a bottleneck to wider adoption. In Constraint Acquisition (CA), the goal is to assist the user by automatically learning the model. In (inter)active CA, this is done by interactively posting queries to the user, e.g., asking whether a partial solution satisfies their (unspecified) constraints or not. While interac tive CA methods learn the constraints, the learning is related to symbolic concept learning, as the goal is to learn an exact representation. However, a large number of queries is still required to learn the model, which is a major limitation. In this paper, we aim to alleviate this limitation by tightening the connection of CA and Machine Learning (ML), by, for the first time in interactive CA, exploiting statistical ML methods. We propose to use probabilistic classification models to guide interactive CA to generate more promising queries. We discuss how to train classifiers to predict whether a candidate expression from the bias is a constraint of the problem or not, using both relation-based and scope-based features. We then show how the predictions can be used in all layers of interactive CA: the query generation, the scope finding, and the lowest-level constraint finding. We experimentally evaluate our proposed methods using different classifiers and show that our methods greatly outperform the state of the art, decreasing the number of queries needed to converge by up to 72%.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2312.10795v1-abstract-full').style.display = 'none'; document.getElementById('2312.10795v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 December, 2023; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2023.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted in AAAI</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2310.17368">arXiv:2310.17368</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2310.17368">pdf</a>, <a href="https://arxiv.org/format/2310.17368">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Leveraging Contextual Information for Robustness in Vehicle Routing Problems
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mahmuto%C4%9Fullar%C4%B1%2C+A+%C4%B0">Ali İrfan Mahmutoğulları</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2310.17368v1-abstract-short" style="display: inline;">
        We investigate the benefit of using contextual information in data-driven demand predictions to solve the robust capacitated vehicle routing problem with time windows. Instead of estimating the demand distribution or its mean, we introduce contextual machine learning models that predict demand quantiles even when the number of historical observations for some or all customers is limited. We invest&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2310.17368v1-abstract-full').style.display = 'inline'; document.getElementById('2310.17368v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2310.17368v1-abstract-full" style="display: none;">
        We investigate the benefit of using contextual information in data-driven demand predictions to solve the robust capacitated vehicle routing problem with time windows. Instead of estimating the demand distribution or its mean, we introduce contextual machine learning models that predict demand quantiles even when the number of historical observations for some or all customers is limited. We investigate the use of such predicted quantiles to make routing decisions, comparing deterministic with robust optimization models. Furthermore, we evaluate the efficiency and robustness of the decisions obtained, both using exact or heuristic methods to solve the optimization models. Our extensive computational experiments show that using a robust optimization model and predicting multiple quantiles is promising when substantial historical data is available. In scenarios with a limited demand history, using a deterministic model with just a single quantile exhibits greater potential. Interestingly, our results also indicate that the use of appropriate quantile demand values within a deterministic model results in solutions with robustness levels comparable to those of robust models. This is important because, in most applications, practitioners use deterministic models as the industry standard, even in an uncertain environment. Furthermore, as they present fewer computational challenges and require only a single demand value prediction, deterministic models paired with an appropriate machine learning model hold the potential for robust decision-making.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2310.17368v1-abstract-full').style.display = 'none'; document.getElementById('2310.17368v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 October, 2023; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2023.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2308.01589">arXiv:2308.01589</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2308.01589">pdf</a>, <a href="https://arxiv.org/format/2308.01589">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Holy Grail 2.0: From Natural Language to Constraint Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tsouros%2C+D">Dimos Tsouros</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Verhaeghe%2C+H">Hélène Verhaeghe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kad%C4%B1o%C4%9Flu%2C+S">Serdar Kadıoğlu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2308.01589v1-abstract-short" style="display: inline;">
        Twenty-seven years ago, E. Freuder highlighted that &#34;Constraint programming represents one of the closest approaches computer science has yet made to the Holy Grail of programming: the user states the problem, the computer solves it&#34;. Nowadays, CP users have great modeling tools available (like Minizinc and CPMpy), allowing them to formulate the problem and then let a solver do the rest of the job&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2308.01589v1-abstract-full').style.display = 'inline'; document.getElementById('2308.01589v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2308.01589v1-abstract-full" style="display: none;">
        Twenty-seven years ago, E. Freuder highlighted that &#34;Constraint programming represents one of the closest approaches computer science has yet made to the Holy Grail of programming: the user states the problem, the computer solves it&#34;. Nowadays, CP users have great modeling tools available (like Minizinc and CPMpy), allowing them to formulate the problem and then let a solver do the rest of the job, getting closer to the stated goal. However, this still requires the CP user to know the formalism and respect it. Another significant challenge lies in the expertise required to effectively model combinatorial problems. All this limits the wider adoption of CP. In this position paper, we investigate a possible approach to leverage pre-trained Large Language Models to extract models from textual problem descriptions. More specifically, we take inspiration from the Natural Language Processing for Optimization (NL4OPT) challenge and present early results with a decomposition-based prompting approach to GPT Models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2308.01589v1-abstract-full').style.display = 'none'; document.getElementById('2308.01589v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 August, 2023; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2023.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2307.13565">arXiv:2307.13565</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2307.13565">pdf</a>, <a href="https://arxiv.org/format/2307.13565">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1613/jair.1.15320">10.1613/jair.1.15320 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Decision-Focused Learning: Foundations, State of the Art, Benchmark and Future Opportunities
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mandi%2C+J">Jayanta Mandi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kotary%2C+J">James Kotary</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berden%2C+S">Senne Berden</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mulamba%2C+M">Maxime Mulamba</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bucarey%2C+V">Victor Bucarey</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fioretto%2C+F">Ferdinando Fioretto</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2307.13565v4-abstract-short" style="display: inline;">
        Decision-focused learning (DFL) is an emerging paradigm that integrates machine learning (ML) and constrained optimization to enhance decision quality by training ML models in an end-to-end system. This approach shows significant potential to revolutionize combinatorial decision-making in real-world applications that operate under uncertainty, where estimating unknown parameters within decision mo&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2307.13565v4-abstract-full').style.display = 'inline'; document.getElementById('2307.13565v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2307.13565v4-abstract-full" style="display: none;">
        Decision-focused learning (DFL) is an emerging paradigm that integrates machine learning (ML) and constrained optimization to enhance decision quality by training ML models in an end-to-end system. This approach shows significant potential to revolutionize combinatorial decision-making in real-world applications that operate under uncertainty, where estimating unknown parameters within decision models is a major challenge. This paper presents a comprehensive review of DFL, providing an in-depth analysis of both gradient-based and gradient-free techniques used to combine ML and constrained optimization. It evaluates the strengths and limitations of these techniques and includes an extensive empirical evaluation of eleven methods across seven problems. The survey also offers insights into recent advancements and future research directions in DFL.
  Code and benchmark: https://github.com/PredOpt/predopt-benchmarks
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2307.13565v4-abstract-full').style.display = 'none'; document.getElementById('2307.13565v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 September, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 25 July, 2023;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2023.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Experimental Survey and Benchmarking</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Journal of Artificial Intelligence Research 81 (2024) 1623-1701
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2307.06126">arXiv:2307.06126</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2307.06126">pdf</a>, <a href="https://arxiv.org/format/2307.06126">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Guided Bottom-Up Interactive Constraint Acquisition
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tsouros%2C+D">Dimos Tsouros</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berden%2C+S">Senne Berden</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2307.06126v1-abstract-short" style="display: inline;">
        Constraint Acquisition (CA) systems can be used to assist in the modeling of constraint satisfaction problems. In (inter)active CA, the system is given a set of candidate constraints and posts queries to the user with the goal of finding the right constraints among the candidates. Current interactive CA algorithms suffer from at least two major bottlenecks. First, in order to converge, they requir&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2307.06126v1-abstract-full').style.display = 'inline'; document.getElementById('2307.06126v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2307.06126v1-abstract-full" style="display: none;">
        Constraint Acquisition (CA) systems can be used to assist in the modeling of constraint satisfaction problems. In (inter)active CA, the system is given a set of candidate constraints and posts queries to the user with the goal of finding the right constraints among the candidates. Current interactive CA algorithms suffer from at least two major bottlenecks. First, in order to converge, they require a large number of queries to be asked to the user. Second, they cannot handle large sets of candidate constraints, since these lead to large waiting times for the user. For this reason, the user must have fairly precise knowledge about what constraints the system should consider. In this paper, we alleviate these bottlenecks by presenting two novel methods that improve the efficiency of CA. First, we introduce a bottom-up approach named GrowAcq that reduces the maximum waiting time for the user and allows the system to handle much larger sets of candidate constraints. It also reduces the total number of queries for problems in which the target constraint network is not sparse. Second, we propose a probability-based method to guide query generation and show that it can significantly reduce the number of queries required to converge. We also propose a new technique that allows the use of openly accessible CP solvers in query generation, removing the dependency of existing methods on less well-maintained custom solvers that are not publicly available. Experimental results show that our proposed methods outperform state-of-the-art CA methods, reducing the number of queries by up to 60%. Our methods work well even in cases where the set of candidate constraints is 50 times larger than the ones commonly used in the literature.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2307.06126v1-abstract-full').style.display = 'none'; document.getElementById('2307.06126v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 July, 2023; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2023.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2307.05213">arXiv:2307.05213</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2307.05213">pdf</a>, <a href="https://arxiv.org/format/2307.05213">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Score Function Gradient Estimation to Widen the Applicability of Decision-Focused Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Silvestri%2C+M">Mattia Silvestri</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berden%2C+S">Senne Berden</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mandi%2C+J">Jayanta Mandi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mahmuto%C4%9Fullar%C4%B1%2C+A+%C4%B0">Ali İrfan Mahmutoğulları</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Amos%2C+B">Brandon Amos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lombardi%2C+M">Michele Lombardi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2307.05213v2-abstract-short" style="display: inline;">
        Many real-world optimization problems contain parameters that are unknown before deployment time, either due to stochasticity or to lack of information (e.g., demand or travel times in delivery problems). A common strategy in such cases is to estimate said parameters via machine learning (ML) models trained to minimize the prediction error, which however is not necessarily aligned with the downstr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2307.05213v2-abstract-full').style.display = 'inline'; document.getElementById('2307.05213v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2307.05213v2-abstract-full" style="display: none;">
        Many real-world optimization problems contain parameters that are unknown before deployment time, either due to stochasticity or to lack of information (e.g., demand or travel times in delivery problems). A common strategy in such cases is to estimate said parameters via machine learning (ML) models trained to minimize the prediction error, which however is not necessarily aligned with the downstream task-level error. The decision-focused learning (DFL) paradigm overcomes this limitation by training to directly minimize a task loss, e.g. regret. Since the latter has non-informative gradients for combinatorial problems, state-of-the-art DFL methods introduce surrogates and approximations that enable training. But these methods exploit specific assumptions about the problem structures (e.g., convex or linear problems, unknown parameters only in the objective function). We propose an alternative method that makes no such assumptions, it combines stochastic smoothing with score function gradient estimation which works on any task loss. This opens up the use of DFL methods to nonlinear objectives, uncertain parameters in the problem constraints, and even two-stage stochastic optimization. Experiments show that it typically requires more epochs, but that it is on par with specialized methods and performs especially well for the difficult case of problems with uncertainty in the constraints, in terms of solution quality, scalability, or both.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2307.05213v2-abstract-full').style.display = 'none'; document.getElementById('2307.05213v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 16 June, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 July, 2023;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2023.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2303.11712">arXiv:2303.11712</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2303.11712">pdf</a>, <a href="https://arxiv.org/format/2303.11712">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Logic in Computer Science">cs.LO</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1613/jair.1.14260">10.1613/jair.1.14260 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Efficiently Explaining CSPs with Unsatisfiable Subset Optimization (extended algorithms and examples)
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gamba%2C+E">Emilio Gamba</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bogaerts%2C+B">Bart Bogaerts</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2303.11712v3-abstract-short" style="display: inline;">
        We build on a recently proposed method for stepwise explaining solutions of Constraint Satisfaction Problems (CSP) in a human-understandable way. An explanation here is a sequence of simple inference steps where simplicity is quantified using a cost function. The algorithms for explanation generation rely on extracting Minimal Unsatisfiable Subsets (MUS) of a derived unsatisfiable formula, exploit&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2303.11712v3-abstract-full').style.display = 'inline'; document.getElementById('2303.11712v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2303.11712v3-abstract-full" style="display: none;">
        We build on a recently proposed method for stepwise explaining solutions of Constraint Satisfaction Problems (CSP) in a human-understandable way. An explanation here is a sequence of simple inference steps where simplicity is quantified using a cost function. The algorithms for explanation generation rely on extracting Minimal Unsatisfiable Subsets (MUS) of a derived unsatisfiable formula, exploiting a one-to-one correspondence between so-called non-redundant explanations and MUSs. However, MUS extraction algorithms do not provide any guarantee of subset minimality or optimality with respect to a given cost function. Therefore, we build on these formal foundations and tackle the main points of improvement, namely how to generate explanations efficiently that are provably optimal (with respect to the given cost metric). For that, we developed (1) a hitting set-based algorithm for finding the optimal constrained unsatisfiable subsets; (2) a method for re-using relevant information over multiple algorithm calls; and (3) methods exploiting domain-specific information to speed up the explanation sequence generation. We experimentally validated our algorithms on a large number of CSP problems. We found that our algorithms outperform the MUS approach in terms of explanation quality and computational time (on average up to 56 % faster than a standard MUS approach).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2303.11712v3-abstract-full').style.display = 'none'; document.getElementById('2303.11712v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 November, 2023; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 March, 2023;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2023.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">arXiv admin note: text overlap with arXiv:2105.11763</span>
    </p>
    

    
      <p class="comments is-size-7">
        
          <span class="has-text-black-bis has-text-weight-semibold">Report number:</span>
          78, 2023
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68T27
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          F.4.1
        
      </p>
    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Journal of Artificial Intelligence Research (JAIR) Volume 78 (2023): pages 709-746
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2301.03094">arXiv:2301.03094</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2301.03094">pdf</a>, <a href="https://arxiv.org/format/2301.03094">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Divide-Align-Conquer Strategy for Program Synthesis
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Witt%2C+J">Jonas Witt</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Duman%C4%8Di%C4%87%2C+S">Sebastijan Dumančić</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Carbon%2C+C">Claus-Christian Carbon</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2301.03094v2-abstract-short" style="display: inline;">
        A major bottleneck in search-based program synthesis is the exponentially growing search space which makes learning large programs intractable. Humans mitigate this problem by leveraging the compositional nature of the real world: In structured domains, a logical specification can often be decomposed into smaller, complementary solution programs. We show that compositional segmentation can be appl&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2301.03094v2-abstract-full').style.display = 'inline'; document.getElementById('2301.03094v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2301.03094v2-abstract-full" style="display: none;">
        A major bottleneck in search-based program synthesis is the exponentially growing search space which makes learning large programs intractable. Humans mitigate this problem by leveraging the compositional nature of the real world: In structured domains, a logical specification can often be decomposed into smaller, complementary solution programs. We show that compositional segmentation can be applied in the programming by examples setting to divide the search for large programs across multiple smaller program synthesis problems. For each example, we search for a decomposition into smaller units which maximizes the reconstruction accuracy in the output under a latent task program. A structural alignment of the constituent parts in the input and output leads to pairwise correspondences used to guide the program synthesis search. In order to align the input/output structures, we make use of the Structure-Mapping Theory (SMT), a formal model of human analogical reasoning which originated in the cognitive sciences. We show that decomposition-driven program synthesis with structural alignment outperforms Inductive Logic Programming (ILP) baselines on string transformation tasks even with minimal knowledge priors. Unlike existing methods, the predictive accuracy of our agent monotonically increases for additional examples and achieves an average time complexity of $\mathcal{O}(m)$ in the number $m$ of partial programs for highly structured domains such as strings. We extend this method to the complex setting of visual reasoning in the Abstraction and Reasoning Corpus (ARC) for which ILP methods were previously infeasible.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2301.03094v2-abstract-full').style.display = 'none'; document.getElementById('2301.03094v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 December, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 8 January, 2023;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2023.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">11 pages, 9 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2205.10157">arXiv:2205.10157</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2205.10157">pdf</a>, <a href="https://arxiv.org/ps/2205.10157">ps</a>, <a href="https://arxiv.org/format/2205.10157">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Machine Learning for Combinatorial Optimisation of Partially-Specified Problems: Regret Minimisation as a Unifying Lens
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Teso%2C+S">Stefano Teso</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bliek%2C+L">Laurens Bliek</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Borghesi%2C+A">Andrea Borghesi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lombardi%2C+M">Michele Lombardi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yorke-Smith%2C+N">Neil Yorke-Smith</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Passerini%2C+A">Andrea Passerini</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2205.10157v1-abstract-short" style="display: inline;">
        It is increasingly common to solve combinatorial optimisation problems that are partially-specified. We survey the case where the objective function or the relations between variables are not known or are only partially specified. The challenge is to learn them from available data, while taking into account a set of hard constraints that a solution must satisfy, and that solving the optimisation p&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.10157v1-abstract-full').style.display = 'inline'; document.getElementById('2205.10157v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2205.10157v1-abstract-full" style="display: none;">
        It is increasingly common to solve combinatorial optimisation problems that are partially-specified. We survey the case where the objective function or the relations between variables are not known or are only partially specified. The challenge is to learn them from available data, while taking into account a set of hard constraints that a solution must satisfy, and that solving the optimisation problem (esp. during learning) is computationally very demanding. This paper overviews four seemingly unrelated approaches, that can each be viewed as learning the objective function of a hard combinatorial optimisation problem: 1) surrogate-based optimisation, 2) empirical model learning, 3) decision-focused learning (`predict + optimise&#39;), and 4) structured-output prediction. We formalise each learning paradigm, at first in the ways commonly found in the literature, and then bring the formalisations together in a compatible way using regret. We discuss the differences and interactions between these frameworks, highlight the opportunities for cross-fertilization and survey open directions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2205.10157v1-abstract-full').style.display = 'none'; document.getElementById('2205.10157v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 May, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2201.10269">arXiv:2201.10269</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2201.10269">pdf</a>, <a href="https://arxiv.org/format/2201.10269">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Probability estimation and structured output prediction for learning preferences in last mile delivery
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Canoy%2C+R">Rocsildes Canoy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bucarey%2C+V">Victor Bucarey</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Molenbruch%2C+Y">Yves Molenbruch</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mulamba%2C+M">Maxime Mulamba</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mandi%2C+J">Jayanta Mandi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2201.10269v1-abstract-short" style="display: inline;">
        We study the problem of learning the preferences of drivers and planners in the context of last mile delivery. Given a data set containing historical decisions and delivery locations, the goal is to capture the implicit preferences of the decision-makers. We consider two ways to use the historical data: one is through a probability estimation method that learns transition probabilities between sto&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.10269v1-abstract-full').style.display = 'inline'; document.getElementById('2201.10269v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2201.10269v1-abstract-full" style="display: none;">
        We study the problem of learning the preferences of drivers and planners in the context of last mile delivery. Given a data set containing historical decisions and delivery locations, the goal is to capture the implicit preferences of the decision-makers. We consider two ways to use the historical data: one is through a probability estimation method that learns transition probabilities between stops (or zones). This is a fast and accurate method, recently studied in a VRP setting. Furthermore, we explore the use of machine learning to infer how to best balance multiple objectives such as distance, probability and penalties. Specifically, we cast the learning problem as a structured output prediction problem, where training is done by repeatedly calling the TSP solver. Another important aspect we consider is that for last-mile delivery, every address is a potential client and hence the data is very sparse. Hence, we propose a two-stage approach that first learns preferences at the zone level in order to compute a zone routing; after which a penalty-based TSP computes the stop routing. Results show that the zone transition probability estimation performs well, and that the structured output prediction learning can improve the results further. We hence showcase a successful combination of both probability estimation and machine learning, all the while using standard TSP solvers, both during learning and to compute the final solution; this means the methodology is applicable to other, real-life, TSP variants, or proprietary solvers.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2201.10269v1-abstract-full').style.display = 'none'; document.getElementById('2201.10269v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 January, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2112.03609">arXiv:2112.03609</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2112.03609">pdf</a>, <a href="https://arxiv.org/format/2112.03609">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Decision-Focused Learning: Through the Lens of Learning to Rank
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mandi%2C+J">Jayanta Mandi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bucarey%2C+V">Víctor Bucarey</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mulamba%2C+M">Maxime Mulamba</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2112.03609v4-abstract-short" style="display: inline;">
        In the last years decision-focused learning framework, also known as predict-and-optimize, have received increasing attention. In this setting, the predictions of a machine learning model are used as estimated cost coefficients in the objective function of a discrete combinatorial optimization problem for decision making. Decision-focused learning proposes to train the ML models, often neural netw&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.03609v4-abstract-full').style.display = 'inline'; document.getElementById('2112.03609v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2112.03609v4-abstract-full" style="display: none;">
        In the last years decision-focused learning framework, also known as predict-and-optimize, have received increasing attention. In this setting, the predictions of a machine learning model are used as estimated cost coefficients in the objective function of a discrete combinatorial optimization problem for decision making. Decision-focused learning proposes to train the ML models, often neural network models, by directly optimizing the quality of decisions made by the optimization solvers. Based on a recent work that proposed a noise contrastive estimation loss over a subset of the solution space, we observe that decision-focused learning can more generally be seen as a learning-to-rank problem, where the goal is to learn an objective function that ranks the feasible points correctly. This observation is independent of the optimization method used and of the form of the objective function. We develop pointwise, pairwise and listwise ranking loss functions, which can be differentiated in closed form given a subset of solutions. We empirically investigate the quality of our generic methods compared to existing decision-focused learning approaches with competitive results. Furthermore, controlling the subset of solutions allows controlling the runtime considerably, with limited effect on regret.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2112.03609v4-abstract-full').style.display = 'none'; document.getElementById('2112.03609v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 June, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 December, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted for presentation at ICML, 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2108.04578">arXiv:2108.04578</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2108.04578">pdf</a>, <a href="https://arxiv.org/format/2108.04578">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Data Driven VRP: A Neural Network Model to Learn Hidden Preferences for VRP
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mandi%2C+J">Jayanta Mandi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Canoy%2C+R">Rocsildes Canoy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bucarey%2C+V">Víctor Bucarey</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2108.04578v2-abstract-short" style="display: inline;">
        The traditional Capacitated Vehicle Routing Problem (CVRP) minimizes the total distance of the routes under the capacity constraints of the vehicles. But more often, the objective involves multiple criteria including not only the total distance of the tour but also other factors such as travel costs, travel time, and fuel consumption.Moreover, in reality, there are numerous implicit preferences in&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.04578v2-abstract-full').style.display = 'inline'; document.getElementById('2108.04578v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2108.04578v2-abstract-full" style="display: none;">
        The traditional Capacitated Vehicle Routing Problem (CVRP) minimizes the total distance of the routes under the capacity constraints of the vehicles. But more often, the objective involves multiple criteria including not only the total distance of the tour but also other factors such as travel costs, travel time, and fuel consumption.Moreover, in reality, there are numerous implicit preferences ingrained in the minds of the route planners and the drivers. Drivers, for instance, have familiarity with certain neighborhoods and knowledge of the state of roads, and often consider the best places for rest and lunch breaks. This knowledge is difficult to formulate and balance when operational routing decisions have to be made. This motivates us to learn the implicit preferences from past solutions and to incorporate these learned preferences in the optimization process. These preferences are in the form of arc probabilities, i.e., the more preferred a route is, the higher is the joint probability. The novelty of this work is the use of a neural network model to estimate the arc probabilities, which allows for additional features and automatic parameter estimation. This first requires identifying suitable features, neural architectures and loss functions, taking into account that there is typically few data available. We investigate the difference with a prior weighted Markov counting approach, and study the applicability of neural networks in this setting.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2108.04578v2-abstract-full').style.display = 'none'; document.getElementById('2108.04578v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 August, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 10 August, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2105.11763">arXiv:2105.11763</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2105.11763">pdf</a>, <a href="https://arxiv.org/format/2105.11763">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Logic in Computer Science">cs.LO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Efficiently Explaining CSPs with Unsatisfiable Subset Optimization
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gamba%2C+E">Emilio Gamba</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bogaerts%2C+B">Bart Bogaerts</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2105.11763v4-abstract-short" style="display: inline;">
        We build on a recently proposed method for explaining solutions of constraint satisfaction problems. An explanation here is a sequence of simple inference steps, where the simplicity of an inference step is measured by the number and types of constraints and facts used, and where the sequence explains all logical consequences of the problem. We build on these formal foundations and tackle two emer&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.11763v4-abstract-full').style.display = 'inline'; document.getElementById('2105.11763v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2105.11763v4-abstract-full" style="display: none;">
        We build on a recently proposed method for explaining solutions of constraint satisfaction problems. An explanation here is a sequence of simple inference steps, where the simplicity of an inference step is measured by the number and types of constraints and facts used, and where the sequence explains all logical consequences of the problem. We build on these formal foundations and tackle two emerging questions, namely how to generate explanations that are provably optimal (with respect to the given cost metric) and how to generate them efficiently. To answer these questions, we develop 1) an implicit hitting set algorithm for finding optimal unsatisfiable subsets; 2) a method to reduce multiple calls for (optimal) unsatisfiable subsets to a single call that takes constraints on the subset into account, and 3) a method for re-using relevant information over multiple calls to these algorithms. The method is also applicable to other problems that require finding cost-optimal unsatiable subsets. We specifically show that this approach can be used to effectively find sequences of optimal explanation steps for constraint satisfaction problems like logic grid puzzles.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2105.11763v4-abstract-full').style.display = 'none'; document.getElementById('2105.11763v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 July, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 25 May, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> May 2021.
      
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68T27
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          F.4.1
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2101.03936">arXiv:2101.03936</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2101.03936">pdf</a>, <a href="https://arxiv.org/format/2101.03936">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learn-n-Route: Learning implicit preferences for vehicle routing
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Canoy%2C+R">Rocsildes Canoy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bucarey%2C+V">Víctor Bucarey</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mandi%2C+J">Jayanta Mandi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2101.03936v1-abstract-short" style="display: inline;">
        We investigate a learning decision support system for vehicle routing, where the routing engine learns implicit preferences that human planners have when manually creating route plans (or routings). The goal is to use these learned subjective preferences on top of the distance-based objective criterion in vehicle routing systems. This is an alternative to the practice of distinctively formulating&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.03936v1-abstract-full').style.display = 'inline'; document.getElementById('2101.03936v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2101.03936v1-abstract-full" style="display: none;">
        We investigate a learning decision support system for vehicle routing, where the routing engine learns implicit preferences that human planners have when manually creating route plans (or routings). The goal is to use these learned subjective preferences on top of the distance-based objective criterion in vehicle routing systems. This is an alternative to the practice of distinctively formulating a custom VRP for every company with its own routing requirements. Instead, we assume the presence of past vehicle routing solutions over similar sets of customers, and learn to make similar choices. The learning approach is based on the concept of learning a Markov model, which corresponds to a probabilistic transition matrix, rather than a deterministic distance matrix. This nevertheless allows us to use existing arc routing VRP software in creating the actual routings, and to optimize over both distances and preferences at the same time. For the learning, we explore different schemes to construct the probabilistic transition matrix that can co-evolve with changing preferences over time. Our results on a use-case with a small transportation company show that our method is able to generate results that are close to the manually created solutions, without needing to characterize all constraints and sub-objectives explicitly. Even in the case of changes in the customer sets, our method is able to find solutions that are closer to the actual routings than when using only distances, and hence, solutions that require fewer manual changes when transformed into practical routings.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2101.03936v1-abstract-full').style.display = 'none'; document.getElementById('2101.03936v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 January, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">arXiv admin note: text overlap with arXiv:1909.07893</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2011.05354">arXiv:2011.05354</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2011.05354">pdf</a>, <a href="https://arxiv.org/format/2011.05354">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Contrastive Losses and Solution Caching for Predict-and-Optimize
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mulamba%2C+M">Maxime Mulamba</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mandi%2C+J">Jayanta Mandi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Diligenti%2C+M">Michelangelo Diligenti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lombardi%2C+M">Michele Lombardi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bucarey%2C+V">Victor Bucarey</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2011.05354v2-abstract-short" style="display: inline;">
        Many decision-making processes involve solving a combinatorial optimization problem with uncertain input that can be estimated from historic data. Recently, problems in this class have been successfully addressed via end-to-end learning approaches, which rely on solving one optimization problem for each training instance at every epoch. In this context, we provide two distinct contributions. First&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.05354v2-abstract-full').style.display = 'inline'; document.getElementById('2011.05354v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2011.05354v2-abstract-full" style="display: none;">
        Many decision-making processes involve solving a combinatorial optimization problem with uncertain input that can be estimated from historic data. Recently, problems in this class have been successfully addressed via end-to-end learning approaches, which rely on solving one optimization problem for each training instance at every epoch. In this context, we provide two distinct contributions. First, we use a Noise Contrastive approach to motivate a family of surrogate loss functions, based on viewing non-optimal solutions as negative examples. Second, we address a major bottleneck of all predict-and-optimize approaches, i.e. the need to frequently recompute optimal solutions at training time. This is done via a solver-agnostic solution caching scheme, and by replacing optimization calls with a lookup in the solution cache. The method is formally based on an inner approximation of the feasible space and, combined with a cache lookup strategy, provides a controllable trade-off between training time and accuracy of the loss approximation. We empirically show that even a very slow growth rate is enough to match the quality of state-of-the-art methods, at a fraction of the computational cost.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2011.05354v2-abstract-full').style.display = 'none'; document.getElementById('2011.05354v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 July, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 10 November, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at IJCAI2021</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2010.13943">arXiv:2010.13943</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2010.13943">pdf</a>, <a href="https://arxiv.org/format/2010.13943">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Interior Point Solving for LP-based prediction+optimisation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mandi%2C+J">Jayanta Mandi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2010.13943v1-abstract-short" style="display: inline;">
        Solving optimization problems is the key to decision making in many real-life analytics applications. However, the coefficients of the optimization problems are often uncertain and dependent on external factors, such as future demand or energy or stock prices. Machine learning (ML) models, especially neural networks, are increasingly being used to estimate these coefficients in a data-driven way.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.13943v1-abstract-full').style.display = 'inline'; document.getElementById('2010.13943v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2010.13943v1-abstract-full" style="display: none;">
        Solving optimization problems is the key to decision making in many real-life analytics applications. However, the coefficients of the optimization problems are often uncertain and dependent on external factors, such as future demand or energy or stock prices. Machine learning (ML) models, especially neural networks, are increasingly being used to estimate these coefficients in a data-driven way. Hence, end-to-end predict-and-optimize approaches, which consider how effective the predicted values are to solve the optimization problem, have received increasing attention. In case of integer linear programming problems, a popular approach to overcome their non-differentiabilty is to add a quadratic penalty term to the continuous relaxation, such that results from differentiating over quadratic programs can be used. Instead we investigate the use of the more principled logarithmic barrier term, as widely used in interior point solvers for linear programming. Specifically, instead of differentiating the KKT conditions, we consider the homogeneous self-dual formulation of the LP and we show the relation between the interior point step direction and corresponding gradients needed for learning. Finally our empirical experiments demonstrate our approach performs as good as if not better than the state-of-the-art QPTL (Quadratic Programming task loss) formulation of Wilder et al. and SPO approach of Elmachtoub and Grigas.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2010.13943v1-abstract-full').style.display = 'none'; document.getElementById('2010.13943v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 October, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2006.06343">arXiv:2006.06343</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2006.06343">pdf</a>, <a href="https://arxiv.org/format/2006.06343">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Logic in Computer Science">cs.LO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1016/j.artint.2021.103550">10.1016/j.artint.2021.103550 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A framework for step-wise explaining how to solve constraint satisfaction problems
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bogaerts%2C+B">Bart Bogaerts</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gamba%2C+E">Emilio Gamba</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2006.06343v1-abstract-short" style="display: inline;">
        We explore the problem of step-wise explaining how to solve constraint satisfaction problems, with a use case on logic grid puzzles. More specifically, we study the problem of explaining the inference steps that one can take during propagation, in a way that is easy to interpret for a person. Thereby, we aim to give the constraint solver explainable agency, which can help in building trust in the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.06343v1-abstract-full').style.display = 'inline'; document.getElementById('2006.06343v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2006.06343v1-abstract-full" style="display: none;">
        We explore the problem of step-wise explaining how to solve constraint satisfaction problems, with a use case on logic grid puzzles. More specifically, we study the problem of explaining the inference steps that one can take during propagation, in a way that is easy to interpret for a person. Thereby, we aim to give the constraint solver explainable agency, which can help in building trust in the solver by being able to understand and even learn from the explanations. The main challenge is that of finding a sequence of simple explanations, where each explanation should aim to be as cognitively easy as possible for a human to verify and understand. This contrasts with the arbitrary combination of facts and constraints that the solver may use when propagating. We propose the use of a cost function to quantify how simple an individual explanation of an inference step is, and identify the explanation-production problem of finding the best sequence of explanations of a CSP. Our approach is agnostic of the underlying constraint propagation mechanisms, and can provide explanations even for inference steps resulting from combinations of constraints. In case multiple constraints are involved, we also develop a mechanism that allows to break the most difficult steps up and thus gives the user the ability to zoom in on specific parts of the explanation. Our proposed algorithm iteratively constructs the explanation sequence by using an optimistic estimate of the cost function to guide the search for the best explanation at each step. Our experiments on logic grid puzzles show the feasibility of the approach in terms of the quality of the individual explanations and the resulting explanation sequences obtained.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.06343v1-abstract-full').style.display = 'none'; document.getElementById('2006.06343v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 June, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2020.
      
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68T27
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          F.4.1
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2004.09931">arXiv:2004.09931</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2004.09931">pdf</a>, <a href="https://arxiv.org/format/2004.09931">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Knowledge Refactoring for Inductive Program Synthesis
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dumancic%2C+S">Sebastijan Dumancic</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cropper%2C+A">Andrew Cropper</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2004.09931v3-abstract-short" style="display: inline;">
        Humans constantly restructure knowledge to use it more efficiently. Our goal is to give a machine learning system similar abilities so that it can learn more efficiently. We introduce the \textit{knowledge refactoring} problem, where the goal is to restructure a learner&#39;s knowledge base to reduce its size and to minimise redundancy in it. We focus on inductive logic programming, where the knowledg&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.09931v3-abstract-full').style.display = 'inline'; document.getElementById('2004.09931v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2004.09931v3-abstract-full" style="display: none;">
        Humans constantly restructure knowledge to use it more efficiently. Our goal is to give a machine learning system similar abilities so that it can learn more efficiently. We introduce the \textit{knowledge refactoring} problem, where the goal is to restructure a learner&#39;s knowledge base to reduce its size and to minimise redundancy in it. We focus on inductive logic programming, where the knowledge base is a logic program. We introduce Knorf, a system which solves the refactoring problem using constraint optimisation. We evaluate our approach on two program induction domains: real-world string transformations and building Lego structures. Our experiments show that learning from refactored knowledge can improve predictive accuracies fourfold and reduce learning times by half.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2004.09931v3-abstract-full').style.display = 'none'; document.getElementById('2004.09931v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 November, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 April, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2020.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">7 pages, 6 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2003.11001">arXiv:2003.11001</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2003.11001">pdf</a>, <a href="https://arxiv.org/format/2003.11001">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Hybrid Classification and Reasoning for Image-based Constraint Solving
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mulamba%2C+M">Maxime Mulamba</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mandi%2C+J">Jayanta Mandi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Canoy%2C+R">Rocsildes Canoy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2003.11001v1-abstract-short" style="display: inline;">
        There is an increased interest in solving complex constrained problems where part of the input is not given as facts but received as raw sensor data such as images or speech. We will use &#34;visual sudoku&#34; as a prototype problem, where the given cell digits are handwritten and provided as an image thereof. In this case, one first has to train and use a classifier to label the images, so that the labe&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.11001v1-abstract-full').style.display = 'inline'; document.getElementById('2003.11001v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2003.11001v1-abstract-full" style="display: none;">
        There is an increased interest in solving complex constrained problems where part of the input is not given as facts but received as raw sensor data such as images or speech. We will use &#34;visual sudoku&#34; as a prototype problem, where the given cell digits are handwritten and provided as an image thereof. In this case, one first has to train and use a classifier to label the images, so that the labels can be used for solving the problem. In this paper, we explore the hybridization of classifying the images with the reasoning of a constraint solver. We show that pure constraint reasoning on predictions does not give satisfactory results. Instead, we explore the possibilities of a tighter integration, by exposing the probabilistic estimates of the classifier to the constraint solver. This allows joint inference on these probabilistic estimates, where we use the solver to find the maximum likelihood solution. We explore the trade-off between the power of the classifier and the power of the constraint reasoning, as well as further integration through the additional use of structural knowledge. Furthermore, we investigate the effect of calibration of the probabilistic estimates on the reasoning. Our results show that such hybrid approaches vastly outperform a separate approach, which encourages a further integration of prediction (probabilities) and constraint solving.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.11001v1-abstract-full').style.display = 'none'; document.getElementById('2003.11001v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 March, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2002.05897">arXiv:2002.05897</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2002.05897">pdf</a>, <a href="https://arxiv.org/format/2002.05897">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning to rank for uplift modeling
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Devriendt%2C+F">Floris Devriendt</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Verbeke%2C+W">Wouter Verbeke</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2002.05897v1-abstract-short" style="display: inline;">
        Uplift modeling has effectively been used in fields such as marketing and customer retention, to target those customers that are most likely to respond due to the campaign or treatment. Uplift models produce uplift scores which are then used to essentially create a ranking. We instead investigate to learn to rank directly by looking into the potential of learning-to-rank techniques in the context&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2002.05897v1-abstract-full').style.display = 'inline'; document.getElementById('2002.05897v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2002.05897v1-abstract-full" style="display: none;">
        Uplift modeling has effectively been used in fields such as marketing and customer retention, to target those customers that are most likely to respond due to the campaign or treatment. Uplift models produce uplift scores which are then used to essentially create a ranking. We instead investigate to learn to rank directly by looking into the potential of learning-to-rank techniques in the context of uplift modeling. We propose a unified formalisation of different global uplift modeling measures in use today and explore how these can be integrated into the learning-to-rank framework. Additionally, we introduce a new metric for learning-to-rank that focusses on optimizing the area under the uplift curve called the promoted cumulative gain (PCG). We employ the learning-to-rank technique LambdaMART to optimize the ranking according to PCG and show improved results over standard learning-to-rank metrics and equal to improved results when compared with state-of-the-art uplift modeling. Finally, we show how learning-to-rank models can learn to optimize a certain targeting depth, however, these results do not generalize on the test set.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2002.05897v1-abstract-full').style.display = 'none'; document.getElementById('2002.05897v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 14 February, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1911.10092">arXiv:1911.10092</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1911.10092">pdf</a>, <a href="https://arxiv.org/format/1911.10092">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Optimization and Control">math.OC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Smart Predict-and-Optimize for Hard Combinatorial Optimization Problems
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mandi%2C+J">Jaynta Mandi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Demirovi%C4%87%2C+E">Emir Demirović</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Stuckey%2C+P+J">Peter. J Stuckey</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1911.10092v1-abstract-short" style="display: inline;">
        Combinatorial optimization assumes that all parameters of the optimization problem, e.g. the weights in the objective function is fixed. Often, these weights are mere estimates and increasingly machine learning techniques are used to for their estimation. Recently, Smart Predict and Optimize (SPO) has been proposed for problems with a linear objective function over the predictions, more specifical&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.10092v1-abstract-full').style.display = 'inline'; document.getElementById('1911.10092v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1911.10092v1-abstract-full" style="display: none;">
        Combinatorial optimization assumes that all parameters of the optimization problem, e.g. the weights in the objective function is fixed. Often, these weights are mere estimates and increasingly machine learning techniques are used to for their estimation. Recently, Smart Predict and Optimize (SPO) has been proposed for problems with a linear objective function over the predictions, more specifically linear programming problems. It takes the regret of the predictions on the linear problem into account, by repeatedly solving it during learning. We investigate the use of SPO to solve more realistic discrete optimization problems. The main challenge is the repeated solving of the optimization problem. To this end, we investigate ways to relax the problem as well as warmstarting the learning and the solving. Our results show that even for discrete problems it often suffices to train by solving the relaxation in the SPO loss. Furthermore, this approach outperforms, for most instances, the state-of-the-art approach of Wilder, Dilkina, and Tambe. We experiment with weighted knapsack problems as well as complex scheduling problems and show for the first time that a predict-and-optimize approach can successfully be used on large-scale combinatorial optimization problems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.10092v1-abstract-full').style.display = 'none'; document.getElementById('1911.10092v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 November, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1910.00505">arXiv:1910.00505</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1910.00505">pdf</a>, <a href="https://arxiv.org/format/1910.00505">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards Improving Solution Dominance with Incomparability Conditions: A case-study using Generator Itemset Mining
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ko%C3%A7ak%2C+G">Gökberk Koçak</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Akg%C3%BCn%2C+%C3%96">Özgür Akgün</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Miguel%2C+I">Ian Miguel</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1910.00505v1-abstract-short" style="display: inline;">
        Finding interesting patterns is a challenging task in data mining. Constraint based mining is a well-known approach to this, and one for which constraint programming has been shown to be a well-suited and generic framework. Dominance programming has been proposed as an extension that can capture an even wider class of constraint-based mining problems, by allowing to compare relations between patte&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1910.00505v1-abstract-full').style.display = 'inline'; document.getElementById('1910.00505v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1910.00505v1-abstract-full" style="display: none;">
        Finding interesting patterns is a challenging task in data mining. Constraint based mining is a well-known approach to this, and one for which constraint programming has been shown to be a well-suited and generic framework. Dominance programming has been proposed as an extension that can capture an even wider class of constraint-based mining problems, by allowing to compare relations between patterns. In this paper, in addition to specifying a dominance relation, we introduce the ability to specify an incomparability condition. Using these two concepts we devise a generic framework that can do a batch-wise search that avoids checking incomparable solutions. We extend the ESSENCE language and underlying modelling pipeline to support this. We use generator itemset mining problem as a test case and give a declarative specification for that. We also present preliminary experimental results on this specific problem class with a CP solver backend to show that using the incomparability condition during search can improve the efficiency of dominance programming and reduces the need for post-processing to filter dominated solutions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1910.00505v1-abstract-full').style.display = 'none'; document.getElementById('1910.00505v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 October, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1909.07893">arXiv:1909.07893</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1909.07893">pdf</a>, <a href="https://arxiv.org/format/1909.07893">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Vehicle routing by learning from historical solutions
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Canoy%2C+R">Rocsildes Canoy</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1909.07893v1-abstract-short" style="display: inline;">
        The goal of this paper is to investigate a decision support system for vehicle routing, where the routing engine learns from the subjective decisions that human planners have made in the past, rather than optimizing a distance-based objective criterion. This is an alternative to the practice of formulating a custom VRP for every company with its own routing requirements. Instead, we assume the pre&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1909.07893v1-abstract-full').style.display = 'inline'; document.getElementById('1909.07893v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1909.07893v1-abstract-full" style="display: none;">
        The goal of this paper is to investigate a decision support system for vehicle routing, where the routing engine learns from the subjective decisions that human planners have made in the past, rather than optimizing a distance-based objective criterion. This is an alternative to the practice of formulating a custom VRP for every company with its own routing requirements. Instead, we assume the presence of past vehicle routing solutions over similar sets of customers, and learn to make similar choices. The approach is based on the concept of learning a first-order Markov model, which corresponds to a probabilistic transition matrix, rather than a deterministic distance matrix. This nevertheless allows us to use existing arc routing VRP software in creating the actual route plans. For the learning, we explore different schemes to construct the probabilistic transition matrix. Our results on a use-case with a small transportation company show that our method is able to generate results that are close to the manually created solutions, without needing to characterize all constraints and sub-objectives explicitly. Even in the case of changes in the client sets, our method is able to find solutions that are closer to the actual route plans than when using distances, and hence, solutions that would require fewer manual changes to transform into the actual route plan.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1909.07893v1-abstract-full').style.display = 'none'; document.getElementById('1909.07893v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 September, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1903.12577">arXiv:1903.12577</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1903.12577">pdf</a>, <a href="https://arxiv.org/format/1903.12577">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning Relational Representations with Auto-encoding Logic Programs
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dumancic%2C+S">Sebastijan Dumancic</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Meert%2C+W">Wannes Meert</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Blockeel%2C+H">Hendrik Blockeel</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1903.12577v2-abstract-short" style="display: inline;">
        Deep learning methods capable of handling relational data have proliferated over the last years. In contrast to traditional relational learning methods that leverage first-order logic for representing such data, these deep learning methods aim at re-representing symbolic relational data in Euclidean spaces. They offer better scalability, but can only numerically approximate relational structures a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.12577v2-abstract-full').style.display = 'inline'; document.getElementById('1903.12577v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1903.12577v2-abstract-full" style="display: none;">
        Deep learning methods capable of handling relational data have proliferated over the last years. In contrast to traditional relational learning methods that leverage first-order logic for representing such data, these deep learning methods aim at re-representing symbolic relational data in Euclidean spaces. They offer better scalability, but can only numerically approximate relational structures and are less flexible in terms of reasoning tasks supported. This paper introduces a novel framework for relational representation learning that combines the best of both worlds. This framework, inspired by the auto-encoding principle, uses first-order logic as a data representation language, and the mapping between the original and latent representation is done by means of logic programs instead of neural networks. We show how learning can be cast as a constraint optimisation problem for which existing solvers can be used. The use of logic as a representation language makes the proposed framework more accurate (as the representation is exact, rather than approximate), more flexible, and more interpretable than deep learning methods. We experimentally show that these latent representations are indeed beneficial in relational learning tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.12577v2-abstract-full').style.display = 'none'; document.getElementById('1903.12577v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 March, 2020; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 29 March, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">8 pages,4 figures, paper + supplement, published at IJCAI</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1812.09207">arXiv:1812.09207</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1812.09207">pdf</a>, <a href="https://arxiv.org/format/1812.09207">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Solution Dominance over Constraint Satisfaction Problems
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Stuckey%2C+P+J">Peter J. Stuckey</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tack%2C+G">Guido Tack</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1812.09207v1-abstract-short" style="display: inline;">
        Constraint Satisfaction Problems (CSPs) typically have many solutions that satisfy all constraints. Often though, some solutions are preferred over others, that is, some solutions dominate other solutions. We present solution dominance as a formal framework to reason about such settings. We define Constraint Dominance Problems (CDPs) as CSPs with a dominance relation, that is, a preorder over the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1812.09207v1-abstract-full').style.display = 'inline'; document.getElementById('1812.09207v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1812.09207v1-abstract-full" style="display: none;">
        Constraint Satisfaction Problems (CSPs) typically have many solutions that satisfy all constraints. Often though, some solutions are preferred over others, that is, some solutions dominate other solutions. We present solution dominance as a formal framework to reason about such settings. We define Constraint Dominance Problems (CDPs) as CSPs with a dominance relation, that is, a preorder over the solutions of the CSP. This framework captures many well-known variants of constraint satisfaction, including optimization, multi-objective optimization, Max-CSP, minimal models, minimum correction subsets as well as optimization over CP-nets and arbitrary dominance relations. We extend MiniZinc, a declarative language for modeling CSPs, to CDPs by introducing dominance nogoods; these can be derived from dominance relations in a principled way. A generic method for solving arbitrary CDPs incrementally calls a CSP solver and is compatible with any existing solver that supports MiniZinc. This encourages experimenting with different solution dominance relations for a problem, as well as comparing different solvers without having to modify their implementations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1812.09207v1-abstract-full').style.display = 'none'; document.getElementById('1812.09207v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 December, 2018; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2018.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Presented at the ModRef18 workshop at CP18</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.8
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1604.01166">arXiv:1604.01166</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1604.01166">pdf</a>, <a href="https://arxiv.org/ps/1604.01166">ps</a>, <a href="https://arxiv.org/format/1604.01166">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        An Efficient Algorithm for Mining Frequent Sequence with Constraint Programming
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Aoga%2C+J+O+R">John O. R. Aoga</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schaus%2C+P">Pierre Schaus</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1604.01166v1-abstract-short" style="display: inline;">
        The main advantage of Constraint Programming (CP) approaches for sequential pattern mining (SPM) is their modularity, which includes the ability to add new constraints (regular expressions, length restrictions, etc). The current best CP approach for SPM uses a global constraint (module) that computes the projected database and enforces the minimum frequency; it does this with a filtering algorithm&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1604.01166v1-abstract-full').style.display = 'inline'; document.getElementById('1604.01166v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1604.01166v1-abstract-full" style="display: none;">
        The main advantage of Constraint Programming (CP) approaches for sequential pattern mining (SPM) is their modularity, which includes the ability to add new constraints (regular expressions, length restrictions, etc). The current best CP approach for SPM uses a global constraint (module) that computes the projected database and enforces the minimum frequency; it does this with a filtering algorithm similar to the PrefixSpan method. However, the resulting system is not as scalable as some of the most advanced mining systems like Zaki&#39;s cSPADE. We show how, using techniques from both data mining and CP, one can use a generic constraint solver and yet outperform existing specialized systems. This is mainly due to two improvements in the module that computes the projected frequencies: first, computing the projected database can be sped up by pre-computing the positions at which an symbol can become unsupported by a sequence, thereby avoiding to scan the full sequence each time; and second by taking inspiration from the trailing used in CP solvers to devise a backtracking-aware data structure that allows fast incremental storing and restoring of the projected database. Detailed experiments show how this approach outperforms existing CP as well as specialized systems for SPM, and that the gain in efficiency translates directly into increased efficiency for other settings such as mining with regular expressions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1604.01166v1-abstract-full').style.display = 'none'; document.getElementById('1604.01166v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 April, 2016; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">frequent sequence mining, constraint programming</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1510.03317">arXiv:1510.03317</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1510.03317">pdf</a>, <a href="https://arxiv.org/format/1510.03317">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Inductive Constraint Programming Loop
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bessiere%2C+C">Christian Bessiere</a>, 
      
      <a href="/search/?searchtype=author&amp;query=De+Raedt%2C+L">Luc De Raedt</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kotthoff%2C+L">Lars Kotthoff</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nanni%2C+M">Mirco Nanni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nijssen%2C+S">Siegfried Nijssen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=O%27Sullivan%2C+B">Barry O&#39;Sullivan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Paparrizou%2C+A">Anastasia Paparrizou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pedreschi%2C+D">Dino Pedreschi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Simonis%2C+H">Helmut Simonis</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1510.03317v1-abstract-short" style="display: inline;">
        Constraint programming is used for a variety of real-world optimisation problems, such as planning, scheduling and resource allocation problems. At the same time, one continuously gathers vast amounts of data about these problems. Current constraint programming software does not exploit such data to update schedules, resources and plans. We propose a new framework, that we call the Inductive Const&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1510.03317v1-abstract-full').style.display = 'inline'; document.getElementById('1510.03317v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1510.03317v1-abstract-full" style="display: none;">
        Constraint programming is used for a variety of real-world optimisation problems, such as planning, scheduling and resource allocation problems. At the same time, one continuously gathers vast amounts of data about these problems. Current constraint programming software does not exploit such data to update schedules, resources and plans. We propose a new framework, that we call the Inductive Constraint Programming loop. In this approach data is gathered and analyzed systematically, in order to dynamically revise and adapt constraints and optimization criteria. Inductive Constraint Programming aims at bridging the gap between the areas of data mining and machine learning on the one hand, and constraint programming on the other hand.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1510.03317v1-abstract-full').style.display = 'none'; document.getElementById('1510.03317v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 October, 2015; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2015.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">17 pages, 9 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1501.01178">arXiv:1501.01178</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1501.01178">pdf</a>, <a href="https://arxiv.org/format/1501.01178">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Constraint-based sequence mining using constraint programming
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Negrevergne%2C+B">Benjamin Negrevergne</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guns%2C+T">Tias Guns</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1501.01178v3-abstract-short" style="display: inline;">
        The goal of constraint-based sequence mining is to find sequences of symbols that are included in a large number of input sequences and that satisfy some constraints specified by the user. Many constraints have been proposed in the literature, but a general framework is still missing. We investigate the use of constraint programming as general framework for this task. We first identify four catego&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1501.01178v3-abstract-full').style.display = 'inline'; document.getElementById('1501.01178v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1501.01178v3-abstract-full" style="display: none;">
        The goal of constraint-based sequence mining is to find sequences of symbols that are included in a large number of input sequences and that satisfy some constraints specified by the user. Many constraints have been proposed in the literature, but a general framework is still missing. We investigate the use of constraint programming as general framework for this task. We first identify four categories of constraints that are applicable to sequence mining. We then propose two constraint programming formulations. The first formulation introduces a new global constraint called exists-embedding. This formulation is the most efficient but does not support one type of constraint. To support such constraints, we develop a second formulation that is more general but incurs more overhead. Both formulations can use the projected database technique used in specialised algorithms. Experiments demonstrate the flexibility towards constraint-based settings and compare the approach to existing methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1501.01178v3-abstract-full').style.display = 'none'; document.getElementById('1501.01178v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 February, 2015; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 January, 2015;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2015.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">In Integration of AI and OR Techniques in Constraint Programming (CPAIOR), 2015</span>
    </p>
    

    

    
  </li>

</ol>


  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>