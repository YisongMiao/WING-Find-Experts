<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;50 of 4,016 results for author: <span class="mathjax">Bo Li</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/"  aria-role="search">
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Bo Li">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Bo+Li&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Bo Li">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?query=Bo+Li&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?query=Bo+Li&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/?query=Bo+Li&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
              class="pagination-link "
              aria-label="Page 2"
              aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/?query=Bo+Li&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=100"
              class="pagination-link "
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/?query=Bo+Li&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=150"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/?query=Bo+Li&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=200"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2509.03348">arXiv:2509.03348</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2509.03348">pdf</a>, <a href="https://arxiv.org/ps/2509.03348">ps</a>, <a href="https://arxiv.org/format/2509.03348">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Science and Game Theory">cs.GT</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Generative Auto-Bidding in Large-Scale Competitive Auctions via Diffusion Completer-Aligner
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yewen Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+J">Jingtong Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+N">Nan Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mao%2C+S">Shuai Mao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=An%2C+R">Ruyi An</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pan%2C+F">Fei Pan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+X">Xiangyu Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=An%2C+B">Bo An</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+Q">Qingpeng Cai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+P">Peng Jiang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2509.03348v1-abstract-short" style="display: inline;">
        Auto-bidding is central to computational advertising, achieving notable commercial success by optimizing advertisers&#39; bids within economic constraints. Recently, large generative models show potential to revolutionize auto-bidding by generating bids that could flexibly adapt to complex, competitive environments. Among them, diffusers stand out for their ability to address sparse-reward challenges&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.03348v1-abstract-full').style.display = 'inline'; document.getElementById('2509.03348v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2509.03348v1-abstract-full" style="display: none;">
        Auto-bidding is central to computational advertising, achieving notable commercial success by optimizing advertisers&#39; bids within economic constraints. Recently, large generative models show potential to revolutionize auto-bidding by generating bids that could flexibly adapt to complex, competitive environments. Among them, diffusers stand out for their ability to address sparse-reward challenges by focusing on trajectory-level accumulated rewards, as well as their explainable capability, i.e., planning a future trajectory of states and executing bids accordingly. However, diffusers struggle with generation uncertainty, particularly regarding dynamic legitimacy between adjacent states, which can lead to poor bids and further cause significant loss of ad impression opportunities when competing with other advertisers in a highly competitive auction environment. To address it, we propose a Causal auto-Bidding method based on a Diffusion completer-aligner framework, termed CBD. Firstly, we augment the diffusion training process with an extra random variable t, where the model observes t-length historical sequences with the goal of completing the remaining sequence, thereby enhancing the generated sequences&#39; dynamic legitimacy. Then, we employ a trajectory-level return model to refine the generated trajectories, aligning more closely with advertisers&#39; objectives. Experimental results across diverse settings demonstrate that our approach not only achieves superior performance on large-scale auto-bidding benchmarks, such as a 29.9% improvement in conversion value in the challenging sparse-reward auction setting, but also delivers significant improvements on the Kuaishou online advertising platform, including a 2.0% increase in target cost.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.03348v1-abstract-full').style.display = 'none'; document.getElementById('2509.03348v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 September, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2509.02969">arXiv:2509.02969</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2509.02969">pdf</a>, <a href="https://arxiv.org/ps/2509.02969">ps</a>, <a href="https://arxiv.org/format/2509.02969">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Social and Information Networks">cs.SI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        VQualA 2025 Challenge on Engagement Prediction for Short Videos: Methods and Results
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+D">Dasong Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+S">Sizhuo Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hua%2C+H">Hang Hua</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+W">Wenjie Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jian Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+C+W">Chris Wei Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guan%2C+F">Fengbin Guan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xin Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+Z">Zihao Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+Y">Yiting Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liao%2C+R">Ru-Ling Liao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ye%2C+Y">Yan Ye</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Z">Zhibo Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+W">Wei Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+L">Linhan Cao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+Y">Yuqin Cao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+W">Weixia Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wen%2C+W">Wen Wen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+K">Kaiwei Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Z">Zijian Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+F">Fangfang Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Min%2C+X">Xiongkuo Min</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhai%2C+G">Guangtao Zhai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiao%2C+E">Erjia Xiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+L">Lingfeng Zhang</a>
      , et al. (18 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2509.02969v1-abstract-short" style="display: inline;">
        This paper presents an overview of the VQualA 2025 Challenge on Engagement Prediction for Short Videos, held in conjunction with ICCV 2025. The challenge focuses on understanding and modeling the popularity of user-generated content (UGC) short videos on social media platforms. To support this goal, the challenge uses a new short-form UGC dataset featuring engagement metrics derived from real-worl&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.02969v1-abstract-full').style.display = 'inline'; document.getElementById('2509.02969v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2509.02969v1-abstract-full" style="display: none;">
        This paper presents an overview of the VQualA 2025 Challenge on Engagement Prediction for Short Videos, held in conjunction with ICCV 2025. The challenge focuses on understanding and modeling the popularity of user-generated content (UGC) short videos on social media platforms. To support this goal, the challenge uses a new short-form UGC dataset featuring engagement metrics derived from real-world user interactions. This objective of the Challenge is to promote robust modeling strategies that capture the complex factors influencing user engagement. Participants explored a variety of multi-modal features, including visual content, audio, and metadata provided by creators. The challenge attracted 97 participants and received 15 valid test submissions, contributing significantly to progress in short-form UGC video engagement prediction.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.02969v1-abstract-full').style.display = 'none'; document.getElementById('2509.02969v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 September, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ICCV 2025 VQualA workshop EVQA track</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        ICCV 2025 Workshop
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2509.02653">arXiv:2509.02653</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2509.02653">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Physics and Society">physics.soc-ph</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="General Economics">econ.GN</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Quantifying the Social Costs of Power Outages and Restoration Disparities Across Four U.S. Hurricanes
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xiangpeng Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+J">Junwei Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+B">Bo Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mostafavi%2C+A">Ali Mostafavi</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2509.02653v1-abstract-short" style="display: inline;">
        The multifaceted nature of disaster impact shows that densely populated areas contribute more to aggregate burden, while sparsely populated but heavily affected regions suffer disproportionately at the individual level. This study introduces a framework for quantifying the societal impacts of power outages by translating customer weighted outage exposure into deprivation measures, integrating welf&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.02653v1-abstract-full').style.display = 'inline'; document.getElementById('2509.02653v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2509.02653v1-abstract-full" style="display: none;">
        The multifaceted nature of disaster impact shows that densely populated areas contribute more to aggregate burden, while sparsely populated but heavily affected regions suffer disproportionately at the individual level. This study introduces a framework for quantifying the societal impacts of power outages by translating customer weighted outage exposure into deprivation measures, integrating welfare metrics with three recovery indicators, average outage days per customer, restoration duration, and relative restoration rate, computed from sequential EAGLE I observations and linked to Zip Code Tabulation Area demographics. Applied to four United States hurricanes, Beryl 2024 Texas, Helene 2024 Florida, Milton 2024 Florida, and Ida 2021 Louisiana, this standardized pipeline provides the first cross event, fine scale evaluation of outage impacts and their drivers. Results demonstrate regressive patterns with greater burdens in lower income areas, mechanistic analysis shows deprivation increases with longer restoration durations and decreases with faster restoration rates, explainable modeling identifies restoration duration as the dominant driver, and clustering reveals distinct recovery typologies not captured by conventional reliability metrics. This framework delivers a transferable method for assessing outage impacts and equity, comparative cross event evidence linking restoration dynamics to social outcomes, and actionable spatial analyses that support equity informed restoration planning and resilience investment.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.02653v1-abstract-full').style.display = 'none'; document.getElementById('2509.02653v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 September, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2509.02544">arXiv:2509.02544</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2509.02544">pdf</a>, <a href="https://arxiv.org/ps/2509.02544">ps</a>, <a href="https://arxiv.org/format/2509.02544">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Haoming Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zou%2C+H">Haoyang Zou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+H">Huatong Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feng%2C+J">Jiazhan Feng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fang%2C+J">Junjie Fang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+J">Junting Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+L">Longxiang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+Q">Qinyu Luo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liang%2C+S">Shihao Liang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+S">Shijue Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhong%2C+W">Wanjun Zhong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ye%2C+Y">Yining Ye</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qin%2C+Y">Yujia Qin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiong%2C+Y">Yuwen Xiong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+Y">Yuxin Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+Z">Zhiyong Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+B">Bo Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dun%2C+C">Chen Dun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+C">Chong Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Leng%2C+F">Fuxing Leng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Hanbin Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+H">Hao Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+H">Haobin Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+H">Hongyi Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Su%2C+J">Jing Su</a>
      , et al. (81 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2509.02544v1-abstract-short" style="display: inline;">
        The development of autonomous agents for graphical user interfaces (GUIs) presents major challenges in artificial intelligence. While recent advances in native agent models have shown promise by unifying perception, reasoning, action, and memory through end-to-end learning, open problems remain in data scalability, multi-turn reinforcement learning (RL), the limitations of GUI-only operation, and&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.02544v1-abstract-full').style.display = 'inline'; document.getElementById('2509.02544v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2509.02544v1-abstract-full" style="display: none;">
        The development of autonomous agents for graphical user interfaces (GUIs) presents major challenges in artificial intelligence. While recent advances in native agent models have shown promise by unifying perception, reasoning, action, and memory through end-to-end learning, open problems remain in data scalability, multi-turn reinforcement learning (RL), the limitations of GUI-only operation, and environment stability. In this technical report, we present UI-TARS-2, a native GUI-centered agent model that addresses these challenges through a systematic training methodology: a data flywheel for scalable data generation, a stabilized multi-turn RL framework, a hybrid GUI environment that integrates file systems and terminals, and a unified sandbox platform for large-scale rollouts. Empirical evaluation demonstrates that UI-TARS-2 achieves significant improvements over its predecessor UI-TARS-1.5. On GUI benchmarks, it reaches 88.2 on Online-Mind2Web, 47.5 on OSWorld, 50.6 on WindowsAgentArena, and 73.3 on AndroidWorld, outperforming strong baselines such as Claude and OpenAI agents. In game environments, it attains a mean normalized score of 59.8 across a 15-game suite-roughly 60% of human-level performance-and remains competitive with frontier proprietary models (e.g., OpenAI o3) on LMGame-Bench. Additionally, the model can generalize to long-horizon information-seeking tasks and software engineering benchmarks, highlighting its robustness across diverse agent tasks. Detailed analyses of training dynamics further provide insights into achieving stability and efficiency in large-scale agent RL. These results underscore UI-TARS-2&#39;s potential to advance the state of GUI agents and exhibit strong generalization to real-world interactive scenarios.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.02544v1-abstract-full').style.display = 'none'; document.getElementById('2509.02544v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 September, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2509.02479">arXiv:2509.02479</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2509.02479">pdf</a>, <a href="https://arxiv.org/ps/2509.02479">ps</a>, <a href="https://arxiv.org/format/2509.02479">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SimpleTIR: End-to-End Reinforcement Learning for Multi-Turn Tool-Integrated Reasoning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xue%2C+Z">Zhenghai Xue</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+L">Longtao Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Q">Qian Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yingru Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+X">Xiaosen Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+Z">Zejun Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=An%2C+B">Bo An</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2509.02479v2-abstract-short" style="display: inline;">
        Large Language Models (LLMs) can significantly improve their reasoning capabilities by interacting with external tools, a paradigm known as Tool-Integrated Reasoning (TIR). However, extending TIR to multi-turn scenarios using Reinforcement Learning (RL) is often hindered by training instability and performance collapse. We identify that such instability is primarily caused by a distributional drif&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.02479v2-abstract-full').style.display = 'inline'; document.getElementById('2509.02479v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2509.02479v2-abstract-full" style="display: none;">
        Large Language Models (LLMs) can significantly improve their reasoning capabilities by interacting with external tools, a paradigm known as Tool-Integrated Reasoning (TIR). However, extending TIR to multi-turn scenarios using Reinforcement Learning (RL) is often hindered by training instability and performance collapse. We identify that such instability is primarily caused by a distributional drift from external tool feedback, leading to the generation of low-probability tokens. This issue compounds over successive turns, causing catastrophic gradient norm explosions that derail the training process. To address this challenge, we introduce SimpleTIR , a plug-and-play algorithm that stabilizes multi-turn TIR training. Its core strategy is to identify and filter out trajectories containing void turns, i.e., turns that yield neither a code block nor a final answer. By removing these problematic trajectories from the policy update, SimpleTIR effectively blocks the harmful, high-magnitude gradients, thus stabilizing the learning dynamics. Extensive experiments show that SimpleTIR achieves state-of-the-art performance on challenging math reasoning benchmarks, notably elevating the AIME24 score from a text-only baseline of 22.1 to 50.5 when starting from the Qwen2.5-7B base model. Furthermore, by avoiding the constraints of supervised fine-tuning, SimpleTIR encourages the model to discover diverse and sophisticated reasoning patterns, such as self-correction and cross-validation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.02479v2-abstract-full').style.display = 'none'; document.getElementById('2509.02479v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 2 September, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2509.01914">arXiv:2509.01914</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2509.01914">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multiagent Systems">cs.MA</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        How Real Is AI Tutoring? Comparing Simulated and Human Dialogues in One-on-One Instruction
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+R">Ruijia Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+Y">Yuan-Hao Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jiatong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+B">Bo Jiang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2509.01914v1-abstract-short" style="display: inline;">
        Heuristic and scaffolded teacher-student dialogues are widely regarded as critical for fostering students&#39; higher-order thinking and deep learning. However, large language models (LLMs) currently face challenges in generating pedagogically rich interactions. This study systematically investigates the structural and behavioral differences between AI-simulated and authentic human tutoring dialogues.&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.01914v1-abstract-full').style.display = 'inline'; document.getElementById('2509.01914v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2509.01914v1-abstract-full" style="display: none;">
        Heuristic and scaffolded teacher-student dialogues are widely regarded as critical for fostering students&#39; higher-order thinking and deep learning. However, large language models (LLMs) currently face challenges in generating pedagogically rich interactions. This study systematically investigates the structural and behavioral differences between AI-simulated and authentic human tutoring dialogues. We conducted a quantitative comparison using an Initiation-Response-Feedback (IRF) coding scheme and Epistemic Network Analysis (ENA). The results show that human dialogues are significantly superior to their AI counterparts in utterance length, as well as in questioning (I-Q) and general feedback (F-F) behaviors. More importantly, ENA results reveal a fundamental divergence in interactional patterns: human dialogues are more cognitively guided and diverse, centered around a &#34;question-factual response-feedback&#34; teaching loop that clearly reflects pedagogical guidance and student-driven thinking; in contrast, simulated dialogues exhibit a pattern of structural simplification and behavioral convergence, revolving around an &#34;explanation-simplistic response&#34; loop that is essentially a simple information transfer between the teacher and student. These findings illuminate key limitations in current AI-generated tutoring and provide empirical guidance for designing and evaluating more pedagogically effective generative educational dialogue systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.01914v1-abstract-full').style.display = 'none'; document.getElementById('2509.01914v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 September, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Proceedings of the 33rd International Conference on Computers in Education (ICCE 2025). Asia-Pacific Society for Computers in Education</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2509.01517">arXiv:2509.01517</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2509.01517">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Emerging Technologies">cs.ET</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Agentic Workflow for Education: Concepts and Applications
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+Y">Yuan-Hao Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+Y">Yijie Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+L">Ling Dai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jiatong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+R">Ruijia Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+B">Bo Jiang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2509.01517v1-abstract-short" style="display: inline;">
        With the rapid advancement of Large Language Models (LLMs) and Artificial Intelligence (AI) agents, agentic workflows are showing transformative potential in education. This study introduces the Agentic Workflow for Education (AWE), a four-component model comprising self-reflection, tool invocation, task planning, and multi-agent collaboration. We distinguish AWE from traditional LLM-based linear&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.01517v1-abstract-full').style.display = 'inline'; document.getElementById('2509.01517v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2509.01517v1-abstract-full" style="display: none;">
        With the rapid advancement of Large Language Models (LLMs) and Artificial Intelligence (AI) agents, agentic workflows are showing transformative potential in education. This study introduces the Agentic Workflow for Education (AWE), a four-component model comprising self-reflection, tool invocation, task planning, and multi-agent collaboration. We distinguish AWE from traditional LLM-based linear interactions and propose a theoretical framework grounded in the von Neumann Multi-Agent System (MAS) architecture. Through a paradigm shift from static prompt-response systems to dynamic, nonlinear workflows, AWE enables scalable, personalized, and collaborative task execution. We further identify four core application domains: integrated learning environments, personalized AI-assisted learning, simulation-based experimentation, and data-driven decision-making. A case study on automated math test generation shows that AWE-generated items are statistically comparable to real exam questions, validating the model&#39;s effectiveness. AWE offers a promising path toward reducing teacher workload, enhancing instructional quality, and enabling broader educational innovation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.01517v1-abstract-full').style.display = 'none'; document.getElementById('2509.01517v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 September, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Proceedings of the 33rd International Conference on Computers in Education (ICCE 2025). Asia-Pacific Society for Computers in Education</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2509.01322">arXiv:2509.01322</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2509.01322">pdf</a>, <a href="https://arxiv.org/ps/2509.01322">ps</a>, <a href="https://arxiv.org/format/2509.01322">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Distributed, Parallel, and Cluster Computing">cs.DC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LongCat-Flash Technical Report
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Meituan+LongCat+Team"> Meituan LongCat Team</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bayan"> Bayan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+B">Bei Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lei%2C+B">Bingye Lei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+B">Bo Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Rong%2C+B">Bolin Rong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+C">Chao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+C">Chao Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+C">Chen Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+C">Chen Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+C">Cheng Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+C">Chengcheng Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xi%2C+C">Chenguang Xi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+C">Chi Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+C">Chong Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qin%2C+C">Chuan Qin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+C">Chuyu Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+C">Cong Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+C">Congkui Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+D">Dan Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pan%2C+D">Daoru Pan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bu%2C+D">Defei Bu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+D">Dengchang Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kong%2C+D">Deyang Kong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+D">Dishan Liu</a>
      , et al. (157 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2509.01322v1-abstract-short" style="display: inline;">
        We introduce LongCat-Flash, a 560-billion-parameter Mixture-of-Experts (MoE) language model designed for both computational efficiency and advanced agentic capabilities. Stemming from the need for scalable efficiency, LongCat-Flash adopts two novel designs: (a) Zero-computation Experts, which enables dynamic computational budget allocation and activates 18.6B-31.3B (27B on average) per token depen&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.01322v1-abstract-full').style.display = 'inline'; document.getElementById('2509.01322v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2509.01322v1-abstract-full" style="display: none;">
        We introduce LongCat-Flash, a 560-billion-parameter Mixture-of-Experts (MoE) language model designed for both computational efficiency and advanced agentic capabilities. Stemming from the need for scalable efficiency, LongCat-Flash adopts two novel designs: (a) Zero-computation Experts, which enables dynamic computational budget allocation and activates 18.6B-31.3B (27B on average) per token depending on contextual demands, optimizing resource usage. (b) Shortcut-connected MoE, which enlarges the computation-communication overlap window, demonstrating notable gains in inference efficiency and throughput compared to models of a comparable scale. We develop a comprehensive scaling framework for large models that combines hyperparameter transfer, model-growth initialization, a multi-pronged stability suite, and deterministic computation to achieve stable and reproducible training. Notably, leveraging the synergy among scalable architectural design and infrastructure efforts, we complete model training on more than 20 trillion tokens within 30 days, while achieving over 100 tokens per second (TPS) for inference at a cost of \$0.70 per million output tokens. To cultivate LongCat-Flash towards agentic intelligence, we conduct a large-scale pre-training on optimized mixtures, followed by targeted mid- and post-training on reasoning, code, and instructions, with further augmentation from synthetic data and tool use tasks. Comprehensive evaluations demonstrate that, as a non-thinking foundation model, LongCat-Flash delivers highly competitive performance among other leading models, with exceptional strengths in agentic tasks. The model checkpoint of LongCat-Flash is open-sourced to foster community research.
  LongCat Chat: https://longcat.ai
  Hugging Face: https://huggingface.co/meituan-longcat
  GitHub: https://github.com/meituan-longcat
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.01322v1-abstract-full').style.display = 'none'; document.getElementById('2509.01322v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 September, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2509.01069">arXiv:2509.01069</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2509.01069">pdf</a>, <a href="https://arxiv.org/ps/2509.01069">ps</a>, <a href="https://arxiv.org/format/2509.01069">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Solar and Stellar Astrophysics">astro-ph.SR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Astrophysics of Galaxies">astro-ph.GA</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A magnetic white dwarf formed through a binary merger within 35 million years
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yan%2C+H">Huahui Yan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+J">Jiamao Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+R">Rizhong Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+L">Li Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+G">Genghao Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ren%2C+L">Liangliang Ren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Z">Zhen Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+S">Siyi Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Z">Zhangliang Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+C">Chun Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+B">Bo Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shao%2C+Y">Yong Shao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zhenwei Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+X">Xianfei Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fremling%2C+C">Christoffer Fremling</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Eldridge%2C+J+J">Jan J. Eldridge</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ge%2C+H">Hongwei Ge</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+C">Chengyuan Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2509.01069v1-abstract-short" style="display: inline;">
        White dwarfs (WDs) represent the final evolutionary stage of most stars, typically originating from progenitor stars with masses below approximately 8 $M_{\odot}$ to 10 $M_{\odot}$. Formation through single-star evolution generally requires at least 25 Myr, with the youngest WDs often near the Chandrasekhar limit of 1.4 $M_{\odot}$. In contrast, WDs formed via binary channels, such as mergers or m&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.01069v1-abstract-full').style.display = 'inline'; document.getElementById('2509.01069v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2509.01069v1-abstract-full" style="display: none;">
        White dwarfs (WDs) represent the final evolutionary stage of most stars, typically originating from progenitor stars with masses below approximately 8 $M_{\odot}$ to 10 $M_{\odot}$. Formation through single-star evolution generally requires at least 25 Myr, with the youngest WDs often near the Chandrasekhar limit of 1.4 $M_{\odot}$. In contrast, WDs formed via binary channels, such as mergers or mass transfer, can develop smaller masses in a shorter timescale and may exhibit unique characteristics, including strong surface magnetic fields and rapid rotation. Accurately determining the ages of these WDs is essential for understanding their formation. A valuable method involves studying WDs in star clusters, where member stars share the same age and chemical composition, allowing for precise constraints on the formation times and metallicities of the WDs&#39; progenitors. Here we report a WD found in the open cluster RSG 5, which is only 35 Myr old. The WD&#39;s mass is lower than 1.05 $M_{\odot}$, indicating it may not have formed through single-star evolution. The WD possesses an exceptionally strong surface magnetic field ($\ge 200$ MG), a short rotational period ($\sim 6.5$ min), and, most notably, a co-rotating half-ring of ionized circumstellar debris. This distinctive feature provides evidence for a binary merger origin, a scenario further substantiated by our stellar evolution models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.01069v1-abstract-full').style.display = 'none'; document.getElementById('2509.01069v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">16 pages, 9 figures, accepted for publication in ApJ Letters</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2509.00930">arXiv:2509.00930</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2509.00930">pdf</a>, <a href="https://arxiv.org/ps/2509.00930">ps</a>, <a href="https://arxiv.org/format/2509.00930">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        SATQuest: A Verifier for Logical Reasoning Evaluation and Reinforcement Fine-Tuning of LLMs
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+Y">Yanxiao Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yaqian Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bo%2C+Z">Zihao Bo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Takezoe%2C+R">Rinyoichi Takezoe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hui%2C+H">Haojia Hui</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guang%2C+M">Mo Guang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ren%2C+L">Lei Ren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qin%2C+X">Xiaolin Qin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Long%2C+K">Kaiwen Long</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2509.00930v1-abstract-short" style="display: inline;">
        Recent advances in Large Language Models (LLMs) have demonstrated remarkable general reasoning capabilities. However, systematically evaluating and enhancing these reasoning capabilities is challenging due to the lack of controllable and scalable tools for fine-grained analysis. Existing benchmarks and datasets often lack the necessary variable control for multi-dimensional, systematic analysis an&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.00930v1-abstract-full').style.display = 'inline'; document.getElementById('2509.00930v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2509.00930v1-abstract-full" style="display: none;">
        Recent advances in Large Language Models (LLMs) have demonstrated remarkable general reasoning capabilities. However, systematically evaluating and enhancing these reasoning capabilities is challenging due to the lack of controllable and scalable tools for fine-grained analysis. Existing benchmarks and datasets often lack the necessary variable control for multi-dimensional, systematic analysis and training, or have narrow problem types and formats. To address these limitations, we introduce SATQuest, a systematic verifier designed to evaluate and enhance logical reasoning in LLMs by generating diverse, Satisfiability-based logical reasoning problems directly from Conjunctive Normal Form (CNF) instances. SATQuest structures these problems along three orthogonal dimensions: instance scale, problem type, and question format, employing randomized, SAT-based problem generation and objective answer verification via PySAT. This design mitigates memorization issues, allows for nuanced insights into reasoning performance, and enables effective reinforcement fine-tuning. Our extensive evaluation of various LLMs using SATQuest identified significant limitations in their logical reasoning, particularly in generalizing beyond familiar mathematical formats. Furthermore, we show that reinforcement fine-tuning with SATQuest rewards substantially improves targeted task performance and generalizes to more complex instances, while highlighting remaining challenges in cross-format adaptation. Through these demonstrations, we showcase SATQuest&#39;s potential as a foundational tool and a valuable starting point for advancing LLM logical reasoning.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.00930v1-abstract-full').style.display = 'none'; document.getElementById('2509.00930v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2509.00676">arXiv:2509.00676</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2509.00676">pdf</a>, <a href="https://arxiv.org/ps/2509.00676">ps</a>, <a href="https://arxiv.org/format/2509.00676">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LLaVA-Critic-R1: Your Critic Model is Secretly a Strong Policy Model
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xiyao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+C">Chunyuan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+J">Jianwei Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+K">Kai Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+B">Bo Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiong%2C+T">Tianyi Xiong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+F">Furong Huang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2509.00676v1-abstract-short" style="display: inline;">
        In vision-language modeling, critic models are typically trained to evaluate outputs -- assigning scalar scores or pairwise preferences -- rather than to generate responses. This separation from policy models, which produce the responses, is so entrenched that critics are rarely considered for direct policy use. In this work, we challenge this convention. We propose to reorganize preference-labele&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.00676v1-abstract-full').style.display = 'inline'; document.getElementById('2509.00676v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2509.00676v1-abstract-full" style="display: none;">
        In vision-language modeling, critic models are typically trained to evaluate outputs -- assigning scalar scores or pairwise preferences -- rather than to generate responses. This separation from policy models, which produce the responses, is so entrenched that critics are rarely considered for direct policy use. In this work, we challenge this convention. We propose to reorganize preference-labeled critic datasets into verifiable training signals and perform reinforcement learning directly on a base generative model, producing LLaVA-Critic-R1, a multimodal critic trained to optimize preference judgments while retaining full generation ability. Surprisingly, LLaVA-Critic-R1 emerges not only as a top-performing critic but also as a competitive policy model -- matching or surpassing specialized reasoning VLMs trained with in-domain data across 26 visual reasoning and understanding benchmarks, with an average gain of +5.7% over its base model (Qwen-2.5-VL-7B). Extending this approach to existing strong reasoning VLMs yields LLaVA-Critic-R1+, which further advances policy performance without sacrificing critic quality, achieving a SoTA performance of 71.9 on MMMU at the 7B scale. Finally, we show that the enhanced critic ability benefits inference: applying self-critique at test time yields an average +13.8% improvement on five representative reasoning tasks without additional training. Our results reveal that RL training on critic data can produce a unified model excelling at both evaluation and generation, offering a simple path toward scalable, self-improving multimodal systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.00676v1-abstract-full').style.display = 'none'; document.getElementById('2509.00676v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2509.00289">arXiv:2509.00289</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2509.00289">pdf</a>, <a href="https://arxiv.org/ps/2509.00289">ps</a>, <a href="https://arxiv.org/format/2509.00289">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="High Energy Physics - Experiment">hep-ex</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Helicity amplitude and branching fraction measurement of $χ_{cJ} \rightarrow Λ\barΛ $
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=BESIII+Collaboration"> BESIII Collaboration</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ablikim%2C+M">M. Ablikim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Achasov%2C+M+N">M. N. Achasov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adlarson%2C+P">P. Adlarson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ai%2C+X+C">X. C. Ai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aliberti%2C+R">R. Aliberti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Amoroso%2C+A">A. Amoroso</a>, 
      
      <a href="/search/?searchtype=author&amp;query=An%2C+Q">Q. An</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+Y">Y. Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bakina%2C+O">O. Bakina</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ban%2C+Y">Y. Ban</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bao%2C+H+-">H. -R. Bao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Batozskaya%2C+V">V. Batozskaya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Begzsuren%2C+K">K. Begzsuren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berger%2C+N">N. Berger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berlowski%2C+M">M. Berlowski</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bertani%2C+M">M. Bertani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bettoni%2C+D">D. Bettoni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianchi%2C+F">F. Bianchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianco%2C+E">E. Bianco</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bortone%2C+A">A. Bortone</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boyko%2C+I">I. Boyko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Briere%2C+R+A">R. A. Briere</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brueggemann%2C+A">A. Brueggemann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+H">H. Cai</a>
      , et al. (697 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2509.00289v1-abstract-short" style="display: inline;">
        Utilizing $2712.4 \pm 14.3$ million $ψ(3686)$ events accumulated by the BESIII experiment, we perform a partial wave analysis of $ψ(3686)\rightarrowγχ_{cJ}\rightarrowγΛ\barΛ$ decay ($J=0,1,2$). The ratio of the helicity amplitudes with same (++) and opposite (+-) helicity for $χ_{c2}\rightarrowΛ\barΛ$ decay is determined for the first time to be $R_{χ_{c2}}=0.575 \pm 0.048 \pm 0.018 $, with a rela&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.00289v1-abstract-full').style.display = 'inline'; document.getElementById('2509.00289v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2509.00289v1-abstract-full" style="display: none;">
        Utilizing $2712.4 \pm 14.3$ million $ψ(3686)$ events accumulated by the BESIII experiment, we perform a partial wave analysis of $ψ(3686)\rightarrowγχ_{cJ}\rightarrowγΛ\barΛ$ decay ($J=0,1,2$). The ratio of the helicity amplitudes with same (++) and opposite (+-) helicity for $χ_{c2}\rightarrowΛ\barΛ$ decay is determined for the first time to be $R_{χ_{c2}}=0.575 \pm 0.048 \pm 0.018 $, with a relative phase angle $ΔΦ_{χ_{c2}} = 0.37 \pm 0.15 \pm 0.05 $~rad. The parameters of the angular distribution of $χ_{c2}$ are determined to be $α_{χ_{c2}} = -0.211 \pm 0.100 \pm 0.050 $ and $β_{χ_{c2}} = -0.039 \pm 0.089 \pm 0.033 $, based on the distribution $dN / d\cosθ= 1 + α_{χ_{c2}} \cos^2θ+ β_{χ_{c2}} \cos^4θ$. The width of $χ_{c0}$ is determined to be $12.31 \pm 0.26 \pm 0.12 $~MeV. Additionally, the branching fractions for $χ_{cJ} \rightarrow Λ\barΛ$ are measured to be $(3.662 \pm 0.048 \pm 0.111) \times 10^{-4}$, $(1.182 \pm 0.026 \pm 0.042) \times 10^{-4}$, and $(1.704 \pm 0.035 \pm 0.057) \times 10^{-4}$ for $χ_{c0}$, $χ_{c1}$ and $χ_{c2}$, respectively, where the first uncertainty is statistical and the second systematic.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.00289v1-abstract-full').style.display = 'none'; document.getElementById('2509.00289v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">This is the first submission of the manuscript. 13 pages, 15 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.21570">arXiv:2508.21570</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.21570">pdf</a>, <a href="https://arxiv.org/ps/2508.21570">ps</a>, <a href="https://arxiv.org/format/2508.21570">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        OASIS: Harnessing Diffusion Adversarial Network for Ocean Salinity Imputation using Sparse Drifter Trajectories
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+B">Bo Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feng%2C+Y">Yingqi Feng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jin%2C+M">Ming Jin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+X">Xin Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+Y">Yufei Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cherubin%2C+L">Laurent Cherubin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liew%2C+A+W">Alan Wee-Chung Liew</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+C">Can Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+Q">Qinghua Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yao%2C+J">Jingwei Yao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pan%2C+S">Shirui Pan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+H">Hong Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+X">Xingquan Zhu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.21570v1-abstract-short" style="display: inline;">
        Ocean salinity plays a vital role in circulation, climate, and marine ecosystems, yet its measurement is often sparse, irregular, and noisy, especially in drifter-based datasets. Traditional approaches, such as remote sensing and optimal interpolation, rely on linearity and stationarity, and are limited by cloud cover, sensor drift, and low satellite revisit rates. While machine learning models of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.21570v1-abstract-full').style.display = 'inline'; document.getElementById('2508.21570v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.21570v1-abstract-full" style="display: none;">
        Ocean salinity plays a vital role in circulation, climate, and marine ecosystems, yet its measurement is often sparse, irregular, and noisy, especially in drifter-based datasets. Traditional approaches, such as remote sensing and optimal interpolation, rely on linearity and stationarity, and are limited by cloud cover, sensor drift, and low satellite revisit rates. While machine learning models offer flexibility, they often fail under severe sparsity and lack principled ways to incorporate physical covariates without specialized sensors. In this paper, we introduce the OceAn Salinity Imputation System (OASIS), a novel diffusion adversarial framework designed to address these challenges.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.21570v1-abstract-full').style.display = 'none'; document.getElementById('2508.21570v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">CIKM 2025 Accepted</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.21432">arXiv:2508.21432</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.21432">pdf</a>, <a href="https://arxiv.org/ps/2508.21432">ps</a>, <a href="https://arxiv.org/format/2508.21432">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        RepoMark: A Code Usage Auditing Framework for Code Large Language Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Qu%2C+W">Wenjie Qu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+Y">Yuguang Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+B">Bo Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+W">Wengrui Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yuexin Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jia%2C+J">Jinyuan Jia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+J">Jiaheng Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.21432v1-abstract-short" style="display: inline;">
        The rapid development of Large Language Models (LLMs) for code generation has transformed software development by automating coding tasks with unprecedented efficiency.
  However, the training of these models on open-source code repositories (e.g., from GitHub) raises critical ethical and legal concerns, particularly regarding data authorization and open-source license compliance. Developers are i&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.21432v1-abstract-full').style.display = 'inline'; document.getElementById('2508.21432v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.21432v1-abstract-full" style="display: none;">
        The rapid development of Large Language Models (LLMs) for code generation has transformed software development by automating coding tasks with unprecedented efficiency.
  However, the training of these models on open-source code repositories (e.g., from GitHub) raises critical ethical and legal concerns, particularly regarding data authorization and open-source license compliance. Developers are increasingly questioning whether model trainers have obtained proper authorization before using repositories for training, especially given the lack of transparency in data collection.
  To address these concerns, we propose a novel data marking framework RepoMark to audit the data usage of code LLMs. Our method enables repository owners to verify whether their code has been used in training, while ensuring semantic preservation, imperceptibility, and theoretical false detection rate (FDR) guarantees. By generating multiple semantically equivalent code variants, RepoMark introduces data marks into the code files, and during detection, RepoMark leverages a novel ranking-based hypothesis test to detect memorization within the model. Compared to prior data auditing approaches, RepoMark significantly enhances sample efficiency, allowing effective auditing even when the user&#39;s repository possesses only a small number of code files.
  Experiments demonstrate that RepoMark achieves a detection success rate over 90\% on small code repositories under a strict FDR guarantee of 5\%. This represents a significant advancement over existing data marking techniques, all of which only achieve accuracy below 55\% under identical settings. This further validates RepoMark as a robust, theoretically sound, and promising solution for enhancing transparency in code LLM training, which can safeguard the rights of repository owners.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.21432v1-abstract-full').style.display = 'none'; document.getElementById('2508.21432v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.21247">arXiv:2508.21247</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.21247">pdf</a>, <a href="https://arxiv.org/ps/2508.21247">ps</a>, <a href="https://arxiv.org/format/2508.21247">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Optics">physics.optics</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Mesoscale and Nanoscale Physics">cond-mat.mes-hall</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Bright yet dark: how strong coupling quenches exciton-polariton radiation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+J">Jiaxun Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+L">Li He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhen%2C+B">Bo Zhen</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.21247v1-abstract-short" style="display: inline;">
        Understanding the radiative decay of exciton-polaritons is essential for achieving long-lived polaritons - a key prerequisite for enhancing nonlinear and quantum polaritonic effects. However, conventional wisdom - the coupled oscillator model - often oversimplifies polariton radiation as independent emissions from uncoupled excitonic and photonic resonances, overlooking the role of strong exciton-&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.21247v1-abstract-full').style.display = 'inline'; document.getElementById('2508.21247v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.21247v1-abstract-full" style="display: none;">
        Understanding the radiative decay of exciton-polaritons is essential for achieving long-lived polaritons - a key prerequisite for enhancing nonlinear and quantum polaritonic effects. However, conventional wisdom - the coupled oscillator model - often oversimplifies polariton radiation as independent emissions from uncoupled excitonic and photonic resonances, overlooking the role of strong exciton-photon coupling in reshaping their radiative behavior. In this work, we present a theoretical framework that goes beyond the conventional coupled oscillator model by fully accounting for the collective and coherent nature of exciton-photon interactions. We demonstrate that these interactions can strongly suppress polariton radiation via destructive interference - both within the excitonic ensemble and between excitonic and photonic radiation channels - giving rise to polaritonic bound states in the continuum with infinitely long radiative lifetimes. Our approach offers a unified description of polariton radiative decay and establishes new design principles for engineering long-lived exciton-polaritons with tailored radiation properties, opening new avenues for nonlinear, topological, and quantum polaritonic applications.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.21247v1-abstract-full').style.display = 'none'; document.getElementById('2508.21247v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.20737">arXiv:2508.20737</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.20737">pdf</a>, <a href="https://arxiv.org/ps/2508.20737">ps</a>, <a href="https://arxiv.org/format/2508.20737">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Rethinking Testing for LLM Applications: Characteristics, Challenges, and a Lightweight Interaction Protocol
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+W">Wei Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Y">Yixiao Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+Q">Qiang Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ying%2C+S">Shi Ying</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jin%2C+Z">Zhi Jin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+B">Bo Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xing%2C+Z">Zhenchang Xing</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+T">Tianlin Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+J">Junjie Shi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+L">Linxiao Jiang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.20737v1-abstract-short" style="display: inline;">
        Applications of Large Language Models~(LLMs) have evolved from simple text generators into complex software systems that integrate retrieval augmentation, tool invocation, and multi-turn interactions. Their inherent non-determinism, dynamism, and context dependence pose fundamental challenges for quality assurance. This paper decomposes LLM applications into a three-layer architecture: \textbf{\te&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.20737v1-abstract-full').style.display = 'inline'; document.getElementById('2508.20737v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.20737v1-abstract-full" style="display: none;">
        Applications of Large Language Models~(LLMs) have evolved from simple text generators into complex software systems that integrate retrieval augmentation, tool invocation, and multi-turn interactions. Their inherent non-determinism, dynamism, and context dependence pose fundamental challenges for quality assurance. This paper decomposes LLM applications into a three-layer architecture: \textbf{\textit{System Shell Layer}}, \textbf{\textit{Prompt Orchestration Layer}}, and \textbf{\textit{LLM Inference Core}}. We then assess the applicability of traditional software testing methods in each layer: directly applicable at the shell layer, requiring semantic reinterpretation at the orchestration layer, and necessitating paradigm shifts at the inference core. A comparative analysis of Testing AI methods from the software engineering community and safety analysis techniques from the AI community reveals structural disconnects in testing unit abstraction, evaluation metrics, and lifecycle management. We identify four fundamental differences that underlie 6 core challenges. To address these, we propose four types of collaborative strategies (\emph{Retain}, \emph{Translate}, \emph{Integrate}, and \emph{Runtime}) and explore a closed-loop, trustworthy quality assurance framework that combines pre-deployment validation with runtime monitoring. Based on these strategies, we offer practical guidance and a protocol proposal to support the standardization and tooling of LLM application testing. We propose a protocol \textbf{\textit{Agent Interaction Communication Language}} (AICL) that is used to communicate between AI agents. AICL has the test-oriented features and is easily integrated in the current agent framework.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.20737v1-abstract-full').style.display = 'none'; document.getElementById('2508.20737v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.20454">arXiv:2508.20454</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.20454">pdf</a>, <a href="https://arxiv.org/ps/2508.20454">ps</a>, <a href="https://arxiv.org/format/2508.20454">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Quantum Physics">quant-ph</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Modulation Instability-Induced Multimode Squeezing in Quadratic Frequency Combs
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+H">Haodong Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+N">Nianqin Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shu%2C+Z">Zijun Shu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shen%2C+Y">Yang Shen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ji%2C+B">Bo Ji</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+A">Aiping Xie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+F">Feng Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+D">Dengcai Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+J">Jing Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gong%2C+H">Hang Gong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+G">Guoxiang Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+C">Chunbo Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+W">Wei Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+T">Tengfei Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+G">Guangqiang He</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.20454v1-abstract-short" style="display: inline;">
        Lithium niobate (LN) microring resonators, characterized by an exceptionally high second-order nonlinear coefficient and superior electro-optic tunability, serve as an outstanding platform for the precise control of integrated quantum frequency combs (QFCs). In this study, we introduce a bipartite entanglement criterion to investigate the pairwise entanglement characteristics of QFCs generated via&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.20454v1-abstract-full').style.display = 'inline'; document.getElementById('2508.20454v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.20454v1-abstract-full" style="display: none;">
        Lithium niobate (LN) microring resonators, characterized by an exceptionally high second-order nonlinear coefficient and superior electro-optic tunability, serve as an outstanding platform for the precise control of integrated quantum frequency combs (QFCs). In this study, we introduce a bipartite entanglement criterion to investigate the pairwise entanglement characteristics of QFCs generated via the spontaneous parametric down-conversion (SPDC) process in lithium niobate microring resonators operating below threshold. Furthermore, we propose a universal framework for analyzing multimode squeezing in quadratic frequency combs, enabling the realization of ultrabroadband and high-degree multimode squeezing. We further reveal the underlying physical mechanism: modulation instability (MI), regulated by temporal walk-off control, not only enables the formation of frequency combs but also induces multimode squeezing in the corresponding resonant modes. This study uncovers the previously unexplored role of on-chip multimode squeezing in quadratic frequency combs while facilitating collective noise suppression across multiple modes, thus holding substantial potential for advancing quantum precision measurement and quantum information processing.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.20454v1-abstract-full').style.display = 'none'; document.getElementById('2508.20454v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.19934">arXiv:2508.19934</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.19934">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Strongly Correlated Electrons">cond-mat.str-el</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multi-origin driven giant planar Hall effect in topological antiferromagnet EuAl2Si2 with tunable spin texture
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+X">Xiangqi Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+Z">Ziyi Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+Y">Yixuan Luo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zhengyang Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+B">Bo Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+J">Jingcheng Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xia Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xi%2C+C">Chuanying Xi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pi%2C+L">Li Pi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+G">Guanxiang Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+L">Leiming Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+W">Wenbo Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xia%2C+W">Wei Xia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Y">Yanfeng Guo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.19934v1-abstract-short" style="display: inline;">
        In topological materials, the planar Hall effect (PHE) is often regarded as a hallmark of profound quantum phenomena-most notably the Adler-Bell-Jackiw chiral anomaly and Berry curvature-rendering it an indispensable tool for deciphering the topological essence of emergent phases. In this study, we delve into the PHE and anisotropic magnetoresistance in the recently discovered layered topological&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.19934v1-abstract-full').style.display = 'inline'; document.getElementById('2508.19934v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.19934v1-abstract-full" style="display: none;">
        In topological materials, the planar Hall effect (PHE) is often regarded as a hallmark of profound quantum phenomena-most notably the Adler-Bell-Jackiw chiral anomaly and Berry curvature-rendering it an indispensable tool for deciphering the topological essence of emergent phases. In this study, we delve into the PHE and anisotropic magnetoresistance in the recently discovered layered topological antiferromagnet EuAl2Si2. Our analysis of the robust PHE signal (~3.8 μΩ cm at 2 K and 8 T) unveils a distinct interplay of mechanisms. While Berry curvature plays a minor role, the dominant contributions stem from classical orbital MR in the field-induced ferromagnetic state and field-suppressed spin fluctuations in the paramagnetic regime. These insights not only position EuAl2Si2-with its highly tunable spin texture-as an exemplary system for probing the intricate coupling between spin configurations and band topology in magnetotransport but also pave the way for designing novel materials with tailored PHE responses, highlighting significant application prospects in quantum sensing, spintronic devices, and topologically protected electronic systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.19934v1-abstract-full').style.display = 'none'; document.getElementById('2508.19934v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">17 pages and 5 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.19092">arXiv:2508.19092</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.19092">pdf</a>, <a href="https://arxiv.org/ps/2508.19092">ps</a>, <a href="https://arxiv.org/format/2508.19092">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="High Energy Physics - Experiment">hep-ex</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Measurement of the branching fraction of $\psip \to ωηη$
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=BESIII+Collaboration"> BESIII Collaboration</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ablikim%2C+M">M. Ablikim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Achasov%2C+M+N">M. N. Achasov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adlarson%2C+P">P. Adlarson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ai%2C+X+C">X. C. Ai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aliberti%2C+R">R. Aliberti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Amoroso%2C+A">A. Amoroso</a>, 
      
      <a href="/search/?searchtype=author&amp;query=An%2C+Q">Q. An</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+Y">Y. Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bakina%2C+O">O. Bakina</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ban%2C+Y">Y. Ban</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bao%2C+H+-">H. -R. Bao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Batozskaya%2C+V">V. Batozskaya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Begzsuren%2C+K">K. Begzsuren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berger%2C+N">N. Berger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berlowski%2C+M">M. Berlowski</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bertani%2C+M">M. Bertani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bettoni%2C+D">D. Bettoni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianchi%2C+F">F. Bianchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianco%2C+E">E. Bianco</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bortone%2C+A">A. Bortone</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boyko%2C+I">I. Boyko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Briere%2C+R+A">R. A. Briere</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brueggemann%2C+A">A. Brueggemann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+H">H. Cai</a>
      , et al. (706 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.19092v1-abstract-short" style="display: inline;">
        Using a sample of (2.712 $\pm$ 0.014)$\times 10^{9}$ $\psip$ events collected with the BESIII detector at the BEPCII collider in 2009, 2012, and 2021, the decay $\psip \to ωηη$ is observed for the first time. The branching fraction of the $ψ(3686)\toωηη$ decay is measured to be (1.65 $\pm$ 0.02 $\pm$ 0.21)$\times 10^{-5}$, where the first uncertainty is statistical and the second systematic. Clear&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.19092v1-abstract-full').style.display = 'inline'; document.getElementById('2508.19092v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.19092v1-abstract-full" style="display: none;">
        Using a sample of (2.712 $\pm$ 0.014)$\times 10^{9}$ $\psip$ events collected with the BESIII detector at the BEPCII collider in 2009, 2012, and 2021, the decay $\psip \to ωηη$ is observed for the first time. The branching fraction of the $ψ(3686)\toωηη$ decay is measured to be (1.65 $\pm$ 0.02 $\pm$ 0.21)$\times 10^{-5}$, where the first uncertainty is statistical and the second systematic. Clear structures associated with the well-established $ω(1420)$ and $f_{0}(1710)$ resonances are observed in the $ωη$ and $ηη$ invariant-mass spectra, respectively.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.19092v1-abstract-full').style.display = 'none'; document.getElementById('2508.19092v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.19005">arXiv:2508.19005</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.19005">pdf</a>, <a href="https://arxiv.org/ps/2508.19005">ps</a>, <a href="https://arxiv.org/format/2508.19005">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+Y">Yuxuan Cai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hao%2C+Y">Yipeng Hao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+J">Jie Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yan%2C+H">Hang Yan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lei%2C+Z">Zhikai Lei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhen%2C+R">Rui Zhen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+Z">Zhenhua Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Y">Yutao Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Junsong Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pan%2C+Q">Qianjun Pan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huai%2C+T">Tianyu Huai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Q">Qin Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xin Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+K">Kai Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+B">Bo Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qiu%2C+X">Xipeng Qiu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+L">Liang He</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.19005v2-abstract-short" style="display: inline;">
        As AI advances toward general intelligence, the focus is shifting from systems optimized for static tasks to creating open-ended agents that learn continuously. In this paper, we introduce Experience-driven Lifelong Learning (ELL), a framework for building self-evolving agents capable of continuous growth through real-world interaction. The framework is built on four core principles: (1) Experienc&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.19005v2-abstract-full').style.display = 'inline'; document.getElementById('2508.19005v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.19005v2-abstract-full" style="display: none;">
        As AI advances toward general intelligence, the focus is shifting from systems optimized for static tasks to creating open-ended agents that learn continuously. In this paper, we introduce Experience-driven Lifelong Learning (ELL), a framework for building self-evolving agents capable of continuous growth through real-world interaction. The framework is built on four core principles: (1) Experience Exploration: Agents learn through continuous, self-motivated interaction with dynamic environments, navigating interdependent tasks and generating rich experiential trajectories. (2) Long-term Memory: Agents preserve and structure historical knowledge, including personal experiences, domain expertise, and commonsense reasoning, into a persistent memory system. (3) Skill Learning: Agents autonomously improve by abstracting recurring patterns from experience into reusable skills, which are actively refined and validated for application in new tasks. (4) Knowledge Internalization: Agents internalize explicit and discrete experiences into implicit and intuitive capabilities as &#34;second nature&#34;.
  We also introduce StuLife, a benchmark dataset for ELL that simulates a student&#39;s holistic college journey, from enrollment to academic and personal development, across three core phases and ten detailed sub-scenarios. StuLife is designed around three key paradigm
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.19005v2-abstract-full').style.display = 'none'; document.getElementById('2508.19005v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 26 August, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.18993">arXiv:2508.18993</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.18993">pdf</a>, <a href="https://arxiv.org/ps/2508.18993">ps</a>, <a href="https://arxiv.org/format/2508.18993">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        GitTaskBench: A Benchmark for Code Agents Solving Real-World Tasks Through Code Repository Leveraging
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ni%2C+Z">Ziyi Ni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Huacan Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+S">Shuo Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+S">Shuo Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+Z">Ziyang He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=You%2C+W">Wang You</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+Z">Zhenheng Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+Y">Yuntao Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+B">Bill Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+H">Hongzhang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+S">Sen Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+R">Ronghao Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+B">Bo Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xin Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+C">Chen Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiao%2C+B">Binxing Jiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+D">Daxin Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lyu%2C+P">Pin Lyu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.18993v1-abstract-short" style="display: inline;">
        Beyond scratch coding, exploiting large-scale code repositories (e.g., GitHub) for practical tasks is vital in real-world software development, yet current benchmarks rarely evaluate code agents in such authentic, workflow-driven scenarios. To bridge this gap, we introduce GitTaskBench, a benchmark designed to systematically assess this capability via 54 realistic tasks across 7 modalities and 7 d&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.18993v1-abstract-full').style.display = 'inline'; document.getElementById('2508.18993v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.18993v1-abstract-full" style="display: none;">
        Beyond scratch coding, exploiting large-scale code repositories (e.g., GitHub) for practical tasks is vital in real-world software development, yet current benchmarks rarely evaluate code agents in such authentic, workflow-driven scenarios. To bridge this gap, we introduce GitTaskBench, a benchmark designed to systematically assess this capability via 54 realistic tasks across 7 modalities and 7 domains. Each task pairs a relevant repository with an automated, human-curated evaluation harness specifying practical success criteria. Beyond measuring execution and task success, we also propose the alpha-value metric to quantify the economic benefit of agent performance, which integrates task success rates, token cost, and average developer salaries. Experiments across three state-of-the-art agent frameworks with multiple advanced LLMs show that leveraging code repositories for complex task solving remains challenging: even the best-performing system, OpenHands+Claude 3.7, solves only 48.15% of tasks. Error analysis attributes over half of failures to seemingly mundane yet critical steps like environment setup and dependency resolution, highlighting the need for more robust workflow management and increased timeout preparedness. By releasing GitTaskBench, we aim to drive progress and attention toward repository-aware code reasoning, execution, and deployment -- moving agents closer to solving complex, end-to-end real-world tasks. The benchmark and code are open-sourced at https://github.com/QuantaAlpha/GitTaskBench.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.18993v1-abstract-full').style.display = 'none'; document.getElementById('2508.18993v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Highly practical, Well-motivated, Actionable</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.18873">arXiv:2508.18873</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.18873">pdf</a>, <a href="https://arxiv.org/ps/2508.18873">ps</a>, <a href="https://arxiv.org/format/2508.18873">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MOCHA: Discovering Multi-Order Dynamic Causality in Temporal Point Processes
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+Y">Yunyang Cao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+J">Juekai Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+W">Wenhao Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jin%2C+B">Bo Jin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.18873v1-abstract-short" style="display: inline;">
        Discovering complex causal dependencies in temporal point processes (TPPs) is critical for modeling real-world event sequences. Existing methods typically rely on static or first-order causal structures, overlooking the multi-order and time-varying nature of causal relationships. In this paper, we propose MOCHA, a novel framework for discovering multi-order dynamic causality in TPPs. MOCHA charact&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.18873v1-abstract-full').style.display = 'inline'; document.getElementById('2508.18873v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.18873v1-abstract-full" style="display: none;">
        Discovering complex causal dependencies in temporal point processes (TPPs) is critical for modeling real-world event sequences. Existing methods typically rely on static or first-order causal structures, overlooking the multi-order and time-varying nature of causal relationships. In this paper, we propose MOCHA, a novel framework for discovering multi-order dynamic causality in TPPs. MOCHA characterizes multi-order influences as multi-hop causal paths over a latent time-evolving graph. To model such dynamics, we introduce a time-varying directed acyclic graph (DAG) with learnable structural weights, where acyclicity and sparsity constraints are enforced to ensure structural validity. We design an end-to-end differentiable framework that jointly models causal discovery and TPP dynamics, enabling accurate event prediction and revealing interpretable structures. Extensive experiments on real-world datasets demonstrate that MOCHA not only achieves state-of-the-art performance in event prediction, but also reveals meaningful and interpretable causal structures.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.18873v1-abstract-full').style.display = 'none'; document.getElementById('2508.18873v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.18761">arXiv:2508.18761</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.18761">pdf</a>, <a href="https://arxiv.org/ps/2508.18761">ps</a>, <a href="https://arxiv.org/format/2508.18761">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="High Energy Physics - Experiment">hep-ex</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Study of the $χ_{cJ}\rightarrowΛ\barΛη^\prime$ decays
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=BESIII+Collaboration"> BESIII Collaboration</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ablikim%2C+M">M. Ablikim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Achasov%2C+M+N">M. N. Achasov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adlarson%2C+P">P. Adlarson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ai%2C+X+C">X. C. Ai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aliberti%2C+R">R. Aliberti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Amoroso%2C+A">A. Amoroso</a>, 
      
      <a href="/search/?searchtype=author&amp;query=An%2C+Q">Q. An</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+Y">Y. Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bakina%2C+O">O. Bakina</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ban%2C+Y">Y. Ban</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bao%2C+H+-">H. -R. Bao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Batozskaya%2C+V">V. Batozskaya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Begzsuren%2C+K">K. Begzsuren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berger%2C+N">N. Berger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berlowski%2C+M">M. Berlowski</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bertani%2C+M+B">M. B. Bertani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bettoni%2C+D">D. Bettoni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianchi%2C+F">F. Bianchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianco%2C+E">E. Bianco</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bortone%2C+A">A. Bortone</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boyko%2C+I">I. Boyko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Briere%2C+R+A">R. A. Briere</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brueggemann%2C+A">A. Brueggemann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+H">H. Cai</a>
      , et al. (683 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.18761v1-abstract-short" style="display: inline;">
        Using a data sample of $(2.712\pm0.014)\times10^{9}$ $ψ(3686)$ events collected with the BESIII detector at the BEPCII collider, we investigate the decays $χ_{cJ} \rightarrow Λ\barΛ η^\prime$ for $J=0,~1,~2$ via the radiative transition $ψ(3686) \rightarrow γχ_{cJ}$. The decays $χ_{c0,2}\rightarrowΛ\barΛη^\prime$ are observed for the first time, with statistical significances of 6.7$\,σ$ and 6.4&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.18761v1-abstract-full').style.display = 'inline'; document.getElementById('2508.18761v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.18761v1-abstract-full" style="display: none;">
        Using a data sample of $(2.712\pm0.014)\times10^{9}$ $ψ(3686)$ events collected with the BESIII detector at the BEPCII collider, we investigate the decays $χ_{cJ} \rightarrow Λ\barΛ η^\prime$ for $J=0,~1,~2$ via the radiative transition $ψ(3686) \rightarrow γχ_{cJ}$. The decays $χ_{c0,2}\rightarrowΛ\barΛη^\prime$ are observed for the first time, with statistical significances of 6.7$\,σ$ and 6.4$\,σ$, respectively. Evidence for the decay $χ_{c1}\rightarrowΛ\barΛη^\prime$ is found with a statistical significance of 3.3$\,σ$. The corresponding branching fractions are measured to be $\mathscr{B}(χ_{c0}\rightarrowΛ\barΛη^\prime)=(7.56\pm1.42\pm0.90)\times10^{-5}$, $\mathscr{B}(χ_{c1}\rightarrowΛ\barΛη^\prime)=(1.54\pm0.51\pm0.16)\times10^{-5}$, and $\mathscr{B}(χ_{c2}\rightarrowΛ\barΛη^\prime)=(3.03\pm0.61\pm0.29)\times10^{-5}$, where the first uncertainties are statistical and the second systematic. No significant excited $Λ$ baryon states or $Λ\barΛ$ near-threshold enhancements are observed.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.18761v1-abstract-full').style.display = 'none'; document.getElementById('2508.18761v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.18601">arXiv:2508.18601</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.18601">pdf</a>, <a href="https://arxiv.org/ps/2508.18601">ps</a>, <a href="https://arxiv.org/format/2508.18601">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="High Energy Physics - Experiment">hep-ex</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Search for $χ_{c1}\to π^{+}π^{-}η_c$ via $ψ(3686)\toγχ_{c1}$
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=BESIII+Collaboration"> BESIII Collaboration</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ablikim%2C+M">M. Ablikim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Achasov%2C+M+N">M. N. Achasov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adlarson%2C+P">P. Adlarson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ai%2C+X+C">X. C. Ai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aliberti%2C+R">R. Aliberti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Amoroso%2C+A">A. Amoroso</a>, 
      
      <a href="/search/?searchtype=author&amp;query=An%2C+Q">Q. An</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+Y">Y. Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bakina%2C+O">O. Bakina</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ban%2C+Y">Y. Ban</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bao%2C+H+-">H. -R. Bao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Batozskaya%2C+V">V. Batozskaya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Begzsuren%2C+K">K. Begzsuren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berger%2C+N">N. Berger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berlowski%2C+M">M. Berlowski</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bertani%2C+M">M. Bertani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bettoni%2C+D">D. Bettoni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianchi%2C+F">F. Bianchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianco%2C+E">E. Bianco</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bortone%2C+A">A. Bortone</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boyko%2C+I">I. Boyko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Briere%2C+R+A">R. A. Briere</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brueggemann%2C+A">A. Brueggemann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+H">H. Cai</a>
      , et al. (697 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.18601v1-abstract-short" style="display: inline;">
        Utilizing $(2712.4 \pm 14.3) \times 10^6$ $ψ(3686)$ events collected with the BESIII detector at the BEPCII collider, we search for the hadronic transition process $χ_{c1} \to π^+π^-η_c$ following the decay $ψ(3686)\to γχ_{c1}$. No significant signal is observed, and an upper limit of $\mathcal{B}(χ_{c1}\toπ^+π^-η_c)$ is determined to be $3.1 times 10^{-4}$~at 90\% confidence level, which is one o&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.18601v1-abstract-full').style.display = 'inline'; document.getElementById('2508.18601v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.18601v1-abstract-full" style="display: none;">
        Utilizing $(2712.4 \pm 14.3) \times 10^6$ $ψ(3686)$ events collected with the BESIII detector at the BEPCII collider, we search for the hadronic transition process $χ_{c1} \to π^+π^-η_c$ following the decay $ψ(3686)\to γχ_{c1}$. No significant signal is observed, and an upper limit of $\mathcal{B}(χ_{c1}\toπ^+π^-η_c)$ is determined to be $3.1 times 10^{-4}$~at 90\% confidence level, which is one order of magnitude more stringent than the previous measurement.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.18601v1-abstract-full').style.display = 'none'; document.getElementById('2508.18601v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.18594">arXiv:2508.18594</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.18594">pdf</a>, <a href="https://arxiv.org/ps/2508.18594">ps</a>, <a href="https://arxiv.org/format/2508.18594">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="High Energy Physics - Experiment">hep-ex</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Search for a bound state of $Λ_{c}\barΣ_{c}$ near threshold
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=BESIII+Collaboration"> BESIII Collaboration</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ablikim%2C+M">M. Ablikim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Achasov%2C+M+N">M. N. Achasov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adlarson%2C+P">P. Adlarson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ai%2C+X+C">X. C. Ai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aliberti%2C+R">R. Aliberti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Amoroso%2C+A">A. Amoroso</a>, 
      
      <a href="/search/?searchtype=author&amp;query=An%2C+Q">Q. An</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+Y">Y. Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bakina%2C+O">O. Bakina</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ban%2C+Y">Y. Ban</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bao%2C+H+-">H. -R. Bao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Batozskaya%2C+V">V. Batozskaya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Begzsuren%2C+K">K. Begzsuren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berger%2C+N">N. Berger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berlowski%2C+M">M. Berlowski</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bertani%2C+M">M. Bertani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bettoni%2C+D">D. Bettoni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianchi%2C+F">F. Bianchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianco%2C+E">E. Bianco</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bortone%2C+A">A. Bortone</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boyko%2C+I">I. Boyko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Briere%2C+R+A">R. A. Briere</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brueggemann%2C+A">A. Brueggemann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+H">H. Cai</a>
      , et al. (706 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.18594v1-abstract-short" style="display: inline;">
        We search for a possible $Λ_{c} \bar{Σ}_{c}$ bound state, denoted as $H_{c}^{\pm}$, via the $ e^{+}e^{-} \to π^{+} π^{-} Λ_{c}^{+}\barΛ_{c}^{-}$ process for the first time. This analysis utilizes 207.8 and 159.3 pb$^{-1}$ of $e^{+}e^{-}$ annihilation data at the center-of-mass energies of 4918.02 and 4950.93 MeV, respectively, collected with the BESIII detector at the BEPCII collider. No statistic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.18594v1-abstract-full').style.display = 'inline'; document.getElementById('2508.18594v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.18594v1-abstract-full" style="display: none;">
        We search for a possible $Λ_{c} \bar{Σ}_{c}$ bound state, denoted as $H_{c}^{\pm}$, via the $ e^{+}e^{-} \to π^{+} π^{-} Λ_{c}^{+}\barΛ_{c}^{-}$ process for the first time. This analysis utilizes 207.8 and 159.3 pb$^{-1}$ of $e^{+}e^{-}$ annihilation data at the center-of-mass energies of 4918.02 and 4950.93 MeV, respectively, collected with the BESIII detector at the BEPCII collider. No statistically significant signal is observed. The upper limits of the product of Born cross section and branching fraction $σ(e^{+}e^{-} \to π^{+} H_c^{-} + c.c.) \times \mathcal{B}(H_c^{-} \rightarrow π^{-}Λ_{c}^{+}\barΛ_{c}^{-})$ at a 90\% confidence level are reported at each energy point and for various $H_{c}$ mass hypotheses (4715, 4720, 4725, 4730, and 4735 MeV/$c^{2}$) and widths (5, 10, or 20 MeV), with the upper limits ranging from 1.1 pb to 6.4 pb.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.18594v1-abstract-full').style.display = 'none'; document.getElementById('2508.18594v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.18032">arXiv:2508.18032</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.18032">pdf</a>, <a href="https://arxiv.org/ps/2508.18032">ps</a>, <a href="https://arxiv.org/format/2508.18032">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yaqi Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+P">Peng Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+M">Mingyang Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bu%2C+P">Pi Bu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+H">Haoxiang Shi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+R">Runzhou Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yao%2C+Y">Yang Yao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+X">Xuan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+J">Jun Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+B">Bo Zheng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.18032v2-abstract-short" style="display: inline;">
        Despite the promising progress of recent autoregressive models in text-to-image (T2I) generation, their ability to handle multi-attribute and ambiguous prompts remains limited. To address these limitations, existing works have applied chain-of-thought (CoT) to enable stage-aware visual synthesis and employed reinforcement learning (RL) to improve reasoning capabilities. However, most models provid&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.18032v2-abstract-full').style.display = 'inline'; document.getElementById('2508.18032v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.18032v2-abstract-full" style="display: none;">
        Despite the promising progress of recent autoregressive models in text-to-image (T2I) generation, their ability to handle multi-attribute and ambiguous prompts remains limited. To address these limitations, existing works have applied chain-of-thought (CoT) to enable stage-aware visual synthesis and employed reinforcement learning (RL) to improve reasoning capabilities. However, most models provide reward signals only at the end of the generation stage. This monolithic final-only guidance makes it difficult to identify which stages contribute positively to the final outcome and may lead to suboptimal policies. To tackle this issue, we propose a Visual-Chain of Guidance (Visual-CoG) paradigm consisting of three stages: semantic reasoning, process refining, and outcome evaluation, with stage-aware rewards providing immediate guidance throughout the image generation pipeline. We further construct a visual cognition benchmark, VisCog-Bench, which comprises four subtasks to evaluate the effectiveness of semantic reasoning. Comprehensive evaluations on GenEval, T2I-CompBench, and the proposed VisCog-Bench show improvements of 15%, 5%, and 19%, respectively, demonstrating the superior performance of the proposed Visual-CoG. We will release all the resources soon.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.18032v2-abstract-full').style.display = 'none'; document.getElementById('2508.18032v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 25 August, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.17976">arXiv:2508.17976</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.17976">pdf</a>, <a href="https://arxiv.org/ps/2508.17976">ps</a>, <a href="https://arxiv.org/format/2508.17976">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Propose and Rectify: A Forensics-Driven MLLM Framework for Image Manipulation Localization
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+K">Keyang Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kong%2C+C">Chenqi Kong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+H">Hui Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ding%2C+B">Bo Ding</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+X">Xinghao Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+H">Haoliang Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.17976v1-abstract-short" style="display: inline;">
        The increasing sophistication of image manipulation techniques demands robust forensic solutions that can both reliably detect alterations and precisely localize tampered regions. Recent Multimodal Large Language Models (MLLMs) show promise by leveraging world knowledge and semantic understanding for context-aware detection, yet they struggle with perceiving subtle, low-level forensic artifacts cr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.17976v1-abstract-full').style.display = 'inline'; document.getElementById('2508.17976v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.17976v1-abstract-full" style="display: none;">
        The increasing sophistication of image manipulation techniques demands robust forensic solutions that can both reliably detect alterations and precisely localize tampered regions. Recent Multimodal Large Language Models (MLLMs) show promise by leveraging world knowledge and semantic understanding for context-aware detection, yet they struggle with perceiving subtle, low-level forensic artifacts crucial for accurate manipulation localization. This paper presents a novel Propose-Rectify framework that effectively bridges semantic reasoning with forensic-specific analysis. In the proposal stage, our approach utilizes a forensic-adapted LLaVA model to generate initial manipulation analysis and preliminary localization of suspicious regions based on semantic understanding and contextual reasoning. In the rectification stage, we introduce a Forensics Rectification Module that systematically validates and refines these initial proposals through multi-scale forensic feature analysis, integrating technical evidence from several specialized filters. Additionally, we present an Enhanced Segmentation Module that incorporates critical forensic cues into SAM&#39;s encoded image embeddings, thereby overcoming inherent semantic biases to achieve precise delineation of manipulated regions. By synergistically combining advanced multimodal reasoning with established forensic methodologies, our framework ensures that initial semantic proposals are systematically validated and enhanced through concrete technical evidence, resulting in comprehensive detection accuracy and localization precision. Extensive experimental validation demonstrates state-of-the-art performance across diverse datasets with exceptional robustness and generalization capabilities.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.17976v1-abstract-full').style.display = 'none'; document.getElementById('2508.17976v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.17916">arXiv:2508.17916</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.17916">pdf</a>, <a href="https://arxiv.org/ps/2508.17916">ps</a>, <a href="https://arxiv.org/format/2508.17916">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        EndoUFM: Utilizing Foundation Models for Monocular depth estimation of endoscopic images
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yao%2C+X">Xinning Yao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+B">Bo Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+B">Bojian Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jingjing Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yue%2C+J">Jinghua Yue</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+F">Fugen Zhou</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.17916v1-abstract-short" style="display: inline;">
        Depth estimation is a foundational component for 3D reconstruction in minimally invasive endoscopic surgeries. However, existing monocular depth estimation techniques often exhibit limited performance to the varying illumination and complex textures of the surgical environment. While powerful visual foundation models offer a promising solution, their training on natural images leads to significant&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.17916v1-abstract-full').style.display = 'inline'; document.getElementById('2508.17916v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.17916v1-abstract-full" style="display: none;">
        Depth estimation is a foundational component for 3D reconstruction in minimally invasive endoscopic surgeries. However, existing monocular depth estimation techniques often exhibit limited performance to the varying illumination and complex textures of the surgical environment. While powerful visual foundation models offer a promising solution, their training on natural images leads to significant domain adaptability limitations and semantic perception deficiencies when applied to endoscopy. In this study, we introduce EndoUFM, an unsupervised monocular depth estimation framework that innovatively integrating dual foundation models for surgical scenes, which enhance the depth estimation performance by leveraging the powerful pre-learned priors. The framework features a novel adaptive fine-tuning strategy that incorporates Random Vector Low-Rank Adaptation (RVLoRA) to enhance model adaptability, and a Residual block based on Depthwise Separable Convolution (Res-DSC) to improve the capture of fine-grained local features. Furthermore, we design a mask-guided smoothness loss to enforce depth consistency within anatomical tissue structures. Extensive experiments on the SCARED, Hamlyn, SERV-CT, and EndoNeRF datasets confirm that our method achieves state-of-the-art performance while maintaining an efficient model size. This work contributes to augmenting surgeons&#39; spatial perception during minimally invasive procedures, thereby enhancing surgical precision and safety, with crucial implications for augmented reality and navigation systems.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.17916v1-abstract-full').style.display = 'none'; document.getElementById('2508.17916v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">12 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.17819">arXiv:2508.17819</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.17819">pdf</a>, <a href="https://arxiv.org/ps/2508.17819">ps</a>, <a href="https://arxiv.org/format/2508.17819">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="High Energy Physics - Experiment">hep-ex</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Search for CP violation in e+e- -&gt; psi(3770) -&gt; DDbar via D -&gt; KsPi0
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=BESIII+Collaboration"> BESIII Collaboration</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ablikim%2C+M">M. Ablikim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Achasov%2C+M+N">M. N. Achasov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adlarson%2C+P">P. Adlarson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ai%2C+X+C">X. C. Ai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aliberti%2C+R">R. Aliberti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Amoroso%2C+A">A. Amoroso</a>, 
      
      <a href="/search/?searchtype=author&amp;query=An%2C+Q">Q. An</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+Y">Y. Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bakina%2C+O">O. Bakina</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ban%2C+Y">Y. Ban</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bao%2C+H+-">H. -R. Bao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Batozskaya%2C+V">V. Batozskaya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Begzsuren%2C+K">K. Begzsuren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berger%2C+N">N. Berger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berlowski%2C+M">M. Berlowski</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bertani%2C+M+B">M. B. Bertani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bettoni%2C+D">D. Bettoni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianchi%2C+F">F. Bianchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianco%2C+E">E. Bianco</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bortone%2C+A">A. Bortone</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boyko%2C+I">I. Boyko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Briere%2C+R+A">R. A. Briere</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brueggemann%2C+A">A. Brueggemann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+H">H. Cai</a>
      , et al. (707 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.17819v2-abstract-short" style="display: inline;">
        Utilizing data sample of electron-positron collisions recorded with the BESIII detector at the center-of-mass energies of 3.773~GeV, corresponding to an integrated luminosity of 20.28~fb$^{-1}$, we report the first search for the CP forbidden process $e^+e^- \to ψ(3773) \to D^0\bar{D}^0 \to (K^0_Sπ^0)(K^0_Sπ^0)$. No significant signal is observed. We set the upper limit on the observed cross secti&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.17819v2-abstract-full').style.display = 'inline'; document.getElementById('2508.17819v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.17819v2-abstract-full" style="display: none;">
        Utilizing data sample of electron-positron collisions recorded with the BESIII detector at the center-of-mass energies of 3.773~GeV, corresponding to an integrated luminosity of 20.28~fb$^{-1}$, we report the first search for the CP forbidden process $e^+e^- \to ψ(3773) \to D^0\bar{D}^0 \to (K^0_Sπ^0)(K^0_Sπ^0)$. No significant signal is observed. We set the upper limit on the observed cross section to be 7.37~fb, and the upper limit on the joint branching fraction of the C-odd correlated neutral $D$ pair $\mathcal{B}[(D^0\bar{D}^0)_{\text{C-odd}} \to (K^0_Sπ^0)(K^0_Sπ^0)]$ to be $2.04 \times 10^{-6}$ at the 90\% confidence level.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.17819v2-abstract-full').style.display = 'none'; document.getElementById('2508.17819v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 26 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 25 August, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages, 4 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.16629">arXiv:2508.16629</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.16629">pdf</a>, <a href="https://arxiv.org/ps/2508.16629">ps</a>, <a href="https://arxiv.org/format/2508.16629">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learn to Memorize: Optimizing LLM-based Agents with Adaptive Memory Framework
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Z">Zeyu Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+Q">Quanyu Dai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+R">Rui Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bo%2C+X">Xiaohe Bo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xu Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dong%2C+Z">Zhenhua Dong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.16629v1-abstract-short" style="display: inline;">
        LLM-based agents have been extensively applied across various domains, where memory stands out as one of their most essential capabilities. Previous memory mechanisms of LLM-based agents are manually predefined by human experts, leading to higher labor costs and suboptimal performance. In addition, these methods overlook the memory cycle effect in interactive scenarios, which is critical to optimi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.16629v1-abstract-full').style.display = 'inline'; document.getElementById('2508.16629v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.16629v1-abstract-full" style="display: none;">
        LLM-based agents have been extensively applied across various domains, where memory stands out as one of their most essential capabilities. Previous memory mechanisms of LLM-based agents are manually predefined by human experts, leading to higher labor costs and suboptimal performance. In addition, these methods overlook the memory cycle effect in interactive scenarios, which is critical to optimizing LLM-based agents for specific environments. To address these challenges, in this paper, we propose to optimize LLM-based agents with an adaptive and data-driven memory framework by modeling memory cycles. Specifically, we design an MoE gate function to facilitate memory retrieval, propose a learnable aggregation process to improve memory utilization, and develop task-specific reflection to adapt memory storage. Our memory framework empowers LLM-based agents to learn how to memorize information effectively in specific environments, with both off-policy and on-policy optimization. In order to evaluate the effectiveness of our proposed methods, we conduct comprehensive experiments across multiple aspects. To benefit the research community in this area, we release our project at https://github.com/nuster1128/learn_to_memorize.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.16629v1-abstract-full').style.display = 'none'; document.getElementById('2508.16629v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">17 pages, 4 figures, 5 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.16557">arXiv:2508.16557</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.16557">pdf</a>, <a href="https://arxiv.org/ps/2508.16557">ps</a>, <a href="https://arxiv.org/format/2508.16557">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Time-Aware One Step Diffusion Network for Real-World Image Super-Resolution
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+T">Tainyi Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Duan%2C+Z">Zheng-Peng Duan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+P">Peng-Tao Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+B">Bo Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+M">Ming-Ming Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+C">Chun-Le Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+C">Chongyi Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.16557v2-abstract-short" style="display: inline;">
        Diffusion-based real-world image super-resolution (Real-ISR) methods have demonstrated impressive performance. To achieve efficient Real-ISR, many works employ Variational Score Distillation (VSD) to distill pre-trained stable-diffusion (SD) model for one-step SR with a fixed timestep. However, due to the different noise injection timesteps, the SD will perform different generative priors. Therefo&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.16557v2-abstract-full').style.display = 'inline'; document.getElementById('2508.16557v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.16557v2-abstract-full" style="display: none;">
        Diffusion-based real-world image super-resolution (Real-ISR) methods have demonstrated impressive performance. To achieve efficient Real-ISR, many works employ Variational Score Distillation (VSD) to distill pre-trained stable-diffusion (SD) model for one-step SR with a fixed timestep. However, due to the different noise injection timesteps, the SD will perform different generative priors. Therefore, a fixed timestep is difficult for these methods to fully leverage the generative priors in SD, leading to suboptimal performance. To address this, we propose a Time-Aware one-step Diffusion Network for Real-ISR (TADSR). We first introduce a Time-Aware VAE Encoder, which projects the same image into different latent features based on timesteps. Through joint dynamic variation of timesteps and latent features, the student model can better align with the input pattern distribution of the pre-trained SD, thereby enabling more effective utilization of SD&#39;s generative capabilities. To better activate the generative prior of SD at different timesteps, we propose a Time-Aware VSD loss that bridges the timesteps of the student model and those of the teacher model, thereby producing more consistent generative prior guidance conditioned on timesteps. Additionally, though utilizing the generative prior in SD at different timesteps, our method can naturally achieve controllable trade-offs between fidelity and realism by changing the timestep condition. Experimental results demonstrate that our method achieves both state-of-the-art performance and controllable SR results with only a single step.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.16557v2-abstract-full').style.display = 'none'; document.getElementById('2508.16557v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 22 August, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.16065">arXiv:2508.16065</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.16065">pdf</a>, <a href="https://arxiv.org/ps/2508.16065">ps</a>, <a href="https://arxiv.org/format/2508.16065">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1007/s11704-025-50136-2">10.1007/s11704-025-50136-2 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Ethical Considerations of Large Language Models in Game Playing
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Q">Qingquan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yuchen Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yuan%2C+B">Bo Yuan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Togelius%2C+J">Julian Togelius</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yannakakis%2C+G+N">Georgios N. Yannakakis</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Jialin Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.16065v1-abstract-short" style="display: inline;">
        Large language models (LLMs) have demonstrated tremendous potential in game playing, while little attention has been paid to their ethical implications in those contexts. This work investigates and analyses the ethical considerations of applying LLMs in game playing, using Werewolf, also known as Mafia, as a case study. Gender bias, which affects game fairness and player experience, has been obser&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.16065v1-abstract-full').style.display = 'inline'; document.getElementById('2508.16065v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.16065v1-abstract-full" style="display: none;">
        Large language models (LLMs) have demonstrated tremendous potential in game playing, while little attention has been paid to their ethical implications in those contexts. This work investigates and analyses the ethical considerations of applying LLMs in game playing, using Werewolf, also known as Mafia, as a case study. Gender bias, which affects game fairness and player experience, has been observed from the behaviour of LLMs. Some roles, such as the Guard and Werewolf, are more sensitive than others to gender information, presented as a higher degree of behavioural change. We further examine scenarios in which gender information is implicitly conveyed through names, revealing that LLMs still exhibit discriminatory tendencies even in the absence of explicit gender labels. This research showcases the importance of developing fair and ethical LLMs. Beyond our research findings, we discuss the challenges and opportunities that lie ahead in this field, emphasising the need for diving deeper into the ethical implications of LLMs in gaming and other interactive domains.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.16065v1-abstract-full').style.display = 'none'; document.getElementById('2508.16065v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">19 pages</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Frontiers of Computer Science (2025)
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.15763">arXiv:2508.15763</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.15763">pdf</a>, <a href="https://arxiv.org/ps/2508.15763">ps</a>, <a href="https://arxiv.org/format/2508.15763">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Intern-S1: A Scientific Multimodal Foundation Model
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+L">Lei Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+Z">Zhongrui Cai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+Y">Yuhang Cao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+M">Maosong Cao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+W">Weihan Cao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+C">Chiyu Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+H">Haojiong Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+K">Kai Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+P">Pengcheng Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Ying Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yongkang Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+Y">Yu Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chu%2C+P">Pei Chu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chu%2C+T">Tao Chu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cui%2C+E">Erfei Cui</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cui%2C+G">Ganqu Cui</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cui%2C+L">Long Cui</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cui%2C+Z">Ziyun Cui</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+N">Nianchen Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ding%2C+N">Ning Ding</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dong%2C+N">Nanqing Dong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dong%2C+P">Peijie Dong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dou%2C+S">Shihan Dou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+S">Sinan Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Duan%2C+H">Haodong Duan</a>
      , et al. (152 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.15763v2-abstract-short" style="display: inline;">
        In recent years, a plethora of open-source foundation models have emerged, achieving remarkable progress in some widely attended fields, with performance being quite close to that of closed-source models. However, in high-value but more challenging scientific professional fields, either the fields still rely on expert models, or the progress of general foundation models lags significantly compared&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.15763v2-abstract-full').style.display = 'inline'; document.getElementById('2508.15763v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.15763v2-abstract-full" style="display: none;">
        In recent years, a plethora of open-source foundation models have emerged, achieving remarkable progress in some widely attended fields, with performance being quite close to that of closed-source models. However, in high-value but more challenging scientific professional fields, either the fields still rely on expert models, or the progress of general foundation models lags significantly compared to those in popular areas, far from sufficient for transforming scientific research and leaving substantial gap between open-source models and closed-source models in these scientific domains. To mitigate this gap and explore a step further toward Artificial General Intelligence (AGI), we introduce Intern-S1, a specialized generalist equipped with general understanding and reasoning capabilities with expertise to analyze multiple science modal data. Intern-S1 is a multimodal Mixture-of-Experts (MoE) model with 28 billion activated parameters and 241 billion total parameters, continually pre-trained on 5T tokens, including over 2.5T tokens from scientific domains. In the post-training stage, Intern-S1 undergoes offline and then online reinforcement learning (RL) in InternBootCamp, where we propose Mixture-of-Rewards (MoR) to synergize the RL training on more than 1000 tasks simultaneously. Through integrated innovations in algorithms, data, and training systems, Intern-S1 achieved top-tier performance in online RL training. On comprehensive evaluation benchmarks, Intern-S1 demonstrates competitive performance on general reasoning tasks among open-source models and significantly outperforms open-source models in scientific domains, surpassing closed-source state-of-the-art models in professional tasks, such as molecular synthesis planning, reaction condition prediction, predicting thermodynamic stabilities for crystals. Our models are available at https://huggingface.co/internlm/Intern-S1.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.15763v2-abstract-full').style.display = 'none'; document.getElementById('2508.15763v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 August, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.15606">arXiv:2508.15606</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.15606">pdf</a>, <a href="https://arxiv.org/ps/2508.15606">ps</a>, <a href="https://arxiv.org/format/2508.15606">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Cryptography and Security">cs.CR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Towards Scalable and Interpretable Mobile App Risk Analysis via Large Language Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Y">Yu Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zhenyuan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ran%2C+X">Xiandong Ran</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Jiahao Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jiahui Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+B">Bo Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ji%2C+S">Shouling Ji</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.15606v1-abstract-short" style="display: inline;">
        Mobile application marketplaces are responsible for vetting apps to identify and mitigate security risks. Current vetting processes are labor-intensive, relying on manual analysis by security professionals aided by semi-automated tools. To address this inefficiency, we propose Mars, a system that leverages Large Language Models (LLMs) for automated risk identification and profiling. Mars is design&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.15606v1-abstract-full').style.display = 'inline'; document.getElementById('2508.15606v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.15606v1-abstract-full" style="display: none;">
        Mobile application marketplaces are responsible for vetting apps to identify and mitigate security risks. Current vetting processes are labor-intensive, relying on manual analysis by security professionals aided by semi-automated tools. To address this inefficiency, we propose Mars, a system that leverages Large Language Models (LLMs) for automated risk identification and profiling. Mars is designed to concurrently analyze multiple applications across diverse risk categories with minimal human intervention. To enhance analytical precision and operational efficiency, Mars leverages a pre-constructed risk identification tree to extract relevant indicators from high-dimensional application features. This initial step filters the data, reducing the input volume for the LLM and mitigating the potential for model hallucination induced by irrelevant features. The extracted indicators are then subjected to LLM analysis for final risk determination. Furthermore, Mars automatically generates a comprehensive evidence chain for each assessment, documenting the analytical process to provide transparent justification. These chains are designed to facilitate subsequent manual review and to inform enforcement decisions, such as application delisting. The performance of Mars was evaluated on a real-world dataset from a partner Android marketplace. The results demonstrate that Mars attained an F1-score of 0.838 in risk identification and an F1-score of 0.934 in evidence retrieval. To assess its practical applicability, a user study involving 20 expert analysts was conducted, which indicated that Mars yielded a substantial efficiency gain, ranging from 60% to 90%, over conventional manual analysis.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.15606v1-abstract-full').style.display = 'none'; document.getElementById('2508.15606v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.15184">arXiv:2508.15184</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.15184">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Superconductivity">cond-mat.supr-con</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Mesoscale and Nanoscale Physics">cond-mat.mes-hall</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Materials Science">cond-mat.mtrl-sci</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1103/4mw4-4rfh">10.1103/4mw4-4rfh <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Quantum-size effect induced Andreev bound states in ultrathin metallic islands proximitized by a superconductor
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+G">Guanyong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+L">Li-Shuo Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+Z">Zhen Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+Y">Yue Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+B">Bo Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guan%2C+D">Dandan Guan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+S">Shiyong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yaoyi Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+C">Canhua Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+W">Wei Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+H">Hao Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jia%2C+J">Jinfeng Jia</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.15184v1-abstract-short" style="display: inline;">
        While Andreev bound states (ABSs) have been realized in engineered superconducting junctions, their direct observation in normal metal/superconductor heterostructures-enabled by quantum confinement-remains experimentally elusive. Here, we report the detection of ABSs in ultrathin metallic islands (Bi, Ag, and SnTe) grown on the s-wave superconductor NbN. Using high-resolution scanning tunneling mi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.15184v1-abstract-full').style.display = 'inline'; document.getElementById('2508.15184v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.15184v1-abstract-full" style="display: none;">
        While Andreev bound states (ABSs) have been realized in engineered superconducting junctions, their direct observation in normal metal/superconductor heterostructures-enabled by quantum confinement-remains experimentally elusive. Here, we report the detection of ABSs in ultrathin metallic islands (Bi, Ag, and SnTe) grown on the s-wave superconductor NbN. Using high-resolution scanning tunneling microscopy and spectroscopy, we clearly reveal in-gap ABSs with energies symmetric about the Fermi level. While the energies of these states show no position dependence, their wave functions exhibit spatial oscillations, demonstrating a quantum size effect. Both the energy levels and spatial distribution of the ABSs can be reproduced by our effective model in which a metallic island is coupled to the superconducting substrate via the proximity effect. We demonstrate that the coupling strength plays a critical role in determining the ABS energies. Our work introduces a novel physical platform for implementing ABSs, which hold promise for significant device applications.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.15184v1-abstract-full').style.display = 'none'; document.getElementById('2508.15184v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Physical Review Letters 135, 076201 (2025)
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.14948">arXiv:2508.14948</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.14948">pdf</a>, <a href="https://arxiv.org/ps/2508.14948">ps</a>, <a href="https://arxiv.org/format/2508.14948">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Large Foundation Model for Ads Recommendation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+S">Shangyu Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Quan%2C+S">Shijie Quan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Z">Zhongren Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pan%2C+J">Junwei Pan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhuang%2C+T">Tianqu Zhuang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fu%2C+B">Bo Fu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+Y">Yilong Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+J">Jieying Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+J">Jushuo Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xiaotian Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feng%2C+Z">Zhixiang Feng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+X">Xian Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+H">Huiting Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+H">Hua Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jinpeng Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+B">Boqi Dai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+X">Xiaoyu Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+B">Bin Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+L">Lili Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+Y">Yanwen Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+Y">Yeshou Cai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+Q">Qi Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+H">Huang Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+C">Chunfeng Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+C">Chengguo Yin</a>
      , et al. (8 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.14948v1-abstract-short" style="display: inline;">
        Online advertising relies on accurate recommendation models, with recent advances using pre-trained large-scale foundation models (LFMs) to capture users&#39; general interests across multiple scenarios and tasks. However, existing methods have critical limitations: they extract and transfer only user representations (URs), ignoring valuable item representations (IRs) and user-item cross representatio&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.14948v1-abstract-full').style.display = 'inline'; document.getElementById('2508.14948v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.14948v1-abstract-full" style="display: none;">
        Online advertising relies on accurate recommendation models, with recent advances using pre-trained large-scale foundation models (LFMs) to capture users&#39; general interests across multiple scenarios and tasks. However, existing methods have critical limitations: they extract and transfer only user representations (URs), ignoring valuable item representations (IRs) and user-item cross representations (CRs); and they simply use a UR as a feature in downstream applications, which fails to bridge upstream-downstream gaps and overlooks more transfer granularities. In this paper, we propose LFM4Ads, an All-Representation Multi-Granularity transfer framework for ads recommendation. It first comprehensively transfers URs, IRs, and CRs, i.e., all available representations in the pre-trained foundation model. To effectively utilize the CRs, it identifies the optimal extraction layer and aggregates them into transferable coarse-grained forms. Furthermore, we enhance the transferability via multi-granularity mechanisms: non-linear adapters for feature-level transfer, an Isomorphic Interaction Module for module-level transfer, and Standalone Retrieval for model-level transfer. LFM4Ads has been successfully deployed in Tencent&#39;s industrial-scale advertising platform, processing tens of billions of daily samples while maintaining terabyte-scale model parameters with billions of sparse embedding keys across approximately two thousand features. Since its production deployment in Q4 2024, LFM4Ads has achieved 10+ successful production launches across various advertising scenarios, including primary ones like Weixin Moments and Channels. These launches achieve an overall GMV lift of 2.45% across the entire platform, translating to estimated annual revenue increases in the hundreds of millions of dollars.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.14948v1-abstract-full').style.display = 'none'; document.getElementById('2508.14948v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.14879">arXiv:2508.14879</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.14879">pdf</a>, <a href="https://arxiv.org/ps/2508.14879">ps</a>, <a href="https://arxiv.org/format/2508.14879">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MeshCoder: LLM-Powered Structured Mesh Code Generation from Point Clouds
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+B">Bingquan Dai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+L+R">Li Ray Luo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+Q">Qihong Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jie Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lian%2C+X">Xinyu Lian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+H">Hao Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qin%2C+M">Minghan Qin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+X">Xudong Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+B">Bo Dai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+H">Haoqian Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lyu%2C+Z">Zhaoyang Lyu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pang%2C+J">Jiangmiao Pang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.14879v2-abstract-short" style="display: inline;">
        Reconstructing 3D objects into editable programs is pivotal for applications like reverse engineering and shape editing. However, existing methods often rely on limited domain-specific languages (DSLs) and small-scale datasets, restricting their ability to model complex geometries and structures. To address these challenges, we introduce MeshCoder, a novel framework that reconstructs complex 3D ob&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.14879v2-abstract-full').style.display = 'inline'; document.getElementById('2508.14879v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.14879v2-abstract-full" style="display: none;">
        Reconstructing 3D objects into editable programs is pivotal for applications like reverse engineering and shape editing. However, existing methods often rely on limited domain-specific languages (DSLs) and small-scale datasets, restricting their ability to model complex geometries and structures. To address these challenges, we introduce MeshCoder, a novel framework that reconstructs complex 3D objects from point clouds into editable Blender Python scripts. We develop a comprehensive set of expressive Blender Python APIs capable of synthesizing intricate geometries. Leveraging these APIs, we construct a large-scale paired object-code dataset, where the code for each object is decomposed into distinct semantic parts. Subsequently, we train a multimodal large language model (LLM) that translates 3D point cloud into executable Blender Python scripts. Our approach not only achieves superior performance in shape-to-code reconstruction tasks but also facilitates intuitive geometric and topological editing through convenient code modifications. Furthermore, our code-based representation enhances the reasoning capabilities of LLMs in 3D shape understanding tasks. Together, these contributions establish MeshCoder as a powerful and flexible solution for programmatic 3D shape reconstruction and understanding. The project homepage is available at \href{https://daibingquan.github.io/MeshCoder}{this link}.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.14879v2-abstract-full').style.display = 'none'; document.getElementById('2508.14879v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 August, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.14405">arXiv:2508.14405</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.14405">pdf</a>, <a href="https://arxiv.org/ps/2508.14405">ps</a>, <a href="https://arxiv.org/format/2508.14405">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CTA-Flux: Integrating Chinese Cultural Semantics into High-Quality English Text-to-Image Communities
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gong%2C+Y">Yue Gong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+S">Shanyuan Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+L">Liuzhuozheng Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+J">Jian Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+B">Bo Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+L">Liebucha Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+X">Xiaoyu Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+Y">Yuhang Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Leng%2C+D">Dawei Leng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+Y">Yuhui Yin</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.14405v1-abstract-short" style="display: inline;">
        We proposed the Chinese Text Adapter-Flux (CTA-Flux). An adaptation method fits the Chinese text inputs to Flux, a powerful text-to-image (TTI) generative model initially trained on the English corpus. Despite the notable image generation ability conditioned on English text inputs, Flux performs poorly when processing non-English prompts, particularly due to linguistic and cultural biases inherent&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.14405v1-abstract-full').style.display = 'inline'; document.getElementById('2508.14405v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.14405v1-abstract-full" style="display: none;">
        We proposed the Chinese Text Adapter-Flux (CTA-Flux). An adaptation method fits the Chinese text inputs to Flux, a powerful text-to-image (TTI) generative model initially trained on the English corpus. Despite the notable image generation ability conditioned on English text inputs, Flux performs poorly when processing non-English prompts, particularly due to linguistic and cultural biases inherent in predominantly English-centric training datasets. Existing approaches, such as translating non-English prompts into English or finetuning models for bilingual mappings, inadequately address culturally specific semantics, compromising image authenticity and quality. To address this issue, we introduce a novel method to bridge Chinese semantic understanding with compatibility in English-centric TTI model communities. Existing approaches relying on ControlNet-like architectures typically require a massive parameter scale and lack direct control over Chinese semantics. In comparison, CTA-flux leverages MultiModal Diffusion Transformer (MMDiT) to control the Flux backbone directly, significantly reducing the number of parameters while enhancing the model&#39;s understanding of Chinese semantics. This integration significantly improves the generation quality and cultural authenticity without extensive retraining of the entire model, thus maintaining compatibility with existing text-to-image plugins such as LoRA, IP-Adapter, and ControlNet. Empirical evaluations demonstrate that CTA-flux supports Chinese and English prompts and achieves superior image generation quality, visual realism, and faithful depiction of Chinese semantics.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.14405v1-abstract-full').style.display = 'none'; document.getElementById('2508.14405v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.14383">arXiv:2508.14383</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.14383">pdf</a>, <a href="https://arxiv.org/ps/2508.14383">ps</a>, <a href="https://arxiv.org/format/2508.14383">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Offline Imitation Learning upon Arbitrary Demonstrations by Pre-Training Dynamics Representations
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+H">Haitong Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dai%2C+B">Bo Dai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ren%2C+Z">Zhaolin Ren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yebin Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+N">Na Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.14383v1-abstract-short" style="display: inline;">
        Limited data has become a major bottleneck in scaling up offline imitation learning (IL). In this paper, we propose enhancing IL performance under limited expert data by introducing a pre-training stage that learns dynamics representations, derived from factorizations of the transition dynamics. We first theoretically justify that the optimal decision variable of offline IL lies in the representat&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.14383v1-abstract-full').style.display = 'inline'; document.getElementById('2508.14383v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.14383v1-abstract-full" style="display: none;">
        Limited data has become a major bottleneck in scaling up offline imitation learning (IL). In this paper, we propose enhancing IL performance under limited expert data by introducing a pre-training stage that learns dynamics representations, derived from factorizations of the transition dynamics. We first theoretically justify that the optimal decision variable of offline IL lies in the representation space, significantly reducing the parameters to learn in the downstream IL. Moreover, the dynamics representations can be learned from arbitrary data collected with the same dynamics, allowing the reuse of massive non-expert data and mitigating the limited data issues. We present a tractable loss function inspired by noise contrastive estimation to learn the dynamics representations at the pre-training stage. Experiments on MuJoCo demonstrate that our proposed algorithm can mimic expert policies with as few as a single trajectory. Experiments on real quadrupeds show that we can leverage pre-trained dynamics representations from simulator data to learn to walk from a few real-world demonstrations.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.14383v1-abstract-full').style.display = 'none'; document.getElementById('2508.14383v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">7 pages, 5 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.13808">arXiv:2508.13808</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.13808">pdf</a>, <a href="https://arxiv.org/ps/2508.13808">ps</a>, <a href="https://arxiv.org/format/2508.13808">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Is-NeRF: In-scattering Neural Radiance Field for Blurred Images
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+N">Nan Luo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ye%2C+C">Chenglin Ye</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Jiaxu Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+G">Gang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wan%2C+B">Bo Wan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+D">Di Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+L">Lupeng Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiao%2C+J">Jun Xiao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.13808v1-abstract-short" style="display: inline;">
        Neural Radiance Fields (NeRF) has gained significant attention for its prominent implicit 3D representation and realistic novel view synthesis capabilities. Available works unexceptionally employ straight-line volume rendering, which struggles to handle sophisticated lightpath scenarios and introduces geometric ambiguities during training, particularly evident when processing motion-blurred images&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.13808v1-abstract-full').style.display = 'inline'; document.getElementById('2508.13808v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.13808v1-abstract-full" style="display: none;">
        Neural Radiance Fields (NeRF) has gained significant attention for its prominent implicit 3D representation and realistic novel view synthesis capabilities. Available works unexceptionally employ straight-line volume rendering, which struggles to handle sophisticated lightpath scenarios and introduces geometric ambiguities during training, particularly evident when processing motion-blurred images. To address these challenges, this work proposes a novel deblur neural radiance field, Is-NeRF, featuring explicit lightpath modeling in real-world environments. By unifying six common light propagation phenomena through an in-scattering representation, we establish a new scattering-aware volume rendering pipeline adaptable to complex lightpaths. Additionally, we introduce an adaptive learning strategy that enables autonomous determining of scattering directions and sampling intervals to capture finer object details. The proposed network jointly optimizes NeRF parameters, scattering parameters, and camera motions to recover fine-grained scene representations from blurry images. Comprehensive evaluations demonstrate that it effectively handles complex real-world scenarios, outperforming state-of-the-art approaches in generating high-fidelity images with accurate geometric details.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.13808v1-abstract-full').style.display = 'none'; document.getElementById('2508.13808v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 19 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.13477">arXiv:2508.13477</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.13477">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Mesoscale and Nanoscale Physics">cond-mat.mes-hall</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1038/s42005-025-02237-4">10.1038/s42005-025-02237-4 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Josephson diode effect in nanowire-based Andreev molecules
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+S">Shang Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+Y">Yiwen Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+J">Jiangbo He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+X">Xiaozhou Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jia%2C+Z">Zhongmou Jia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wei%2C+M">Min Wei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiao%2C+Y">Yiping Jiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+J">Jiezhong He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhuo%2C+E">Enna Zhuo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+X">Xuewei Cao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tong%2C+B">Bingbing Tong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dou%2C+Z">Ziwei Dou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+P">Peiling Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shen%2C+J">Jie Shen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+X">Xiaohui Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lyu%2C+Z">Zhaozheng Lyu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+G">Guangtong Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pan%2C+D">Dong Pan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+J">Jianhua Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+B">Bo Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+L">Li Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qu%2C+F">Fanming Qu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.13477v2-abstract-short" style="display: inline;">
        Superconducting systems exhibit non-reciprocal current transport under certain conditions of symmetry breaking, a phenomenon known as the superconducting diode effect. This effect allows for perfect rectification of supercurrent, and has received considerable research interest. We report the observation of the Josephson diode effect (JDE) in nanowire-based Andreev molecules, where the time-reversa&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.13477v2-abstract-full').style.display = 'inline'; document.getElementById('2508.13477v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.13477v2-abstract-full" style="display: none;">
        Superconducting systems exhibit non-reciprocal current transport under certain conditions of symmetry breaking, a phenomenon known as the superconducting diode effect. This effect allows for perfect rectification of supercurrent, and has received considerable research interest. We report the observation of the Josephson diode effect (JDE) in nanowire-based Andreev molecules, where the time-reversal and spatial-inversion symmetries of a Josephson junction (JJ) can be nonlocally broken by coherently coupling to another JJ. The JDE can be controlled using both non-local phase and gate voltages. Notably, the non-local phase can induce a sign reversal of the diode efficiency, a manifestation of regulating the probabilities of double elastic cotunneling and double-crossed Andreev reflection. Additionally, the diode efficiency can be further modulated by local and non-local gate voltages, exhibiting a central-peak feature in the gate-voltage space. Our theoretical calculations of the energy spectrum and the Josephson currents align well with the experimental results. These results demonstrate the non-local regulation of the JDE in Andreev molecules, offering significant implications for the control of multi-JJ devices and the development of advanced superconducting devices.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.13477v2-abstract-full').style.display = 'none'; document.getElementById('2508.13477v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 August, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">24 pages, 13 figures</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Communications Physics 8, 330 (2025)
      </p>
    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.13434">arXiv:2508.13434</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.13434">pdf</a>, <a href="https://arxiv.org/ps/2508.13434">ps</a>, <a href="https://arxiv.org/format/2508.13434">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        EventTSF: Event-Aware Non-Stationary Time Series Forecasting
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ge%2C+Y">Yunfeng Ge</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jin%2C+M">Ming Jin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+Y">Yiji Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+H">Hongyan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+B">Bo Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+C">Chang Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pan%2C+S">Shirui Pan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.13434v1-abstract-short" style="display: inline;">
        Time series forecasting plays a vital role in critical domains like energy and transportation, where non-stationary dynamics are deeply intertwined with events in other modalities such as texts. However, incorporating natural language-based external events to improve non-stationary forecasting remains largely unexplored, as most approaches still rely on a single modality, resulting in limited cont&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.13434v1-abstract-full').style.display = 'inline'; document.getElementById('2508.13434v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.13434v1-abstract-full" style="display: none;">
        Time series forecasting plays a vital role in critical domains like energy and transportation, where non-stationary dynamics are deeply intertwined with events in other modalities such as texts. However, incorporating natural language-based external events to improve non-stationary forecasting remains largely unexplored, as most approaches still rely on a single modality, resulting in limited contextual knowledge and model underperformance. Enabling fine-grained multimodal interactions between temporal and textual data is challenged by three fundamental issues: (1) the difficulty of fine-grained synchronization between time-varying discrete textual events and continuous time series; (2) the inherent temporal uncertainty introduced by textual semantics; and (3) the misalignment between textual event embeddings and multi-resolution temporal patterns. In this work, we address these challenges by introducing event-aware non-stationary time series forecasting (EventTSF), an autoregressive generation framework that integrates historical time series with textual events to make subsequent forecasts. Specifically, EventTSF uses autoregressive diffusion with flow matching at each step to capture nuanced temporal-event interactions. To handle event-induced uncertainty, flow matching timesteps are adaptively controlled according to event semantic signals. The underlying denoiser employs a multimodal U-shaped diffusion transformer that efficiently fuses temporal and textual modalities across different resolutions. Extensive experiments on 8 synthetic and real-world datasets show that EventTSF outperforms 12 baselines across diverse event-aware non-stationary time series forecasting scenarios, achieving substantial improvements of 10.7% higher forecasting accuracy and $1.13\times$ faster training efficiency.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.13434v1-abstract-full').style.display = 'none'; document.getElementById('2508.13434v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">13 pages, 10 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.13157">arXiv:2508.13157</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.13157">pdf</a>, <a href="https://arxiv.org/ps/2508.13157">ps</a>, <a href="https://arxiv.org/format/2508.13157">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Hardware Architecture">cs.AR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Image and Video Processing">eess.IV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Image2Net: Datasets, Benchmark and Hybrid Framework to Convert Analog Circuit Diagrams into Netlists
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+H">Haohang Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+C">Chengjie Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Q">Qihang Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+W">Wenhao Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+Y">Yongjian Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+W">Weiyu Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+A">Anlan Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zhijun Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+B">Bo Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qi%2C+L">Lei Qi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+J">Jun Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+Y">Yuan Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+L">Li Du</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.13157v1-abstract-short" style="display: inline;">
        Large Language Model (LLM) exhibits great potential in designing of analog integrated circuits (IC) because of its excellence in abstraction and generalization for knowledge. However, further development of LLM-based analog ICs heavily relies on textual description of analog ICs, while existing analog ICs are mostly illustrated in image-based circuit diagrams rather than text-based netlists. Conve&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.13157v1-abstract-full').style.display = 'inline'; document.getElementById('2508.13157v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.13157v1-abstract-full" style="display: none;">
        Large Language Model (LLM) exhibits great potential in designing of analog integrated circuits (IC) because of its excellence in abstraction and generalization for knowledge. However, further development of LLM-based analog ICs heavily relies on textual description of analog ICs, while existing analog ICs are mostly illustrated in image-based circuit diagrams rather than text-based netlists. Converting circuit diagrams to netlists help LLMs to enrich the knowledge of analog IC. Nevertheless, previously proposed conversion frameworks face challenges in further application because of limited support of image styles and circuit elements. Up to now, it still remains a challenging task to effectively convert complex circuit diagrams into netlists. To this end, this paper constructs and opensources a new dataset with rich styles of circuit diagrams as well as balanced distribution of simple and complex analog ICs. And a hybrid framework, named Image2Net, is proposed for practical conversion from circuit diagrams to netlists. The netlist edit distance (NED) is also introduced to precisely assess the difference between the converted netlists and ground truth. Based on our benchmark, Image2Net achieves 80.77\% successful rate, which is 34.62\%-45.19\% higher than previous works. Specifically, the proposed work shows 0.116 averaged NED, which is 62.1\%-69.6\% lower than state-of-the-arts.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.13157v1-abstract-full').style.display = 'none'; document.getElementById('2508.13157v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages, 12 figures, 6 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.13142">arXiv:2508.13142</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.13142">pdf</a>, <a href="https://arxiv.org/ps/2508.13142">ps</a>, <a href="https://arxiv.org/format/2508.13142">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Has GPT-5 Achieved Spatial Intelligence? An Empirical Study
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+Z">Zhongang Cai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yubo Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+Q">Qingping Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+R">Ruisi Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gu%2C+C">Chenyang Gu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+W">Wanqi Yin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+Z">Zhiqian Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Z">Zhitao Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wei%2C+C">Chen Wei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shi%2C+X">Xuanke Shi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+K">Kewang Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+X">Xiaoyang Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Z">Zukai Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Jiaqi Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fan%2C+X">Xiangyu Fan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+H">Hanming Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+L">Lewei Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+B">Bo Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Z">Ziwei Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Q">Quan Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+D">Dahua Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+L">Lei Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.13142v1-abstract-short" style="display: inline;">
        Multi-modal models have achieved remarkable progress in recent years. Nevertheless, they continue to exhibit notable limitations in spatial understanding and reasoning, which are fundamental capabilities to achieving artificial general intelligence. With the recent release of GPT-5, allegedly the most powerful AI model to date, it is timely to examine where the leading models stand on the path tow&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.13142v1-abstract-full').style.display = 'inline'; document.getElementById('2508.13142v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.13142v1-abstract-full" style="display: none;">
        Multi-modal models have achieved remarkable progress in recent years. Nevertheless, they continue to exhibit notable limitations in spatial understanding and reasoning, which are fundamental capabilities to achieving artificial general intelligence. With the recent release of GPT-5, allegedly the most powerful AI model to date, it is timely to examine where the leading models stand on the path toward spatial intelligence. First, we propose a comprehensive taxonomy of spatial tasks that unifies existing benchmarks and discuss the challenges in ensuring fair evaluation. We then evaluate state-of-the-art proprietary and open-source models on eight key benchmarks, at a cost exceeding one billion total tokens. Our empirical study reveals that (1) GPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2) still falls short of human performance across a broad spectrum of tasks. Moreover, we (3) identify the more challenging spatial intelligence problems for multi-modal models, and (4) proprietary models do not exhibit a decisive advantage when facing the most difficult problems. In addition, we conduct a qualitative evaluation across a diverse set of scenarios that are intuitive for humans yet fail even the most advanced multi-modal models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.13142v1-abstract-full').style.display = 'none'; document.getElementById('2508.13142v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.12857">arXiv:2508.12857</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.12857">pdf</a>, <a href="https://arxiv.org/ps/2508.12857">ps</a>, <a href="https://arxiv.org/format/2508.12857">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Networking and Internet Architecture">cs.NI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        REACH: Reinforcement Learning for Efficient Allocation in Community and Heterogeneous Networks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+Z">Zhiwei Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+C">Chengze Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+H">Heng Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+Y">Ying Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+B">Bo Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Jialong Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.12857v1-abstract-short" style="display: inline;">
        Community GPU platforms are emerging as a cost-effective and democratized alternative to centralized GPU clusters for AI workloads, aggregating idle consumer GPUs from globally distributed and heterogeneous environments. However, their extreme hardware/software diversity, volatile availability, and variable network conditions render traditional schedulers ineffective, leading to suboptimal task co&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.12857v1-abstract-full').style.display = 'inline'; document.getElementById('2508.12857v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.12857v1-abstract-full" style="display: none;">
        Community GPU platforms are emerging as a cost-effective and democratized alternative to centralized GPU clusters for AI workloads, aggregating idle consumer GPUs from globally distributed and heterogeneous environments. However, their extreme hardware/software diversity, volatile availability, and variable network conditions render traditional schedulers ineffective, leading to suboptimal task completion. In this work, we present REACH (Reinforcement Learning for Efficient Allocation in Community and Heterogeneous Networks), a Transformer-based reinforcement learning framework that redefines task scheduling as a sequence scoring problem to balance performance, reliability, cost, and network efficiency. By modeling both global GPU states and task requirements, REACH learns to adaptively co-locate computation with data, prioritize critical jobs, and mitigate the impact of unreliable resources. Extensive simulation results show that REACH improves task completion rates by up to 17%, more than doubles the success rate for high-priority tasks, and reduces bandwidth penalties by over 80% compared to state-of-the-art baselines. Stress tests further demonstrate its robustness to GPU churn and network congestion, while scalability experiments confirm its effectiveness in large-scale, high-contention scenarios.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.12857v1-abstract-full').style.display = 'none'; document.getElementById('2508.12857v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.12750">arXiv:2508.12750</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.12750">pdf</a>, <a href="https://arxiv.org/ps/2508.12750">ps</a>, <a href="https://arxiv.org/format/2508.12750">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        D2-Mamba: Dual-Scale Fusion and Dual-Path Scanning with SSMs for Shadow Removal
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+L">Linhao Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jin%2C+B">Boya Jin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zizhe Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+L">Lanqing Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+H">Hao Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+B">Bo Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dong%2C+Y">Yongfeng Dong</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.12750v2-abstract-short" style="display: inline;">
        Shadow removal aims to restore images that are partially degraded by shadows, where the degradation is spatially localized and non-uniform. Unlike general restoration tasks that assume global degradation, shadow removal can leverage abundant information from non-shadow regions for guidance. However, the transformation required to correct shadowed areas often differs significantly from that of well&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.12750v2-abstract-full').style.display = 'inline'; document.getElementById('2508.12750v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.12750v2-abstract-full" style="display: none;">
        Shadow removal aims to restore images that are partially degraded by shadows, where the degradation is spatially localized and non-uniform. Unlike general restoration tasks that assume global degradation, shadow removal can leverage abundant information from non-shadow regions for guidance. However, the transformation required to correct shadowed areas often differs significantly from that of well-lit regions, making it challenging to apply uniform correction strategies. This necessitates the effective integration of non-local contextual cues and adaptive modeling of region-specific transformations. To this end, we propose a novel Mamba-based network featuring dual-scale fusion and dual-path scanning to selectively propagate contextual information based on transformation similarity across regions. Specifically, the proposed Dual-Scale Fusion Mamba Block (DFMB) enhances multi-scale feature representation by fusing original features with low-resolution features, effectively reducing boundary artifacts. The Dual-Path Mamba Group (DPMG) captures global features via horizontal scanning and incorporates a mask-aware adaptive scanning strategy, which improves structural continuity and fine-grained region modeling. Experimental results demonstrate that our method significantly outperforms existing state-of-the-art approaches on shadow removal benchmarks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.12750v2-abstract-full').style.display = 'none'; document.getElementById('2508.12750v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 18 August, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Paper Under Review</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.12726">arXiv:2508.12726</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.12726">pdf</a>, <a href="https://arxiv.org/ps/2508.12726">ps</a>, <a href="https://arxiv.org/format/2508.12726">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        DESIGNER: Design-Logic-Guided Multidisciplinary Data Synthesis for LLM Reasoning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+W">Weize Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+Y">Yongchi Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+Y">Yijia Luo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+M">Mingyu Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Jiaheng Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yanan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+X">Xiguo Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+Y">Yuchi Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Su%2C+W">Wenbo Su</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+B">Bo Zheng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.12726v1-abstract-short" style="display: inline;">
        Large language models (LLMs) have achieved remarkable success in many natural language tasks but still struggle with complex, multi-step reasoning, particularly across diverse disciplines. Existing reasoning datasets often either lack disciplinary breadth or the structural depth necessary to elicit robust reasoning behaviors. We propose DESIGNER: a DESIGN-logic-guidEd Reasoning data synthesis pipe&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.12726v1-abstract-full').style.display = 'inline'; document.getElementById('2508.12726v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.12726v1-abstract-full" style="display: none;">
        Large language models (LLMs) have achieved remarkable success in many natural language tasks but still struggle with complex, multi-step reasoning, particularly across diverse disciplines. Existing reasoning datasets often either lack disciplinary breadth or the structural depth necessary to elicit robust reasoning behaviors. We propose DESIGNER: a DESIGN-logic-guidEd Reasoning data synthesis pipeline that leverages naturally available, extensive raw documents (book corpus and web corpus) to generate multidisciplinary challenging questions. A core innovation of our approach is the introduction of a Design Logic concept, which mimics the question-creation process of human educators. We use LLMs to reverse-engineer and abstract over 120,000 design logics from existing questions across various disciplines. By matching these design logics with disciplinary source materials, we are able to create reasoning questions that far surpass the difficulty and diversity of existing datasets. Based on this pipeline, we synthesized two large-scale reasoning datasets that span 75 disciplines: Design-Logic-Reasoning-Book (DLR-Book), containing 3.04 million challenging questions synthesized from the book corpus, and Design-Logic-Reasoning-Web (DLR-Web), with 1.66 million challenging questions from the web corpus. Our data analysis demonstrates that the questions synthesized by our method exhibit substantially greater difficulty and diversity than those in the baseline datasets. We validate the effectiveness of these datasets by conducting SFT experiments on the Qwen3-8B-Base and Qwen3-4B-Base models. The results show that our dataset significantly outperforms existing multidisciplinary datasets of the same volume. Training with the full datasets further enables the models to surpass the multidisciplinary reasoning performance of the official Qwen3-8B and Qwen3-4B models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.12726v1-abstract-full').style.display = 'none'; document.getElementById('2508.12726v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.12524">arXiv:2508.12524</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.12524">pdf</a>, <a href="https://arxiv.org/ps/2508.12524">ps</a>, <a href="https://arxiv.org/format/2508.12524">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Results of the NeurIPS 2023 Neural MMO Competition on Multi-task Reinforcement Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Su%C3%A1rez%2C+J">Joseph Suárez</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Choe%2C+K+W">Kyoung Whan Choe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bloomin%2C+D">David Bloomin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+J">Jianming Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yunkun Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Feng%2C+Y">Yao Feng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pola%2C+S">Saidinesh Pola</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+K">Kun Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+Y">Yonghui Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pinnaparaju%2C+N">Nikhil Pinnaparaju</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+H+X">Hao Xiang Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kanna%2C+N">Nishaanth Kanna</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Scott%2C+D">Daniel Scott</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sullivan%2C+R">Ryan Sullivan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shuman%2C+R+S">Rose S. Shuman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=de+Alc%C3%A2ntara%2C+L">Lucas de Alcântara</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bradley%2C+H">Herbie Bradley</a>, 
      
      <a href="/search/?searchtype=author&amp;query=You%2C+K">Kirsty You</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+B">Bo Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+Y">Yuhao Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Q">Qimai Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+J">Jiaxin Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castricato%2C+L">Louis Castricato</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+X">Xiaolong Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Isola%2C+P">Phillip Isola</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.12524v1-abstract-short" style="display: inline;">
        We present the results of the NeurIPS 2023 Neural MMO Competition, which attracted over 200 participants and submissions. Participants trained goal-conditional policies that generalize to tasks, maps, and opponents never seen during training. The top solution achieved a score 4x higher than our baseline within 8 hours of training on a single 4090 GPU. We open-source everything relating to Neural M&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.12524v1-abstract-full').style.display = 'inline'; document.getElementById('2508.12524v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.12524v1-abstract-full" style="display: none;">
        We present the results of the NeurIPS 2023 Neural MMO Competition, which attracted over 200 participants and submissions. Participants trained goal-conditional policies that generalize to tasks, maps, and opponents never seen during training. The top solution achieved a score 4x higher than our baseline within 8 hours of training on a single 4090 GPU. We open-source everything relating to Neural MMO and the competition under the MIT license, including the policy weights and training code for our baseline and for the top submissions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.12524v1-abstract-full').style.display = 'none'; document.getElementById('2508.12524v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 17 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.11883">arXiv:2508.11883</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.11883">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Bioinspired underwater soft robots: from biology to robotics and back
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+L">Lei Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qin%2C+B">Boyang Qin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+W">Wenzhuo Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yanyu Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yiyuan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+B">Bo Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kong%2C+S">Shihan Kong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jian Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+D">Dekui He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+J">Junzhi Yu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.11883v1-abstract-short" style="display: inline;">
        The ocean vast unexplored regions and diverse soft-bodied marine organisms have spurred interest in bio-inspired underwater soft robotics. Recent advances have enabled new capabilities in underwater movement, sensing, and interaction. However, these efforts are largely unidirectional, with biology guiding robotics while insights from robotics rarely feed back into biology. Here we propose a holist&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.11883v1-abstract-full').style.display = 'inline'; document.getElementById('2508.11883v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.11883v1-abstract-full" style="display: none;">
        The ocean vast unexplored regions and diverse soft-bodied marine organisms have spurred interest in bio-inspired underwater soft robotics. Recent advances have enabled new capabilities in underwater movement, sensing, and interaction. However, these efforts are largely unidirectional, with biology guiding robotics while insights from robotics rarely feed back into biology. Here we propose a holistic, bidirectional framework that integrates biological principles, robotic implementation, and biological validation. We show that soft robots can serve as experimental tools to probe biological functions and even test evolutionary hypotheses. Their inherent compliance also allows them to outperform rigid systems in unstructured environments, supporting applications in marine exploration, manipulation, and medicine. Looking forward, we introduce bio-universal-inspired robotics, a paradigm that transcends species-specific mimicry by identifying convergent principles across species to inspire more adaptable designs. Despite rapid progress, challenges persist in material robustness, actuation efficiency, autonomy, and intelligence. By uniting biology and engineering, soft robots can advance ocean exploration and deepen scientific discovery.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.11883v1-abstract-full').style.display = 'none'; document.getElementById('2508.11883v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.11400">arXiv:2508.11400</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.11400">pdf</a>, <a href="https://arxiv.org/ps/2508.11400">ps</a>, <a href="https://arxiv.org/format/2508.11400">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="High Energy Physics - Experiment">hep-ex</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        The Production and Decay Dynamics of the Charmed Baryon $Λ_c^+$ in $e^+e^-$ Annihilations near Threshold
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=BESIII+Collaboration"> BESIII Collaboration</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ablikim%2C+M">M. Ablikim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Achasov%2C+M+N">M. N. Achasov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Adlarson%2C+P">P. Adlarson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ai%2C+X+C">X. C. Ai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Aliberti%2C+R">R. Aliberti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Amoroso%2C+A">A. Amoroso</a>, 
      
      <a href="/search/?searchtype=author&amp;query=An%2C+Q">Q. An</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+Y">Y. Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bakina%2C+O">O. Bakina</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ban%2C+Y">Y. Ban</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bao%2C+H+-">H. -R. Bao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Batozskaya%2C+V">V. Batozskaya</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Begzsuren%2C+K">K. Begzsuren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berger%2C+N">N. Berger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Berlowski%2C+M">M. Berlowski</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bertani%2C+M+B">M. B. Bertani</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bettoni%2C+D">D. Bettoni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianchi%2C+F">F. Bianchi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bianco%2C+E">E. Bianco</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bortone%2C+A">A. Bortone</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Boyko%2C+I">I. Boyko</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Briere%2C+R+A">R. A. Briere</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brueggemann%2C+A">A. Brueggemann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+H">H. Cai</a>
      , et al. (706 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.11400v2-abstract-short" style="display: inline;">
        The study of the charmed baryons is crucial for investigating the strong and weak interactions in the Standard Model and for gaining insights into the internal structure of baryons. In an $e^+e^-$ experiment the lightest charmed baryon, $Λ_c^+$, can be produced in pairs through the single photon annihilation process. This process can be described by two complex electromagnetic form factors. The pr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.11400v2-abstract-full').style.display = 'inline'; document.getElementById('2508.11400v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.11400v2-abstract-full" style="display: none;">
        The study of the charmed baryons is crucial for investigating the strong and weak interactions in the Standard Model and for gaining insights into the internal structure of baryons. In an $e^+e^-$ experiment the lightest charmed baryon, $Λ_c^+$, can be produced in pairs through the single photon annihilation process. This process can be described by two complex electromagnetic form factors. The presence of a non-zero relative phase between these form factors gives rise to a transverse polarization of the charmed baryon and provides additional constraints on the dynamic parameters in the decays. In this article, we present the first observation of the transverse polarization of $Λ_{c}^{+}$ in the reaction $e^+e^- \to Λ_c^{+}\barΛ_c^-$, based on $6.4~\text{fb}^{-1}$ of $e^{+}e^{-}$ annihilation data collected at center-of-mass energies between 4600 MeV and 4951 MeV with the BESIII detector. The decay asymmetry parameters and strong phase shift in the decays $Λ_c^+ \to pK_S^0$, $Λπ^+$, $Σ^0π^+$, $Σ^+π^0$ are also simultaneously extracted from the joint angular distributions. These results are vital for understanding CP violation and its role in the matter-antimatter asymmetry of the Universe.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.11400v2-abstract-full').style.display = 'none'; document.getElementById('2508.11400v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 15 August, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">21 pages, 8 figures</span>
    </p>
    

    

    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?query=Bo+Li&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?query=Bo+Li&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/?query=Bo+Li&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
              class="pagination-link "
              aria-label="Page 2"
              aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/?query=Bo+Li&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=100"
              class="pagination-link "
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/?query=Bo+Li&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=150"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/?query=Bo+Li&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=200"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  

  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>