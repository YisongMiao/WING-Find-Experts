<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;26 of 26 results for author: <span class="mathjax">Ana Paiva</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/"  aria-role="search">
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Ana Paiva">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Ana+Paiva&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Ana Paiva">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      




<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.21631">arXiv:2507.21631</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.21631">pdf</a>, <a href="https://arxiv.org/ps/2507.21631">ps</a>, <a href="https://arxiv.org/format/2507.21631">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multiagent Systems">cs.MA</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        &#34;Teammates, Am I Clear?&#34;: Analysing Legible Behaviours in Teams
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Faria%2C+M">Miguel Faria</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Melo%2C+F+S">Francisco S. Melo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Paiva%2C+A">Ana Paiva</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.21631v1-abstract-short" style="display: inline;">
        In this paper we investigate the notion of legibility in sequential decision-making in the context of teams and teamwork. There have been works that extend the notion of legibility to sequential decision making, for deterministic and for stochastic scenarios. However, these works focus on one agent interacting with one human, foregoing the benefits of having legible decision making in teams of age&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.21631v1-abstract-full').style.display = 'inline'; document.getElementById('2507.21631v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.21631v1-abstract-full" style="display: none;">
        In this paper we investigate the notion of legibility in sequential decision-making in the context of teams and teamwork. There have been works that extend the notion of legibility to sequential decision making, for deterministic and for stochastic scenarios. However, these works focus on one agent interacting with one human, foregoing the benefits of having legible decision making in teams of agents or in team configurations with humans. In this work we propose an extension of legible decision-making to multi-agent settings that improves the performance of agents working in collaboration. We showcase the performance of legible decision making in team scenarios using our proposed extension in multi-agent benchmark scenarios. We show that a team with a legible agent is able to outperform a team composed solely of agents with standard optimal behaviour.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.21631v1-abstract-full').style.display = 'none'; document.getElementById('2507.21631v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 July, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.04972">arXiv:2506.04972</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.04972">pdf</a>, <a href="https://arxiv.org/ps/2506.04972">ps</a>, <a href="https://arxiv.org/format/2506.04972">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Emerging Technologies">cs.ET</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        From Screen to Space: Evaluating Siemens&#39; Cinematic Reality
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Luijten%2C+G">Gijs Luijten</a>, 
      
      <a href="/search/?searchtype=author&amp;query=de+Paiva%2C+L+F">Lisle Faray de Paiva</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Krueger%2C+S">Sebastian Krueger</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Brost%2C+A">Alexander Brost</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mazilescu%2C+L">Laura Mazilescu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Santos%2C+A+S+F">Ana Sofia Ferreira Santos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hoyer%2C+P">Peter Hoyer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kleesiek%2C+J">Jens Kleesiek</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schmitz%2C+S+M">Sophia Marie-Therese Schmitz</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Neumann%2C+U+P">Ulf Peter Neumann</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Egger%2C+J">Jan Egger</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.04972v1-abstract-short" style="display: inline;">
        As one of the first research teams with full access to Siemens&#39; Cinematic Reality, we evaluate its usability and clinical potential for cinematic volume rendering on the Apple Vision Pro. We visualized venous-phase liver computed tomography and magnetic resonance cholangiopancreatography scans from the CHAOS and MRCP\_DLRecon datasets. Fourteen medical experts assessed usability and anticipated cl&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.04972v1-abstract-full').style.display = 'inline'; document.getElementById('2506.04972v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.04972v1-abstract-full" style="display: none;">
        As one of the first research teams with full access to Siemens&#39; Cinematic Reality, we evaluate its usability and clinical potential for cinematic volume rendering on the Apple Vision Pro. We visualized venous-phase liver computed tomography and magnetic resonance cholangiopancreatography scans from the CHAOS and MRCP\_DLRecon datasets. Fourteen medical experts assessed usability and anticipated clinical integration potential using the System Usability Scale, ISONORM 9242-110-S questionnaire, and an open-ended survey. Their feedback identified feasibility, key usability strengths, and required features to catalyze the adaptation in real-world clinical workflows. The findings provide insights into the potential of immersive cinematic rendering in medical imaging.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.04972v1-abstract-full').style.display = 'none'; document.getElementById('2506.04972v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">16 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2506.04858">arXiv:2506.04858</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2506.04858">pdf</a>, <a href="https://arxiv.org/ps/2506.04858">ps</a>, <a href="https://arxiv.org/format/2506.04858">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Beyond the Desktop: XR-Driven Segmentation with Meta Quest 3 and MX Ink
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=de+Paiva%2C+L+F">Lisle Faray de Paiva</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luijten%2C+G">Gijs Luijten</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Santos%2C+A+S+F">Ana Sofia Ferreira Santos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+M">Moon Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Puladi%2C+B">Behrus Puladi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kleesiek%2C+J">Jens Kleesiek</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Egger%2C+J">Jan Egger</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2506.04858v1-abstract-short" style="display: inline;">
        Medical imaging segmentation is essential in clinical settings for diagnosing diseases, planning surgeries, and other procedures. However, manual annotation is a cumbersome and effortful task. To mitigate these aspects, this study implements and evaluates the usability and clinical applicability of an extended reality (XR)-based segmentation tool for anatomical CT scans, using the Meta Quest 3 hea&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.04858v1-abstract-full').style.display = 'inline'; document.getElementById('2506.04858v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2506.04858v1-abstract-full" style="display: none;">
        Medical imaging segmentation is essential in clinical settings for diagnosing diseases, planning surgeries, and other procedures. However, manual annotation is a cumbersome and effortful task. To mitigate these aspects, this study implements and evaluates the usability and clinical applicability of an extended reality (XR)-based segmentation tool for anatomical CT scans, using the Meta Quest 3 headset and Logitech MX Ink stylus. We develop an immersive interface enabling real-time interaction with 2D and 3D medical imaging data in a customizable workspace designed to mitigate workflow fragmentation and cognitive demands inherent to conventional manual segmentation tools. The platform combines stylus-driven annotation, mirroring traditional pen-on-paper workflows, with instant 3D volumetric rendering. A user study with a public craniofacial CT dataset demonstrated the tool&#39;s foundational viability, achieving a System Usability Scale (SUS) score of 66, within the expected range for medical applications. Participants highlighted the system&#39;s intuitive controls (scoring 4.1/5 for self-descriptiveness on ISONORM metrics) and spatial interaction design, with qualitative feedback highlighting strengths in hybrid 2D/3D navigation and realistic stylus ergonomics. While users identified opportunities to enhance task-specific precision and error management, the platform&#39;s core workflow enabled dynamic slice adjustment, reducing cognitive load compared to desktop tools. Results position the XR-stylus paradigm as a promising foundation for immersive segmentation tools, with iterative refinements targeting haptic feedback calibration and workflow personalization to advance adoption in preoperative planning.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2506.04858v1-abstract-full').style.display = 'none'; document.getElementById('2506.04858v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 June, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">10 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2501.02875">arXiv:2501.02875</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2501.02875">pdf</a>, <a href="https://arxiv.org/format/2501.02875">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        METFORD -- Mutation tEsTing Framework fOR anDroid
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Vincenzi%2C+A+M+R">Auri M. R. Vincenzi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kuroishi%2C+P+H">Pedro H. Kuroishi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bispo%2C+J+C+M">João C. M. Bispo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=da+Veiga%2C+A+R+C">Ana R. C. da Veiga</a>, 
      
      <a href="/search/?searchtype=author&amp;query=da+Mata%2C+D+R+C">David R. C. da Mata</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Azevedo%2C+F+B">Francisco B. Azevedo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Paiva%2C+A+C+R">Ana C. R. Paiva</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2501.02875v2-abstract-short" style="display: inline;">
        Mutation testing may be used to guide test case generation and as a technique to assess the quality of test suites. Despite being used frequently, mutation testing is not so commonly applied in the mobile world. One critical challenge in mutation testing is dealing with its computational cost. Generating mutants, running test cases over each mutant, and analyzing the results may require significan&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2501.02875v2-abstract-full').style.display = 'inline'; document.getElementById('2501.02875v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2501.02875v2-abstract-full" style="display: none;">
        Mutation testing may be used to guide test case generation and as a technique to assess the quality of test suites. Despite being used frequently, mutation testing is not so commonly applied in the mobile world. One critical challenge in mutation testing is dealing with its computational cost. Generating mutants, running test cases over each mutant, and analyzing the results may require significant time and resources. This research aims to contribute to reducing Android mutation testing costs. It implements mutation testing operators (traditional and Android-specific) according to mutant schemata (implementing multiple mutants into a single code file). It also describes an Android mutation testing framework developed to execute test cases and determine mutation scores. Additional mutation operators can be implemented in JavaScript and easily integrated into the framework. The overall approach is validated through case studies showing that mutant schemata have advantages over the traditional mutation strategy (one file per mutant). The results show mutant schemata overcome traditional mutation in all evaluated aspects with no additional cost: it takes 8.50% less time for mutant generation, requires 99.78% less disk space, and runs, on average, 6.45% faster than traditional mutation. Moreover, considering sustainability metrics, mutant schemata have 8,18% less carbon footprint than traditional strategy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2501.02875v2-abstract-full').style.display = 'none'; document.getElementById('2501.02875v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 January, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 6 January, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accept for publication in the Journal of System and Software - JSS. This work is partially supported by Brazilian Funding Agencies FAPESP (Grant n. 2019/23160-0 and 2023/00001-9), CAPES, and CNPq</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2307.06007">arXiv:2307.06007</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2307.06007">pdf</a>, <a href="https://arxiv.org/format/2307.06007">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Building Persuasive Robots with Social Power Strategies
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hashemian%2C+M">Mojgan Hashemian</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Couto%2C+M">Marta Couto</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mascarenhas%2C+S">Samuel Mascarenhas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Paiva%2C+A">Ana Paiva</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Santos%2C+P+A">Pedro A. Santos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Prada%2C+R">Rui Prada</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2307.06007v2-abstract-short" style="display: inline;">
        Can social power endow social robots with the capacity to persuade? This paper represents our recent endeavor to design persuasive social robots. We have designed and run three different user studies to investigate the effectiveness of different bases of social power (inspired by French and Raven&#39;s theory) on peoples&#39; compliance to the requests of social robots. The results show that robotic persu&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2307.06007v2-abstract-full').style.display = 'inline'; document.getElementById('2307.06007v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2307.06007v2-abstract-full" style="display: none;">
        Can social power endow social robots with the capacity to persuade? This paper represents our recent endeavor to design persuasive social robots. We have designed and run three different user studies to investigate the effectiveness of different bases of social power (inspired by French and Raven&#39;s theory) on peoples&#39; compliance to the requests of social robots. The results show that robotic persuaders that exert social power (specifically from expert, reward, and coercion bases) demonstrate increased ability to influence humans. The first study provides a positive answer and shows that under the same circumstances, people with different personalities prefer robots using a specific social power base. In addition, social rewards can be useful in persuading individuals. The second study suggests that by employing social power, social robots are capable of persuading people objectively to select a less desirable choice among others. Finally, the third study shows that the effect of power on persuasion does not decay over time and might strengthen under specific circumstances. Moreover, exerting stronger social power does not necessarily lead to higher persuasion. Overall, we argue that the results of these studies are relevant for designing human--robot-interaction scenarios especially the ones aiming at behavioral change.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2307.06007v2-abstract-full').style.display = 'none'; document.getElementById('2307.06007v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 September, 2023; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 12 July, 2023;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2023.
      
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.9; H.5
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2302.09014">arXiv:2302.09014</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2302.09014">pdf</a>, <a href="https://arxiv.org/ps/2302.09014">ps</a>, <a href="https://arxiv.org/format/2302.09014">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Algebraic Geometry">math.AG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Automorphisms of quartic surfaces and Cremona transformations
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Paiva%2C+D">Daniela Paiva</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Quedo%2C+A">Ana Quedo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2302.09014v2-abstract-short" style="display: inline;">
        In this paper, we consider the problem of determining which automorphisms of a smooth quartic surface $S \subset \mathbb{P}^3$ are induced by a Cremona transformation of $\mathbb{P}^3$. We provide the first steps towards a complete solution of this problem when $ρ(S)=2$. In particular, we give several examples of quartics whose automorphism groups are generated by involutions, but no non-trivial a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2302.09014v2-abstract-full').style.display = 'inline'; document.getElementById('2302.09014v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2302.09014v2-abstract-full" style="display: none;">
        In this paper, we consider the problem of determining which automorphisms of a smooth quartic surface $S \subset \mathbb{P}^3$ are induced by a Cremona transformation of $\mathbb{P}^3$. We provide the first steps towards a complete solution of this problem when $ρ(S)=2$. In particular, we give several examples of quartics whose automorphism groups are generated by involutions, but no non-trivial automorphism is induced by a Cremona transformation of $\mathbb{P}^3$, giving a negative answer for Oguiso&#39;s question of whether every automorphism of finite order of a smooth quartic surface $S\subset \mathbb{P}^3$ is induced by a Cremona transformation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2302.09014v2-abstract-full').style.display = 'none'; document.getElementById('2302.09014v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 April, 2024; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 17 February, 2023;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2023.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2212.01897">arXiv:2212.01897</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2212.01897">pdf</a>, <a href="https://arxiv.org/format/2212.01897">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Characterizing instance hardness in classification and regression problems
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Torquette%2C+G+P">Gustavo P. Torquette</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nunes%2C+V+S">Victor S. Nunes</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Paiva%2C+P+Y+A">Pedro Y. A. Paiva</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Neto%2C+L+B+C">Lourenço B. C. Neto</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lorena%2C+A+C">Ana C. Lorena</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2212.01897v1-abstract-short" style="display: inline;">
        Some recent pieces of work in the Machine Learning (ML) literature have demonstrated the usefulness of assessing which observations are hardest to have their label predicted accurately. By identifying such instances, one may inspect whether they have any quality issues that should be addressed. Learning strategies based on the difficulty level of the observations can also be devised. This paper pr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2212.01897v1-abstract-full').style.display = 'inline'; document.getElementById('2212.01897v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2212.01897v1-abstract-full" style="display: none;">
        Some recent pieces of work in the Machine Learning (ML) literature have demonstrated the usefulness of assessing which observations are hardest to have their label predicted accurately. By identifying such instances, one may inspect whether they have any quality issues that should be addressed. Learning strategies based on the difficulty level of the observations can also be devised. This paper presents a set of meta-features that aim at characterizing which instances of a dataset are hardest to have their label predicted accurately and why they are so, aka instance hardness measures. Both classification and regression problems are considered. Synthetic datasets with different levels of complexity are built and analyzed. A Python package containing all implementations is also provided.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2212.01897v1-abstract-full').style.display = 'none'; document.getElementById('2212.01897v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 December, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> December 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2210.11161">arXiv:2210.11161</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2210.11161">pdf</a>, <a href="https://arxiv.org/ps/2210.11161">ps</a>, <a href="https://arxiv.org/format/2210.11161">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        From Modelling to Understanding Children&#39;s Behaviour in the Context of Robotics and Social Artificial Intelligence
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Thill%2C+S">Serge Thill</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Charisi%2C+V">Vicky Charisi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Belpaeme%2C+T">Tony Belpaeme</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Paiva%2C+A">Ana Paiva</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2210.11161v1-abstract-short" style="display: inline;">
        Understanding and modelling children&#39;s cognitive processes and their behaviour in the context of their interaction with robots and social artificial intelligence systems is a fundamental prerequisite for meaningful and effective robot interventions. However, children&#39;s development involve complex faculties such as exploration, creativity and curiosity which are challenging to model. Also, often ch&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2210.11161v1-abstract-full').style.display = 'inline'; document.getElementById('2210.11161v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2210.11161v1-abstract-full" style="display: none;">
        Understanding and modelling children&#39;s cognitive processes and their behaviour in the context of their interaction with robots and social artificial intelligence systems is a fundamental prerequisite for meaningful and effective robot interventions. However, children&#39;s development involve complex faculties such as exploration, creativity and curiosity which are challenging to model. Also, often children express themselves in a playful way which is different from a typical adult behaviour. Different children also have different needs, and it remains a challenge in the current state of the art that those of neurodiverse children are under-addressed. With this workshop, we aim to promote a common ground among different disciplines such as developmental sciences, artificial intelligence and social robotics and discuss cutting-edge research in the area of user modelling and adaptive systems for children.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2210.11161v1-abstract-full').style.display = 'none'; document.getElementById('2210.11161v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 October, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted proposal for a workshop to be held in conjunction with the 14th International Conference on Social Robotics (ICSR&#39;22)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2210.06274">arXiv:2210.06274</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2210.06274">pdf</a>, <a href="https://arxiv.org/format/2210.06274">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Centralized Training with Hybrid Execution in Multi-Agent Reinforcement Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Santos%2C+P+P">Pedro P. Santos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Carvalho%2C+D+S">Diogo S. Carvalho</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vasco%2C+M">Miguel Vasco</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sardinha%2C+A">Alberto Sardinha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Santos%2C+P+A">Pedro A. Santos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Paiva%2C+A">Ana Paiva</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Melo%2C+F+S">Francisco S. Melo</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2210.06274v2-abstract-short" style="display: inline;">
        We introduce hybrid execution in multi-agent reinforcement learning (MARL), a new paradigm in which agents aim to successfully complete cooperative tasks with arbitrary communication levels at execution time by taking advantage of information-sharing among the agents. Under hybrid execution, the communication level can range from a setting in which no communication is allowed between agents (fully&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2210.06274v2-abstract-full').style.display = 'inline'; document.getElementById('2210.06274v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2210.06274v2-abstract-full" style="display: none;">
        We introduce hybrid execution in multi-agent reinforcement learning (MARL), a new paradigm in which agents aim to successfully complete cooperative tasks with arbitrary communication levels at execution time by taking advantage of information-sharing among the agents. Under hybrid execution, the communication level can range from a setting in which no communication is allowed between agents (fully decentralized), to a setting featuring full communication (fully centralized), but the agents do not know beforehand which communication level they will encounter at execution time. To formalize our setting, we define a new class of multi-agent partially observable Markov decision processes (POMDPs) that we name hybrid-POMDPs, which explicitly model a communication process between the agents. We contribute MARO, an approach that makes use of an auto-regressive predictive model, trained in a centralized manner, to estimate missing agents&#39; observations at execution time. We evaluate MARO on standard scenarios and extensions of previous benchmarks tailored to emphasize the negative impact of partial observability in MARL. Experimental results show that our method consistently outperforms relevant baselines, allowing agents to act with faulty communication while successfully exploiting shared information.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2210.06274v2-abstract-full').style.display = 'none'; document.getElementById('2210.06274v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 June, 2023; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 12 October, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2209.09141">arXiv:2209.09141</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2209.09141">pdf</a>, <a href="https://arxiv.org/format/2209.09141">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1016/j.artint.2024.104107">10.1016/j.artint.2024.104107 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        &#34;Guess what I&#39;m doing&#34;: Extending legibility to sequential decision tasks
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Faria%2C+M">Miguel Faria</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Melo%2C+F+S">Francisco S. Melo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Paiva%2C+A">Ana Paiva</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2209.09141v2-abstract-short" style="display: inline;">
        In this paper we investigate the notion of legibility in sequential decision tasks under uncertainty. Previous works that extend legibility to scenarios beyond robot motion either focus on deterministic settings or are computationally too expensive. Our proposed approach, dubbed PoL-MDP, is able to handle uncertainty while remaining computationally tractable. We establish the advantages of our app&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2209.09141v2-abstract-full').style.display = 'inline'; document.getElementById('2209.09141v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2209.09141v2-abstract-full" style="display: none;">
        In this paper we investigate the notion of legibility in sequential decision tasks under uncertainty. Previous works that extend legibility to scenarios beyond robot motion either focus on deterministic settings or are computationally too expensive. Our proposed approach, dubbed PoL-MDP, is able to handle uncertainty while remaining computationally tractable. We establish the advantages of our approach against state-of-the-art approaches in several simulated scenarios of different complexity. We also showcase the use of our legible policies as demonstrations for an inverse reinforcement learning agent, establishing their superiority against the commonly used demonstrations based on the optimal policy. Finally, we assess the legibility of our computed policies through a user study where people are asked to infer the goal of a mobile robot following a legible policy by observing its actions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2209.09141v2-abstract-full').style.display = 'none'; document.getElementById('2209.09141v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 December, 2023; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 19 September, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2022.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2203.01176">arXiv:2203.01176</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2203.01176">pdf</a>, <a href="https://arxiv.org/format/2203.01176">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Avant-Satie! Using ERIK to encode task-relevant expressivity into the animation of autonomous social robots
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ribeiro%2C+T">Tiago Ribeiro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Paiva%2C+A">Ana Paiva</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2203.01176v1-abstract-short" style="display: inline;">
        ERIK is an expressive inverse kinematics technique that has been previously presented and evaluated both algorithmically and in a limited user-interaction scenario. It allows autonomous social robots to convey posture-based expressive information while gaze-tracking users. We have developed a new scenario aimed at further validating some of the unsupported claims from the previous scenario. Our ex&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.01176v1-abstract-full').style.display = 'inline'; document.getElementById('2203.01176v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2203.01176v1-abstract-full" style="display: none;">
        ERIK is an expressive inverse kinematics technique that has been previously presented and evaluated both algorithmically and in a limited user-interaction scenario. It allows autonomous social robots to convey posture-based expressive information while gaze-tracking users. We have developed a new scenario aimed at further validating some of the unsupported claims from the previous scenario. Our experiment features a fully autonomous Adelino robot, and concludes that ERIK can be used to direct a user&#39;s choice of actions during execution of a given task, fully through its non-verbal expressive queues.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2203.01176v1-abstract-full').style.display = 'none'; document.getElementById('2203.01176v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 March, 2022; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2022.
      
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          68T40 (Primary) 68U99 (Secondary)
        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.9; I.2.1; I.3.6; J.0
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2202.03390">arXiv:2202.03390</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2202.03390">pdf</a>, <a href="https://arxiv.org/format/2202.03390">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Geometric Multimodal Contrastive Representation Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Poklukar%2C+P">Petra Poklukar</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vasco%2C+M">Miguel Vasco</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+H">Hang Yin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Melo%2C+F+S">Francisco S. Melo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Paiva%2C+A">Ana Paiva</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kragic%2C+D">Danica Kragic</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2202.03390v4-abstract-short" style="display: inline;">
        Learning representations of multimodal data that are both informative and robust to missing modalities at test time remains a challenging problem due to the inherent heterogeneity of data obtained from different channels. To address it, we present a novel Geometric Multimodal Contrastive (GMC) representation learning method consisting of two main components: i) a two-level architecture consisting&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.03390v4-abstract-full').style.display = 'inline'; document.getElementById('2202.03390v4-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2202.03390v4-abstract-full" style="display: none;">
        Learning representations of multimodal data that are both informative and robust to missing modalities at test time remains a challenging problem due to the inherent heterogeneity of data obtained from different channels. To address it, we present a novel Geometric Multimodal Contrastive (GMC) representation learning method consisting of two main components: i) a two-level architecture consisting of modality-specific base encoders, allowing to process an arbitrary number of modalities to an intermediate representation of fixed dimensionality, and a shared projection head, mapping the intermediate representations to a latent representation space; ii) a multimodal contrastive loss function that encourages the geometric alignment of the learned representations. We experimentally demonstrate that GMC representations are semantically rich and achieve state-of-the-art performance with missing modality information on three different learning problems including prediction and reinforcement learning tasks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2202.03390v4-abstract-full').style.display = 'none'; document.getElementById('2202.03390v4-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 November, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 February, 2022;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2022.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ICML 2022 Camera ready version (update)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2110.03608">arXiv:2110.03608</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2110.03608">pdf</a>, <a href="https://arxiv.org/format/2110.03608">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        How to Sense the World: Leveraging Hierarchy in Multimodal Perception for Robust Reinforcement Learning Agents
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Vasco%2C+M">Miguel Vasco</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+H">Hang Yin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Melo%2C+F+S">Francisco S. Melo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Paiva%2C+A">Ana Paiva</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2110.03608v2-abstract-short" style="display: inline;">
        This work addresses the problem of sensing the world: how to learn a multimodal representation of a reinforcement learning agent&#39;s environment that allows the execution of tasks under incomplete perceptual conditions. To address such problem, we argue for hierarchy in the design of representation models and contribute with a novel multimodal representation model, MUSE. The proposed model learns hi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.03608v2-abstract-full').style.display = 'inline'; document.getElementById('2110.03608v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2110.03608v2-abstract-full" style="display: none;">
        This work addresses the problem of sensing the world: how to learn a multimodal representation of a reinforcement learning agent&#39;s environment that allows the execution of tasks under incomplete perceptual conditions. To address such problem, we argue for hierarchy in the design of representation models and contribute with a novel multimodal representation model, MUSE. The proposed model learns hierarchical representations: low-level modality-specific representations, encoded from raw observation data, and a high-level multimodal representation, encoding joint-modality information to allow robust state estimation. We employ MUSE as the sensory representation model of deep reinforcement learning agents provided with multimodal observations in Atari games. We perform a comparative study over different designs of reinforcement learning agents, showing that MUSE allows agents to perform tasks under incomplete perceptual experience with minimal performance loss. Finally, we evaluate the performance of MUSE in literature-standard multimodal scenarios with higher number and more complex modalities, showing that it outperforms state-of-the-art multimodal variational autoencoders in single and cross-modality generation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2110.03608v2-abstract-full').style.display = 'none'; document.getElementById('2110.03608v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 January, 2022; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 October, 2021;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> October 2021.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted at the International Conference on Autonomous Agents and MultiAgent Systems (AAMAS) 2022</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2109.14430">arXiv:2109.14430</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2109.14430">pdf</a>, <a href="https://arxiv.org/format/2109.14430">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        PyHard: a novel tool for generating hardness embeddings to support data-centric analysis
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Paiva%2C+P+Y+A">Pedro Yuri Arbs Paiva</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Smith-Miles%2C+K">Kate Smith-Miles</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Valeriano%2C+M+G">Maria Gabriela Valeriano</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lorena%2C+A+C">Ana Carolina Lorena</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2109.14430v1-abstract-short" style="display: inline;">
        For building successful Machine Learning (ML) systems, it is imperative to have high quality data and well tuned learning models. But how can one assess the quality of a given dataset? And how can the strengths and weaknesses of a model on a dataset be revealed? Our new tool PyHard employs a methodology known as Instance Space Analysis (ISA) to produce a hardness embedding of a dataset relating th&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.14430v1-abstract-full').style.display = 'inline'; document.getElementById('2109.14430v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2109.14430v1-abstract-full" style="display: none;">
        For building successful Machine Learning (ML) systems, it is imperative to have high quality data and well tuned learning models. But how can one assess the quality of a given dataset? And how can the strengths and weaknesses of a model on a dataset be revealed? Our new tool PyHard employs a methodology known as Instance Space Analysis (ISA) to produce a hardness embedding of a dataset relating the predictive performance of multiple ML models to estimated instance hardness meta-features. This space is built so that observations are distributed linearly regarding how hard they are to classify. The user can visually interact with this embedding in multiple ways and obtain useful insights about data and algorithmic performance along the individual observations of the dataset. We show in a COVID prognosis dataset how this analysis supported the identification of pockets of hard observations that challenge ML models and are therefore worth closer inspection, and the delineation of regions of strengths and weaknesses of ML models.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2109.14430v1-abstract-full').style.display = 'none'; document.getElementById('2109.14430v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 29 September, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2103.03020">arXiv:2103.03020</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2103.03020">pdf</a>, <a href="https://arxiv.org/format/2103.03020">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Multiagent Systems">cs.MA</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FAtiMA Toolkit -- Toward an effective and accessible tool for the development of intelligent virtual agents and social robots
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Mascarenhas%2C+S">Samuel Mascarenhas</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guimar%C3%A3es%2C+M">Manuel Guimarães</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Santos%2C+P+A">Pedro A. Santos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dias%2C+J">João Dias</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Prada%2C+R">Rui Prada</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Paiva%2C+A">Ana Paiva</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2103.03020v1-abstract-short" style="display: inline;">
        More than a decade has passed since the development of FearNot!, an application designed to help children deal with bullying through role-playing with virtual characters. It was also the application that led to the creation of FAtiMA, an affective agent architecture for creating autonomous characters that can evoke empathic responses. In this paper, we describe FAtiMA Toolkit, a collection of open&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.03020v1-abstract-full').style.display = 'inline'; document.getElementById('2103.03020v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2103.03020v1-abstract-full" style="display: none;">
        More than a decade has passed since the development of FearNot!, an application designed to help children deal with bullying through role-playing with virtual characters. It was also the application that led to the creation of FAtiMA, an affective agent architecture for creating autonomous characters that can evoke empathic responses. In this paper, we describe FAtiMA Toolkit, a collection of open-source tools that is designed to help researchers, game developers and roboticists incorporate a computational model of emotion and decision-making in their work. The toolkit was developed with the goal of making FAtiMA more accessible, easier to incorporate into different projects and more flexible in its capabilities for human-agent interaction, based upon the experience gathered over the years across different virtual environments and human-robot interaction scenarios. As a result, this work makes several different contributions to the field of Agent-Based Architectures. More precisely, FAtiMA Toolkit&#39;s library based design allows developers to easily integrate it with other frameworks, its meta-cognitive model affords different internal reasoners and affective components and its explicit dialogue structure gives control to the author even within highly complex scenarios. To demonstrate the use of FAtiMA Toolkit, several different use cases where the toolkit was successfully applied are described and discussed.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2103.03020v1-abstract-full').style.display = 'none'; document.getElementById('2103.03020v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 March, 2021; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2021.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2006.02991">arXiv:2006.02991</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2006.02991">pdf</a>, <a href="https://arxiv.org/format/2006.02991">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MHVAE: a Human-Inspired Deep Hierarchical Generative Model for Multimodal Representation Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Vasco%2C+M">Miguel Vasco</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Melo%2C+F+S">Francisco S. Melo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Paiva%2C+A">Ana Paiva</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2006.02991v1-abstract-short" style="display: inline;">
        Humans are able to create rich representations of their external reality. Their internal representations allow for cross-modality inference, where available perceptions can induce the perceptual experience of missing input modalities. In this paper, we contribute the Multimodal Hierarchical Variational Auto-encoder (MHVAE), a hierarchical multimodal generative model for representation learning. In&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.02991v1-abstract-full').style.display = 'inline'; document.getElementById('2006.02991v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2006.02991v1-abstract-full" style="display: none;">
        Humans are able to create rich representations of their external reality. Their internal representations allow for cross-modality inference, where available perceptions can induce the perceptual experience of missing input modalities. In this paper, we contribute the Multimodal Hierarchical Variational Auto-encoder (MHVAE), a hierarchical multimodal generative model for representation learning. Inspired by human cognitive models, the MHVAE is able to learn modality-specific distributions, of an arbitrary number of modalities, and a joint-modality distribution, responsible for cross-modality inference. We formally derive the model&#39;s evidence lower bound and propose a novel methodology to approximate the joint-modality posterior based on modality-specific representation dropout. We evaluate the MHVAE on standard multimodal datasets. Our model performs on par with other state-of-the-art generative models regarding joint-modality reconstruction from arbitrary input modalities and cross-modality inference.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2006.02991v1-abstract-full').style.display = 'none'; document.getElementById('2006.02991v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 June, 2020; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> June 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2003.05251">arXiv:2003.05251</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2003.05251">pdf</a>, <a href="https://arxiv.org/format/2003.05251">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Explainable Agents Through Social Cues: A Review
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wallkotter%2C+S">Sebastian Wallkotter</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tulli%2C+S">Silvia Tulli</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castellano%2C+G">Ginevra Castellano</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Paiva%2C+A">Ana Paiva</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chetouani%2C+M">Mohamed Chetouani</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2003.05251v3-abstract-short" style="display: inline;">
        The issue of how to make embodied agents explainable has experienced a surge of interest over the last three years, and, there are many terms that refer to this concept, e.g., transparency or legibility. One reason for this high variance in terminology is the unique array of social cues that embodied agents can access in contrast to that accessed by non-embodied agents. Another reason is that diff&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.05251v3-abstract-full').style.display = 'inline'; document.getElementById('2003.05251v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2003.05251v3-abstract-full" style="display: none;">
        The issue of how to make embodied agents explainable has experienced a surge of interest over the last three years, and, there are many terms that refer to this concept, e.g., transparency or legibility. One reason for this high variance in terminology is the unique array of social cues that embodied agents can access in contrast to that accessed by non-embodied agents. Another reason is that different authors use these terms in different ways. Hence, we review the existing literature on explainability and organize it by (1) providing an overview of existing definitions, (2) showing how explainability is implemented and how it exploits different social cues, and (3) showing how the impact of explainability is measured. Additionally, we present a list of open questions and challenges that highlight areas that require further investigation by the community. This provides the interested reader with an overview of the current state-of-the-art.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2003.05251v3-abstract-full').style.display = 'none'; document.getElementById('2003.05251v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 February, 2021; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 11 March, 2020;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2001.10080">arXiv:2001.10080</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2001.10080">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Chemical Physics">physics.chem-ph</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        How do we drive deep eutectic systems towards an industrial reality?
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Paiva%2C+A">Alexandre Paiva</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Matias%2C+A+A">Ana A. Matias</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Duarte%2C+A+R+C">Ana Rita C. Duarte</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2001.10080v1-abstract-short" style="display: inline;">
        Deep eutectic systems (DES) have received considerable attention in the past 5 years of research. Up to 2013 little had been explored over these systems both in which concerns fundamentals and applications. The definition of a deep eutectic system is still controversial and in this manuscript we highlight the different definitions proposed in the literature and discuss what, in our opinion, should&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2001.10080v1-abstract-full').style.display = 'inline'; document.getElementById('2001.10080v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2001.10080v1-abstract-full" style="display: none;">
        Deep eutectic systems (DES) have received considerable attention in the past 5 years of research. Up to 2013 little had been explored over these systems both in which concerns fundamentals and applications. The definition of a deep eutectic system is still controversial and in this manuscript we highlight the different definitions proposed in the literature and discuss what, in our opinion, should be pursued in order to have an agreement in the scientific community studying this field. From the definition of DES to its applications also interesting changes have been observed in the topics published. From electrochemistry to newer applications, such as cryopreservation, the field is evolving as new outcomes on DES properties are unraveled. We herein present and discuss new trends on DES research and present some perspectives on what we consider to be the most promising applications of DES at industrial scale.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2001.10080v1-abstract-full').style.display = 'none'; document.getElementById('2001.10080v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 November, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> January 2020.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1911.12851">arXiv:1911.12851</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1911.12851">pdf</a>, <a href="https://arxiv.org/ps/1911.12851">ps</a>, <a href="https://arxiv.org/format/1911.12851">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Playing Games in the Dark: An approach for cross-modality transfer in reinforcement learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Silva%2C+R">Rui Silva</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Vasco%2C+M">Miguel Vasco</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Melo%2C+F+S">Francisco S. Melo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Paiva%2C+A">Ana Paiva</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Veloso%2C+M">Manuela Veloso</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1911.12851v1-abstract-short" style="display: inline;">
        In this work we explore the use of latent representations obtained from multiple input sensory modalities (such as images or sounds) in allowing an agent to learn and exploit policies over different subsets of input modalities. We propose a three-stage architecture that allows a reinforcement learning agent trained over a given sensory modality, to execute its task on a different sensory modality-&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.12851v1-abstract-full').style.display = 'inline'; document.getElementById('1911.12851v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1911.12851v1-abstract-full" style="display: none;">
        In this work we explore the use of latent representations obtained from multiple input sensory modalities (such as images or sounds) in allowing an agent to learn and exploit policies over different subsets of input modalities. We propose a three-stage architecture that allows a reinforcement learning agent trained over a given sensory modality, to execute its task on a different sensory modality-for example, learning a visual policy over image inputs, and then execute such policy when only sound inputs are available. We show that the generalized policies achieve better out-of-the-box performance when compared to different baselines. Moreover, we show this holds in different OpenAI gym and video game environments, even when using different multimodal generative models and reinforcement learning algorithms.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1911.12851v1-abstract-full').style.display = 'none'; document.getElementById('1911.12851v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 November, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> November 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1909.13875">arXiv:1909.13875</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1909.13875">pdf</a>, <a href="https://arxiv.org/format/1909.13875">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Expressive Inverse Kinematics Solving in Real-time for Virtual and Robotic Interactive Characters
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ribeiro%2C+T">Tiago Ribeiro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Paiva%2C+A">Ana Paiva</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1909.13875v2-abstract-short" style="display: inline;">
        With new advancements in interaction techniques, character animation also requires new methods, to support fields such as robotics, and VR/AR. Interactive characters in such fields are becoming driven by AI which opens up the possibility of non-linear and open-ended narratives that may even include interaction with the real, physical world. This paper presents and describes ERIK, an expressive inv&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1909.13875v2-abstract-full').style.display = 'inline'; document.getElementById('1909.13875v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1909.13875v2-abstract-full" style="display: none;">
        With new advancements in interaction techniques, character animation also requires new methods, to support fields such as robotics, and VR/AR. Interactive characters in such fields are becoming driven by AI which opens up the possibility of non-linear and open-ended narratives that may even include interaction with the real, physical world. This paper presents and describes ERIK, an expressive inverse kinematics technique aimed at such applications. Our technique allows an arbitrary kinematic chain, such as an arm, snake, or robotic manipulator, to exhibit an expressive posture while aiming its end-point towards a given target orientation. The technique runs in interactive-time and does not require any pre-processing step such as e.g. training in machine learning techniques, in order to support new embodiments or new postures. That allows it to be integrated in an artist-friendly workflow, bringing artists closer to the development of such AI-driven expressive characters, by allowing them to use their typical animation tools of choice, and to properly pre-visualize the animation during design-time, even on a real robot. The full algorithmic specification is presented and described so that it can be implemented and used throughout the communities of the various fields we address. We demonstrate ERIK on different virtual kinematic structures, and also on a low-fidelity robot that was crafted using wood and hobby-grade servos, to show how well the technique performs even on a low-grade robot. Our evaluation shows how well the technique performs, i.e., how well the character is able to point at the target orientation, while minimally disrupting its target expressive posture, and respecting its mechanical rotation limits.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1909.13875v2-abstract-full').style.display = 'none'; document.getElementById('1909.13875v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 November, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 30 September, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2019.
      
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.9; H.5.2; I.3.8; J.5
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1909.10823">arXiv:1909.10823</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1909.10823">pdf</a>, <a href="https://arxiv.org/format/1909.10823">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Software architecture for YOLO, a creativity-stimulating robot
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Alves-Oliveira%2C+P">Patrícia Alves-Oliveira</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gomes%2C+S">Samuel Gomes</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chandak%2C+A">Ankita Chandak</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Arriaga%2C+P">Patrícia Arriaga</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hoffman%2C+G">Guy Hoffman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Paiva%2C+A">Ana Paiva</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1909.10823v1-abstract-short" style="display: inline;">
        YOLO is a social robot designed and developed to stimulate creativity in children through storytelling activities. Children use it as a character in their stories. This article details the artificial intelligence software developed for YOLO. The implemented software schedules through several Creativity Behaviors to find the ones that stimulate creativity more effectively. YOLO can choose between c&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1909.10823v1-abstract-full').style.display = 'inline'; document.getElementById('1909.10823v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1909.10823v1-abstract-full" style="display: none;">
        YOLO is a social robot designed and developed to stimulate creativity in children through storytelling activities. Children use it as a character in their stories. This article details the artificial intelligence software developed for YOLO. The implemented software schedules through several Creativity Behaviors to find the ones that stimulate creativity more effectively. YOLO can choose between convergent and divergent thinking techniques, two important processes of creative thought. These techniques were developed based on the psychological theories of creativity development and on research from creativity experts who work with children. Additionally, this software allows the creation of Social Behaviors that enable the robot to behave as a believable character. On top of our framework, we built 3 main social behavior parameters: Exuberant, Aloof, and Harmonious. These behaviors are meant to ease immersive play and the process of character creation. The 3 social behaviors were based on psychological theories of personality and developed using children&#39;s input during co-design studies. Overall, this work presents an attempt to design, develop, and deploy social robots that nurture intrinsic human abilities, such as the ability to be creative.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1909.10823v1-abstract-full').style.display = 'none'; document.getElementById('1909.10823v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 September, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">17 pages, 7 figurs, 2 tables, open-source code, submitted to SoftwareX journal</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1907.02275">arXiv:1907.02275</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1907.02275">pdf</a>, <a href="https://arxiv.org/format/1907.02275">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Software Engineering">cs.SE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Sharing and Learning Alloy on the Web
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Macedo%2C+N">Nuno Macedo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cunha%2C+A">Alcino Cunha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pereira%2C+J">José Pereira</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Carvalho%2C+R">Renato Carvalho</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Silva%2C+R">Ricardo Silva</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Paiva%2C+A+C+R">Ana C. R. Paiva</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ramalho%2C+M+S">Miguel S. Ramalho</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Silva%2C+D">Daniel Silva</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1907.02275v1-abstract-short" style="display: inline;">
        We present Alloy4Fun, a web application that enables online editing and sharing of Alloy models and instances, to be used mainly in an educational context. By introducing the notion of secret paragraphs and commands in the models, it also allows the distribution and automatic evaluation of simple specification challenges, a useful mechanism that enables students to learn relational logic at their&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1907.02275v1-abstract-full').style.display = 'inline'; document.getElementById('1907.02275v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1907.02275v1-abstract-full" style="display: none;">
        We present Alloy4Fun, a web application that enables online editing and sharing of Alloy models and instances, to be used mainly in an educational context. By introducing the notion of secret paragraphs and commands in the models, it also allows the distribution and automatic evaluation of simple specification challenges, a useful mechanism that enables students to learn relational logic at their own pace. Alloy4Fun stores all versions of shared and analyzed models, as well as derivation trees that depict how those models evolved over time: this wealth of information can be mined by researchers or tutors to identify, for example, learning breakdowns in the class or typical mistakes made by students and other Alloy users. A beta version of Alloy4Fun was already used in two formal methods courses, and we present some results of this preliminary evaluation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1907.02275v1-abstract-full').style.display = 'none'; document.getElementById('1907.02275v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 July, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2019.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1904.02898">arXiv:1904.02898</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1904.02898">pdf</a>, <a href="https://arxiv.org/format/1904.02898">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Nutty-based Robot Animation -- Principles and Practices
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ribeiro%2C+T">Tiago Ribeiro</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Paiva%2C+A">Ana Paiva</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1904.02898v3-abstract-short" style="display: inline;">
        Robot animation is a new form of character animation that extends the traditional process by allowing the animated motion to become more interactive and adaptable during interaction with users in real-world settings. This paper reviews how this new type of character animation has evolved and been shaped from character animation principles and practices. We outline some new paradigms that aim at al&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1904.02898v3-abstract-full').style.display = 'inline'; document.getElementById('1904.02898v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1904.02898v3-abstract-full" style="display: none;">
        Robot animation is a new form of character animation that extends the traditional process by allowing the animated motion to become more interactive and adaptable during interaction with users in real-world settings. This paper reviews how this new type of character animation has evolved and been shaped from character animation principles and practices. We outline some new paradigms that aim at allowing character animators to become robot animators, and to properly take part in the development of social robots. One such paradigm consists of the 12 principles of robot animation, which describes general concepts that both animators and robot developers should consider in order to properly understand each other. We also introduce the concept of Kinematronics, for specifying the controllable and programmable expressive abilities of robots, and the Nutty Workflow and Pipeline. The Nutty Pipeline introduces the concept of the Programmable Robot Animation Engine, which allows to generate, compose and blend various types of animation sources into a final, interaction-enabled motion that can be rendered on robots in real-time during real-world interactions. The Nutty Motion Filter is described and exemplified as a technique that allows an open-loop motion controller to apply physical limits to the motion while still allowing to tweak the shape and expressivity of the resulting motion. Additionally, we describe some types of tools that can be developed and integrated into Nutty-based workflows and pipelines, which allow animation artists to perform an integral part of the expressive behaviour development within social robots, and thus to evolve from standard (3D) character animators, towards a full-stack type of robot animators.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1904.02898v3-abstract-full').style.display = 'none'; document.getElementById('1904.02898v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 June, 2019; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 April, 2019;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> April 2019.
      
    </p>
    

    
      <p class="comments is-size-7">
        

        

        
          <span class="has-text-black-bis has-text-weight-semibold">ACM Class:</span>
          I.2.9; H.5.2; I.3.8; J.5
        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1903.02511">arXiv:1903.02511</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1903.02511">pdf</a>, <a href="https://arxiv.org/format/1903.02511">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Learning multimodal representations for sample-efficient recognition of human actions
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Vasco%2C+M">Miguel Vasco</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Melo%2C+F+S">Francisco S. Melo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=de+Matos%2C+D+M">David Martins de Matos</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Paiva%2C+A">Ana Paiva</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Inamura%2C+T">Tetsunari Inamura</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1903.02511v1-abstract-short" style="display: inline;">
        Humans interact in rich and diverse ways with the environment. However, the representation of such behavior by artificial agents is often limited. In this work we present \textit{motion concepts}, a novel multimodal representation of human actions in a household environment. A motion concept encompasses a probabilistic description of the kinematics of the action along with its contextual backgroun&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.02511v1-abstract-full').style.display = 'inline'; document.getElementById('1903.02511v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1903.02511v1-abstract-full" style="display: none;">
        Humans interact in rich and diverse ways with the environment. However, the representation of such behavior by artificial agents is often limited. In this work we present \textit{motion concepts}, a novel multimodal representation of human actions in a household environment. A motion concept encompasses a probabilistic description of the kinematics of the action along with its contextual background, namely the location and the objects held during the performance. Furthermore, we present Online Motion Concept Learning (OMCL), a new algorithm which learns novel motion concepts from action demonstrations and recognizes previously learned motion concepts. The algorithm is evaluated on a virtual-reality household environment with the presence of a human avatar. OMCL outperforms standard motion recognition algorithms on an one-shot recognition task, attesting to its potential for sample-efficient recognition of human actions.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1903.02511v1-abstract-full').style.display = 'none'; document.getElementById('1903.02511v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 March, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> March 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">7 pages, 6 figures, submitted to 2019 IEEE/RSJ International Conference on Intelligent Robots and Systems (IROS)</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1902.01800">arXiv:1902.01800</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1902.01800">pdf</a>, <a href="https://arxiv.org/format/1902.01800">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Human-Computer Interaction">cs.HC</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Empathic Robot for Group Learning: A Field Study
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Alves-Oliveira%2C+P">Patricia Alves-Oliveira</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sequeira%2C+P">Pedro Sequeira</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Melo%2C+F+S">Francisco S. Melo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Castellano%2C+G">Ginevra Castellano</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Paiva%2C+A">Ana Paiva</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1902.01800v1-abstract-short" style="display: inline;">
        This work explores a group learning scenario with an autonomous empathic robot. We address two research questions: (1) Can an autonomous robot designed with empathic competencies foster collaborative learning in a group context? (2) Can an empathic robot sustain positive educational outcomes in long-term collaborative learning interactions with groups of students? To answer these questions, we dev&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1902.01800v1-abstract-full').style.display = 'inline'; document.getElementById('1902.01800v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1902.01800v1-abstract-full" style="display: none;">
        This work explores a group learning scenario with an autonomous empathic robot. We address two research questions: (1) Can an autonomous robot designed with empathic competencies foster collaborative learning in a group context? (2) Can an empathic robot sustain positive educational outcomes in long-term collaborative learning interactions with groups of students? To answer these questions, we developed an autonomous robot with empathic competencies that is able to interact with a group of students in a learning activity about sustainable development. Two studies were conducted. The first study compares learning outcomes in children across 3 conditions: learning with an empathic robot; learning with a robot without empathic capabilities; and learning without a robot. The results show that the autonomous robot with empathy fosters meaningful discussions about sustainability, which is a learning outcome in sustainability education. The second study features groups of students who interact with the robot in a school classroom for two months. The long-term educational interaction did not seem to provide significant learning gains, although there was a change in game-actions to achieve more sustainability during game-play. This result reflects the need to perform more long-term research in the field of educational robots for group learning.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1902.01800v1-abstract-full').style.display = 'none'; document.getElementById('1902.01800v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 February, 2019; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2019.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">ACM Transactions on Human-Robot Interaction, In press</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/1602.06703">arXiv:1602.06703</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/1602.06703">pdf</a>, <a href="https://arxiv.org/format/1602.06703">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computers and Society">cs.CY</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Cognitive Architecture for Mutual Modelling
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Jacq%2C+A">Alexis Jacq</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Johal%2C+W">Wafa Johal</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dillenbourg%2C+P">Pierre Dillenbourg</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Paiva%2C+A">Ana Paiva</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="1602.06703v1-abstract-short" style="display: inline;">
        In social robotics, robots needs to be able to be understood by humans. Especially in collaborative tasks where they have to share mutual knowledge. For instance, in an educative scenario, learners share their knowledge and they must adapt their behaviour in order to make sure they are understood by others. Learners display behaviours in order to show their understanding and teachers adapt in orde&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1602.06703v1-abstract-full').style.display = 'inline'; document.getElementById('1602.06703v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="1602.06703v1-abstract-full" style="display: none;">
        In social robotics, robots needs to be able to be understood by humans. Especially in collaborative tasks where they have to share mutual knowledge. For instance, in an educative scenario, learners share their knowledge and they must adapt their behaviour in order to make sure they are understood by others. Learners display behaviours in order to show their understanding and teachers adapt in order to make sure that the learners&#39; knowledge is the required one. This ability requires a model of their own mental states perceived by others: \textit{&#34;has the human understood that I(robot) need this object for the task or should I explain it once again ?&#34;} In this paper, we discuss the importance of a cognitive architecture enabling second-order Mutual Modelling for Human-Robot Interaction in educative contexts.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('1602.06703v1-abstract-full').style.display = 'none'; document.getElementById('1602.06703v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 February, 2016; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> February 2016.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Presented at &#34;2nd Workshop on Cognitive Architectures for Social Human-Robot Interaction 2016 (arXiv:1602.01868)</span>
    </p>
    

    
      <p class="comments is-size-7">
        
          <span class="has-text-black-bis has-text-weight-semibold">Report number:</span>
          CogArch4sHRI/2016/07
        

        

        
      </p>
    

    
  </li>

</ol>


  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>