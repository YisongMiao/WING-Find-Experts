<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8"/>
<meta name="viewport" content="width=device-width, initial-scale=1"/>
<!-- new favicon config and versions by realfavicongenerator.net -->
<link rel="apple-touch-icon" sizes="180x180" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/apple-touch-icon.png">
<link rel="icon" type="image/png" sizes="32x32" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-32x32.png">
<link rel="icon" type="image/png" sizes="16x16" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon-16x16.png">
<link rel="manifest" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/site.webmanifest">
<link rel="mask-icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/safari-pinned-tab.svg" color="#b31b1b">
<link rel="shortcut icon" href="https://static.arxiv.org/static/base/1.0.0a5/images/icons/favicon.ico">
<meta name="msapplication-TileColor" content="#b31b1b">
<meta name="msapplication-config" content="images/icons/browserconfig.xml">
<meta name="theme-color" content="#b31b1b">
<!-- end favicon config -->
<title>Search | arXiv e-print repository</title>
<script defer src="https://static.arxiv.org/static/base/1.0.0a5/fontawesome-free-5.11.2-web/js/all.js"></script>
<link rel="stylesheet" href="https://static.arxiv.org/static/base/1.0.0a5/css/arxivstyle.css" />
<script type="text/x-mathjax-config">
  MathJax.Hub.Config({
    messageStyle: "none",
    extensions: ["tex2jax.js"],
    jax: ["input/TeX", "output/HTML-CSS"],
    tex2jax: {
      inlineMath: [ ['$','$'], ["\\(","\\)"] ],
      displayMath: [ ['$$','$$'], ["\\[","\\]"] ],
      processEscapes: true,
      ignoreClass: '.*',
      processClass: 'mathjax.*'
    },
    TeX: {
        extensions: ["AMSmath.js", "AMSsymbols.js", "noErrors.js"],
        noErrors: {
          inlineDelimiters: ["$","$"],
          multiLine: false,
          style: {
            "font-size": "normal",
            "border": ""
          }
        }
    },
    "HTML-CSS": { availableFonts: ["TeX"] }
  });
</script>
<script src='//static.arxiv.org/MathJax-2.7.3/MathJax.js'></script>
<script src="https://static.arxiv.org/static/base/1.0.0a5/js/notification.js"></script>

    
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/bulma-tooltip.min.css" />
  <link rel="stylesheet" href="https://static.arxiv.org/static/search/0.5.6/css/search.css" />
  <script
    src="https://code.jquery.com/jquery-3.2.1.slim.min.js"
    integrity="sha256-k2WSCIexGzOj3Euiig+TlR8gA0EmPjuc79OEeY5L45g="
    crossorigin="anonymous"></script>

  <script src="https://static.arxiv.org/static/search/0.5.6/js/fieldset.js"></script>
  <style>
  radio#cf-customfield_11400 {
    display: none;
  }
  </style>

  </head>
  <body>
  
  
  <header><a href="#main-container" class="is-sr-only">Skip to main content</a>
    
    <!-- contains Cornell logo and sponsor statement -->
<div class="attribution level is-marginless" role="banner">
  <div class="level-left">
    <a class="level-item" href="https://cornell.edu/"><img src="https://static.arxiv.org/static/base/1.0.0a5/images/cornell-reduced-white-SMALL.svg" alt="Cornell University" width="200" aria-label="logo" /></a>
  </div>
  <div class="level-right is-marginless"><p class="sponsors level-item is-marginless"><span id="support-ack-url">We gratefully acknowledge support from<br /> the Simons Foundation, <a href="https://info.arxiv.org/about/ourmembers.html">member institutions</a>, and all contributors. <a href="https://info.arxiv.org/about/donate.html">Donate</a></span></p></div>
</div>
<!-- contains arXiv identity and search bar -->
<div class="identity level is-marginless">
  <div class="level-left">
    <div class="level-item">
      <a class="arxiv" href="https://arxiv.org/" aria-label="arxiv-logo">
        <img src="https://static.arxiv.org/static/base/1.0.0a5/images/arxiv-logo-one-color-white.svg" aria-label="logo" alt="arxiv logo" width="85" style="width:85px;"/>
      </a>
    </div>
  </div>
  
  <div class="search-block level-right">
    <form class="level-item mini-search" method="GET" action="https://arxiv.org/search">
      <div class="field has-addons">
        <div class="control">
          <input class="input is-small" type="text" name="query" placeholder="Search..." aria-label="Search term or terms" />
          <p class="help"><a href="https://info.arxiv.org/help">Help</a> | <a href="https://arxiv.org/search/advanced">Advanced Search</a></p>
        </div>
        <div class="control">
          <div class="select is-small">
            <select name="searchtype" aria-label="Field to search">
              <option value="all" selected="selected">All fields</option>
              <option value="title">Title</option>
              <option value="author">Author</option>
              <option value="abstract">Abstract</option>
              <option value="comments">Comments</option>
              <option value="journal_ref">Journal reference</option>
              <option value="acm_class">ACM classification</option>
              <option value="msc_class">MSC classification</option>
              <option value="report_num">Report number</option>
              <option value="paper_id">arXiv identifier</option>
              <option value="doi">DOI</option>
              <option value="orcid">ORCID</option>
              <option value="author_id">arXiv author ID</option>
              <option value="help">Help pages</option>
              <option value="full_text">Full text</option>
            </select>
          </div>
        </div>
        <input type="hidden" name="source" value="header">
        <button class="button is-small is-cul-darker">Search</button>
      </div>
    </form>
  </div>
</div> <!-- closes identity -->

<div class="container">
    <div class="user-tools is-size-7 has-text-right has-text-weight-bold" role="navigation" aria-label="User menu">
      <a href="https://arxiv.org/login">Login</a>
    </div>
</div>
    
  </header>
  <main class="container" id="main-container">
    


    
  <div class="level is-marginless">
    <div class="level-left">
      <h1 class="title is-clearfix">
    
        Showing 1&ndash;50 of 1,711 results for author: <span class="mathjax">Fei Liu</span>
    
</h1>
    </div>
    <div class="level-right is-hidden-mobile">
      <!-- feedback for mobile is moved to footer -->
      <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
    </div>
  </div>
    <div class="content">
      
  <form method="GET" action="/search/"  aria-role="search">
    

    
    <div class="field has-addons-tablet">
      <div class="control is-expanded">
        <label for="query" class="hidden-label">Search term or terms</label>
        
          <input class="input is-medium" id="query" name="query" placeholder="Search term..." type="text" value="Fei Liu">
        
        
      </div>
      <div class="select control is-medium">
        <label class="is-hidden" for="searchtype">Field</label>
        <select class="is-medium" id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
      </div>
      <div class="control">
          <button class="button is-link is-medium">Search</button>
      </div>
    </div>
    <div class="field">
      <div class="control is-size-7">
        
        <label class="radio">
          <input checked id="abstracts-0" name="abstracts" type="radio" value="show"> Show abstracts
        </label>
        
        <label class="radio">
          <input id="abstracts-1" name="abstracts" type="radio" value="hide"> Hide abstracts
        </label>
        
      </div>
    </div>
    <div class="is-clearfix" style="height: 2.5em"> 
      <div class="is-pulled-right">
        
        <a href="/search/advanced?terms-0-term=Fei+Liu&amp;terms-0-field=author&amp;size=50&amp;order=-announced_date_first">Advanced Search</a>
        
      </div>
    </div>
    <input type="hidden" name="order" value="-announced_date_first">
    <input type="hidden" name="size" value="50">
  </form>

  

  
      
<div class="level breathe-horizontal">
  <div class="level-left">
    <form method="GET" action="/search/">
      <div style="display: none;">
        
          
            <select id="searchtype" name="searchtype"><option value="all">All fields</option><option value="title">Title</option><option selected value="author">Author(s)</option><option value="abstract">Abstract</option><option value="comments">Comments</option><option value="journal_ref">Journal reference</option><option value="acm_class">ACM classification</option><option value="msc_class">MSC classification</option><option value="report_num">Report number</option><option value="paper_id">arXiv identifier</option><option value="doi">DOI</option><option value="orcid">ORCID</option><option value="license">License (URI)</option><option value="author_id">arXiv author ID</option><option value="help">Help pages</option><option value="full_text">Full text</option></select>
          
        
          
            <input id="query" name="query" type="text" value="Fei Liu">
          
        
          
        
          
        
          
            <ul id="abstracts"><li><input checked id="abstracts-0" name="abstracts" type="radio" value="show"> <label for="abstracts-0">Show abstracts</label></li><li><input id="abstracts-1" name="abstracts" type="radio" value="hide"> <label for="abstracts-1">Hide abstracts</label></li></ul>
          
        
      </div>
      <div class="box field is-grouped is-grouped-multiline level-item">
        <div class="control">
          <span class="select is-small">
            <select id="size" name="size"><option value="25">25</option><option selected value="50">50</option><option value="100">100</option><option value="200">200</option></select>
          </span>
          <label for="size">results per page</label>.
        </div>
        <div class="control">
          <label for="order">Sort results by</label>
          <span class="select is-small">
            <select id="order" name="order"><option selected value="-announced_date_first">Announcement date (newest first)</option><option value="announced_date_first">Announcement date (oldest first)</option><option value="-submitted_date">Submission date (newest first)</option><option value="submitted_date">Submission date (oldest first)</option><option value="">Relevance</option></select>
          </span>
        </div>
        <div class="control">
          <button class="button is-small is-link">Go</button>
        </div>
      </div>
    </form>
  </div>
</div>
      


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?query=Fei+Liu&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?query=Fei+Liu&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/?query=Fei+Liu&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
              class="pagination-link "
              aria-label="Page 2"
              aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/?query=Fei+Liu&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=100"
              class="pagination-link "
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/?query=Fei+Liu&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=150"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/?query=Fei+Liu&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=200"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  



<ol class="breathe-horizontal" start="1"> 


  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2509.03244">arXiv:2509.03244</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2509.03244">pdf</a>, <a href="https://arxiv.org/ps/2509.03244">ps</a>, <a href="https://arxiv.org/format/2509.03244">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FoMEMO: Towards Foundation Models for Expensive Multi-objective Optimization
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yao%2C+Y">Yiming Yao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+F">Fei Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+L">Liang Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+X">Xi Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Q">Qingfu Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2509.03244v1-abstract-short" style="display: inline;">
        Expensive multi-objective optimization is a prevalent and crucial concern in many real-world scenarios, where sample-efficiency is vital due to the limited evaluations to recover the true Pareto front for decision making. Existing works either involve rebuilding Gaussian process surrogates from scratch for each objective in each new problem encountered, or rely on extensive past domain experiments&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.03244v1-abstract-full').style.display = 'inline'; document.getElementById('2509.03244v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2509.03244v1-abstract-full" style="display: none;">
        Expensive multi-objective optimization is a prevalent and crucial concern in many real-world scenarios, where sample-efficiency is vital due to the limited evaluations to recover the true Pareto front for decision making. Existing works either involve rebuilding Gaussian process surrogates from scratch for each objective in each new problem encountered, or rely on extensive past domain experiments for pre-training deep learning models, making them hard to generalize and impractical to cope with various emerging applications in the real world. To address this issue, we propose a new paradigm named FoMEMO (Foundation Models for Expensive Multi-objective Optimization), which enables the establishment of a foundation model conditioned on any domain trajectory and user preference, and facilitates fast in-context optimization based on the predicted preference-wise aggregation posteriors. Rather than accessing extensive domain experiments in the real world, we demonstrate that pre-training the foundation model with a diverse set of hundreds of millions of synthetic data can lead to superior adaptability to unknown problems, without necessitating any subsequent model training or updates in the optimization process. We evaluate our method across a variety of synthetic benchmarks and real-word applications, and demonstrate its superior generality and competitive performance compared to existing methods.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.03244v1-abstract-full').style.display = 'none'; document.getElementById('2509.03244v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 3 September, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2509.02974">arXiv:2509.02974</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2509.02974">pdf</a>, <a href="https://arxiv.org/ps/2509.02974">ps</a>, <a href="https://arxiv.org/format/2509.02974">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="High Energy Physics - Phenomenology">hep-ph</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="High Energy Physics - Theory">hep-th</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Nuclear Theory">nucl-th</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Revisiting the first-order QCD phase transition in dense strong interaction matter
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+Y">Yi Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+F">Fei Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yu-xin Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2509.02974v1-abstract-short" style="display: inline;">
        We revisit the phase structure and thermodynamics of QCD in the low temperature and high density region, where a strong, first-order phase transition is expected beyond the critical end point. By solving the quark gap equation in the continuum QCD approach, we reveal the coexistence of the multi-phases both in the microscopic dynamics of chiral symmetry breaking and also in the thermodynamic obser&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.02974v1-abstract-full').style.display = 'inline'; document.getElementById('2509.02974v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2509.02974v1-abstract-full" style="display: none;">
        We revisit the phase structure and thermodynamics of QCD in the low temperature and high density region, where a strong, first-order phase transition is expected beyond the critical end point. By solving the quark gap equation in the continuum QCD approach, we reveal the coexistence of the multi-phases both in the microscopic dynamics of chiral symmetry breaking and also in the thermodynamic observables, which suggests the existence of spinodal decomposition during the first-order QCD phase transitions. We also analyse the interface structure of the co-exist Nambu and Wigner phases in the isothermal process during the first-order transition. In particular, the interface tension and interface entropy density are extracted from the isothermal trajectories, which further allows for an analysis on the formation of nuclear bubble, including the bubble radius and its stability at different temperatures. Our predictions may serve as useful inputs for further investigations in heavy-ion physics or astrophysics research.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.02974v1-abstract-full').style.display = 'none'; document.getElementById('2509.02974v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 September, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">25 pages, 13 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2509.02208">arXiv:2509.02208</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2509.02208">pdf</a>, <a href="https://arxiv.org/ps/2509.02208">ps</a>, <a href="https://arxiv.org/format/2509.02208">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Baichuan-M2: Scaling Medical Capability with Large Verifier System
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Team%2C+B">Baichuan-M2 Team</a>, 
      
      <a href="/search/?searchtype=author&amp;query=%3A"> :</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dou%2C+C">Chengfeng Dou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+C">Chong Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+F">Fan Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+F">Fei Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jia%2C+J">Jiyuan Jia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+M">Mingyang Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ju%2C+Q">Qiang Ju</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+S">Shuai Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dang%2C+S">Shunya Dang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+T">Tianpeng Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zeng%2C+X">Xiangrong Zeng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+Y">Yijie Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+C">Chenzheng Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pan%2C+D">Da Pan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+F">Fei Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ai%2C+G">Guangwei Ai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dong%2C+G">Guosheng Dong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+H">Hongda Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tai%2C+J">Jinyang Tai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hong%2C+J">Jixiang Hong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+K">Kai Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+L">Linzhuang Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+P">Peidong Guo</a>
      , et al. (10 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2509.02208v1-abstract-short" style="display: inline;">
        As large language models (LLMs) advance in conversational and reasoning capabilities, their practical application in healthcare has become a critical research focus. However, there is a notable gap between the performance of medical LLMs on static benchmarks such as USMLE and their utility in real-world clinical decision-making. This discrepancy arises because traditional exams fail to capture the&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.02208v1-abstract-full').style.display = 'inline'; document.getElementById('2509.02208v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2509.02208v1-abstract-full" style="display: none;">
        As large language models (LLMs) advance in conversational and reasoning capabilities, their practical application in healthcare has become a critical research focus. However, there is a notable gap between the performance of medical LLMs on static benchmarks such as USMLE and their utility in real-world clinical decision-making. This discrepancy arises because traditional exams fail to capture the dynamic, interactive nature of medical consultations. To address this challenge, we introduce a novel dynamic verification framework that moves beyond static answer verifier, establishing a large-scale, high-fidelity interactive reinforcement learning system. Our framework comprises two key components: a Patient Simulator that creates realistic clinical environments using de-identified medical records, and a Clinical Rubrics Generator that dynamically produces multi-dimensional evaluation metrics. Building on this foundation, we develop Baichuan-M2, a 32B-parameter medical augmented reasoning model trained through a multi-stage reinforcement learning strategy with an improved Group Relative Policy Optimization (GRPO) algorithm. Evaluated on HealthBench, Baichuan-M2 outperforms all other open-source models and most advanced closed-source counterparts, achieving a score above 32 on the challenging HealthBench Hard benchmark-previously exceeded only by GPT-5. Our work demonstrates that robust dynamic verifier system is essential for aligning LLM capabilities with practical clinical applications, establishing a new Pareto front in the performance-parameter trade-off for medical AI deployment.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2509.02208v1-abstract-full').style.display = 'none'; document.getElementById('2509.02208v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 2 September, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> September 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Baichuan-M2 Technical Report</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.21148">arXiv:2508.21148</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.21148">pdf</a>, <a href="https://arxiv.org/ps/2508.21148">ps</a>, <a href="https://arxiv.org/format/2508.21148">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Survey of Scientific Large Language Models: From Data Foundations to Agent Frontiers
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+M">Ming Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+C">Chenglong Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+W">Wei Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+W">Wanghan Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+J">Jiamin Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+J">Jucheng Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+T">Tianbin Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhuang%2C+G">Guohang Zhuang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Jiaqi Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+Y">Yingzhou Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Ying Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+C">Chaoyang Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tan%2C+C">Cheng Tan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ying%2C+J">Jie Ying</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+G">Guocheng Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+S">Shujian Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+P">Pengcheng Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+J">Jiashi Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+H">Haitao Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+L">Lulu Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+F">Fengxiang Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yuanyuan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+X">Xiangyu Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+F">Feilong Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Su%2C+E">Encheng Su</a>
      , et al. (78 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.21148v1-abstract-short" style="display: inline;">
        Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is represented, integrated, and applied in scientific research, yet their progress is shaped by the complex nature of scientific data. This survey presents a comprehensive, data-centric synthesis that reframes the development of Sci-LLMs as a co-evolution between models and their underlying data substrate. We formulate a un&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.21148v1-abstract-full').style.display = 'inline'; document.getElementById('2508.21148v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.21148v1-abstract-full" style="display: none;">
        Scientific Large Language Models (Sci-LLMs) are transforming how knowledge is represented, integrated, and applied in scientific research, yet their progress is shaped by the complex nature of scientific data. This survey presents a comprehensive, data-centric synthesis that reframes the development of Sci-LLMs as a co-evolution between models and their underlying data substrate. We formulate a unified taxonomy of scientific data and a hierarchical model of scientific knowledge, emphasizing the multimodal, cross-scale, and domain-specific challenges that differentiate scientific corpora from general natural language processing datasets. We systematically review recent Sci-LLMs, from general-purpose foundations to specialized models across diverse scientific disciplines, alongside an extensive analysis of over 270 pre-/post-training datasets, showing why Sci-LLMs pose distinct demands -- heterogeneous, multi-scale, uncertainty-laden corpora that require representations preserving domain invariance and enabling cross-modal reasoning. On evaluation, we examine over 190 benchmark datasets and trace a shift from static exams toward process- and discovery-oriented assessments with advanced evaluation protocols. These data-centric analyses highlight persistent issues in scientific data development and discuss emerging solutions involving semi-automated annotation pipelines and expert validation. Finally, we outline a paradigm shift toward closed-loop systems where autonomous agents based on Sci-LLMs actively experiment, validate, and contribute to a living, evolving knowledge base. Collectively, this work provides a roadmap for building trustworthy, continually evolving artificial intelligence (AI) systems that function as a true partner in accelerating scientific discovery.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.21148v1-abstract-full').style.display = 'none'; document.getElementById('2508.21148v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.17627">arXiv:2508.17627</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.17627">pdf</a>, <a href="https://arxiv.org/ps/2508.17627">ps</a>, <a href="https://arxiv.org/format/2508.17627">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Stop Spinning Wheels: Mitigating LLM Overthinking via Mining Patterns for Early Reasoning Exit
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wei%2C+Z">Zihao Wei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pang%2C+L">Liang Pang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Jiahao Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+J">Jingcheng Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+S">Shicheng Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Duan%2C+Z">Zenghao Duan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jingang Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+F">Fei Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+X">Xunliang Cai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shen%2C+H">Huawei Shen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+X">Xueqi Cheng</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.17627v1-abstract-short" style="display: inline;">
        Large language models (LLMs) enhance complex reasoning tasks by scaling the individual thinking process. However, prior work shows that overthinking can degrade overall performance. Motivated by observed patterns in thinking length and content length, we categorize reasoning into three stages: insufficient exploration stage, compensatory reasoning stage, and reasoning convergence stage. Typically,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.17627v1-abstract-full').style.display = 'inline'; document.getElementById('2508.17627v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.17627v1-abstract-full" style="display: none;">
        Large language models (LLMs) enhance complex reasoning tasks by scaling the individual thinking process. However, prior work shows that overthinking can degrade overall performance. Motivated by observed patterns in thinking length and content length, we categorize reasoning into three stages: insufficient exploration stage, compensatory reasoning stage, and reasoning convergence stage. Typically, LLMs produce correct answers in the compensatory reasoning stage, whereas reasoning convergence often triggers overthinking, causing increased resource usage or even infinite loops. Therefore, mitigating overthinking hinges on detecting the end of the compensatory reasoning stage, defined as the Reasoning Completion Point (RCP). RCP typically appears at the end of the first complete reasoning cycle and can be identified by querying the LLM sentence by sentence or monitoring the probability of an end-of-thinking token (e.g., \texttt{&lt;/think&gt;}), though these methods lack an efficient and precise balance. To improve this, we mine more sensitive and consistent RCP patterns and develop a lightweight thresholding strategy based on heuristic rules. Experimental evaluations on benchmarks (AIME24, AIME25, GPQA-D) demonstrate that the proposed method reduces token consumption while preserving or enhancing reasoning accuracy.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.17627v1-abstract-full').style.display = 'none'; document.getElementById('2508.17627v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.17423">arXiv:2508.17423</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.17423">pdf</a>, <a href="https://arxiv.org/ps/2508.17423">ps</a>, <a href="https://arxiv.org/format/2508.17423">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="General Economics">econ.GN</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Carbon Disclosure Effect, Corporate Fundamentals, and Net-zero Emission Target: Evidence from China
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+X">Xiyuan Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xinlei Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fei%2C+X">Xiang Fei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+W">Wenxuan Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+B">Bai-Chen Xie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+J">Junhua Zhao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.17423v1-abstract-short" style="display: inline;">
        In response to China&#39;s national carbon neutrality goals, this study examines how corporate carbon emissions disclosure affects the financial performance of Chinese A-share listed companies. Leveraging artificial intelligence tools, including natural language processing, we analyzed emissions disclosures for 4,336 companies from 2017 to 2022. The research demonstrates that high-quality carbon discl&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.17423v1-abstract-full').style.display = 'inline'; document.getElementById('2508.17423v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.17423v1-abstract-full" style="display: none;">
        In response to China&#39;s national carbon neutrality goals, this study examines how corporate carbon emissions disclosure affects the financial performance of Chinese A-share listed companies. Leveraging artificial intelligence tools, including natural language processing, we analyzed emissions disclosures for 4,336 companies from 2017 to 2022. The research demonstrates that high-quality carbon disclosure positively impacts financial performance with higher stock returns, improved return on equity, increased Tobin&#39;s Q ratio, and reduced stock price volatility. Our findings underscore the emerging importance of carbon transparency in financial markets, highlighting how environmental reporting can serve as a strategic mechanism to create corporate value and adapt to climate change.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.17423v1-abstract-full').style.display = 'none'; document.getElementById('2508.17423v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.17011">arXiv:2508.17011</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.17011">pdf</a>, <a href="https://arxiv.org/ps/2508.17011">ps</a>, <a href="https://arxiv.org/format/2508.17011">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Survey of Deep Learning-based Point Cloud Denoising
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jinxi Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fei%2C+B">Ben Fei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Edirimuni%2C+D+d+S">Dasith de Silva Edirimuni</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Z">Zheng Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+Y">Ying He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+X">Xuequan Lu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.17011v1-abstract-short" style="display: inline;">
        Accurate 3D geometry acquisition is essential for a wide range of applications, such as computer graphics, autonomous driving, robotics, and augmented reality. However, raw point clouds acquired in real-world environments are often corrupted with noise due to various factors such as sensor, lighting, material, environment etc, which reduces geometric fidelity and degrades downstream performance. P&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.17011v1-abstract-full').style.display = 'inline'; document.getElementById('2508.17011v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.17011v1-abstract-full" style="display: none;">
        Accurate 3D geometry acquisition is essential for a wide range of applications, such as computer graphics, autonomous driving, robotics, and augmented reality. However, raw point clouds acquired in real-world environments are often corrupted with noise due to various factors such as sensor, lighting, material, environment etc, which reduces geometric fidelity and degrades downstream performance. Point cloud denoising is a fundamental problem, aiming to recover clean point sets while preserving underlying structures. Classical optimization-based methods, guided by hand-crafted filters or geometric priors, have been extensively studied but struggle to handle diverse and complex noise patterns. Recent deep learning approaches leverage neural network architectures to learn distinctive representations and demonstrate strong outcomes, particularly on complex and large-scale point clouds. Provided these significant advances, this survey provides a comprehensive and up-to-date review of deep learning-based point cloud denoising methods up to August 2025. We organize the literature from two perspectives: (1) supervision level (supervised vs. unsupervised), and (2) modeling perspective, proposing a functional taxonomy that unifies diverse approaches by their denoising principles. We further analyze architectural trends both structurally and chronologically, establish a unified benchmark with consistent training settings, and evaluate methods in terms of denoising quality, surface fidelity, point distribution, and computational efficiency. Finally, we discuss open challenges and outline directions for future research in this rapidly evolving field.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.17011v1-abstract-full').style.display = 'none'; document.getElementById('2508.17011v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.15763">arXiv:2508.15763</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.15763">pdf</a>, <a href="https://arxiv.org/ps/2508.15763">ps</a>, <a href="https://arxiv.org/format/2508.15763">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Intern-S1: A Scientific Multimodal Foundation Model
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+L">Lei Bai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+Z">Zhongrui Cai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+Y">Yuhang Cao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+M">Maosong Cao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+W">Weihan Cao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+C">Chiyu Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+H">Haojiong Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+K">Kai Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+P">Pengcheng Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Ying Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yongkang Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+Y">Yu Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chu%2C+P">Pei Chu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chu%2C+T">Tao Chu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cui%2C+E">Erfei Cui</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cui%2C+G">Ganqu Cui</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cui%2C+L">Long Cui</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cui%2C+Z">Ziyun Cui</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+N">Nianchen Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ding%2C+N">Ning Ding</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dong%2C+N">Nanqing Dong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dong%2C+P">Peijie Dong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dou%2C+S">Shihan Dou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Du%2C+S">Sinan Du</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Duan%2C+H">Haodong Duan</a>
      , et al. (152 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.15763v2-abstract-short" style="display: inline;">
        In recent years, a plethora of open-source foundation models have emerged, achieving remarkable progress in some widely attended fields, with performance being quite close to that of closed-source models. However, in high-value but more challenging scientific professional fields, either the fields still rely on expert models, or the progress of general foundation models lags significantly compared&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.15763v2-abstract-full').style.display = 'inline'; document.getElementById('2508.15763v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.15763v2-abstract-full" style="display: none;">
        In recent years, a plethora of open-source foundation models have emerged, achieving remarkable progress in some widely attended fields, with performance being quite close to that of closed-source models. However, in high-value but more challenging scientific professional fields, either the fields still rely on expert models, or the progress of general foundation models lags significantly compared to those in popular areas, far from sufficient for transforming scientific research and leaving substantial gap between open-source models and closed-source models in these scientific domains. To mitigate this gap and explore a step further toward Artificial General Intelligence (AGI), we introduce Intern-S1, a specialized generalist equipped with general understanding and reasoning capabilities with expertise to analyze multiple science modal data. Intern-S1 is a multimodal Mixture-of-Experts (MoE) model with 28 billion activated parameters and 241 billion total parameters, continually pre-trained on 5T tokens, including over 2.5T tokens from scientific domains. In the post-training stage, Intern-S1 undergoes offline and then online reinforcement learning (RL) in InternBootCamp, where we propose Mixture-of-Rewards (MoR) to synergize the RL training on more than 1000 tasks simultaneously. Through integrated innovations in algorithms, data, and training systems, Intern-S1 achieved top-tier performance in online RL training. On comprehensive evaluation benchmarks, Intern-S1 demonstrates competitive performance on general reasoning tasks among open-source models and significantly outperforms open-source models in scientific domains, surpassing closed-source state-of-the-art models in professional tasks, such as molecular synthesis planning, reaction condition prediction, predicting thermodynamic stabilities for crystals. Our models are available at https://huggingface.co/internlm/Intern-S1.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.15763v2-abstract-full').style.display = 'none'; document.getElementById('2508.15763v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 21 August, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.15505">arXiv:2508.15505</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.15505">pdf</a>, <a href="https://arxiv.org/ps/2508.15505">ps</a>, <a href="https://arxiv.org/format/2508.15505">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Task-Generalized Adaptive Cross-Domain Learning for Multimodal Image Fusion
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+M">Mengyu Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Z">Zhenyu Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+K">Kun Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yu Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yuwei Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wei%2C+Y">Yanyan Wei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+F">Fei Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.15505v1-abstract-short" style="display: inline;">
        Multimodal Image Fusion (MMIF) aims to integrate complementary information from different imaging modalities to overcome the limitations of individual sensors. It enhances image quality and facilitates downstream applications such as remote sensing, medical diagnostics, and robotics. Despite significant advancements, current MMIF methods still face challenges such as modality misalignment, high-fr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.15505v1-abstract-full').style.display = 'inline'; document.getElementById('2508.15505v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.15505v1-abstract-full" style="display: none;">
        Multimodal Image Fusion (MMIF) aims to integrate complementary information from different imaging modalities to overcome the limitations of individual sensors. It enhances image quality and facilitates downstream applications such as remote sensing, medical diagnostics, and robotics. Despite significant advancements, current MMIF methods still face challenges such as modality misalignment, high-frequency detail destruction, and task-specific limitations. To address these challenges, we propose AdaSFFuse, a novel framework for task-generalized MMIF through adaptive cross-domain co-fusion learning. AdaSFFuse introduces two key innovations: the Adaptive Approximate Wavelet Transform (AdaWAT) for frequency decoupling, and the Spatial-Frequency Mamba Blocks for efficient multimodal fusion. AdaWAT adaptively separates the high- and low-frequency components of multimodal images from different scenes, enabling fine-grained extraction and alignment of distinct frequency characteristics for each modality. The Spatial-Frequency Mamba Blocks facilitate cross-domain fusion in both spatial and frequency domains, enhancing this process. These blocks dynamically adjust through learnable mappings to ensure robust fusion across diverse modalities. By combining these components, AdaSFFuse improves the alignment and integration of multimodal features, reduces frequency loss, and preserves critical details. Extensive experiments on four MMIF tasks -- Infrared-Visible Image Fusion (IVF), Multi-Focus Image Fusion (MFF), Multi-Exposure Image Fusion (MEF), and Medical Image Fusion (MIF) -- demonstrate AdaSFFuse&#39;s superior fusion performance, ensuring both low computational cost and a compact network, offering a strong balance between performance and efficiency. The code will be publicly available at https://github.com/Zhen-yu-Liu/AdaSFFuse.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.15505v1-abstract-full').style.display = 'none'; document.getElementById('2508.15505v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 21 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by IEEE Transactions on Multimedia</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.15144">arXiv:2508.15144</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.15144">pdf</a>, <a href="https://arxiv.org/ps/2508.15144">ps</a>, <a href="https://arxiv.org/format/2508.15144">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Mobile-Agent-v3: Fundamental Agents for GUI Automation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ye%2C+J">Jiabo Ye</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+X">Xi Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+H">Haiyang Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+H">Haowei Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Junyang Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+Z">Zhaoqing Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+Z">Ziwei Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+F">Feiyu Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+J">Junjie Cao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+Z">Zhengxi Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liao%2C+J">Jitong Liao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+Q">Qi Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+F">Fei Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+J">Jingren Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yan%2C+M">Ming Yan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.15144v2-abstract-short" style="display: inline;">
        This paper introduces GUI-Owl, a foundational GUI agent model that achieves state-of-the-art performance among open-source end-to-end models on ten GUI benchmarks across desktop and mobile environments, covering grounding, question answering, planning, decision-making, and procedural knowledge. GUI-Owl-7B achieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose Mobile-Agent-&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.15144v2-abstract-full').style.display = 'inline'; document.getElementById('2508.15144v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.15144v2-abstract-full" style="display: none;">
        This paper introduces GUI-Owl, a foundational GUI agent model that achieves state-of-the-art performance among open-source end-to-end models on ten GUI benchmarks across desktop and mobile environments, covering grounding, question answering, planning, decision-making, and procedural knowledge. GUI-Owl-7B achieves 66.4 on AndroidWorld and 29.4 on OSWorld. Building on this, we propose Mobile-Agent-v3, a general-purpose GUI agent framework that further improves performance to 73.3 on AndroidWorld and 37.7 on OSWorld, setting a new state-of-the-art for open-source GUI agent frameworks. GUI-Owl incorporates three key innovations: (1) Large-scale Environment Infrastructure: a cloud-based virtual environment spanning Android, Ubuntu, macOS, and Windows, enabling our Self-Evolving GUI Trajectory Production framework. This generates high-quality interaction data via automated query generation and correctness validation, leveraging GUI-Owl to refine trajectories iteratively, forming a self-improving loop. It supports diverse data pipelines and reduces manual annotation. (2) Diverse Foundational Agent Capabilities: by integrating UI grounding, planning, action semantics, and reasoning patterns, GUI-Owl supports end-to-end decision-making and can act as a modular component in multi-agent systems. (3) Scalable Environment RL: we develop a scalable reinforcement learning framework with fully asynchronous training for real-world alignment. We also introduce Trajectory-aware Relative Policy Optimization (TRPO) for online RL, achieving 34.9 on OSWorld. GUI-Owl and Mobile-Agent-v3 are open-sourced at https://github.com/X-PLUG/MobileAgent.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.15144v2-abstract-full').style.display = 'none'; document.getElementById('2508.15144v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 1 September, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 20 August, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.13009">arXiv:2508.13009</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.13009">pdf</a>, <a href="https://arxiv.org/ps/2508.13009">ps</a>, <a href="https://arxiv.org/format/2508.13009">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Matrix-Game 2.0: An Open-Source, Real-Time, and Streaming Interactive World Model
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=He%2C+X">Xianglong He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+C">Chunli Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Z">Zexiang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+B">Boyang Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yifan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cui%2C+Q">Qi Cui</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kang%2C+F">Fei Kang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+B">Biao Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=An%2C+M">Mengyin An</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ren%2C+Y">Yangyang Ren</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+B">Baixin Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+H">Hao-Xiang Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gong%2C+K">Kaixiong Gong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+C">Cyrus Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+W">Wei Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+X">Xuchen Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+E">Eric Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+Y">Yahui Zhou</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.13009v1-abstract-short" style="display: inline;">
        Recent advances in interactive video generations have demonstrated diffusion model&#39;s potential as world models by capturing complex physical dynamics and interactive behaviors. However, existing interactive world models depend on bidirectional attention and lengthy inference steps, severely limiting real-time performance. Consequently, they are hard to simulate real-world dynamics, where outcomes&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.13009v1-abstract-full').style.display = 'inline'; document.getElementById('2508.13009v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.13009v1-abstract-full" style="display: none;">
        Recent advances in interactive video generations have demonstrated diffusion model&#39;s potential as world models by capturing complex physical dynamics and interactive behaviors. However, existing interactive world models depend on bidirectional attention and lengthy inference steps, severely limiting real-time performance. Consequently, they are hard to simulate real-world dynamics, where outcomes must update instantaneously based on historical context and current actions. To address this, we present Matrix-Game 2.0, an interactive world model generates long videos on-the-fly via few-step auto-regressive diffusion. Our framework consists of three key components: (1) A scalable data production pipeline for Unreal Engine and GTA5 environments to effectively produce massive amounts (about 1200 hours) of video data with diverse interaction annotations; (2) An action injection module that enables frame-level mouse and keyboard inputs as interactive conditions; (3) A few-step distillation based on the casual architecture for real-time and streaming video generation. Matrix Game 2.0 can generate high-quality minute-level videos across diverse scenes at an ultra-fast speed of 25 FPS. We open-source our model weights and codebase to advance research in interactive world modeling.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.13009v1-abstract-full').style.display = 'none'; document.getElementById('2508.13009v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Project Page: https://matrix-game-v2.github.io</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.12685">arXiv:2508.12685</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.12685">pdf</a>, <a href="https://arxiv.org/ps/2508.12685">ps</a>, <a href="https://arxiv.org/format/2508.12685">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        ToolACE-MT: Non-Autoregressive Generation for Agentic Multi-Turn Interaction
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zeng%2C+X">Xingshan Zeng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+W">Weiwen Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+L">Lingzhi Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+L">Liangyou Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mi%2C+F">Fei Mi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yasheng Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shang%2C+L">Lifeng Shang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+X">Xin Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Q">Qun Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.12685v1-abstract-short" style="display: inline;">
        Agentic task-solving with Large Language Models (LLMs) requires multi-turn, multi-step interactions, often involving complex function calls and dynamic user-agent exchanges. Existing simulation-based data generation methods for such scenarios rely heavily on costly autoregressive interactions between multiple LLM agents, thereby limiting real-world performance of agentic tasks. In this paper, we p&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.12685v1-abstract-full').style.display = 'inline'; document.getElementById('2508.12685v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.12685v1-abstract-full" style="display: none;">
        Agentic task-solving with Large Language Models (LLMs) requires multi-turn, multi-step interactions, often involving complex function calls and dynamic user-agent exchanges. Existing simulation-based data generation methods for such scenarios rely heavily on costly autoregressive interactions between multiple LLM agents, thereby limiting real-world performance of agentic tasks. In this paper, we propose a novel Non-Autoregressive Iterative Generation framework, called ToolACE-MT, for constructing high-quality multi-turn agentic dialogues. ToolACE-MT generates full conversational trajectories through three stages: coarse-grained initialization, iterative refinement, and offline verification. The initialization phase builds a structurally complete yet semantically coarse dialogue skeleton; the iterative refinement phase introduces realistic complexities and continued refinement via mask-and-fill operations; and the offline verification phase ensures correctness and coherence via rule- and model-based checks. Experiments demonstrate that ToolACE-MT enables efficient, effective and generalizable agentic data generation, offering a new paradigm for high-quality data construction in tool-augmented LLM scenarios.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.12685v1-abstract-full').style.display = 'none'; document.getElementById('2508.12685v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 18 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.11238">arXiv:2508.11238</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.11238">pdf</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Plasma Physics">physics.plasm-ph</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Accelerator Physics">physics.acc-ph</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Scaling Laws in Plasma Channels for Laser Wakefield Accelerators
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+T">Tianliang Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Jianyi Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+S">Shuang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+R">Ran Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+F">Fei Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hua%2C+J">Jianfei Hua</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+W">Wei Lu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.11238v1-abstract-short" style="display: inline;">
        Preformed plasma channels are essential for guiding high-power laser pulses over extended distances in laser wakefield accelerators, enabling the generation of multi-GeV electron beams for applications such as free-electron lasers and particle colliders. Above-threshold ionization heating provides a robust mechanism for creating laser-matched plasma channels across a wide parameter range, owing to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.11238v1-abstract-full').style.display = 'inline'; document.getElementById('2508.11238v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.11238v1-abstract-full" style="display: none;">
        Preformed plasma channels are essential for guiding high-power laser pulses over extended distances in laser wakefield accelerators, enabling the generation of multi-GeV electron beams for applications such as free-electron lasers and particle colliders. Above-threshold ionization heating provides a robust mechanism for creating laser-matched plasma channels across a wide parameter range, owing to its density- and geometry-independent heating effect. Establishing predictive scaling laws between channel parameters and formation conditions is critical for designing channels optimized for electron acceleration across energies spanning hundreds of MeV to tens of GeV. Through combined timescale analysis and numerical simulations, hydrodynamic expansion is identified as the dominant mechanism governing density profile evolution during ATI channel formation. Remarkably, this process maintains effective laser-guiding channel structures across a wide range of initial gas density, as evidenced by the persistent profile similarity observed despite these significant parameter variations. For parabolic channels matched to Gaussian laser drivers, rigorous scaling laws are established that, the on-axis density scales linearly with the initial gas density, while the matching radius has an exponential dependence on both the initial gas density and the ionization laser radius. These findings provide a systematic framework for the predictive design and optimization of plasma channels in high-efficiency and high-energy LWFA applications.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.11238v1-abstract-full').style.display = 'none'; document.getElementById('2508.11238v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.10833">arXiv:2508.10833</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.10833">pdf</a>, <a href="https://arxiv.org/ps/2508.10833">ps</a>, <a href="https://arxiv.org/format/2508.10833">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        UI-Venus Technical Report: Building High-performance UI Agents with RFT
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gu%2C+Z">Zhangxuan Gu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zeng%2C+Z">Zhengwen Zeng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+Z">Zhenyu Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+X">Xingran Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shen%2C+S">Shuheng Shen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yunfei Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+B">Beitong Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Meng%2C+C">Changhua Meng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xia%2C+T">Tianyu Xia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+W">Weizhi Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wen%2C+Y">Yue Wen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dou%2C+J">Jingya Dou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+F">Fei Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+J">Jinzhen Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yulin Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Z">Zhenlin Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gong%2C+Y">Yichen Gong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jia%2C+H">Heng Jia</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+C">Changlong Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Y">Yuan Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+Y">Yong Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+Z">Zhenyu Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+L">Liang Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+W">Weiqiang Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.10833v2-abstract-short" style="display: inline;">
        We present UI-Venus, a native UI agent that takes only screenshots as input based on a multimodal large language model. UI-Venus achieves SOTA performance on both UI grounding and navigation tasks using only several hundred thousand high-quality training samples through reinforcement finetune (RFT) based on Qwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% / 50.8% and 95.3&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.10833v2-abstract-full').style.display = 'inline'; document.getElementById('2508.10833v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.10833v2-abstract-full" style="display: none;">
        We present UI-Venus, a native UI agent that takes only screenshots as input based on a multimodal large language model. UI-Venus achieves SOTA performance on both UI grounding and navigation tasks using only several hundred thousand high-quality training samples through reinforcement finetune (RFT) based on Qwen2.5-VL. Specifically, the 7B and 72B variants of UI-Venus obtain 94.1% / 50.8% and 95.3% / 61.9% on the standard grounding benchmarks, i.e., Screenspot-V2 / Pro, surpassing the previous SOTA baselines including open-source GTA1 and closed-source UI-TARS-1.5. To show UI-Venus&#39;s summary and planing ability, we also evaluate it on the AndroidWorld, an online UI navigation arena, on which our 7B and 72B variants achieve 49.1% and 65.9% success rate, also beating existing models. To achieve this, we introduce carefully designed reward functions for both UI grounding and navigation tasks and corresponding efficient data cleaning strategies. To further boost navigation performance, we propose Self-Evolving Trajectory History Alignment &amp; Sparse Action Enhancement that refine historical reasoning traces and balances the distribution of sparse but critical actions, leading to more coherent planning and better generalization in complex UI tasks. Our contributions include the publish of SOTA open-source UI agents, comprehensive data cleaning protocols and a novel self-evolving framework for improving navigation performance, which encourage further research and development in the community. Code is available at https://github.com/inclusionAI/UI-Venus.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.10833v2-abstract-full').style.display = 'none'; document.getElementById('2508.10833v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 15 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 14 August, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.08192">arXiv:2508.08192</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.08192">pdf</a>, <a href="https://arxiv.org/ps/2508.08192">ps</a>, <a href="https://arxiv.org/format/2508.08192">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Efficient Speculative Decoding for Llama at Scale: Challenges and Solutions
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+B">Bangsheng Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fu%2C+C+C">Carl Chengyan Fu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kou%2C+F">Fei Kou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sizov%2C+G">Grigory Sizov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+H">Haoci Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Park%2C+J">Jason Park</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Jiawen Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=You%2C+J">Jie You</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Q">Qirui Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mehta%2C+S">Sachin Mehta</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+S">Shengyong Cai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xiaodong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+X">Xingyu Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yunlu Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+Y">Yanjun Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wei%2C+W">Wei Wei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+Z">Zhiwei Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qi%2C+Z">Zixi Qi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Victoria%2C+A">Adolfo Victoria</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ibrahim%2C+A">Aya Ibrahim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wasti%2C+B">Bram Wasti</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kim%2C+C">Changkyu Kim</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Haziza%2C+D">Daniel Haziza</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+F">Fei Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Delfin%2C+G">Giancarlo Delfin</a>
      , et al. (13 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.08192v1-abstract-short" style="display: inline;">
        Speculative decoding is a standard method for accelerating the inference speed of large language models. However, scaling it for production environments poses several engineering challenges, including efficiently implementing different operations (e.g., tree attention and multi-round speculative decoding) on GPU. In this paper, we detail the training and inference optimization techniques that we h&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.08192v1-abstract-full').style.display = 'inline'; document.getElementById('2508.08192v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.08192v1-abstract-full" style="display: none;">
        Speculative decoding is a standard method for accelerating the inference speed of large language models. However, scaling it for production environments poses several engineering challenges, including efficiently implementing different operations (e.g., tree attention and multi-round speculative decoding) on GPU. In this paper, we detail the training and inference optimization techniques that we have implemented to enable EAGLE-based speculative decoding at a production scale for Llama models. With these changes, we achieve a new state-of-the-art inference latency for Llama models. For example, Llama4 Maverick decodes at a speed of about 4 ms per token (with a batch size of one) on 8 NVIDIA H100 GPUs, which is 10% faster than the previously best known method. Furthermore, for EAGLE-based speculative decoding, our optimizations enable us to achieve a speed-up for large batch sizes between 1.4x and 2.0x at production scale.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.08192v1-abstract-full').style.display = 'none'; document.getElementById('2508.08192v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">15 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.08090">arXiv:2508.08090</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.08090">pdf</a>, <a href="https://arxiv.org/ps/2508.08090">ps</a>, <a href="https://arxiv.org/format/2508.08090">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Analysis of PDEs">math.AP</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Weak solutions and incompressible limit of a quasi-incompressible Navier--Stokes/Cahn--Hilliard model for viscous two-phase flows
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Fei%2C+M">Mingwen Fei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fei%2C+X">Xiang Fei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+D">Daozhi Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yadong Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.08090v1-abstract-short" style="display: inline;">
        We study a quasi-incompressible Navier--Stokes/Cahn--Hilliard coupled system which describes the motion of two macroscopically immiscible incompressible viscous fluids with partial mixing in a small interfacial region and long-range interactions. The case of unmatched densities with mass-averaged velocity is considered so that the velocity field is no longer divergence-free, and the pressure enter&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.08090v1-abstract-full').style.display = 'inline'; document.getElementById('2508.08090v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.08090v1-abstract-full" style="display: none;">
        We study a quasi-incompressible Navier--Stokes/Cahn--Hilliard coupled system which describes the motion of two macroscopically immiscible incompressible viscous fluids with partial mixing in a small interfacial region and long-range interactions. The case of unmatched densities with mass-averaged velocity is considered so that the velocity field is no longer divergence-free, and the pressure enters the equation of the chemical potential. We first prove the existence of global weak solutions to the model in a three-dimensional periodic domain, for which the implicit time discretization together with a fixed-point argument to the approximate system is employed. In particular, we obtain a new regularity estimate of the order parameter by exploiting the partial damping effect of the capillary force. Then utilizing the relative entropy method, we establish the incompressible limit -- the quasi-incompressible two-phase model converges to model H as the density difference tends to zero. Crucial to the passage of the incompressible limit, due to the lack of regularity of the pressure, are some non-standard uniform-in-density difference controls of the pressure, which are derived from the structure of the momentum equations and the improved regularity of the order parameter.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.08090v1-abstract-full').style.display = 'none'; document.getElementById('2508.08090v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">32 pages</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          35Q35; 76T06; 76T99; 35D30; 35B25; 35Q30
        

        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.08086">arXiv:2508.08086</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.08086">pdf</a>, <a href="https://arxiv.org/ps/2508.08086">ps</a>, <a href="https://arxiv.org/format/2508.08086">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Graphics">cs.GR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Matrix-3D: Omnidirectional Explorable 3D World Generation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Z">Zhongqi Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ge%2C+W">Wenhang Ge</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yuqi Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+J">Jiaqi Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+H">Haoyuan Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=An%2C+M">Mengyin An</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kang%2C+F">Fei Kang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xue%2C+H">Hua Xue</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+B">Baixin Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+Y">Yuyang Yin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+E">Eric Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yikai Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+H">Hao-Xiang Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+Y">Yahui Zhou</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.08086v1-abstract-short" style="display: inline;">
        Explorable 3D world generation from a single image or text prompt forms a cornerstone of spatial intelligence. Recent works utilize video model to achieve wide-scope and generalizable 3D world generation. However, existing approaches often suffer from a limited scope in the generated scenes. In this work, we propose Matrix-3D, a framework that utilize panoramic representation for wide-coverage omn&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.08086v1-abstract-full').style.display = 'inline'; document.getElementById('2508.08086v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.08086v1-abstract-full" style="display: none;">
        Explorable 3D world generation from a single image or text prompt forms a cornerstone of spatial intelligence. Recent works utilize video model to achieve wide-scope and generalizable 3D world generation. However, existing approaches often suffer from a limited scope in the generated scenes. In this work, we propose Matrix-3D, a framework that utilize panoramic representation for wide-coverage omnidirectional explorable 3D world generation that combines conditional video generation and panoramic 3D reconstruction. We first train a trajectory-guided panoramic video diffusion model that employs scene mesh renders as condition, to enable high-quality and geometrically consistent scene video generation. To lift the panorama scene video to 3D world, we propose two separate methods: (1) a feed-forward large panorama reconstruction model for rapid 3D scene reconstruction and (2) an optimization-based pipeline for accurate and detailed 3D scene reconstruction. To facilitate effective training, we also introduce the Matrix-Pano dataset, the first large-scale synthetic collection comprising 116K high-quality static panoramic video sequences with depth and trajectory annotations. Extensive experiments demonstrate that our proposed framework achieves state-of-the-art performance in panoramic video generation and 3D world generation. See more in https://matrix-3d.github.io.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.08086v1-abstract-full').style.display = 'none'; document.getElementById('2508.08086v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Technical Report</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.07654">arXiv:2508.07654</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.07654">pdf</a>, <a href="https://arxiv.org/ps/2508.07654">ps</a>, <a href="https://arxiv.org/format/2508.07654">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Databases">cs.DB</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MLego: Interactive and Scalable Topic Exploration Through Model Reuse
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ye%2C+F">Fei Ye</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Jiapan Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jing%2C+Y">Yinan Jing</a>, 
      
      <a href="/search/?searchtype=author&amp;query=He%2C+Z">Zhenying He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+W">Weirao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X+S">X. Sean Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.07654v1-abstract-short" style="display: inline;">
        With massive texts on social media, users and analysts often rely on topic modeling techniques to quickly extract key themes and gain insights. Traditional topic modeling techniques, such as Latent Dirichlet Allocation (LDA), provide valuable insights but are computationally expensive, making them impractical for real-time data analysis. Although recent advances in distributed training and fast sa&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.07654v1-abstract-full').style.display = 'inline'; document.getElementById('2508.07654v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.07654v1-abstract-full" style="display: none;">
        With massive texts on social media, users and analysts often rely on topic modeling techniques to quickly extract key themes and gain insights. Traditional topic modeling techniques, such as Latent Dirichlet Allocation (LDA), provide valuable insights but are computationally expensive, making them impractical for real-time data analysis. Although recent advances in distributed training and fast sampling methods have improved efficiency, real-time topic exploration remains a significant challenge. In this paper, we present MLego, an interactive query framework designed to support real-time topic modeling analysis by leveraging model materialization and reuse. Instead of retraining models from scratch, MLego efficiently merges materialized topic models to construct approximate results at interactive speeds. To further enhance efficiency, we introduce a hierarchical plan search strategy for single queries and an optimized query reordering technique for batch queries. We integrate MLego into a visual analytics prototype system, enabling users to explore large-scale textual datasets through interactive queries. Extensive experiments demonstrate that MLego significantly reduces computation costs while maintaining high-quality topic modeling results. MLego enhances existing visual analytics approaches, which primarily focus on user-driven topic modeling, by enabling real-time, query-driven exploration. This complements traditional methods and bridges the gap between scalable topic modeling and interactive data analysis.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.07654v1-abstract-full').style.display = 'none'; document.getElementById('2508.07654v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">14 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.07608">arXiv:2508.07608</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.07608">pdf</a>, <a href="https://arxiv.org/ps/2508.07608">ps</a>, <a href="https://arxiv.org/format/2508.07608">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Multimedia">cs.MM</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Sound">cs.SD</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Audio and Speech Processing">eess.AS</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        AD-AVSR: Asymmetric Dual-stream Enhancement for Robust Audio-Visual Speech Recognition
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xue%2C+J">Junxiao Xue</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+X">Xiaozhen Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+X">Xuecheng Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+X">Xinyi Yin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+D">Danlei Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+F">Fei Yu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.07608v1-abstract-short" style="display: inline;">
        Audio-visual speech recognition (AVSR) combines audio-visual modalities to improve speech recognition, especially in noisy environments. However, most existing methods deploy the unidirectional enhancement or symmetric fusion manner, which limits their capability to capture heterogeneous and complementary correlations of audio-visual data-especially under asymmetric information conditions. To tack&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.07608v1-abstract-full').style.display = 'inline'; document.getElementById('2508.07608v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.07608v1-abstract-full" style="display: none;">
        Audio-visual speech recognition (AVSR) combines audio-visual modalities to improve speech recognition, especially in noisy environments. However, most existing methods deploy the unidirectional enhancement or symmetric fusion manner, which limits their capability to capture heterogeneous and complementary correlations of audio-visual data-especially under asymmetric information conditions. To tackle these gaps, we introduce a new AVSR framework termed AD-AVSR based on bidirectional modality enhancement. Specifically, we first introduce the audio dual-stream encoding strategy to enrich audio representations from multiple perspectives and intentionally establish asymmetry to support subsequent cross-modal interactions. The enhancement process involves two key components, Audio-aware Visual Refinement Module for enhanced visual representations under audio guidance, and Cross-modal Noise Suppression Masking Module which refines audio representations using visual cues, collaboratively leading to the closed-loop and bidirectional information flow. To further enhance correlation robustness, we adopt a threshold-based selection mechanism to filter out irrelevant or weakly correlated audio-visual pairs. Extensive experimental results on the LRS2 and LRS3 datasets indicate that our AD-AVSR consistently surpasses SOTA methods in both performance and noise robustness, highlighting the effectiveness of our model design.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.07608v1-abstract-full').style.display = 'none'; document.getElementById('2508.07608v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 11 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted by the ACM MM 2025 Workshop on SVC</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.07409">arXiv:2508.07409</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.07409">pdf</a>, <a href="https://arxiv.org/ps/2508.07409">ps</a>, <a href="https://arxiv.org/format/2508.07409">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        CharacterShot: Controllable and Consistent 4D Character Animation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+J">Junyao Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Jiaxing Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+W">Wenran Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zeng%2C+Y">Yanhong Zeng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shen%2C+F">Fei Shen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+K">Kai Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+Y">Yanan Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+C">Cairong Zhao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.07409v1-abstract-short" style="display: inline;">
        In this paper, we propose \textbf{CharacterShot}, a controllable and consistent 4D character animation framework that enables any individual designer to create dynamic 3D characters (i.e., 4D character animation) from a single reference character image and a 2D pose sequence. We begin by pretraining a powerful 2D character animation model based on a cutting-edge DiT-based image-to-video model, whi&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.07409v1-abstract-full').style.display = 'inline'; document.getElementById('2508.07409v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.07409v1-abstract-full" style="display: none;">
        In this paper, we propose \textbf{CharacterShot}, a controllable and consistent 4D character animation framework that enables any individual designer to create dynamic 3D characters (i.e., 4D character animation) from a single reference character image and a 2D pose sequence. We begin by pretraining a powerful 2D character animation model based on a cutting-edge DiT-based image-to-video model, which allows for any 2D pose sequnce as controllable signal. We then lift the animation model from 2D to 3D through introducing dual-attention module together with camera prior to generate multi-view videos with spatial-temporal and spatial-view consistency. Finally, we employ a novel neighbor-constrained 4D gaussian splatting optimization on these multi-view videos, resulting in continuous and stable 4D character representations. Moreover, to improve character-centric performance, we construct a large-scale dataset Character4D, containing 13,115 unique characters with diverse appearances and motions, rendered from multiple viewpoints. Extensive experiments on our newly constructed benchmark, CharacterBench, demonstrate that our approach outperforms current state-of-the-art methods. Code, models, and datasets will be publicly available at https://github.com/Jeoyal/CharacterShot.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.07409v1-abstract-full').style.display = 'none'; document.getElementById('2508.07409v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">13 pages, 10 figures. Code at https://github.com/Jeoyal/CharacterShot</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.07307">arXiv:2508.07307</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.07307">pdf</a>, <a href="https://arxiv.org/ps/2508.07307">ps</a>, <a href="https://arxiv.org/format/2508.07307">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MCITlib: Multimodal Continual Instruction Tuning Library and Benchmark
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+H">Haiyang Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+F">Fei Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhao%2C+H">Hongbo Zhao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zeng%2C+F">Fanhu Zeng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+W">Wenzhuo Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+S">Shijie Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+D">Da-Han Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+X">Xu-Yao Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.07307v1-abstract-short" style="display: inline;">
        Continual learning aims to equip AI systems with the ability to continuously acquire and adapt to new knowledge without forgetting previously learned information, similar to human learning. While traditional continual learning methods focusing on unimodal tasks have achieved notable success, the emergence of Multimodal Large Language Models has brought increasing attention to Multimodal Continual&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.07307v1-abstract-full').style.display = 'inline'; document.getElementById('2508.07307v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.07307v1-abstract-full" style="display: none;">
        Continual learning aims to equip AI systems with the ability to continuously acquire and adapt to new knowledge without forgetting previously learned information, similar to human learning. While traditional continual learning methods focusing on unimodal tasks have achieved notable success, the emergence of Multimodal Large Language Models has brought increasing attention to Multimodal Continual Learning tasks involving multiple modalities, such as vision and language. In this setting, models are expected to not only mitigate catastrophic forgetting but also handle the challenges posed by cross-modal interactions and coordination. To facilitate research in this direction, we introduce MCITlib, a comprehensive and constantly evolving code library for continual instruction tuning of Multimodal Large Language Models. In MCITlib, we have currently implemented 8 representative algorithms for Multimodal Continual Instruction Tuning and systematically evaluated them on 2 carefully selected benchmarks. MCITlib will be continuously updated to reflect advances in the Multimodal Continual Learning field. The codebase is released at https://github.com/Ghy0501/MCITlib.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.07307v1-abstract-full').style.display = 'none'; document.getElementById('2508.07307v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 10 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Preprint</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.06471">arXiv:2508.06471</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.06471">pdf</a>, <a href="https://arxiv.org/ps/2508.06471">ps</a>, <a href="https://arxiv.org/format/2508.06471">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        GLM-4.5: Agentic, Reasoning, and Coding (ARC) Foundation Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Team%2C+G+5">GLM-4. 5 Team</a>, 
      
      <a href="/search/?searchtype=author&amp;query=%3A"> :</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zeng%2C+A">Aohan Zeng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lv%2C+X">Xin Lv</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zheng%2C+Q">Qinkai Zheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hou%2C+Z">Zhenyu Hou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+B">Bin Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+C">Chengxing Xie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+C">Cunxiang Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yin%2C+D">Da Yin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zeng%2C+H">Hao Zeng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+J">Jiajie Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+K">Kedong Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhong%2C+L">Lucen Zhong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+M">Mingdao Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+R">Rui Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+S">Shulin Cao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+X">Xiaohan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+X">Xuancheng Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wei%2C+Y">Yao Wei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+Y">Yean Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=An%2C+Y">Yifan An</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Niu%2C+Y">Yilin Niu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wen%2C+Y">Yuanhao Wen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bai%2C+Y">Yushi Bai</a>
      , et al. (147 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.06471v1-abstract-short" style="display: inline;">
        We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance acro&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.06471v1-abstract-full').style.display = 'inline'; document.getElementById('2508.06471v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.06471v1-abstract-full" style="display: none;">
        We present GLM-4.5, an open-source Mixture-of-Experts (MoE) large language model with 355B total parameters and 32B activated parameters, featuring a hybrid reasoning method that supports both thinking and direct response modes. Through multi-stage training on 23T tokens and comprehensive post-training with expert model iteration and reinforcement learning, GLM-4.5 achieves strong performance across agentic, reasoning, and coding (ARC) tasks, scoring 70.1% on TAU-Bench, 91.0% on AIME 24, and 64.2% on SWE-bench Verified. With much fewer parameters than several competitors, GLM-4.5 ranks 3rd overall among all evaluated models and 2nd on agentic benchmarks. We release both GLM-4.5 (355B parameters) and a compact version, GLM-4.5-Air (106B parameters), to advance research in reasoning and agentic AI systems. Code, models, and more information are available at https://github.com/zai-org/GLM-4.5.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.06471v1-abstract-full').style.display = 'none'; document.getElementById('2508.06471v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.05731">arXiv:2508.05731</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.05731">pdf</a>, <a href="https://arxiv.org/ps/2508.05731">ps</a>, <a href="https://arxiv.org/format/2508.05731">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        InfiGUI-G1: Advancing GUI Grounding with Adaptive Exploration Policy Optimization
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yuhang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Z">Zeyu Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+S">Shuanghe Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+P">Pengxiang Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+C">Congkai Xie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jiasheng Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+X">Xueyu Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+X">Xiaotian Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yuan%2C+J">Jianbo Yuan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xinyao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+S">Shengyu Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+H">Hongxia Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+F">Fei Wu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.05731v1-abstract-short" style="display: inline;">
        The emergence of Multimodal Large Language Models (MLLMs) has propelled the development of autonomous agents that operate on Graphical User Interfaces (GUIs) using pure visual input. A fundamental challenge is robustly grounding natural language instructions. This requires a precise spatial alignment, which accurately locates the coordinates of each element, and, more critically, a correct semanti&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.05731v1-abstract-full').style.display = 'inline'; document.getElementById('2508.05731v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.05731v1-abstract-full" style="display: none;">
        The emergence of Multimodal Large Language Models (MLLMs) has propelled the development of autonomous agents that operate on Graphical User Interfaces (GUIs) using pure visual input. A fundamental challenge is robustly grounding natural language instructions. This requires a precise spatial alignment, which accurately locates the coordinates of each element, and, more critically, a correct semantic alignment, which matches the instructions to the functionally appropriate UI element. Although Reinforcement Learning with Verifiable Rewards (RLVR) has proven to be effective at improving spatial alignment for these MLLMs, we find that inefficient exploration bottlenecks semantic alignment, which prevent models from learning difficult semantic associations. To address this exploration problem, we present Adaptive Exploration Policy Optimization (AEPO), a new policy optimization framework. AEPO employs a multi-answer generation strategy to enforce broader exploration, which is then guided by a theoretically grounded Adaptive Exploration Reward (AER) function derived from first principles of efficiency eta=U/C. Our AEPO-trained models, InfiGUI-G1-3B and InfiGUI-G1-7B, establish new state-of-the-art results across multiple challenging GUI grounding benchmarks, achieving significant relative improvements of up to 9.0% against the naive RLVR baseline on benchmarks designed to test generalization and semantic understanding. Resources are available at https://github.com/InfiXAI/InfiGUI-G1.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.05731v1-abstract-full').style.display = 'none'; document.getElementById('2508.05731v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">11 pages, 3 figures</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.05452">arXiv:2508.05452</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.05452">pdf</a>, <a href="https://arxiv.org/ps/2508.05452">ps</a>, <a href="https://arxiv.org/format/2508.05452">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        LLMEval-3: A Large-Scale Longitudinal Study on Robust and Fair Evaluation of Large Language Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+M">Ming Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shen%2C+Y">Yujiong Shen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Deng%2C+J">Jingyi Deng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Y">Yuhui Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yue Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Junzhe Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+S">Shichun Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dou%2C+S">Shihan Dou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sha%2C+H">Huayu Sha</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Peng%2C+Q">Qiyuan Peng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+C">Changhao Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tong%2C+J">Jingqi Tong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+Y">Yilong Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Z">Zhihao Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+M">Mingqi Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xi%2C+Z">Zhiheng Xi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chai%2C+M">Mingxu Chai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liang%2C+T">Tao Liang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fei%2C+Z">Zhihui Fei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Z">Zhen Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wan%2C+M">Mingyang Wan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+G">Guojun Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gui%2C+T">Tao Gui</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Q">Qi Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+X">Xuanjing Huang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.05452v2-abstract-short" style="display: inline;">
        Existing evaluation of Large Language Models (LLMs) on static benchmarks is vulnerable to data contamination and leaderboard overfitting, critical issues that obscure true model capabilities. To address this, we introduce LLMEval-3, a framework for dynamic evaluation of LLMs. LLMEval-3 is built on a proprietary bank of 220k graduate-level questions, from which it dynamically samples unseen test se&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.05452v2-abstract-full').style.display = 'inline'; document.getElementById('2508.05452v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.05452v2-abstract-full" style="display: none;">
        Existing evaluation of Large Language Models (LLMs) on static benchmarks is vulnerable to data contamination and leaderboard overfitting, critical issues that obscure true model capabilities. To address this, we introduce LLMEval-3, a framework for dynamic evaluation of LLMs. LLMEval-3 is built on a proprietary bank of 220k graduate-level questions, from which it dynamically samples unseen test sets for each evaluation run. Its automated pipeline ensures integrity via contamination-resistant data curation, a novel anti-cheating architecture, and a calibrated LLM-as-a-judge process achieving 90% agreement with human experts, complemented by a relative ranking system for fair comparison. An 20-month longitudinal study of nearly 50 leading models reveals a performance ceiling on knowledge memorization and exposes data contamination vulnerabilities undetectable by static benchmarks. The framework demonstrates exceptional robustness in ranking stability and consistency, providing strong empirical validation for the dynamic evaluation paradigm. LLMEval-3 offers a robust and credible methodology for assessing the true capabilities of LLMs beyond leaderboard scores, promoting the development of more trustworthy evaluation standards.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.05452v2-abstract-full').style.display = 'none'; document.getElementById('2508.05452v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 12 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 August, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.05433">arXiv:2508.05433</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.05433">pdf</a>, <a href="https://arxiv.org/ps/2508.05433">ps</a>, <a href="https://arxiv.org/format/2508.05433">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Neural and Evolutionary Computing">cs.NE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Discovering Interpretable Programmatic Policies via Multimodal LLM-assisted Evolutionary Search
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+Q">Qinglong Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tong%2C+X">Xialiang Tong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yuan%2C+M">Mingxuan Yuan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+F">Fei Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+Z">Zhichao Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Q">Qingfu Zhang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.05433v1-abstract-short" style="display: inline;">
        Interpretability and high performance are essential goals in designing control policies, particularly for safety-critical tasks. Deep reinforcement learning has greatly enhanced performance, yet its inherent lack of interpretability often undermines trust and hinders real-world deployment. This work addresses these dual challenges by introducing a novel approach for programmatic policy discovery,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.05433v1-abstract-full').style.display = 'inline'; document.getElementById('2508.05433v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.05433v1-abstract-full" style="display: none;">
        Interpretability and high performance are essential goals in designing control policies, particularly for safety-critical tasks. Deep reinforcement learning has greatly enhanced performance, yet its inherent lack of interpretability often undermines trust and hinders real-world deployment. This work addresses these dual challenges by introducing a novel approach for programmatic policy discovery, called Multimodal Large Language Model-assisted Evolutionary Search (MLES). MLES utilizes multimodal large language models as policy generators, combining them with evolutionary mechanisms for automatic policy optimization. It integrates visual feedback-driven behavior analysis within the policy generation process to identify failure patterns and facilitate targeted improvements, enhancing the efficiency of policy discovery and producing adaptable, human-aligned policies. Experimental results show that MLES achieves policy discovery capabilities and efficiency comparable to Proximal Policy Optimization (PPO) across two control tasks, while offering transparent control logic and traceable design processes. This paradigm overcomes the limitations of predefined domain-specific languages, facilitates knowledge transfer and reuse, and is scalable across various control tasks. MLES shows promise as a leading approach for the next generation of interpretable control policy discovery.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.05433v1-abstract-full').style.display = 'none'; document.getElementById('2508.05433v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.05401">arXiv:2508.05401</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.05401">pdf</a>, <a href="https://arxiv.org/ps/2508.05401">ps</a>, <a href="https://arxiv.org/format/2508.05401">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Analysis of PDEs">math.AP</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Geometrical characterizations of radiating and non-radiating elastic sources and mediums with applications
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Diao%2C+H">Huaian Diao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fei%2C+X">Xiaoxu Fei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+H">Hongyu Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.05401v2-abstract-short" style="display: inline;">
        In this paper, we investigate two types of time-harmonic elastic wave scattering problems. The first one involves the scattered wave generated by an active elastic source with compact support. The second one concerns elastic wave scattering caused by an inhomogeneous medium, also with compact support. We derive several novel quantitative results concerning the geometrical properties of the underly&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.05401v2-abstract-full').style.display = 'inline'; document.getElementById('2508.05401v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.05401v2-abstract-full" style="display: none;">
        In this paper, we investigate two types of time-harmonic elastic wave scattering problems. The first one involves the scattered wave generated by an active elastic source with compact support. The second one concerns elastic wave scattering caused by an inhomogeneous medium, also with compact support. We derive several novel quantitative results concerning the geometrical properties of the underlying scatterer, the associated source or incident wave field, and the physical parameters. In particular, we show that a scatterer with either a small support or high-curvature boundary points must radiate at any frequency. These qualitative characterizations allow us to establish several local and global uniqueness results for determining the support of the source or medium scatterer from a single far-field measurement. Furthermore, we reveal new geometric properties of elastic transmission eigenfunctions. To derive a quantitative relationship between the intensity of a radiating or non-radiating source and the diameter of its support, we utilize the Helmholtz decomposition, the translation-invariant $L^2$-norm estimate for the Lamé operator, and global energy estimates. Another pivotal technical approach combines complex geometric optics (CGO) solutions with local regularity estimates, facilitating microlocal analysis near admissible $K$-curvature boundary points.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.05401v2-abstract-full').style.display = 'none'; document.getElementById('2508.05401v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 8 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 7 August, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.04681">arXiv:2508.04681</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.04681">pdf</a>, <a href="https://arxiv.org/ps/2508.04681">ps</a>, <a href="https://arxiv.org/format/2508.04681">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Perceiving and Acting in First-Person: A Dataset and Benchmark for Egocentric Human-Object-Human Interactions
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+L">Liang Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+C">Chengqun Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+Z">Zili Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+F">Fei Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yifan Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+C">Congsheng Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yiyi Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Qin%2C+J">Jie Qin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sheng%2C+X">Xingdong Sheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yunhui Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jin%2C+X">Xin Jin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yan%2C+Y">Yichao Yan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zeng%2C+W">Wenjun Zeng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+X">Xiaokang Yang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.04681v1-abstract-short" style="display: inline;">
        Learning action models from real-world human-centric interaction datasets is important towards building general-purpose intelligent assistants with efficiency. However, most existing datasets only offer specialist interaction category and ignore that AI assistants perceive and act based on first-person acquisition. We urge that both the generalist interaction knowledge and egocentric modality are&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.04681v1-abstract-full').style.display = 'inline'; document.getElementById('2508.04681v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.04681v1-abstract-full" style="display: none;">
        Learning action models from real-world human-centric interaction datasets is important towards building general-purpose intelligent assistants with efficiency. However, most existing datasets only offer specialist interaction category and ignore that AI assistants perceive and act based on first-person acquisition. We urge that both the generalist interaction knowledge and egocentric modality are indispensable. In this paper, we embed the manual-assisted task into a vision-language-action framework, where the assistant provides services to the instructor following egocentric vision and commands. With our hybrid RGB-MoCap system, pairs of assistants and instructors engage with multiple objects and the scene following GPT-generated scripts. Under this setting, we accomplish InterVLA, the first large-scale human-object-human interaction dataset with 11.4 hours and 1.2M frames of multimodal data, spanning 2 egocentric and 5 exocentric videos, accurate human/object motions and verbal commands. Furthermore, we establish novel benchmarks on egocentric human motion estimation, interaction synthesis, and interaction prediction with comprehensive analysis. We believe that our InterVLA testbed and the benchmarks will foster future works on building AI agents in the physical world.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.04681v1-abstract-full').style.display = 'none'; document.getElementById('2508.04681v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Accepted to ICCV 2025. Project Page: https://liangxuy.github.io/InterVLA/</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.04463">arXiv:2508.04463</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.04463">pdf</a>, <a href="https://arxiv.org/ps/2508.04463">ps</a>, <a href="https://arxiv.org/format/2508.04463">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        GFocal: A Global-Focal Neural Operator for Solving PDEs on Arbitrary Geometries
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Fei%2C+F">Fangzhi Fei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hu%2C+J">Jiaxin Hu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Q">Qiaofeng Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Z">Zhenyu Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.04463v1-abstract-short" style="display: inline;">
        Transformer-based neural operators have emerged as promising surrogate solvers for partial differential equations, by leveraging the effectiveness of Transformers for capturing long-range dependencies and global correlations, profoundly proven in language modeling. However, existing methodologies overlook the coordinated learning of interdependencies between local physical details and global featu&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.04463v1-abstract-full').style.display = 'inline'; document.getElementById('2508.04463v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.04463v1-abstract-full" style="display: none;">
        Transformer-based neural operators have emerged as promising surrogate solvers for partial differential equations, by leveraging the effectiveness of Transformers for capturing long-range dependencies and global correlations, profoundly proven in language modeling. However, existing methodologies overlook the coordinated learning of interdependencies between local physical details and global features, which are essential for tackling multiscale problems, preserving physical consistency and numerical stability in long-term rollouts, and accurately capturing transitional dynamics. In this work, we propose GFocal, a Transformer-based neural operator method that enforces simultaneous global and local feature learning and fusion. Global correlations and local features are harnessed through Nyström attention-based \textbf{g}lobal blocks and slices-based \textbf{focal} blocks to generate physics-aware tokens, subsequently modulated and integrated via convolution-based gating blocks, enabling dynamic fusion of multiscale information. GFocal achieves accurate modeling and prediction of physical features given arbitrary geometries and initial conditions. Experiments show that GFocal achieves state-of-the-art performance with an average 15.2\% relative gain in five out of six benchmarks and also excels in industry-scale simulations such as aerodynamics simulation of automotives and airfoils.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.04463v1-abstract-full').style.display = 'none'; document.getElementById('2508.04463v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.03691">arXiv:2508.03691</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.03691">pdf</a>, <a href="https://arxiv.org/ps/2508.03691">ps</a>, <a href="https://arxiv.org/format/2508.03691">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        La La LiDAR: Large-Scale Layout Generation from LiDAR Data
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Youquan Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kong%2C+L">Lingdong Kong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+W">Weidong Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xin Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liang%2C+A">Ao Liang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+R">Runnan Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fei%2C+B">Ben Fei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tongliang Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.03691v1-abstract-short" style="display: inline;">
        Controllable generation of realistic LiDAR scenes is crucial for applications such as autonomous driving and robotics. While recent diffusion-based models achieve high-fidelity LiDAR generation, they lack explicit control over foreground objects and spatial relationships, limiting their usefulness for scenario simulation and safety validation. To address these limitations, we propose Large-scale L&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.03691v1-abstract-full').style.display = 'inline'; document.getElementById('2508.03691v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.03691v1-abstract-full" style="display: none;">
        Controllable generation of realistic LiDAR scenes is crucial for applications such as autonomous driving and robotics. While recent diffusion-based models achieve high-fidelity LiDAR generation, they lack explicit control over foreground objects and spatial relationships, limiting their usefulness for scenario simulation and safety validation. To address these limitations, we propose Large-scale Layout-guided LiDAR generation model (&#34;La La LiDAR&#34;), a novel layout-guided generative framework that introduces semantic-enhanced scene graph diffusion with relation-aware contextual conditioning for structured LiDAR layout generation, followed by foreground-aware control injection for complete scene generation. This enables customizable control over object placement while ensuring spatial and semantic consistency. To support our structured LiDAR generation, we introduce Waymo-SG and nuScenes-SG, two large-scale LiDAR scene graph datasets, along with new evaluation metrics for layout synthesis. Extensive experiments demonstrate that La La LiDAR achieves state-of-the-art performance in both LiDAR generation and downstream perception tasks, establishing a new benchmark for controllable 3D scene generation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.03691v1-abstract-full').style.display = 'none'; document.getElementById('2508.03691v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Preprint; 10 pages, 6 figures, 7 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.03690">arXiv:2508.03690</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.03690">pdf</a>, <a href="https://arxiv.org/ps/2508.03690">ps</a>, <a href="https://arxiv.org/format/2508.03690">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Veila: Panoramic LiDAR Generation from a Monocular RGB Image
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Youquan Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kong%2C+L">Lingdong Kong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+W">Weidong Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liang%2C+A">Ao Liang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+J">Jianxiong Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+Y">Yang Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+X">Xiang Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xin Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+L">Linfeng Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+R">Runnan Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fei%2C+B">Ben Fei</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.03690v1-abstract-short" style="display: inline;">
        Realistic and controllable panoramic LiDAR data generation is critical for scalable 3D perception in autonomous driving and robotics. Existing methods either perform unconditional generation with poor controllability or adopt text-guided synthesis, which lacks fine-grained spatial control. Leveraging a monocular RGB image as a spatial control signal offers a scalable and low-cost alternative, whic&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.03690v1-abstract-full').style.display = 'inline'; document.getElementById('2508.03690v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.03690v1-abstract-full" style="display: none;">
        Realistic and controllable panoramic LiDAR data generation is critical for scalable 3D perception in autonomous driving and robotics. Existing methods either perform unconditional generation with poor controllability or adopt text-guided synthesis, which lacks fine-grained spatial control. Leveraging a monocular RGB image as a spatial control signal offers a scalable and low-cost alternative, which remains an open problem. However, it faces three core challenges: (i) semantic and depth cues from RGB are vary spatially, complicating reliable conditioning generation; (ii) modality gaps between RGB appearance and LiDAR geometry amplify alignment errors under noisy diffusion; and (iii) maintaining structural coherence between monocular RGB and panoramic LiDAR is challenging, particularly in non-overlap regions between images and LiDAR. To address these challenges, we propose Veila, a novel conditional diffusion framework that integrates: a Confidence-Aware Conditioning Mechanism (CACM) that strengthens RGB conditioning by adaptively balancing semantic and depth cues according to their local reliability; a Geometric Cross-Modal Alignment (GCMA) for robust RGB-LiDAR alignment under noisy diffusion; and a Panoramic Feature Coherence (PFC) for enforcing global structural consistency across monocular RGB and panoramic LiDAR. Additionally, we introduce two metrics, Cross-Modal Semantic Consistency and Cross-Modal Depth Consistency, to evaluate alignment quality across modalities. Experiments on nuScenes, SemanticKITTI, and our proposed KITTI-Weather benchmark demonstrate that Veila achieves state-of-the-art generation fidelity and cross-modal consistency, while enabling generative data augmentation that improves downstream LiDAR semantic segmentation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.03690v1-abstract-full').style.display = 'none'; document.getElementById('2508.03690v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">Preprint; 10 pages, 6 figures, 7 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.03405">arXiv:2508.03405</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.03405">pdf</a>, <a href="https://arxiv.org/ps/2508.03405">ps</a>, <a href="https://arxiv.org/format/2508.03405">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Materials Science">cond-mat.mtrl-sci</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Model Accuracy and Data Heterogeneity Shape Uncertainty Quantification in Machine Learning Interatomic Potentials
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Shuang%2C+F">Fei Shuang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wei%2C+Z">Zixiong Wei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+K">Kai Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+W">Wei Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dey%2C+P">Poulumi Dey</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.03405v1-abstract-short" style="display: inline;">
        Machine learning interatomic potentials (MLIPs) enable accurate atomistic modelling, but reliable uncertainty quantification (UQ) remains elusive. In this study, we investigate two UQ strategies, ensemble learning and D-optimality, within the atomic cluster expansion framework. It is revealed that higher model accuracy strengthens the correlation between predicted uncertainties and actual errors a&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.03405v1-abstract-full').style.display = 'inline'; document.getElementById('2508.03405v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.03405v1-abstract-full" style="display: none;">
        Machine learning interatomic potentials (MLIPs) enable accurate atomistic modelling, but reliable uncertainty quantification (UQ) remains elusive. In this study, we investigate two UQ strategies, ensemble learning and D-optimality, within the atomic cluster expansion framework. It is revealed that higher model accuracy strengthens the correlation between predicted uncertainties and actual errors and improves novelty detection, with D-optimality yielding more conservative estimates. Both methods deliver well calibrated uncertainties on homogeneous training sets, yet they underpredict errors and exhibit reduced novelty sensitivity on heterogeneous datasets. To address this limitation, we introduce clustering-enhanced local D-optimality, which partitions configuration space into clusters during training and applies D-optimality within each cluster. This approach substantially improves the detection of novel atomic environments in heterogeneous datasets. Our findings clarify the roles of model fidelity and data heterogeneity in UQ performance and provide a practical route to robust active learning and adaptive sampling strategies for MLIP development.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.03405v1-abstract-full').style.display = 'none'; document.getElementById('2508.03405v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 5 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.03082">arXiv:2508.03082</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.03082">pdf</a>, <a href="https://arxiv.org/ps/2508.03082">ps</a>, <a href="https://arxiv.org/format/2508.03082">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        EoH-S: Evolution of Heuristic Set using LLMs for Automated Heuristic Design
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+F">Fei Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yilu Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Q">Qingfu Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tong%2C+X">Xialiang Tong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yuan%2C+M">Mingxuan Yuan</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.03082v2-abstract-short" style="display: inline;">
        Automated Heuristic Design (AHD) using Large Language Models (LLMs) has achieved notable success in recent years. Despite the effectiveness of existing approaches, they only design a single heuristic to serve all problem instances, often inducing poor generalization across different distributions or settings. To address this issue, we propose Automated Heuristic Set Design (AHSD), a new formulatio&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.03082v2-abstract-full').style.display = 'inline'; document.getElementById('2508.03082v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.03082v2-abstract-full" style="display: none;">
        Automated Heuristic Design (AHD) using Large Language Models (LLMs) has achieved notable success in recent years. Despite the effectiveness of existing approaches, they only design a single heuristic to serve all problem instances, often inducing poor generalization across different distributions or settings. To address this issue, we propose Automated Heuristic Set Design (AHSD), a new formulation for LLM-driven AHD. The aim of AHSD is to automatically generate a small-sized complementary heuristic set to serve diverse problem instances, such that each problem instance could be optimized by at least one heuristic in this set. We show that the objective function of AHSD is monotone and supermodular. Then, we propose Evolution of Heuristic Set (EoH-S) to apply the AHSD formulation for LLM-driven AHD. With two novel mechanisms of complementary population management and complementary-aware memetic search, EoH-S could effectively generate a set of high-quality and complementary heuristics. Comprehensive experimental results on three AHD tasks with diverse instances spanning various sizes and distributions demonstrate that EoH-S consistently outperforms existing state-of-the-art AHD methods and achieves up to 60\% performance improvements.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.03082v2-abstract-full').style.display = 'none'; document.getElementById('2508.03082v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 20 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 5 August, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.03016">arXiv:2508.03016</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.03016">pdf</a>, <a href="https://arxiv.org/ps/2508.03016">ps</a>, <a href="https://arxiv.org/format/2508.03016">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        KBest: Efficient Vector Search on Kunpeng CPU
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+K">Kaihao Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+M">Meiling Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Oleg%2C+S">Senkevich Oleg</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Z">Zijian Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xue%2C+D">Daihao Xue</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Malyshev%2C+D">Dmitriy Malyshev</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lv%2C+Y">Yangming Lv</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiao%2C+S">Shihai Xiao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yan%2C+X">Xiao Yan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Alexander%2C+R">Radionov Alexander</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zeng%2C+W">Weidi Zeng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+Y">Yuanzhan Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zou%2C+Z">Zhiyu Zou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yao%2C+X">Xin Yao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+L">Lin Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+J">Junhao Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yiding Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fu%2C+Y">Yaoyao Fu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+G">Gongyi Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+G">Gong Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yi%2C+F">Fei Yi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yingfan Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.03016v2-abstract-short" style="display: inline;">
        Vector search, which returns the vectors most similar to a given query vector from a large vector dataset, underlies many important applications such as search, recommendation, and LLMs. To be economic, vector search needs to be efficient to reduce the resources required by a given query workload. However, existing vector search libraries (e.g., Faiss and DiskANN) are optimized for x86 CPU archite&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.03016v2-abstract-full').style.display = 'inline'; document.getElementById('2508.03016v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.03016v2-abstract-full" style="display: none;">
        Vector search, which returns the vectors most similar to a given query vector from a large vector dataset, underlies many important applications such as search, recommendation, and LLMs. To be economic, vector search needs to be efficient to reduce the resources required by a given query workload. However, existing vector search libraries (e.g., Faiss and DiskANN) are optimized for x86 CPU architectures (i.e., Intel and AMD CPUs) while Huawei Kunpeng CPUs are based on the ARM architecture and competitive in compute power. In this paper, we present KBest as a vector search library tailored for the latest Kunpeng 920 CPUs. To be efficient, KBest incorporates extensive hardware-aware and algorithmic optimizations, which include single-instruction-multiple-data (SIMD) accelerated distance computation, data prefetch, index refinement, early termination, and vector quantization. Experiment results show that KBest outperforms SOTA vector search libraries running on x86 CPUs, and our optimizations can improve the query throughput by over 2x. Currently, KBest serves applications from both our internal business and external enterprise clients with tens of millions of queries on a daily basis.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.03016v2-abstract-full').style.display = 'none'; document.getElementById('2508.03016v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 4 August, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.03009">arXiv:2508.03009</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.03009">pdf</a>, <a href="https://arxiv.org/ps/2508.03009">ps</a>, <a href="https://arxiv.org/format/2508.03009">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Enhancing Long Video Question Answering with Scene-Localized Frame Grouping
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+X">Xuyi Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+W">Wenhao Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jin%2C+H">Hongbo Jin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+L">Lin Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+H">Hongbo Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nie%2C+Y">Yongwei Nie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+F">Fei Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+F">Fei Ma</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.03009v1-abstract-short" style="display: inline;">
        Current Multimodal Large Language Models (MLLMs) often perform poorly in long video understanding, primarily due to resource limitations that prevent them from processing all video frames and their associated information. Efficiently extracting relevant information becomes a challenging task. Existing frameworks and evaluation tasks focus on identifying specific frames containing core objects from&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.03009v1-abstract-full').style.display = 'inline'; document.getElementById('2508.03009v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.03009v1-abstract-full" style="display: none;">
        Current Multimodal Large Language Models (MLLMs) often perform poorly in long video understanding, primarily due to resource limitations that prevent them from processing all video frames and their associated information. Efficiently extracting relevant information becomes a challenging task. Existing frameworks and evaluation tasks focus on identifying specific frames containing core objects from a large number of irrelevant frames, which does not align with the practical needs of real-world applications. To address this issue, we propose a new scenario under the video question-answering task, SceneQA, which emphasizes scene-based detail perception and reasoning abilities. And we develop the LVSQA dataset to support the SceneQA task, which is built upon carefully selected videos from LVBench and contains a new collection of question-answer pairs to promote a more fair evaluation of MLLMs&#39; scene perception abilities in long videos. Inspired by human cognition, we introduce a novel method called SLFG. The core idea of SLFG is to combine individual frames into semantically coherent scene frames. By leveraging scene localization methods and dynamic frame reassembly mechanisms, SLFG significantly enhances the understanding capabilities of existing MLLMs in long videos. SLFG requires no modification to the original model architecture and boasts excellent plug-and-play usability. Experimental results show that this method performs exceptionally well in several long video benchmark tests. Code and dataset will be released at http://www.slfg.pkuzwh.cn.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.03009v1-abstract-full').style.display = 'none'; document.getElementById('2508.03009v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.02460">arXiv:2508.02460</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.02460">pdf</a>, <a href="https://arxiv.org/ps/2508.02460">ps</a>, <a href="https://arxiv.org/format/2508.02460">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        InfoSyncNet: Information Synchronization Temporal Convolutional Network for Visual Speech Recognition
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xue%2C+J">Junxiao Xue</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+X">Xiaozhen Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+X">Xuecheng Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+F">Fei Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jun Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.02460v1-abstract-short" style="display: inline;">
        Estimating spoken content from silent videos is crucial for applications in Assistive Technology (AT) and Augmented Reality (AR). However, accurately mapping lip movement sequences in videos to words poses significant challenges due to variability across sequences and the uneven distribution of information within each sequence. To tackle this, we introduce InfoSyncNet, a non-uniform sequence model&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.02460v1-abstract-full').style.display = 'inline'; document.getElementById('2508.02460v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.02460v1-abstract-full" style="display: none;">
        Estimating spoken content from silent videos is crucial for applications in Assistive Technology (AT) and Augmented Reality (AR). However, accurately mapping lip movement sequences in videos to words poses significant challenges due to variability across sequences and the uneven distribution of information within each sequence. To tackle this, we introduce InfoSyncNet, a non-uniform sequence modeling network enhanced by tailored data augmentation techniques. Central to InfoSyncNet is a non-uniform quantization module positioned between the encoder and decoder, enabling dynamic adjustment to the network&#39;s focus and effectively handling the natural inconsistencies in visual speech data. Additionally, multiple training strategies are incorporated to enhance the model&#39;s capability to handle variations in lighting and the speaker&#39;s orientation. Comprehensive experiments on the LRW and LRW1000 datasets confirm the superiority of InfoSyncNet, achieving new state-of-the-art accuracies of 92.0% and 60.7% Top-1 ACC. The code is available for download (see comments).
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.02460v1-abstract-full').style.display = 'none'; document.getElementById('2508.02460v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">https://github.com/liuxiaozhen123/InfoSyncNet</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.02242">arXiv:2508.02242</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.02242">pdf</a>, <a href="https://arxiv.org/ps/2508.02242">ps</a>, <a href="https://arxiv.org/format/2508.02242">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        From Generation to Consumption: Personalized List Value Estimation for Re-ranking
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+K">Kaike Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xiaobei Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+X">Xiaoyu Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+S">Shuchang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+H">Hailan Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xiang Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+F">Fei Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+Q">Qi Cao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.02242v2-abstract-short" style="display: inline;">
        Re-ranking is critical in recommender systems for optimizing the order of recommendation lists, thus improving user satisfaction and platform revenue. Most existing methods follow a generator-evaluator paradigm, where the evaluator estimates the overall value of each candidate list. However, they often ignore the fact that users may exit before consuming the full list, leading to a mismatch betwee&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.02242v2-abstract-full').style.display = 'inline'; document.getElementById('2508.02242v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.02242v2-abstract-full" style="display: none;">
        Re-ranking is critical in recommender systems for optimizing the order of recommendation lists, thus improving user satisfaction and platform revenue. Most existing methods follow a generator-evaluator paradigm, where the evaluator estimates the overall value of each candidate list. However, they often ignore the fact that users may exit before consuming the full list, leading to a mismatch between estimated generation value and actual consumption value. To bridge this gap, we propose CAVE, a personalized Consumption-Aware list Value Estimation framework. CAVE formulates the list value as the expectation over sub-list values, weighted by user-specific exit probabilities at each position. The exit probability is decomposed into an interest-driven component and a stochastic component, the latter modeled via a Weibull distribution to capture random external factors such as fatigue. By jointly modeling sub-list values and user exit behavior, CAVE yields a more faithful estimate of actual list consumption value. We further contribute three large-scale real-world list-wise benchmarks from the Kuaishou platform, varying in size and user activity patterns. Extensive experiments on these benchmarks, two Amazon datasets, and online A/B testing on Kuaishou show that CAVE consistently outperforms strong baselines, highlighting the benefit of explicitly modeling user exits in re-ranking.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.02242v2-abstract-full').style.display = 'none'; document.getElementById('2508.02242v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 7 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 4 August, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.02222">arXiv:2508.02222</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.02222">pdf</a>, <a href="https://arxiv.org/ps/2508.02222">ps</a>, <a href="https://arxiv.org/format/2508.02222">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computational Engineering, Finance, and Science">cs.CE</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        FinCPRG: A Bidirectional Generation Pipeline for Hierarchical Queries and Rich Relevance in Financial Chinese Passage Retrieval
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+X">Xuan Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chu%2C+B">Beilin Chu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lin%2C+Q">Qinhong Lin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhong%2C+Y">Yixiao Zhong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wen%2C+F">Fufang Wen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Jiaqi Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fei%2C+B">Binjie Fei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yu Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+Z">Zhongliang Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+L">Linna Zhou</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.02222v1-abstract-short" style="display: inline;">
        In recent years, large language models (LLMs) have demonstrated significant potential in constructing passage retrieval datasets. However, existing methods still face limitations in expressing cross-doc query needs and controlling annotation quality. To address these issues, this paper proposes a bidirectional generation pipeline, which aims to generate 3-level hierarchical queries for both intra-&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.02222v1-abstract-full').style.display = 'inline'; document.getElementById('2508.02222v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.02222v1-abstract-full" style="display: none;">
        In recent years, large language models (LLMs) have demonstrated significant potential in constructing passage retrieval datasets. However, existing methods still face limitations in expressing cross-doc query needs and controlling annotation quality. To address these issues, this paper proposes a bidirectional generation pipeline, which aims to generate 3-level hierarchical queries for both intra-doc and cross-doc scenarios and mine additional relevance labels on top of direct mapping annotation. The pipeline introduces two query generation methods: bottom-up from single-doc text and top-down from multi-doc titles. The bottom-up method uses LLMs to disassemble and generate structured queries at both sentence-level and passage-level simultaneously from intra-doc passages. The top-down approach incorporates three key financial elements--industry, topic, and time--to divide report titles into clusters and prompts LLMs to generate topic-level queries from each cluster. For relevance annotation, our pipeline not only relies on direct mapping annotation from the generation relationship but also implements an indirect positives mining method to enrich the relevant query-passage pairs. Using this pipeline, we constructed a Financial Passage Retrieval Generated dataset (FinCPRG) from almost 1.3k Chinese financial research reports, which includes hierarchical queries and rich relevance labels. Through evaluations of mined relevance labels, benchmarking and training experiments, we assessed the quality of FinCPRG and validated its effectiveness as a passage retrieval dataset for both training and benchmarking.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.02222v1-abstract-full').style.display = 'none'; document.getElementById('2508.02222v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.02121">arXiv:2508.02121</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.02121">pdf</a>, <a href="https://arxiv.org/ps/2508.02121">ps</a>, <a href="https://arxiv.org/format/2508.02121">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Multiagent Systems">cs.MA</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Survey on AgentOps: Categorization, Challenges, and Future Directions
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+Z">Zexin Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Jingjing Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+Q">Quan Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Si%2C+H">Haotian Si</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yuanhao Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Jianhui Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xie%2C+G">Gaogang Xie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+F">Fei Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pei%2C+D">Dan Pei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pei%2C+C">Changhua Pei</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.02121v1-abstract-short" style="display: inline;">
        As the reasoning capabilities of Large Language Models (LLMs) continue to advance, LLM-based agent systems offer advantages in flexibility and interpretability over traditional systems, garnering increasing attention. However, despite the widespread research interest and industrial application of agent systems, these systems, like their traditional counterparts, frequently encounter anomalies. The&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.02121v1-abstract-full').style.display = 'inline'; document.getElementById('2508.02121v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.02121v1-abstract-full" style="display: none;">
        As the reasoning capabilities of Large Language Models (LLMs) continue to advance, LLM-based agent systems offer advantages in flexibility and interpretability over traditional systems, garnering increasing attention. However, despite the widespread research interest and industrial application of agent systems, these systems, like their traditional counterparts, frequently encounter anomalies. These anomalies lead to instability and insecurity, hindering their further development. Therefore, a comprehensive and systematic approach to the operation and maintenance of agent systems is urgently needed. Unfortunately, current research on the operations of agent systems is sparse. To address this gap, we have undertaken a survey on agent system operations with the aim of establishing a clear framework for the field, defining the challenges, and facilitating further development. Specifically, this paper begins by systematically defining anomalies within agent systems, categorizing them into intra-agent anomalies and inter-agent anomalies. Next, we introduce a novel and comprehensive operational framework for agent systems, dubbed Agent System Operations (AgentOps). We provide detailed definitions and explanations of its four key stages: monitoring, anomaly detection, root cause analysis, and resolution.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.02121v1-abstract-full').style.display = 'none'; document.getElementById('2508.02121v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 4 August, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">35 pages</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.00222">arXiv:2508.00222</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.00222">pdf</a>, <a href="https://arxiv.org/ps/2508.00222">ps</a>, <a href="https://arxiv.org/format/2508.00222">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        RL-PLUS: Countering Capability Boundary Collapse of LLMs in Reinforcement Learning with Hybrid-policy Optimization
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Dong%2C+Y">Yihong Dong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+X">Xue Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tao%2C+Y">Yongding Tao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+H">Huanyu Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+K">Kechi Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mou%2C+L">Lili Mou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cao%2C+R">Rongyu Cao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+Y">Yingwei Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+J">Jue Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+B">Binhua Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jin%2C+Z">Zhi Jin</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Huang%2C+F">Fei Huang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yongbin Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+G">Ge Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.00222v3-abstract-short" style="display: inline;">
        Reinforcement Learning with Verifiable Reward (RLVR) has significantly advanced the complex reasoning abilities of Large Language Models (LLMs). However, it struggles to break through the inherent capability boundaries of the base LLM, due to its essentially on-policy strategy coupled with LLM&#39;s immense action space and sparse reward. Critically, RLVR can lead to the capability boundary collapse,&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.00222v3-abstract-full').style.display = 'inline'; document.getElementById('2508.00222v3-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.00222v3-abstract-full" style="display: none;">
        Reinforcement Learning with Verifiable Reward (RLVR) has significantly advanced the complex reasoning abilities of Large Language Models (LLMs). However, it struggles to break through the inherent capability boundaries of the base LLM, due to its essentially on-policy strategy coupled with LLM&#39;s immense action space and sparse reward. Critically, RLVR can lead to the capability boundary collapse, narrowing the LLM&#39;s problem-solving scope. To address this problem, we propose RL-PLUS, a novel hybrid-policy optimization approach for LLMs that synergizes internal exploitation with external data to achieve stronger reasoning capabilities and surpass the boundaries of base models. RL-PLUS integrates two core components, i.e., Multiple Importance Sampling to address distributional mismatch from external data, and Exploration-Based Advantage Function to guide the model towards high-value, unexplored reasoning paths. We provide both theoretical analysis and extensive experiments to demonstrate the superiority and generalizability of our approach. Compared with existing RLVR methods, RL-PLUS achieves 1) state-of-the-art performance on six math reasoning benchmarks; 2) superior performance on six out-of-distribution reasoning tasks; 3) consistent and significant gains across diverse model families, with average relative improvements up to 69.2\%. Moreover, the analysis of Pass@k curves indicates that RL-PLUS effectively resolves the capability boundary collapse problem.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.00222v3-abstract-full').style.display = 'none'; document.getElementById('2508.00222v3-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 6 August, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 31 July, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2508.00057">arXiv:2508.00057</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2508.00057">pdf</a>, <a href="https://arxiv.org/ps/2508.00057">ps</a>, <a href="https://arxiv.org/format/2508.00057">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Astrophysics of Galaxies">astro-ph.GA</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Discovery of a Little Red Dot candidate at $z\gtrsim10$ in COSMOS-Web based on MIRI-NIRCam selection
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Tanaka%2C+T+S">Takumi S. Tanaka</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Akins%2C+H+B">Hollis B. Akins</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Harikane%2C+Y">Yuichi Harikane</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Silverman%2C+J+D">John D. Silverman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Casey%2C+C+M">Caitlin M. Casey</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Inayoshi%2C+K">Kohei Inayoshi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Schindler%2C+J">Jan-Torge Schindler</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shimasaku%2C+K">Kazuhiro Shimasaku</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kocevski%2C+D+D">Dale D. Kocevski</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Onoue%2C+M">Masafusa Onoue</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Faisst%2C+A+L">Andreas L. Faisst</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Robertson%2C+B">Brant Robertson</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kokorev%2C+V">Vasily Kokorev</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shuntov%2C+M">Marko Shuntov</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Koekemoer%2C+A+M">Anton M. Koekemoer</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Franco%2C+M">Maximilien Franco</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+D">Daizhong Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Taylor%2C+A+J">Anthony J. Taylor</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kartaltepe%2C+J+S">Jeyhan S. Kartaltepe</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Bosman%2C+S+E">Sarah E. Bosman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Champagne%2C+J+B">Jaclyn B. Champagne</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kakiichi%2C+K">Koki Kakiichi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Z">Zijian Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Newman%2C+S+L">Sophie L. Newman</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Kakkad%2C+D">Darshan Kakkad</a>
      , et al. (38 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2508.00057v1-abstract-short" style="display: inline;">
        JWST has revealed a new high-redshift population called little red dots (LRDs). Since LRDs may be in the early phase of black hole growth, identifying them in the early universe is crucial for understanding the formation of the first supermassive black holes. However, no robust LRD candidates have been identified at $z&gt;10$, because commonly-used NIRCam photometry covers wavelengths up to&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.00057v1-abstract-full').style.display = 'inline'; document.getElementById('2508.00057v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2508.00057v1-abstract-full" style="display: none;">
        JWST has revealed a new high-redshift population called little red dots (LRDs). Since LRDs may be in the early phase of black hole growth, identifying them in the early universe is crucial for understanding the formation of the first supermassive black holes. However, no robust LRD candidates have been identified at $z&gt;10$, because commonly-used NIRCam photometry covers wavelengths up to $\sim5\,{\rm μm}$ and is insufficient to capture the characteristic V-shaped spectral energy distributions (SEDs) of LRDs. In this study, we present the first search for $z\gtrsim10$ LRD candidates using both NIRCam and MIRI imaging from COSMOS-Web, which provides the largest joint NIRCam-MIRI coverage to date ($0.20\,{\rm deg^2}$). Taking advantage of MIRI/F770W to remove contaminants, we identify one robust candidate, CW-LRD-z10 at $z_{\rm phot}=10.5^{+0.7}_{-0.6}$ with $M_{\rm UV}=-19.9^{+0.1}_{-0.2}\,{\rm mag}$. CW-LRD-z10 exhibits a compact morphology, a distinct V-shaped SED, and a non-detection in F115W, all consistent with being an LRD at $z\sim10$. Based on this discovery, we place the first constraint on the number density of LRDs at $z\sim10$ with $M_{\rm UV}\sim-20$ of $1.2^{+2.7}_{-1.0}\times10^{-6}\,{\rm Mpc^{-3}\,mag^{-1}}$, suggesting that the fraction of LRDs among the overall galaxy population increases with redshift, reaching $\sim3\%$ at $z\sim10$. Although deep spectroscopy is necessary to confirm the redshift and the nature of CW-LRD-z10, our results imply that LRDs may be a common population at $z&gt;10$, playing a key role in the first supermassive black hole formation.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2508.00057v1-abstract-full').style.display = 'none'; document.getElementById('2508.00057v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 July, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> August 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">32 pages, 12 figures, and 5 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.22773">arXiv:2507.22773</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.22773">pdf</a>, <a href="https://arxiv.org/ps/2507.22773">ps</a>, <a href="https://arxiv.org/format/2507.22773">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Quantum Physics">quant-ph</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        General quantum computation on photons assisted with double single-sided cavity system
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Jiu-Ming Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+J">Jun-Yan Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Y">Yuan-Yuan Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiu%2C+X">Xiao-Ming Xiu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fei%2C+S">Shao-Ming Fei</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.22773v1-abstract-short" style="display: inline;">
        We propose a physical system consisting of two optical cavities and a two-level system (TLS), which can be viewed as a double single-sided cavity system. The two cavities are crossed each other in a mutually perpendicular way and are both single-sided. The TLS is coupled to the two cavities. The universal input-output relation of the system, the reflection and transmission coefficients are derived&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.22773v1-abstract-full').style.display = 'inline'; document.getElementById('2507.22773v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.22773v1-abstract-full" style="display: none;">
        We propose a physical system consisting of two optical cavities and a two-level system (TLS), which can be viewed as a double single-sided cavity system. The two cavities are crossed each other in a mutually perpendicular way and are both single-sided. The TLS is coupled to the two cavities. The universal input-output relation of the system, the reflection and transmission coefficients are derived by exploiting the probability amplitude method. Then by using the nitrogen-vacancy center instead of the TLS, we generate the controlled-phase gate and the controlled-controlled-phase gate on the photon qubits, with simple protocols that can be accomplished in both weak and strong coupling regimes. The protocols are shown to give rise to high fidelities and gate efficiencies.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.22773v1-abstract-full').style.display = 'none'; document.getElementById('2507.22773v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 July, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.22473">arXiv:2507.22473</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.22473">pdf</a>, <a href="https://arxiv.org/ps/2507.22473">ps</a>, <a href="https://arxiv.org/format/2507.22473">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Robotics">cs.RO</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        A Two-Stage Lightweight Framework for Efficient Land-Air Bimodal Robot Autonomous Navigation
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yongjie Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Z">Zhou Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+W">Wenshuai Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+Z">Zhangji Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+C">Chenyang Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+F">Fei Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Q">Qingquan Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.22473v1-abstract-short" style="display: inline;">
        Land-air bimodal robots (LABR) are gaining attention for autonomous navigation, combining high mobility from aerial vehicles with long endurance from ground vehicles. However, existing LABR navigation methods are limited by suboptimal trajectories from mapping-based approaches and the excessive computational demands of learning-based methods. To address this, we propose a two-stage lightweight fra&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.22473v1-abstract-full').style.display = 'inline'; document.getElementById('2507.22473v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.22473v1-abstract-full" style="display: none;">
        Land-air bimodal robots (LABR) are gaining attention for autonomous navigation, combining high mobility from aerial vehicles with long endurance from ground vehicles. However, existing LABR navigation methods are limited by suboptimal trajectories from mapping-based approaches and the excessive computational demands of learning-based methods. To address this, we propose a two-stage lightweight framework that integrates global key points prediction with local trajectory refinement to generate efficient and reachable trajectories. In the first stage, the Global Key points Prediction Network (GKPN) was used to generate a hybrid land-air keypoint path. The GKPN includes a Sobel Perception Network (SPN) for improved obstacle detection and a Lightweight Attention Planning Network (LAPN) to improves predictive ability by capturing contextual information. In the second stage, the global path is segmented based on predicted key points and refined using a mapping-based planner to create smooth, collision-free trajectories. Experiments conducted on our LABR platform show that our framework reduces network parameters by 14\% and energy consumption during land-air transitions by 35\% compared to existing approaches. The framework achieves real-time navigation without GPU acceleration and enables zero-shot transfer from simulation to reality during
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.22473v1-abstract-full').style.display = 'none'; document.getElementById('2507.22473v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 30 July, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">IROS2025</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.21297">arXiv:2507.21297</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.21297">pdf</a>, <a href="https://arxiv.org/ps/2507.21297">ps</a>, <a href="https://arxiv.org/format/2507.21297">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Materials Science">cond-mat.mtrl-sci</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Heterogeneous Ensemble Enables a Universal Uncertainty Metric for Atomistic Foundation Models
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+K">Kai Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wei%2C+Z">Zixiong Wei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gao%2C+W">Wei Gao</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Dey%2C+P">Poulumi Dey</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sluiter%2C+M+H+F">Marcel H. F. Sluiter</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shuang%2C+F">Fei Shuang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.21297v1-abstract-short" style="display: inline;">
        Universal machine learning interatomic potentials (uMLIPs) are reshaping atomistic simulation as foundation models, delivering near \textit{ab initio} accuracy at a fraction of the cost. Yet the lack of reliable, general uncertainty quantification limits their safe, wide-scale use. Here we introduce a unified, scalable uncertainty metric \(U\) based on a heterogeneous model ensemble with reuse of&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.21297v1-abstract-full').style.display = 'inline'; document.getElementById('2507.21297v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.21297v1-abstract-full" style="display: none;">
        Universal machine learning interatomic potentials (uMLIPs) are reshaping atomistic simulation as foundation models, delivering near \textit{ab initio} accuracy at a fraction of the cost. Yet the lack of reliable, general uncertainty quantification limits their safe, wide-scale use. Here we introduce a unified, scalable uncertainty metric \(U\) based on a heterogeneous model ensemble with reuse of pretrained uMLIPs. Across chemically and structurally diverse datasets, \(U\) shows a strong correlation with the true prediction errors and provides a robust ranking of configuration-level risk. Leveraging this metric, we propose an uncertainty-aware model distillation framework to produce system-specific potentials: for W, an accuracy comparable to full-DFT training is achieved using only \(4\%\) of the DFT labels; for MoNbTaW, no additional DFT calculations are required. Notably, by filtering numerical label noise, the distilled models can, in some cases, surpass the accuracy of the DFT reference labels. The uncertainty-aware approach offers a practical monitor of uMLIP reliability in deployment, and guides data selection and fine-tuning strategies, thereby advancing the construction and safe use of foundation models and enabling cost-efficient development of accurate, system-specific potentials.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.21297v1-abstract-full').style.display = 'none'; document.getElementById('2507.21297v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 28 July, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.20335">arXiv:2507.20335</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.20335">pdf</a>, <a href="https://arxiv.org/ps/2507.20335">ps</a>, <a href="https://arxiv.org/format/2507.20335">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Cultivating Helpful, Personalized, and Creative AI Tutors: A Framework for Pedagogical Alignment using Reinforcement Learning
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Song%2C+S">Siyu Song</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+W">Wentao Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+Y">Ye Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+R">Ruohua Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+T">Tao Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lv%2C+J">Jinze Lv</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+X">Xinyun Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+A">Aimin Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tan%2C+F">Fei Tan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+B">Bo Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Hao%2C+H">Hao Hao</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.20335v1-abstract-short" style="display: inline;">
        The integration of large language models (LLMs) into education presents unprecedented opportunities for scalable personalized learning. However, standard LLMs often function as generic information providers, lacking alignment with fundamental pedagogical principles such as helpfulness, student-centered personalization, and creativity cultivation. To bridge this gap, we propose EduAlign, a novel fr&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.20335v1-abstract-full').style.display = 'inline'; document.getElementById('2507.20335v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.20335v1-abstract-full" style="display: none;">
        The integration of large language models (LLMs) into education presents unprecedented opportunities for scalable personalized learning. However, standard LLMs often function as generic information providers, lacking alignment with fundamental pedagogical principles such as helpfulness, student-centered personalization, and creativity cultivation. To bridge this gap, we propose EduAlign, a novel framework designed to guide LLMs toward becoming more effective and responsible educational assistants. EduAlign consists of two main stages. In the first stage, we curate a dataset of 8k educational interactions and annotate them-both manually and automatically-along three key educational dimensions: Helpfulness, Personalization, and Creativity (HPC). These annotations are used to train HPC-RM, a multi-dimensional reward model capable of accurately scoring LLM outputs according to these educational principles. We further evaluate the consistency and reliability of this reward model. In the second stage, we leverage HPC-RM as a reward signal to fine-tune a pre-trained LLM using Group Relative Policy Optimization (GRPO) on a set of 2k diverse prompts. We then assess the pre- and post-finetuning models on both educational and general-domain benchmarks across the three HPC dimensions. Experimental results demonstrate that the fine-tuned model exhibits significantly improved alignment with pedagogical helpfulness, personalization, and creativity stimulation. This study presents a scalable and effective approach to aligning LLMs with nuanced and desirable educational traits, paving the way for the development of more engaging, pedagogically aligned AI tutors.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.20335v1-abstract-full').style.display = 'none'; document.getElementById('2507.20335v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 27 July, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.19672">arXiv:2507.19672</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.19672">pdf</a>, <a href="https://arxiv.org/ps/2507.19672">ps</a>, <a href="https://arxiv.org/format/2507.19672">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Machine Learning">stat.ML</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Alignment and Safety in Large Language Models: Safety Mechanisms, Training Paradigms, and Emerging Challenges
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Lu%2C+H">Haoran Lu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fang%2C+L">Luyang Fang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+R">Ruidong Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+X">Xinliang Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cai%2C+J">Jiazhang Cai</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Cheng%2C+H">Huimin Cheng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Tang%2C+L">Lin Tang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Z">Ziyu Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+Z">Zeliang Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+T">Tao Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+Y">Yingchuan Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zidan%2C+A+H">Arif Hassan Zidan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xu%2C+J">Jinwen Xu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+J">Jincheng Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yu%2C+M">Meizhi Yu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Jiang%2C+H">Hanqi Jiang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Gong%2C+X">Xilin Gong</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Luo%2C+W">Weidi Luo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Sun%2C+B">Bolun Sun</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+Y">Yongkai Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+T">Terry Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wu%2C+S">Shushan Wu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+Y">Yifan Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Chen%2C+J">Junhao Chen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Xiang%2C+H">Haotian Xiang</a>
      , et al. (25 additional authors not shown)
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.19672v1-abstract-short" style="display: inline;">
        Due to the remarkable capabilities and growing impact of large language models (LLMs), they have been deeply integrated into many aspects of society. Thus, ensuring their alignment with human values and intentions has emerged as a critical challenge. This survey provides a comprehensive overview of practical alignment techniques, training protocols, and empirical findings in LLM alignment. We anal&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.19672v1-abstract-full').style.display = 'inline'; document.getElementById('2507.19672v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.19672v1-abstract-full" style="display: none;">
        Due to the remarkable capabilities and growing impact of large language models (LLMs), they have been deeply integrated into many aspects of society. Thus, ensuring their alignment with human values and intentions has emerged as a critical challenge. This survey provides a comprehensive overview of practical alignment techniques, training protocols, and empirical findings in LLM alignment. We analyze the development of alignment methods across diverse paradigms, characterizing the fundamental trade-offs between core alignment objectives. Our analysis shows that while supervised fine-tuning enables basic instruction-following, preference-based methods offer more flexibility for aligning with nuanced human intent. We discuss state-of-the-art techniques, including Direct Preference Optimization (DPO), Constitutional AI, brain-inspired methods, and alignment uncertainty quantification (AUQ), highlighting their approaches to balancing quality and efficiency. We review existing evaluation frameworks and benchmarking datasets, emphasizing limitations such as reward misspecification, distributional robustness, and scalable oversight. We summarize strategies adopted by leading AI labs to illustrate the current state of practice. We conclude by outlining open problems in oversight, value pluralism, robustness, and continuous alignment. This survey aims to inform both researchers and practitioners navigating the evolving landscape of LLM alignment.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.19672v1-abstract-full').style.display = 'none'; document.getElementById('2507.19672v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 July, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">119 pages, 10 figures, 7 tables</span>
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.19017">arXiv:2507.19017</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.19017">pdf</a>, <a href="https://arxiv.org/ps/2507.19017">ps</a>, <a href="https://arxiv.org/format/2507.19017">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Machine Learning">cs.LG</span>
        
          
            <span class="tag is-small is-grey tooltip is-tooltip-top" data-tooltip="Artificial Intelligence">cs.AI</span>
          
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        MindSpeed RL: Distributed Dataflow for Scalable and Efficient RL Training on Ascend NPU Cluster
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Feng%2C+L">Laingjun Feng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Pan%2C+C">Chenyi Pan</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Guo%2C+X">Xinjie Guo</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Mei%2C+F">Fei Mei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ning%2C+B">Benzhe Ning</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+J">Jianxiang Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+X">Xinyang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhou%2C+B">Beirong Zhou</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shu%2C+Z">Zeng Shu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+C">Chang Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+G">Guang Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Han%2C+Z">Zhenyu Han</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jiangben Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+B">Bo Wang</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.19017v1-abstract-short" style="display: inline;">
        Reinforcement learning (RL) is a paradigm increasingly used to align large language models. Popular RL algorithms utilize multiple workers and can be modeled as a graph, where each node is the status of a worker and each edge represents dataflow between nodes. Owing to the heavy cross-node dependencies, the RL training system usually suffers from poor cluster scalability and low memory utilization&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.19017v1-abstract-full').style.display = 'inline'; document.getElementById('2507.19017v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.19017v1-abstract-full" style="display: none;">
        Reinforcement learning (RL) is a paradigm increasingly used to align large language models. Popular RL algorithms utilize multiple workers and can be modeled as a graph, where each node is the status of a worker and each edge represents dataflow between nodes. Owing to the heavy cross-node dependencies, the RL training system usually suffers from poor cluster scalability and low memory utilization. In this article, we introduce MindSpeed RL, an effective and efficient system for large-scale RL training. Unlike existing centralized methods, MindSpeed RL organizes the essential data dependencies in RL training, i.e., sample flow and resharding flow, from a distributed view. On the one hand, a distributed transfer dock strategy, which sets controllers and warehouses on the basis of the conventional replay buffer, is designed to release the dispatch overhead in the sample flow. A practical allgather--swap strategy is presented to eliminate redundant memory usage in resharding flow. In addition, MindSpeed RL further integrates numerous parallelization strategies and acceleration techniques for systematic optimization. Compared with existing state-of-the-art systems, comprehensive experiments on the RL training of popular Qwen2.5-Dense-7B/32B, Qwen3-MoE-30B, and DeepSeek-R1-MoE-671B show that MindSpeed RL increases the throughput by 1.42 ~ 3.97 times. Finally, we open--source MindSpeed RL and perform all the experiments on a super pod of Ascend with 384 neural processing units (NPUs) to demonstrate the powerful performance and reliability of Ascend.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.19017v1-abstract-full').style.display = 'none'; document.getElementById('2507.19017v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 25 July, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">9 pages</span>
    </p>
    

    
      <p class="comments is-size-7">
        

        
          <span class="has-text-black-bis has-text-weight-semibold">MSC Class:</span>
          CS
        

        
      </p>
    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.18518">arXiv:2507.18518</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.18518">pdf</a>, <a href="https://arxiv.org/ps/2507.18518">ps</a>, <a href="https://arxiv.org/format/2507.18518">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Information Retrieval">cs.IR</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Transform Before You Query: A Privacy-Preserving Approach for Vector Retrieval with Embedding Space Alignment
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=He%2C+R">Ruiqi He</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fei%2C+Z">Zekun Fei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+J">Jiaqi Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Zhu%2C+X">Xinyuan Zhu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yi%2C+B">Biao Yi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Lv%2C+S">Siyi Lv</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+W">Weijie Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Z">Zheli Liu</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.18518v2-abstract-short" style="display: inline;">
        Vector Database (VDB) can efficiently index and search high-dimensional vector embeddings from unstructured data, crucially enabling fast semantic similarity search essential for modern AI applications like generative AI and recommendation systems. Since current VDB service providers predominantly use proprietary black-box models, users are forced to expose raw query text to them via API in exchan&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.18518v2-abstract-full').style.display = 'inline'; document.getElementById('2507.18518v2-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.18518v2-abstract-full" style="display: none;">
        Vector Database (VDB) can efficiently index and search high-dimensional vector embeddings from unstructured data, crucially enabling fast semantic similarity search essential for modern AI applications like generative AI and recommendation systems. Since current VDB service providers predominantly use proprietary black-box models, users are forced to expose raw query text to them via API in exchange for the vector retrieval services. Consequently, if query text involves confidential records from finance or healthcare domains, this mechanism inevitably leads to critical leakage of user&#39;s sensitive information. To address this issue, we introduce STEER (\textbf{S}ecure \textbf{T}ransformed \textbf{E}mbedding v\textbf{E}ctor\textbf{ R}etrieval), a private vector retrieval framework that leverages the alignment relationship between the semantic spaces of different embedding models to derive approximate embeddings for the query text. STEER performs the retrieval using the approximate embeddings within the original VDB and requires no modifications to the server side. Our theoretical and experimental analyses demonstrate that STEER effectively safeguards query text privacy while maintaining the retrieval accuracy. Even though approximate embeddings are approximations of the embeddings from proprietary models, they still prevent the providers from recovering the query text through Embedding Inversion Attacks (EIAs). Extensive experimental results show that Recall@100 of STEER can basically achieve a decrease of less than 5\%. Furthermore, even when searching within a text corpus of millions of entries, STEER achieves a Recall@20 accuracy 20\% higher than current baselines.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.18518v2-abstract-full').style.display = 'none'; document.getElementById('2507.18518v2-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 31 July, 2025; <span class="has-text-black-bis has-text-weight-semibold">v1</span> submitted 24 July, 2025;
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.18305">arXiv:2507.18305</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.18305">pdf</a>, <a href="https://arxiv.org/ps/2507.18305">ps</a>, <a href="https://arxiv.org/format/2507.18305">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computation and Language">cs.CL</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        BadReasoner: Planting Tunable Overthinking Backdoors into Large Reasoning Models for Fun or Profit
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Yi%2C+B">Biao Yi</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fei%2C+Z">Zekun Fei</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Geng%2C+J">Jianing Geng</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+T">Tong Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Nie%2C+L">Lihai Nie</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+Z">Zheli Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+Y">Yiming Li</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.18305v1-abstract-short" style="display: inline;">
        Large reasoning models (LRMs) have emerged as a significant advancement in artificial intelligence, representing a specialized class of large language models (LLMs) designed to tackle complex reasoning tasks. The defining characteristic of LRMs lies in their extensive chain-of-thought (CoT) reasoning capabilities. In this paper, we identify a previously unexplored attack vector against LRMs, which&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.18305v1-abstract-full').style.display = 'inline'; document.getElementById('2507.18305v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.18305v1-abstract-full" style="display: none;">
        Large reasoning models (LRMs) have emerged as a significant advancement in artificial intelligence, representing a specialized class of large language models (LLMs) designed to tackle complex reasoning tasks. The defining characteristic of LRMs lies in their extensive chain-of-thought (CoT) reasoning capabilities. In this paper, we identify a previously unexplored attack vector against LRMs, which we term &#34;overthinking backdoors&#34;. We advance this concept by proposing a novel tunable backdoor, which moves beyond simple on/off attacks to one where an attacker can precisely control the extent of the model&#39;s reasoning verbosity. Our attack is implemented through a novel data poisoning methodology. It pairs a tunable trigger-where the number of repetitions signals the desired intensity-with a correspondingly verbose CoT response. These responses are programmatically generated by instructing a teacher LLM to inject a controlled number of redundant refinement steps into a correct reasoning process. The approach preserves output correctness, which ensures stealth and establishes the attack as a pure resource-consumption vector. Extensive empirical results on various LRMs demonstrate that our method can reliably trigger a controllable, multi-fold increase in the length of the reasoning process, without degrading the final answer&#39;s correctness. Our source code is available at https://github.com/FZaKK/BadReasoner.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.18305v1-abstract-full').style.display = 'none'; document.getElementById('2507.18305v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 24 July, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.17533">arXiv:2507.17533</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.17533">pdf</a>, <a href="https://arxiv.org/ps/2507.17533">ps</a>, <a href="https://arxiv.org/format/2507.17533">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Computer Vision and Pattern Recognition">cs.CV</span>
        </div>
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Multi-modal Multi-task Pre-training for Improved Point Cloud Understanding
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+L">Liwen Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Yang%2C+W">Weidong Yang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Ma%2C+L">Lipeng Ma</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fei%2C+B">Ben Fei</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.17533v1-abstract-short" style="display: inline;">
        Recent advances in multi-modal pre-training methods have shown promising effectiveness in learning 3D representations by aligning multi-modal features between 3D shapes and their corresponding 2D counterparts. However, existing multi-modal pre-training frameworks primarily rely on a single pre-training task to gather multi-modal data in 3D applications. This limitation prevents the models from obt&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.17533v1-abstract-full').style.display = 'inline'; document.getElementById('2507.17533v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.17533v1-abstract-full" style="display: none;">
        Recent advances in multi-modal pre-training methods have shown promising effectiveness in learning 3D representations by aligning multi-modal features between 3D shapes and their corresponding 2D counterparts. However, existing multi-modal pre-training frameworks primarily rely on a single pre-training task to gather multi-modal data in 3D applications. This limitation prevents the models from obtaining the abundant information provided by other relevant tasks, which can hinder their performance in downstream tasks, particularly in complex and diverse domains. In order to tackle this issue, we propose MMPT, a Multi-modal Multi-task Pre-training framework designed to enhance point cloud understanding. Specifically, three pre-training tasks are devised: (i) Token-level reconstruction (TLR) aims to recover masked point tokens, endowing the model with representative learning abilities. (ii) Point-level reconstruction (PLR) is integrated to predict the masked point positions directly, and the reconstructed point cloud can be considered as a transformed point cloud used in the subsequent task. (iii) Multi-modal contrastive learning (MCL) combines feature correspondences within and across modalities, thus assembling a rich learning signal from both 3D point cloud and 2D image modalities in a self-supervised manner. Moreover, this framework operates without requiring any 3D annotations, making it scalable for use with large datasets. The trained encoder can be effectively transferred to various downstream tasks. To demonstrate its effectiveness, we evaluated its performance compared to state-of-the-art methods in various discriminant and generative applications under widely-used benchmarks.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.17533v1-abstract-full').style.display = 'none'; document.getElementById('2507.17533v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 23 July, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    

    

    
  </li>

  <li class="arxiv-result">
    <div class="is-marginless">
      <p class="list-title is-inline-block"><a href="https://arxiv.org/abs/2507.17117">arXiv:2507.17117</a>
        <span>&nbsp;[<a href="https://arxiv.org/pdf/2507.17117">pdf</a>, <a href="https://arxiv.org/ps/2507.17117">ps</a>, <a href="https://arxiv.org/format/2507.17117">other</a>]&nbsp;</span>
      </p>
      <div class="tags is-inline-block">
        <span class="tag is-small is-link tooltip is-tooltip-top" data-tooltip="Quantum Physics">quant-ph</span>
        </div>
      
        
          <div class="is-inline-block" style="margin-left: 0.5rem">
            <div class="tags has-addons">
              <span class="tag is-dark is-size-7">doi</span>
              <span class="tag is-light is-size-7"><a class="" href="https://doi.org/10.1002/qute.202500376">10.1002/qute.202500376 <i class="fa fa-external-link" aria-hidden="true"></i></a></span>
            </div>
          </div>
        
      
    </div>
    
    <p class="title is-5 mathjax">
      
        Explicit Formulas for Estimating Trace of Reduced Density Matrix Powers via Single-Circuit Measurement Probabilities
      
    </p>
    <p class="authors">
      <span class="search-hit">Authors:</span>
      
      <a href="/search/?searchtype=author&amp;query=Zhang%2C+R">Rui-Qi Zhang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Liu%2C+X">Xiao-Qi Liu</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Wang%2C+J">Jing Wang</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Li%2C+M">Ming Li</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Shen%2C+S">Shu-Qian Shen</a>, 
      
      <a href="/search/?searchtype=author&amp;query=Fei%2C+S">Shao-Ming Fei</a>
      
    </p>
    
    <p class="abstract mathjax">
      <span class="has-text-black-bis has-text-weight-semibold">Abstract</span>:
      <span class="abstract-short has-text-grey-dark mathjax" id="2507.17117v1-abstract-short" style="display: inline;">
        In the fields of quantum mechanics and quantum information science, the traces of reduced density matrix powers play a crucial role in the study of quantum systems and have numerous important applications. In this paper, we propose a universal framework to simultaneously estimate the traces of the $2$nd to the $n$th powers of a reduced density matrix using a single quantum circuit with $n$ copies&hellip;
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.17117v1-abstract-full').style.display = 'inline'; document.getElementById('2507.17117v1-abstract-short').style.display = 'none';">&#9661; More</a>
      </span>
      <span class="abstract-full has-text-grey-dark mathjax" id="2507.17117v1-abstract-full" style="display: none;">
        In the fields of quantum mechanics and quantum information science, the traces of reduced density matrix powers play a crucial role in the study of quantum systems and have numerous important applications. In this paper, we propose a universal framework to simultaneously estimate the traces of the $2$nd to the $n$th powers of a reduced density matrix using a single quantum circuit with $n$ copies of the quantum state. Specifically, our approach leverages the controlled SWAP test and establishes explicit formulas connecting measurement probabilities to these traces. We further develop two algorithms: a purely quantum method and a hybrid quantum-classical approach combining Newton-Girard iteration. Rigorous analysis via Hoeffding inequality demonstrates the method&#39;s efficiency, requiring only $M=O\left(\frac{1}{ε^2}\log(\frac{n}δ)\right)$ measurements to achieve precision $ε$ with confidence $1-δ$. Additionally, we explore various applications including the estimation of nonlinear functions and the representation of entanglement measures. Numerical simulations are conducted for two maximally entangled states, the GHZ state and the W state, to validate the proposed method.
        <a class="is-size-7" style="white-space: nowrap;" onclick="document.getElementById('2507.17117v1-abstract-full').style.display = 'none'; document.getElementById('2507.17117v1-abstract-short').style.display = 'inline';">&#9651; Less</a>
      </span>
    </p>
    

    <p class="is-size-7"><span class="has-text-black-bis has-text-weight-semibold">Submitted</span> 22 July, 2025; 
      <span class="has-text-black-bis has-text-weight-semibold">originally announced</span> July 2025.
      
    </p>
    
    <p class="comments is-size-7">
      <span class="has-text-black-bis has-text-weight-semibold">Comments:</span>
      <span class="has-text-grey-dark mathjax">29 pages, 19 figures</span>
    </p>
    

    

    
      <p class="comments is-size-7">
        <span class="has-text-black-bis has-text-weight-semibold">Journal ref:</span>
        Adv. Quantum Technol. 2025, e2500376
      </p>
    
  </li>

</ol>


  <nav class="pagination is-small is-centered breathe-horizontal" role="navigation" aria-label="pagination">
    
    <a href=""
      class="pagination-previous is-invisible">Previous
    </a>
    
    
      <a href="/search/?query=Fei+Liu&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
        class="pagination-next" >Next
      </a>
    
    <ul class="pagination-list">

      <li>
        <a href="/search/?query=Fei+Liu&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=0"
          class="pagination-link is-current"
          aria-label="Goto page 1">1
        </a>
      </li>

      
                                     
          
          <li>
            <a href="/search/?query=Fei+Liu&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=50"
              class="pagination-link "
              aria-label="Page 2"
              aria-current="page">2
            </a>
          </li>
          
          <li>
            <a href="/search/?query=Fei+Liu&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=100"
              class="pagination-link "
              aria-label="Page 3"
              aria-current="page">3
            </a>
          </li>
          
          <li>
            <a href="/search/?query=Fei+Liu&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=150"
              class="pagination-link "
              aria-label="Page 4"
              aria-current="page">4
            </a>
          </li>
          
          <li>
            <a href="/search/?query=Fei+Liu&amp;searchtype=author&amp;abstracts=show&amp;order=-announced_date_first&amp;size=50&amp;start=200"
              class="pagination-link "
              aria-label="Page 5"
              aria-current="page">5
            </a>
          </li>
          
          <li><span class="pagination-ellipsis">&hellip;</span></li>
        
      
    </ul>
  </nav>
  

  


      <div class="is-hidden-tablet">
        <!-- feedback for mobile only -->
        <span class="help" style="display: inline-block;"><a href="https://github.com/arXiv/arxiv-search/releases">Search v0.5.6 released 2020-02-24</a>&nbsp;&nbsp;</span>
      </div>
    </div>

  </main>
  <footer>
    
    <div class="columns is-desktop" role="navigation" aria-label="Secondary">
  <!-- MetaColumn 1 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/about">About</a></li>
          <li><a href="https://info.arxiv.org/help">Help</a></li>
        </ul>
      </div>
      <div class="column">
        <ul class="nav-spaced">
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>contact arXiv</title><desc>Click here to contact arXiv</desc><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>
            <a href="https://info.arxiv.org/help/contact.html"> Contact</a>
          </li>
          <li>
            <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><title>subscribe to arXiv mailings</title><desc>Click here to subscribe</desc><path d="M476 3.2L12.5 270.6c-18.1 10.4-15.8 35.6 2.2 43.2L121 358.4l287.3-253.2c5.5-4.9 13.3 2.6 8.6 8.3L176 407v80.5c0 23.6 28.5 32.9 42.5 15.8L282 426l124.6 52.2c14.2 6 30.4-2.9 33-18.2l72-432C515 7.8 493.3-6.8 476 3.2z"/></svg>
            <a href="https://info.arxiv.org/help/subscribe"> Subscribe</a>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 1 -->
  <!-- MetaColumn 2 -->
  <div class="column">
    <div class="columns">
      <div class="column">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/license/index.html">Copyright</a></li>
          <li><a href="https://info.arxiv.org/help/policies/privacy_policy.html">Privacy Policy</a></li>
        </ul>
      </div>
      <div class="column sorry-app-links">
        <ul class="nav-spaced">
          <li><a href="https://info.arxiv.org/help/web_accessibility.html">Web Accessibility Assistance</a></li>
          <li>
            <p class="help">
              <a class="a11y-main-link" href="https://status.arxiv.org" target="_blank">arXiv Operational Status <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 256 512" class="icon filter-dark_grey" role="presentation"><path d="M224.3 273l-136 136c-9.4 9.4-24.6 9.4-33.9 0l-22.6-22.6c-9.4-9.4-9.4-24.6 0-33.9l96.4-96.4-96.4-96.4c-9.4-9.4-9.4-24.6 0-33.9L54.3 103c9.4-9.4 24.6-9.4 33.9 0l136 136c9.5 9.4 9.5 24.6.1 34z"/></svg></a><br>
              Get status notifications via
              <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/email/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 512 512" class="icon filter-black" role="presentation"><path d="M502.3 190.8c3.9-3.1 9.7-.2 9.7 4.7V400c0 26.5-21.5 48-48 48H48c-26.5 0-48-21.5-48-48V195.6c0-5 5.7-7.8 9.7-4.7 22.4 17.4 52.1 39.5 154.1 113.6 21.1 15.4 56.7 47.8 92.2 47.6 35.7.3 72-32.8 92.3-47.6 102-74.1 131.6-96.3 154-113.7zM256 320c23.2.4 56.6-29.2 73.4-41.4 132.7-96.3 142.8-104.7 173.4-128.7 5.8-4.5 9.2-11.5 9.2-18.9v-19c0-26.5-21.5-48-48-48H48C21.5 64 0 85.5 0 112v19c0 7.4 3.4 14.3 9.2 18.9 30.6 23.9 40.7 32.4 173.4 128.7 16.8 12.2 50.2 41.8 73.4 41.4z"/></svg>email</a>
              or <a class="is-link" href="https://subscribe.sorryapp.com/24846f03/slack/new" target="_blank"><svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512" class="icon filter-black" role="presentation"><path d="M94.12 315.1c0 25.9-21.16 47.06-47.06 47.06S0 341 0 315.1c0-25.9 21.16-47.06 47.06-47.06h47.06v47.06zm23.72 0c0-25.9 21.16-47.06 47.06-47.06s47.06 21.16 47.06 47.06v117.84c0 25.9-21.16 47.06-47.06 47.06s-47.06-21.16-47.06-47.06V315.1zm47.06-188.98c-25.9 0-47.06-21.16-47.06-47.06S139 32 164.9 32s47.06 21.16 47.06 47.06v47.06H164.9zm0 23.72c25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06H47.06C21.16 243.96 0 222.8 0 196.9s21.16-47.06 47.06-47.06H164.9zm188.98 47.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06s-21.16 47.06-47.06 47.06h-47.06V196.9zm-23.72 0c0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06V79.06c0-25.9 21.16-47.06 47.06-47.06 25.9 0 47.06 21.16 47.06 47.06V196.9zM283.1 385.88c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06-25.9 0-47.06-21.16-47.06-47.06v-47.06h47.06zm0-23.72c-25.9 0-47.06-21.16-47.06-47.06 0-25.9 21.16-47.06 47.06-47.06h117.84c25.9 0 47.06 21.16 47.06 47.06 0 25.9-21.16 47.06-47.06 47.06H283.1z"/></svg>slack</a>
            </p>
          </li>
        </ul>
      </div>
    </div>
  </div> <!-- end MetaColumn 2 -->
</div>
    
  </footer>
  <script src="https://static.arxiv.org/static/base/1.0.0a5/js/member_acknowledgement.js"></script>
  </body>
</html>