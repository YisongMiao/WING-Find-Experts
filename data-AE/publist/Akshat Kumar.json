{
  "author": "Akshat Kumar",
  "results": [
    {
      "title": "Exploiting solute segregation and partitioning to the deformation-induced planar defects and nano-martensite in designing ultra-strong Co-Ni base alloys",
      "abstract": "Single-phase, multi-elements (three or more) with high concentrations show exceptional tensile strength up to ~ 0.8-1.2 GPa. However, they possess a very low 0.2% yield strength (YS), i.e., they can be permanently deformed at very low-stress levels of 300 to 600 MPa. Here, we reveal by exploiting atomic-scale solute interactions with the deformation-induced structures to design ultra-strong single-phase alloys with YS > 2 GPa. This was achieved by controlled thermomechanical processing that introduces stacking-faults (SFs), nano-twins (NTs), and nano-martensite ε-laths (NMLs) during cold deformation followed by facilitating solute segregation/partitioning to them by tempering at intermediate temperature. We demonstrate the phenomena in a low stacking faulty energy multi-component (face-centered-cubic, fcc structured) Co-33Ni-24Cr alloy (all in at.%) containing 5at.% Mo as a solute. It is also shown that the degree of strengthening after tempering scales up with the fraction of these structures (before tempering) in the alloy microstructure that can be tuned by the amount and temperature of cold deformation. Cold-rolling with 45% and 65% thickness reduction, followed by tempering at 600°C for 4 hours, led to an YS of 1.5 GPa and 2 GPa with elongation to fracture (%El) 14% and 7%, respectively. The YS is further enhanced to ~ 2.2 GPa without reduction in %El upon cryo-rolling followed by tempering. The alloy microstructure is stable at 600°C up to 100 hours and also retains an YS of ~ 1.5 GPa with %El of 18% during tensile test at 600°C. The derived high YS and high-temperature stability are critically a consequence of solute partitioning to the NMLs that we termed as Solute-Partitioned NMLs (SP-NMLs) in the microstructure. △ Less",
      "url": "https://arxiv.org/abs/2507.22078"
    },
    {
      "title": "TraCeS: Trajectory Based Credit Assignment From Sparse Safety Feedback",
      "abstract": "In safe reinforcement learning (RL), auxiliary safety costs are used to align the agent to safe decision making. In practice, safety constraints, including cost functions and budgets, are unknown or hard to specify, as it requires anticipation of all possible unsafe behaviors. We therefore address a general setting where the true safety definition is unknown, and has to be learned from sparsely labeled data. Our key contributions are: first, we design a safety model that performs credit assignment to estimate each decision step's impact on the overall safety using a dataset of diverse trajectories and their corresponding binary safety labels (i.e., whether the corresponding trajectory is safe/unsafe). Second, we illustrate the architecture of our safety model to demonstrate its ability to learn a separate safety score for each timestep. Third, we reformulate the safe RL problem using the proposed safety model and derive an effective algorithm to optimize a safe yet rewarding policy. Finally, our empirical results corroborate our findings and show that this approach is effective in satisfying unknown safety definition, and scalable to various continuous control tasks. △ Less",
      "url": "https://arxiv.org/abs/2504.12557"
    },
    {
      "title": "Constraining the 3HDM Parameter Space using Active Learning",
      "abstract": "One of the standard ways to study scenarios beyond the Standard Model involves extending the Higgs Sector. This work examines the Three Higgs Doublet Model (3HDM) in a Type-Z or democratic setup, where each Higgs doublet couples exclusively to a specific type of fermion. The particle spectrum of the 3HDM includes four charged Higgs bosons, two CP-odd scalars, and three CP-even scalars. This work investigates the allowed mass and coupling parameter space in the Type-Z 3HDM after imposing all theoretical and experimental constraints. We extract the allowed parameter space under three distinct alignment-limit conditions or mass hierarchies leveraging machine learning techniques. Specifically, we analyze scenarios where the 125 GeV Higgs is the lightest, an intermediary, or the heaviest CP-even Higgs boson. Our findings indicate that while a single lighter CP-even Higgs boson below 125 GeV still remains a possibility, the presence of two lighter Higgses is ruled out. △ Less",
      "url": "https://arxiv.org/abs/2504.07489"
    },
    {
      "title": "Chitrarth: Bridging Vision and Language for a Billion People",
      "abstract": "Recent multimodal foundation models are primarily trained on English or high resource European language data, which hinders their applicability to other medium and low-resource languages. To address this limitation, we introduce Chitrarth (Chitra: Image; Artha: Meaning), an inclusive Vision-Language Model (VLM), specifically targeting the rich linguistic diversity and visual reasoning across 10 prominent Indian languages. Our model effectively integrates a state-of-the-art (SOTA) multilingual Large Language Model (LLM) with a vision module, primarily trained on multilingual image-text data. Furthermore, we also introduce BharatBench, a comprehensive framework for evaluating VLMs across various Indian languages, ultimately contributing to more diverse and effective AI systems. Our model achieves SOTA results for benchmarks across low resource languages while retaining its efficiency in English. Through our research, we aim to set new benchmarks in multilingual-multimodal capabilities, offering substantial improvements over existing models and establishing a foundation to facilitate future advancements in this arena. △ Less",
      "url": "https://arxiv.org/abs/2502.15392"
    },
    {
      "title": "Leveraging Constraint Violation Signals For Action-Constrained Reinforcement Learning",
      "abstract": "In many RL applications, ensuring an agent's actions adhere to constraints is crucial for safety. Most previous methods in Action-Constrained Reinforcement Learning (ACRL) employ a projection layer after the policy network to correct the action. However projection-based methods suffer from issues like the zero gradient problem and higher runtime due to the usage of optimization solvers. Recently methods were proposed to train generative models to learn a differentiable mapping between latent variables and feasible actions to address this issue. However, generative models require training using samples from the constrained action space, which itself is challenging. To address such limitations, first, we define a target distribution for feasible actions based on constraint violation signals, and train normalizing flows by minimizing the KL divergence between an approximated distribution over feasible actions and the target. This eliminates the need to generate feasible action samples, greatly simplifying the flow model learning. Second, we integrate the learned flow model with existing deep RL methods, which restrict it to exploring only the feasible action space. Third, we extend our approach beyond ACRL to handle state-wise constraints by learning the constraint violation signal from the environment. Empirically, our approach has significantly fewer constraint violations while achieving similar or better quality in several control tasks than previous best methods. △ Less",
      "url": "https://arxiv.org/abs/2502.10431"
    },
    {
      "title": "Krutrim LLM: Multilingual Foundational Model for over a Billion People",
      "abstract": "India is a diverse society with unique challenges in developing AI systems, including linguistic diversity, oral traditions, data accessibility, and scalability. Existing foundation models are primarily trained on English, limiting their effectiveness for India's population. Indic languages comprise only 1 percent of Common Crawl corpora despite India representing 18 percent of the global population, leading to linguistic biases. Thousands of regional languages, dialects, and code mixing create additional representation challenges due to sparse training data. We introduce Krutrim LLM, a 2 trillion token multilingual model designed for India's linguistic landscape. It incorporates the largest known Indic dataset, mitigating data scarcity and ensuring balanced performance across dialects. Krutrim outperforms or matches state-of-the-art models on Indic benchmarks while maintaining competitive English performance. Despite being significantly smaller in training flops, Krutrim LLM matches or exceeds models like LLAMA-2 on 10 out of 16 tasks, with an average score of 0.57 versus 0.55. This evidences Krutrim's flexible multilingual fluency across diverse linguistic contexts. Krutrim is integrated with real-time search to improve factual accuracy in conversational AI applications. This enhances accessibility for over 1 billion users worldwide. Through intentional design choices addressing data imbalances, Krutrim LLM signifies meaningful progress in building ethical, globally representative AI models. △ Less",
      "url": "https://arxiv.org/abs/2502.09642"
    },
    {
      "title": "Offline Safe Reinforcement Learning Using Trajectory Classification",
      "abstract": "Offline safe reinforcement learning (RL) has emerged as a promising approach for learning safe behaviors without engaging in risky online interactions with the environment. Most existing methods in offline safe RL rely on cost constraints at each time step (derived from global cost constraints) and this can result in either overly conservative policies or violation of safety constraints. In this paper, we propose to learn a policy that generates desirable trajectories and avoids undesirable trajectories. To be specific, we first partition the pre-collected dataset of state-action trajectories into desirable and undesirable subsets. Intuitively, the desirable set contains high reward and safe trajectories, and undesirable set contains unsafe trajectories and low-reward safe trajectories. Second, we learn a policy that generates desirable trajectories and avoids undesirable trajectories, where (un)desirability scores are provided by a classifier learnt from the dataset of desirable and undesirable trajectories. This approach bypasses the computational complexity and stability issues of a min-max objective that is employed in existing methods. Theoretically, we also show our approach's strong connections to existing learning paradigms involving human feedback. Finally, we extensively evaluate our method using the DSRL benchmark for offline safe RL. Empirically, our method outperforms competitive baselines, achieving higher rewards and better constraint satisfaction across a wide variety of benchmark tasks. △ Less",
      "url": "https://arxiv.org/abs/2412.15429"
    },
    {
      "title": "Real-Time Weapon Detection Using YOLOv8 for Enhanced Safety",
      "abstract": "This research paper presents the development of an AI model utilizing YOLOv8 for real-time weapon detection, aimed at enhancing safety in public spaces such as schools, airports, and public transportation systems. As incidents of violence continue to rise globally, there is an urgent need for effective surveillance technologies that can quickly identify potential threats. Our approach focuses on leveraging advanced deep learning techniques to create a highly accurate and efficient system capable of detecting weapons in real-time video streams. The model was trained on a comprehensive dataset containing thousands of images depicting various types of firearms and edged weapons, ensuring a robust learning process. We evaluated the model's performance using key metrics such as precision, recall, F1-score, and mean Average Precision (mAP) across multiple Intersection over Union (IoU) thresholds, revealing a significant capability to differentiate between weapon and non-weapon classes with minimal error. Furthermore, we assessed the system's operational efficiency, demonstrating that it can process frames at high speeds suitable for real-time applications. The findings indicate that our YOLOv8-based weapon detection model not only contributes to the existing body of knowledge in computer vision but also addresses critical societal needs for improved safety measures in vulnerable environments. By harnessing the power of artificial intelligence, this research lays the groundwork for developing practical solutions that can be deployed in security settings, ultimately enhancing the protective capabilities of law enforcement and public safety agencies. △ Less",
      "url": "https://arxiv.org/abs/2410.19862"
    },
    {
      "title": "A Factored MDP Approach To Moving Target Defense With Dynamic Threat Modeling and Cost Efficiency",
      "abstract": "Moving Target Defense (MTD) has emerged as a proactive and dynamic framework to counteract evolving cyber threats. Traditional MTD approaches often rely on assumptions about the attackers knowledge and behavior. However, real-world scenarios are inherently more complex, with adaptive attackers and limited prior knowledge of their payoffs and intentions. This paper introduces a novel approach to MTD using a Markov Decision Process (MDP) model that does not rely on predefined attacker payoffs. Our framework integrates the attackers real-time responses into the defenders MDP using a dynamic Bayesian Network. By employing a factored MDP model, we provide a comprehensive and realistic system representation. We also incorporate incremental updates to an attack response predictor as new data emerges. This ensures an adaptive and robust defense mechanism. Additionally, we consider the costs of switching configurations in MTD, integrating them into the reward structure to balance execution and defense costs. We first highlight the challenges of the problem through a theoretical negative result on regret. However, empirical evaluations demonstrate the frameworks effectiveness in scenarios marked by high uncertainty and dynamically changing attack landscapes. △ Less",
      "url": "https://arxiv.org/abs/2408.08934"
    },
    {
      "title": "Safe Reinforcement Learning with Learned Non-Markovian Safety Constraints",
      "abstract": "In safe Reinforcement Learning (RL), safety cost is typically defined as a function dependent on the immediate state and actions. In practice, safety constraints can often be non-Markovian due to the insufficient fidelity of state representation, and safety cost may not be known. We therefore address a general setting where safety labels (e.g., safe or unsafe) are associated with state-action trajectories. Our key contributions are: first, we design a safety model that specifically performs credit assignment to assess contributions of partial state-action trajectories on safety. This safety model is trained using a labeled safety dataset. Second, using RL-as-inference strategy we derive an effective algorithm for optimizing a safe policy using the learned safety model. Finally, we devise a method to dynamically adapt the tradeoff coefficient between reward maximization and safety compliance. We rewrite the constrained optimization problem into its dual problem and derive a gradient-based method to dynamically adjust the tradeoff coefficient during training. Our empirical results demonstrate that this approach is highly scalable and able to satisfy sophisticated non-Markovian safety constraints. △ Less",
      "url": "https://arxiv.org/abs/2405.03005"
    },
    {
      "title": "CloudLens: Modeling and Detecting Cloud Security Vulnerabilities",
      "abstract": "Cloud computing services provide scalable and cost-effective solutions for data storage, processing, and collaboration. With their growing popularity, concerns about security vulnerabilities are increasing. To address this, first, we provide a formal model, called CloudLens, that expresses relations between different cloud objects such as users, datastores, security roles, representing access control policies in cloud systems. Second, as access control misconfigurations are often the primary driver for cloud attacks, we develop a planning model for detecting security vulnerabilities. Such vulnerabilities can lead to widespread attacks such as ransomware, sensitive data exfiltration among others. A planner generates attacks to identify such vulnerabilities in the cloud. Finally, we test our approach on 14 real Amazon AWS cloud configurations of different commercial organizations. Our system can identify a broad range of security vulnerabilities, which state-of-the-art industry tools cannot detect. △ Less",
      "url": "https://arxiv.org/abs/2402.10985"
    },
    {
      "title": "FlowPG: Action-constrained Policy Gradient with Normalizing Flows",
      "abstract": "Action-constrained reinforcement learning (ACRL) is a popular approach for solving safety-critical and resource-allocation related decision making problems. A major challenge in ACRL is to ensure agent taking a valid action satisfying constraints in each RL step. Commonly used approach of using a projection layer on top of the policy network requires solving an optimization program which can result in longer training time, slow convergence, and zero gradient problem. To address this, first we use a normalizing flow model to learn an invertible, differentiable mapping between the feasible action space and the support of a simple distribution on a latent variable, such as Gaussian. Second, learning the flow model requires sampling from the feasible action space, which is also challenging. We develop multiple methods, based on Hamiltonian Monte-Carlo and probabilistic sentential decision diagrams for such action sampling for convex and non-convex constraints. Third, we integrate the learned normalizing flow with the DDPG algorithm. By design, a well-trained normalizing flow will transform policy output into a valid action without requiring an optimization solver. Empirically, our approach results in significantly fewer constraint violations (upto an order-of-magnitude for several instances) and is multiple times faster on a variety of continuous control tasks. △ Less",
      "url": "https://arxiv.org/abs/2402.05149"
    },
    {
      "title": "Unified Training of Universal Time Series Forecasting Transformers",
      "abstract": "Deep learning for time series forecasting has traditionally operated within a one-model-per-dataset framework, limiting its potential to leverage the game-changing impact of large pre-trained models. The concept of universal forecasting, emerging from pre-training on a vast collection of time series datasets, envisions a single Large Time Series Model capable of addressing diverse downstream forecasting tasks. However, constructing such a model poses unique challenges specific to time series data: i) cross-frequency learning, ii) accommodating an arbitrary number of variates for multivariate time series, and iii) addressing the varying distributional properties inherent in large-scale data. To address these challenges, we present novel enhancements to the conventional time series Transformer architecture, resulting in our proposed Masked Encoder-based Universal Time Series Forecasting Transformer (Moirai). Trained on our newly introduced Large-scale Open Time Series Archive (LOTSA) featuring over 27B observations across nine domains, Moirai achieves competitive or superior performance as a zero-shot forecaster when compared to full-shot models. Code, data, and model weights can be found at https://github.com/SalesforceAIResearch/uni2ts. △ Less",
      "url": "https://arxiv.org/abs/2402.02592"
    },
    {
      "title": "Integrating Human Vision Perception in Vision Transformers for Classifying Waste Items",
      "abstract": "In this paper, we propose an novel methodology aimed at simulating the learning phenomenon of nystagmus through the application of differential blurring on datasets. Nystagmus is a biological phenomenon that influences human vision throughout life, notably by diminishing head shake from infancy to adulthood. Leveraging this concept, we address the issue of waste classification, a pressing global concern. The proposed framework comprises two modules, with the second module closely resembling the original Vision Transformer, a state-of-the-art model model in classification tasks. The primary motivation behind our approach is to enhance the model's precision and adaptability, mirroring the real-world conditions that the human visual system undergoes. This novel methodology surpasses the standard Vision Transformer model in waste classification tasks, exhibiting an improvement with a margin of 2%. This improvement underscores the potential of our methodology in improving model precision by drawing inspiration from human vision perception. Further research in the proposed methodology could yield greater performance results, and can be extrapolated to other global issues. △ Less",
      "url": "https://arxiv.org/abs/2312.12143"
    },
    {
      "title": "Mean-field games among teams",
      "abstract": "In this paper, we present a model of a game among teams. Each team consists of a homogeneous population of agents. Agents within a team are cooperative while the teams compete with other teams. The dynamics and the costs are coupled through the empirical distribution (or the mean field) of the state of agents in each team. This mean-field is assumed to be observed by all agents. Agents have asymmetric information (also called a non-classical information structure). We propose a mean-field based refinement of the Team-Nash equilibrium of the game, which we call mean-field Markov perfect equilibrium (MF-MPE). We identify a dynamic programming decomposition to characterize MF-MPE. We then consider the case where each team has a large number of players and present a mean-field approximation which approximates the game among large-population teams as a game among infinite-population teams. We show that MF-MPE of the game among teams of infinite population is easier to compute and is an $\\varepsilon$-approximate MF-MPE of the game among teams of finite population. △ Less",
      "url": "https://arxiv.org/abs/2310.12282"
    },
    {
      "title": "Pushing the Limits of Pre-training for Time Series Forecasting in the CloudOps Domain",
      "abstract": "Time series has been left behind in the era of pre-training and transfer learning. While research in the fields of natural language processing and computer vision are enjoying progressively larger datasets to train massive models, the most popular time series datasets consist of only tens of thousands of time steps, limiting our ability to study the effectiveness of pre-training and scaling. Recent studies have also cast doubt on the need for expressive models and scale. To alleviate these issues, we introduce three large-scale time series forecasting datasets from the cloud operations (CloudOps) domain, the largest having billions of observations, enabling further study into pre-training and scaling of time series models. We build the empirical groundwork for studying pre-training and scaling of time series models and pave the way for future research by identifying a promising candidate architecture. We show that it is a strong zero-shot baseline and benefits from further scaling, both in model and dataset size. Accompanying these datasets and results is a suite of comprehensive benchmark results comparing classical and deep learning baselines to our pre-trained method - achieving a 27% reduction in error on the largest dataset. Code and datasets can be found https://github.com/SalesforceAIResearch/pretrain-time-series-cloudops. △ Less",
      "url": "https://arxiv.org/abs/2310.05063"
    },
    {
      "title": "Safe MDP Planning by Learning Temporal Patterns of Undesirable Trajectories and Averting Negative Side Effects",
      "abstract": "In safe MDP planning, a cost function based on the current state and action is often used to specify safety aspects. In the real world, often the state representation used may lack sufficient fidelity to specify such safety constraints. Operating based on an incomplete model can often produce unintended negative side effects (NSEs). To address these challenges, first, we associate safety signals with state-action trajectories (rather than just an immediate state-action). This makes our safety model highly general. We also assume categorical safety labels are given for different trajectories, rather than a numerical cost function, which is harder to specify by the problem designer. We then employ a supervised learning model to learn such non-Markovian safety patterns. Second, we develop a Lagrange multiplier method, which incorporates the safety model and the underlying MDP model in a single computation graph to facilitate agent learning of safe behaviors. Finally, our empirical results on a variety of discrete and continuous domains show that this approach can satisfy complex non-Markovian safety constraints while optimizing an agent's total returns, is highly scalable, and is also better than the previous best approach for Markovian NSEs. △ Less",
      "url": "https://arxiv.org/abs/2304.03081"
    },
    {
      "title": "Shining light on data: Geometric data analysis through quantum dynamics",
      "abstract": "Experimental sciences have come to depend heavily on our ability to organize and interpret high-dimensional datasets. Natural laws, conservation principles, and inter-dependencies among observed variables yield geometric structure, with fewer degrees of freedom, on the dataset. We introduce the frameworks of semiclassical and microlocal analysis to data analysis and develop a novel, yet natural uncertainty principle for extracting fine-scale features of this geometric structure in data, crucially dependent on data-driven approximations to quantum mechanical processes underlying geometric optics. This leads to the first tractable algorithm for approximation of wave dynamics and geodesics on data manifolds with rigorous probabilistic convergence rates under the manifold hypothesis. We demonstrate our algorithm on real-world datasets, including an analysis of population mobility information during the COVID-19 pandemic to achieve four-fold improvement in dimensionality reduction over existing state-of-the-art and reveal anomalous behavior exhibited by less than 1.2% of the entire dataset. Our work initiates the study of data-driven quantum dynamics for analyzing datasets, and we outline several future directions for research. △ Less",
      "url": "https://arxiv.org/abs/2212.00682"
    },
    {
      "title": "Daksha: On Alert for High Energy Transients",
      "abstract": "We present Daksha, a proposed high energy transients mission for the study of electromagnetic counterparts of gravitational wave sources, and gamma ray bursts. Daksha will comprise of two satellites in low earth equatorial orbits, on opposite sides of earth. Each satellite will carry three types of detectors to cover the entire sky in an energy range from 1 keV to >1 MeV. Any transients detected on-board will be announced publicly within minutes of discovery. All photon data will be downloaded in ground station passes to obtain source positions, spectra, and light curves. In addition, Daksha will address a wide range of science cases including monitoring X-ray pulsars, studies of magnetars, solar flares, searches for fast radio burst counterparts, routine monitoring of bright persistent high energy sources, terrestrial gamma-ray flashes, and probing primordial black hole abundances through lensing. In this paper, we discuss the technical capabilities of Daksha, while the detailed science case is discussed in a separate paper. △ Less",
      "url": "https://arxiv.org/abs/2211.12055"
    },
    {
      "title": "Science with the Daksha High Energy Transients Mission",
      "abstract": "We present the science case for the proposed Daksha high energy transients mission. Daksha will comprise of two satellites covering the entire sky from 1~keV to $>1$~MeV. The primary objectives of the mission are to discover and characterize electromagnetic counterparts to gravitational wave source; and to study Gamma Ray Bursts (GRBs). Daksha is a versatile all-sky monitor that can address a wide variety of science cases. With its broadband spectral response, high sensitivity, and continuous all-sky coverage, it will discover fainter and rarer sources than any other existing or proposed mission. Daksha can make key strides in GRB research with polarization studies, prompt soft spectroscopy, and fine time-resolved spectral studies. Daksha will provide continuous monitoring of X-ray pulsars. It will detect magnetar outbursts and high energy counterparts to Fast Radio Bursts. Using Earth occultation to measure source fluxes, the two satellites together will obtain daily flux measurements of bright hard X-ray sources including active galactic nuclei, X-ray binaries, and slow transients like Novae. Correlation studies between the two satellites can be used to probe primordial black holes through lensing. Daksha will have a set of detectors continuously pointing towards the Sun, providing excellent hard X-ray monitoring data. Closer to home, the high sensitivity and time resolution of Daksha can be leveraged for the characterization of Terrestrial Gamma-ray Flashes. △ Less",
      "url": "https://arxiv.org/abs/2211.12052"
    },
    {
      "title": "SpeechNet: Weakly Supervised, End-to-End Speech Recognition at Industrial Scale",
      "abstract": "End-to-end automatic speech recognition systems represent the state of the art, but they rely on thousands of hours of manually annotated speech for training, as well as heavyweight computation for inference. Of course, this impedes commercialization since most companies lack vast human and computational resources. In this paper, we explore training and deploying an ASR system in the label-scarce, compute-limited setting. To reduce human labor, we use a third-party ASR system as a weak supervision source, supplemented with labeling functions derived from implicit user feedback. To accelerate inference, we propose to route production-time queries across a pool of CUDA graphs of varying input lengths, the distribution of which best matches the traffic's. Compared to our third-party ASR, we achieve a relative improvement in word-error rate of 8% and a speedup of 600%. Our system, called SpeechNet, currently serves 12 million queries per day on our voice-enabled smart television. To our knowledge, this is the first time a large-scale, Wav2vec-based deployment has been described in the academic literature. △ Less",
      "url": "https://arxiv.org/abs/2211.11740"
    },
    {
      "title": "UATTA-ENS: Uncertainty Aware Test Time Augmented Ensemble for PIRC Diabetic Retinopathy Detection",
      "abstract": "Deep Ensemble Convolutional Neural Networks has become a methodology of choice for analyzing medical images with a diagnostic performance comparable to a physician, including the diagnosis of Diabetic Retinopathy. However, commonly used techniques are deterministic and are therefore unable to provide any estimate of predictive uncertainty. Quantifying model uncertainty is crucial for reducing the risk of misdiagnosis. A reliable architecture should be well-calibrated to avoid over-confident predictions. To address this, we propose a UATTA-ENS: Uncertainty-Aware Test-Time Augmented Ensemble Technique for 5 Class PIRC Diabetic Retinopathy Classification to produce reliable and well-calibrated predictions. △ Less",
      "url": "https://arxiv.org/abs/2211.03148"
    },
    {
      "title": "What the DAAM: Interpreting Stable Diffusion Using Cross Attention",
      "abstract": "Large-scale diffusion neural networks represent a substantial milestone in text-to-image generation, but they remain poorly understood, lacking interpretability analyses. In this paper, we perform a text-image attribution analysis on Stable Diffusion, a recently open-sourced model. To produce pixel-level attribution maps, we upscale and aggregate cross-attention word-pixel scores in the denoising subnetwork, naming our method DAAM. We evaluate its correctness by testing its semantic segmentation ability on nouns, as well as its generalized attribution quality on all parts of speech, rated by humans. We then apply DAAM to study the role of syntax in the pixel space, characterizing head--dependent heat map interaction patterns for ten common dependency relations. Finally, we study several semantic phenomena using DAAM, with a focus on feature entanglement, where we find that cohyponyms worsen generation quality and descriptive adjectives attend too broadly. To our knowledge, we are the first to interpret large diffusion models from a visuolinguistic perspective, which enables future lines of research. Our code is at https://github.com/castorini/daam. △ Less",
      "url": "https://arxiv.org/abs/2210.04885"
    },
    {
      "title": "EmoSens: Emotion Recognition based on Sensor data analysis using LightGBM",
      "abstract": "Smart wearables have played an integral part in our day to day life. From recording ECG signals to analysing body fat composition, the smart wearables can do it all. The smart devices encompass various sensors which can be employed to derive meaningful information regarding the user's physical and psychological conditions. Our approach focuses on employing such sensors to identify and obtain the variations in the mood of a user at a given instance through the use of supervised machine learning techniques. The study examines the performance of various supervised learning models such as Decision Trees, Random Forests, XGBoost, LightGBM on the dataset. With our proposed model, we obtained a high recognition rate of 92.5% using XGBoost and LightGBM for 9 different emotion classes. By utilizing this, we aim to improvise and suggest methods to aid emotion recognition for better mental health analysis and mood monitoring. △ Less",
      "url": "https://arxiv.org/abs/2207.14640"
    },
    {
      "title": "Learning Deep Time-index Models for Time Series Forecasting",
      "abstract": "Deep learning has been actively applied to time series forecasting, leading to a deluge of new methods, belonging to the class of historical-value models. Yet, despite the attractive properties of time-index models, such as being able to model the continuous nature of underlying time series dynamics, little attention has been given to them. Indeed, while naive deep time-index models are far more expressive than the manually predefined function representations of classical time-index models, they are inadequate for forecasting, being unable to generalize to unseen time steps due to the lack of inductive bias. In this paper, we propose DeepTime, a meta-optimization framework to learn deep time-index models which overcome these limitations, yielding an efficient and accurate forecasting model. Extensive experiments on real world datasets in the long sequence time-series forecasting setting demonstrate that our approach achieves competitive results with state-of-the-art methods, and is highly efficient. Code is available at https://github.com/salesforce/DeepTime. △ Less",
      "url": "https://arxiv.org/abs/2207.06046"
    },
    {
      "title": "Beyond the Imitation Game: Quantifying and extrapolating the capabilities of language models",
      "abstract": "Language models demonstrate both quantitative improvement and new qualitative capabilities with increasing scale. Despite their potentially transformative impact, these new capabilities are as yet poorly characterized. In order to inform future research, prepare for disruptive new model capabilities, and ameliorate socially harmful effects, it is vital that we understand the present and near-future capabilities and limitations of language models. To address this challenge, we introduce the Beyond the Imitation Game benchmark (BIG-bench). BIG-bench currently consists of 204 tasks, contributed by 450 authors across 132 institutions. Task topics are diverse, drawing problems from linguistics, childhood development, math, common-sense reasoning, biology, physics, social bias, software development, and beyond. BIG-bench focuses on tasks that are believed to be beyond the capabilities of current language models. We evaluate the behavior of OpenAI's GPT models, Google-internal dense transformer architectures, and Switch-style sparse transformers on BIG-bench, across model sizes spanning millions to hundreds of billions of parameters. In addition, a team of human expert raters performed all tasks in order to provide a strong baseline. Findings include: model performance and calibration both improve with scale, but are poor in absolute terms (and when compared with rater performance); performance is remarkably similar across model classes, though with benefits from sparsity; tasks that improve gradually and predictably commonly involve a large knowledge or memorization component, whereas tasks that exhibit \"breakthrough\" behavior at a critical scale often involve multiple steps or components, or brittle metrics; social bias typically increases with scale in settings with ambiguous context, but this can be improved with prompting. △ Less",
      "url": "https://arxiv.org/abs/2206.04615"
    },
    {
      "title": "Using Constraint Programming and Graph Representation Learning for Generating Interpretable Cloud Security Policies",
      "abstract": "Modern software systems rely on mining insights from business sensitive data stored in public clouds. A data breach usually incurs significant (monetary) loss for a commercial organization. Conceptually, cloud security heavily relies on Identity Access Management (IAM) policies that IT admins need to properly configure and periodically update. Security negligence and human errors often lead to misconfiguring IAM policies which may open a backdoor for attackers. To address these challenges, first, we develop a novel framework that encodes generating optimal IAM policies using constraint programming (CP). We identify reducing dark permissions of cloud users as an optimality criterion, which intuitively implies minimizing unnecessary datastore access permissions. Second, to make IAM policies interpretable, we use graph representation learning applied to historical access patterns of users to augment our CP model with similarity constraints: similar users should be grouped together and share common IAM policies. Third, we describe multiple attack models and show that our optimized IAM policies significantly reduce the impact of security attacks using real data from 8 commercial organizations, and synthetic instances. △ Less",
      "url": "https://arxiv.org/abs/2205.01240"
    },
    {
      "title": "Gravitational Waves and Electromagnetic Transients",
      "abstract": "The advanced gravitational wave (GW) detector network has started {routine detection of } signals from merging compact binaries. Data indicate that in a fair fraction of these sources, at least one component was a neutron star, bringing with it the possibility of electromagnetic (EM) radiation. So far, a confirmed link between EM and GW radiation has been established for only one source, GW170817. Joint analysis of broadband multiwavelength data and the GW signal have yielded rich information spanning fields as varied as jet physics, cosmology, and nucleosynthesis. Here, we discuss the importance of such joint observations, as well as current and near-future efforts to discover and study more EM counterparts to GW sources. △ Less",
      "url": "https://arxiv.org/abs/2204.05648"
    },
    {
      "title": "Sample-efficient Iterative Lower Bound Optimization of Deep Reactive Policies for Planning in Continuous MDPs",
      "abstract": "Recent advances in deep learning have enabled optimization of deep reactive policies (DRPs) for continuous MDP planning by encoding a parametric policy as a deep neural network and exploiting automatic differentiation in an end-to-end model-based gradient descent framework. This approach has proven effective for optimizing DRPs in nonlinear continuous MDPs, but it requires a large number of sampled trajectories to learn effectively and can suffer from high variance in solution quality. In this work, we revisit the overall model-based DRP objective and instead take a minorization-maximization perspective to iteratively optimize the DRP w.r.t. a locally tight lower-bounded objective. This novel formulation of DRP learning as iterative lower bound optimization (ILBO) is particularly appealing because (i) each step is structurally easier to optimize than the overall objective, (ii) it guarantees a monotonically improving objective under certain theoretical conditions, and (iii) it reuses samples between iterations thus lowering sample complexity. Empirical evaluation confirms that ILBO is significantly more sample-efficient than the state-of-the-art DRP planner and consistently produces better solution quality with lower variance. We additionally demonstrate that ILBO generalizes well to new problem instances (i.e., different initial states) without requiring retraining. △ Less",
      "url": "https://arxiv.org/abs/2203.12679"
    },
    {
      "title": "A Next-Generation Liquid Xenon Observatory for Dark Matter and Neutrino Physics",
      "abstract": "The nature of dark matter and properties of neutrinos are among the most pressing issues in contemporary particle physics. The dual-phase xenon time-projection chamber is the leading technology to cover the available parameter space for Weakly Interacting Massive Particles (WIMPs), while featuring extensive sensitivity to many alternative dark matter candidates. These detectors can also study neutrinos through neutrinoless double-beta decay and through a variety of astrophysical sources. A next-generation xenon-based detector will therefore be a true multi-purpose observatory to significantly advance particle physics, nuclear physics, astrophysics, solar physics, and cosmology. This review article presents the science cases for such a detector. △ Less",
      "url": "https://arxiv.org/abs/2203.02309"
    },
    {
      "title": "InfraredTags: Embedding Invisible AR Markers and Barcodes Using Low-Cost, Infrared-Based 3D Printing and Imaging Tools",
      "abstract": "Existing approaches for embedding unobtrusive tags inside 3D objects require either complex fabrication or high-cost imaging equipment. We present InfraredTags, which are 2D markers and barcodes imperceptible to the naked eye that can be 3D printed as part of objects, and detected rapidly by low-cost near-infrared cameras. We achieve this by printing objects from an infrared-transmitting filament, which infrared cameras can see through, and by having air gaps inside for the tag's bits, which appear at a different intensity in the infrared image. We built a user interface that facilitates the integration of common tags (QR codes, ArUco markers) with the object geometry to make them 3D printable as InfraredTags. We also developed a low-cost infrared imaging module that augments existing mobile devices and decodes tags using our image processing pipeline. Our evaluation shows that the tags can be detected with little near-infrared illumination (0.2lux) and from distances as far as 250cm. We demonstrate how our method enables various applications, such as object tracking and embedding metadata for augmented reality and tangible interactions. △ Less",
      "url": "https://arxiv.org/abs/2202.06165"
    },
    {
      "title": "CoST: Contrastive Learning of Disentangled Seasonal-Trend Representations for Time Series Forecasting",
      "abstract": "Deep learning has been actively studied for time series forecasting, and the mainstream paradigm is based on the end-to-end training of neural network architectures, ranging from classical LSTM/RNNs to more recent TCNs and Transformers. Motivated by the recent success of representation learning in computer vision and natural language processing, we argue that a more promising paradigm for time series forecasting, is to first learn disentangled feature representations, followed by a simple regression fine-tuning step -- we justify such a paradigm from a causal perspective. Following this principle, we propose a new time series representation learning framework for time series forecasting named CoST, which applies contrastive learning methods to learn disentangled seasonal-trend representations. CoST comprises both time domain and frequency domain contrastive losses to learn discriminative trend and seasonal representations, respectively. Extensive experiments on real-world datasets show that CoST consistently outperforms the state-of-the-art methods by a considerable margin, achieving a 21.3% improvement in MSE on multivariate benchmarks. It is also robust to various choices of backbone encoders, as well as downstream regressors. Code is available at https://github.com/salesforce/CoST. △ Less",
      "url": "https://arxiv.org/abs/2202.01575"
    },
    {
      "title": "ETSformer: Exponential Smoothing Transformers for Time-series Forecasting",
      "abstract": "Transformers have been actively studied for time-series forecasting in recent years. While often showing promising results in various scenarios, traditional Transformers are not designed to fully exploit the characteristics of time-series data and thus suffer some fundamental limitations, e.g., they generally lack of decomposition capability and interpretability, and are neither effective nor efficient for long-term forecasting. In this paper, we propose ETSFormer, a novel time-series Transformer architecture, which exploits the principle of exponential smoothing in improving Transformers for time-series forecasting. In particular, inspired by the classical exponential smoothing methods in time-series forecasting, we propose the novel exponential smoothing attention (ESA) and frequency attention (FA) to replace the self-attention mechanism in vanilla Transformers, thus improving both accuracy and efficiency. Based on these, we redesign the Transformer architecture with modular decomposition blocks such that it can learn to decompose the time-series data into interpretable time-series components such as level, growth and seasonality. Extensive experiments on various time-series benchmarks validate the efficacy and advantages of the proposed method. Code is available at https://github.com/salesforce/ETSformer. △ Less",
      "url": "https://arxiv.org/abs/2202.01381"
    },
    {
      "title": "Manifold learning via quantum dynamics",
      "abstract": "We introduce an algorithm for computing geodesics on sampled manifolds that relies on simulation of quantum dynamics on a graph embedding of the sampled data. Our approach exploits classic results in semiclassical analysis and the quantum-classical correspondence, and forms a basis for techniques to learn the manifold from which a dataset is sampled, and subsequently for nonlinear dimensionality reduction of high-dimensional datasets. We illustrate the new algorithm with data sampled from model manifolds and also by a clustering demonstration based on COVID-19 mobility data. Finally, our method reveals interesting connections between the discretization provided by data sampling and quantization. △ Less",
      "url": "https://arxiv.org/abs/2112.11161"
    },
    {
      "title": "On a quantum-classical correspondence: from graphs to manifolds",
      "abstract": "We establish conditions for which graph Laplacians $Δ_{λ,ε}$ on compact, boundaryless, smooth submanifolds $\\mathcal{M}$ of Euclidean space are semiclassical pseudodifferential operators ($Ψ$DOs): essentially, that the graph Laplacian's kernel bandwidth ($\\textit{bias term}$) $\\sqrtε$ decays faster than the semiclassical parameter $h$, $\\textit{i.e.}$, $h \\gg \\sqrtε$ and we compute the symbol. Coupling this with Egorov's theorem and coherent states $ψ_h$ localized at $(x_0, ξ_0) \\in T^*\\mathcal{M}$, we show that with $U_{λ,ε}^t := e^{-i t \\sqrtΔ_{λ,ε}}$ spectrally defined, the (co-)geodesic flow $Γ^t$ on $T^*\\mathcal{M}$ is approximated by $\\langle U_{λ,ε}^{-t} \\operatorname{Op}_h(a) U_{λ,ε}^t ψ_h, ψ_h \\rangle = a \\circ Γ^t(x_0, ξ_0) + O(h)$. Then, we turn to the discrete setting: for $Δ_{λ,ε,N}$ a normalized graph Laplacian defined on a set of $N$ points $x_1, \\ldots, x_N$ sampled $\\textit{i.i.d.}$ from a probability distribution with smooth density, we establish Bernstein-type lower bounds on the probability that $||U_{λ,ε,N}^t[u] - U_{λ,ε}^t[u]||_{L^{\\infty}} \\leq δ$ with $U_{λ,ε,N}^t := e^{-i t \\sqrtΔ_{λ,ε,N}}$. We apply this to coherent states to show that the geodesic flow on $\\mathcal{M}$ can be approximated by matrix dynamics on the discrete sample set, namely that $\\textit{with high probability}$, $c_{t,N}^{-1} \\sum_{j=1}^N |U_{λ,ε,N}^t[ψ_h](x_j)|^2 u(x_j) = u(x_t) + O(h)$ for $c_{t,N} := \\sum_{j=1}^N |U_{λ,ε,N}^t[ψ_h](x_j)|^2$ and $x_t$ the projection of $Γ^t(x_0, ξ_0)$ onto $\\mathcal{M}$. △ Less",
      "url": "https://arxiv.org/abs/2112.10748"
    },
    {
      "title": "Cuttlefish: Library for Achieving Energy Efficiency in Multicore Parallel Programs",
      "abstract": "A low-cap power budget is challenging for exascale computing. Dynamic Voltage and Frequency Scaling (DVFS) and Uncore Frequency Scaling (UFS) are the two widely used techniques for limiting the HPC application's energy footprint. However, existing approaches fail to provide a unified solution that can work with different types of parallel programming models and applications. This paper proposes Cuttlefish, a programming model oblivious C/C++ library for achieving energy efficiency in multicore parallel programs running over Intel processors. An online profiler periodically profiles model-specific registers to discover a running application's memory access pattern. Using a combination of DVFS and UFS, Cuttlefish then dynamically adapts the processor's core and uncore frequencies, thereby improving its energy efficiency. The evaluation on a 20-core Intel Xeon processor using a set of widely used OpenMP benchmarks, consisting of several irregular-tasking and work-sharing pragmas, achieves geometric mean energy savings of 19.4% with a 3.6% slowdown. △ Less",
      "url": "https://arxiv.org/abs/2110.00617"
    },
    {
      "title": "One to rule them all: Towards Joint Indic Language Hate Speech Detection",
      "abstract": "This paper is a contribution to the Hate Speech and Offensive Content Identification in Indo-European Languages (HASOC) 2021 shared task. Social media today is a hotbed of toxic and hateful conversations, in various languages. Recent news reports have shown that current models struggle to automatically identify hate posted in minority languages. Therefore, efficiently curbing hate speech is a critical challenge and problem of interest. We present a multilingual architecture using state-of-the-art transformer language models to jointly learn hate and offensive speech detection across three languages namely, English, Hindi, and Marathi. On the provided testing corpora, we achieve Macro F1 scores of 0.7996, 0.7748, 0.8651 for sub-task 1A and 0.6268, 0.5603 during the fine-grained classification of sub-task 1B. These results show the efficacy of exploiting a multilingual training scheme. △ Less",
      "url": "https://arxiv.org/abs/2109.13711"
    },
    {
      "title": "Microstructural engineering of medium entropy NiCo(CrAl) alloy for enhanced room and high-temperature mechanical properties",
      "abstract": "This work demonstrates the development of a strong and ductile medium entropy alloy by employing conventional alloying and thermomechanical processing to induce partial recrystallization (PR) and precipitation strengthening in the microstructure. The combined usage of electron microscopy and atom probe tomography reveals the sequence of microstructural evolution during the process. First, the cold working of homogenized alloy resulted in a highly deformed microstructure. On annealing at 700°C, B2 ordered precipitates heterogeneously nucleate on the highly misoriented sites. These B2 promotes particle stimulated nucleation (PSN) of new recrystallized strain-free grains. The migration of recrystallized grain boundaries leads to discontinuous precipitation of L12 ordered regions in highly dense lamellae structures. Atomic-scale compositional analysis reveals a significant amount of Ni confined to the GB regions between B2 and L12 precipitates, indicating Ni as a rate-controlling element for coarsening the microstructure. On 20 hours of annealing, the alloy comprises a composite microstructure of soft recrystallized and hard non-recrystallized zones, B2 particles at the grain boundaries (GBs), and coherent L12 precipitates inside the grains. The B2 pins the GB movement during recrystallization while the latter provides high strength. The microstructure results in a 0.2% yield stress (YS) value of 1030 MPa with 32% elongation at ambient temperature and retains up to 910 MPa at 670°C. Also, it shows exceptional microstructural stability at 700 °C and resistance to deformation at high temperatures up to 770°C. Examination of deformed microstructure reveals excessive twinning, formation of stacking faults, shearing of L12 precipitates, and accumulation of dislocations at around the B2 precipitates and GBs attributed to high strain hardening of the alloy. △ Less",
      "url": "https://arxiv.org/abs/2109.08894"
    },
    {
      "title": "Quantum speedup for track reconstruction in particle accelerators",
      "abstract": "To investigate the fundamental nature of matter and its interactions, particles are accelerated to very high energies and collided inside detectors, producing a multitude of other particles that are scattered in all directions. As charged particles traverse the detector, they leave signals of their passage. The problem of track reconstruction is to recover the original trajectories from these signals. This challenging data analysis task will become even more demanding as the luminosity of future accelerators increases, leading to collision events with a more complex structure. We identify four fundamental routines present in every local tracking method and analyse how they scale in the context of a standard tracking algorithm. We show that for some of these routines we can reach a lower computational complexity with quantum search algorithms. Although the found quantum speedups are mild, this constitutes, to the best of our knowledge, the first rigorous evidence of a quantum advantage for a high-energy physics data processing task. △ Less",
      "url": "https://arxiv.org/abs/2104.11583"
    },
    {
      "title": "Combining Propositional Logic Based Decision Diagrams with Decision Making in Urban Systems",
      "abstract": "Solving multiagent problems can be an uphill task due to uncertainty in the environment, partial observability, and scalability of the problem at hand. Especially in an urban setting, there are more challenges since we also need to maintain safety for all users while minimizing congestion of the agents as well as their travel times. To this end, we tackle the problem of multiagent pathfinding under uncertainty and partial observability where the agents are tasked to move from their starting points to ending points while also satisfying some constraints, e.g., low congestion, and model it as a multiagent reinforcement learning problem. We compile the domain constraints using propositional logic and integrate them with the RL algorithms to enable fast simulation for RL. △ Less",
      "url": "https://arxiv.org/abs/2011.04405"
    },
    {
      "title": "End to End Binarized Neural Networks for Text Classification",
      "abstract": "Deep neural networks have demonstrated their superior performance in almost every Natural Language Processing task, however, their increasing complexity raises concerns. In particular, these networks require high expenses on computational hardware, and training budget is a concern for many. Even for a trained network, the inference phase can be too demanding for resource-constrained devices, thus limiting its applicability. The state-of-the-art transformer models are a vivid example. Simplifying the computations performed by a network is one way of relaxing the complexity requirements. In this paper, we propose an end to end binarized neural network architecture for the intent classification task. In order to fully utilize the potential of end to end binarization, both input representations (vector embeddings of tokens statistics) and the classifier are binarized. We demonstrate the efficiency of such architecture on the intent classification of short texts over three datasets and for text classification with a larger dataset. The proposed architecture achieves comparable to the state-of-the-art results on standard intent classification datasets while utilizing ~ 20-40% lesser memory and training time. Furthermore, the individual components of the architecture, such as binarized vector embeddings of documents or binarized classifiers, can be used separately with not necessarily fully binary architectures. △ Less",
      "url": "https://arxiv.org/abs/2010.05223"
    },
    {
      "title": "Conversational Semantic Parsing",
      "abstract": "The structured representation for semantic parsing in task-oriented assistant systems is geared towards simple understanding of one-turn queries. Due to the limitations of the representation, the session-based properties such as co-reference resolution and context carryover are processed downstream in a pipelined system. In this paper, we propose a semantic representation for such task-oriented conversational systems that can represent concepts such as co-reference and context carryover, enabling comprehensive understanding of queries in a session. We release a new session-based, compositional task-oriented parsing dataset of 20k sessions consisting of 60k utterances. Unlike Dialog State Tracking Challenges, the queries in the dataset have compositional forms. We propose a new family of Seq2Seq models for the session-based parsing above, which achieve better or comparable performance to the current state-of-the-art on ATIS, SNIPS, TOP and DSTC2. Notably, we improve the best known results on DSTC2 by up to 5 points for slot-carryover. △ Less",
      "url": "https://arxiv.org/abs/2009.13655"
    },
    {
      "title": "Learning Transferable Cooperative Behavior in Multi-Agent Teams",
      "abstract": "While multi-agent interactions can be naturally modeled as a graph, the environment has traditionally been considered as a black box. We propose to create a shared agent-entity graph, where agents and environmental entities form vertices, and edges exist between the vertices which can communicate with each other. Agents learn to cooperate by exchanging messages along the edges of this graph. Our proposed multi-agent reinforcement learning framework is invariant to the number of agents or entities present in the system as well as permutation invariance, both of which are desirable properties for any multi-agent system representation. We present state-of-the-art results on coverage, formation and line control tasks for multi-agent teams in a fully decentralized framework and further show that the learned policies quickly transfer to scenarios with different team sizes along with strong zero-shot generalization performance. This is an important step towards developing multi-agent teams which can be realistically deployed in the real world without assuming complete prior knowledge or instantaneous communication at unbounded distances. △ Less",
      "url": "https://arxiv.org/abs/1906.01202"
    },
    {
      "title": "Resource Constrained Deep Reinforcement Learning",
      "abstract": "In urban environments, supply resources have to be constantly matched to the \"right\" locations (where customer demand is present) so as to improve quality of life. For instance, ambulances have to be matched to base stations regularly so as to reduce response time for emergency incidents in EMS (Emergency Management Systems); vehicles (cars, bikes, scooters etc.) have to be matched to docking stations so as to reduce lost demand in shared mobility systems. Such problem domains are challenging owing to the demand uncertainty, combinatorial action spaces (due to allocation) and constraints on allocation of resources (e.g., total resources, minimum and maximum number of resources at locations and regions). Existing systems typically employ myopic and greedy optimization approaches to optimize allocation of supply resources to locations. Such approaches typically are unable to handle surges or variances in demand patterns well. Recent research has demonstrated the ability of Deep RL methods in adapting well to highly uncertain environments. However, existing Deep RL methods are unable to handle combinatorial action spaces and constraints on allocation of resources. To that end, we have developed three approaches on top of the well known actor critic approach, DDPG (Deep Deterministic Policy Gradient) that are able to handle constraints on resource allocation. More importantly, we demonstrate that they are able to outperform leading approaches on simulators validated on semi-real and real data sets. △ Less",
      "url": "https://arxiv.org/abs/1812.00600"
    },
    {
      "title": "Better Safe than Sorry: Evidence Accumulation Allows for Safe Reinforcement Learning",
      "abstract": "In the real world, agents often have to operate in situations with incomplete information, limited sensing capabilities, and inherently stochastic environments, making individual observations incomplete and unreliable. Moreover, in many situations it is preferable to delay a decision rather than run the risk of making a bad decision. In such situations it is necessary to aggregate information before taking an action; however, most state of the art reinforcement learning (RL) algorithms are biased towards taking actions \\textit{at every time step}, even if the agent is not particularly confident in its chosen action. This lack of caution can lead the agent to make critical mistakes, regardless of prior experience and acclimation to the environment. Motivated by theories of dynamic resolution of uncertainty during decision making in biological brains, we propose a simple accumulator module which accumulates evidence in favor of each possible decision, encodes uncertainty as a dynamic competition between actions, and acts on the environment only when it is sufficiently confident in the chosen action. The agent makes no decision by default, and the burden of proof to make a decision falls on the policy to accrue evidence strongly in favor of a single decision. Our results show that this accumulator module achieves near-optimal performance on a simple guessing game, far outperforming deep recurrent networks using traditional, forced action selection policies. △ Less",
      "url": "https://arxiv.org/abs/1809.09147"
    },
    {
      "title": "Where The Light Gets In: Analyzing Web Censorship Mechanisms in India",
      "abstract": "This paper presents a detailed study of the Internet censorship in India. We consolidated a list of potentially blocked websites from various public sources to assess censorship mechanisms used by nine major ISPs. To begin with, we demonstrate that existing censorship detection tools like OONI are grossly inaccurate. We thus developed various techniques and heuristics to correctly assess censorship and study the underlying mechanism involved in these ISPs. At every step we corroborated our finding manually to test the efficacy of our approach, a step largely ignored by others. We fortify our findings by adjudging the coverage and consistency of censorship infrastructure, broadly in terms of average number of network paths and requested domains the infrastructure surveils. Our results indicate a clear disparity among the ISPs, on how they install censorship infrastructure. For instance, in Idea network we observed the censorious middleboxes on over 90% of our tested intra-AS paths whereas for Vodafone, it is as low as 2.5%. We conclude our research by devising our own novel anti-censorship strategies, that does not depend on third party tools (like proxies, Tor and VPNs etc.). We managed to anti-censor all blocked websites in all ISPs under test. △ Less",
      "url": "https://arxiv.org/abs/1808.01708"
    },
    {
      "title": "Policy Gradient With Value Function Approximation For Collective Multiagent Planning",
      "abstract": "Decentralized (PO)MDPs provide an expressive framework for sequential decision making in a multiagent system. Given their computational complexity, recent research has focused on tractable yet practical subclasses of Dec-POMDPs. We address such a subclass called CDEC-POMDP where the collective behavior of a population of agents affects the joint-reward and environment dynamics. Our main contribution is an actor-critic (AC) reinforcement learning method for optimizing CDEC-POMDP policies. Vanilla AC has slow convergence for larger problems. To address this, we show how a particular decomposition of the approximate action-value function over agents leads to effective updates, and also derive a new way to train the critic based on local reward signals. Comparisons on a synthetic benchmark and a real-world taxi fleet optimization problem show that our new AC approach provides better quality solutions than previous best approaches. △ Less",
      "url": "https://arxiv.org/abs/1804.02884"
    },
    {
      "title": "Solving Inverse Computational Imaging Problems using Deep Pixel-level Prior",
      "abstract": "Signal reconstruction is a challenging aspect of computational imaging as it often involves solving ill-posed inverse problems. Recently, deep feed-forward neural networks have led to state-of-the-art results in solving various inverse imaging problems. However, being task specific, these networks have to be learned for each inverse problem. On the other hand, a more flexible approach would be to learn a deep generative model once and then use it as a signal prior for solving various inverse problems. We show that among the various state of the art deep generative models, autoregressive models are especially suitable for our purpose for the following reasons. First, they explicitly model the pixel level dependencies and hence are capable of reconstructing low-level details such as texture patterns and edges better. Second, they provide an explicit expression for the image prior which can then be used for MAP based inference along with the forward model. Third, they can model long range dependencies in images which make them ideal for handling global multiplexing as encountered in various compressive imaging systems. We demonstrate the efficacy of our proposed approach in solving three computational imaging problems: Single Pixel Camera (SPC), LiSens and FlatCam. For both real and simulated cases, we obtain better reconstructions than the state-of-the-art methods in terms of perceptual and quantitative metrics. △ Less",
      "url": "https://arxiv.org/abs/1802.09850"
    },
    {
      "title": "Compressive Image Recovery Using Recurrent Generative Model",
      "abstract": "Reconstruction of signals from compressively sensed measurements is an ill-posed problem. In this paper, we leverage the recurrent generative model, RIDE, as an image prior for compressive image reconstruction. Recurrent networks can model long-range dependencies in images and hence are suitable to handle global multiplexing in reconstruction from compressive imaging. We perform MAP inference with RIDE using back-propagation to the inputs and projected gradient method. We propose an entropy thresholding based approach for preserving texture in images well. Our approach shows superior reconstructions compared to recent global reconstruction approaches like D-AMP and TVAL3 on both simulated and real data. △ Less",
      "url": "https://arxiv.org/abs/1612.04229"
    },
    {
      "title": "Robust Optimization for Tree-Structured Stochastic Network Design",
      "abstract": "Stochastic network design is a general framework for optimizing network connectivity. It has several applications in computational sustainability including spatial conservation planning, pre-disaster network preparation, and river network optimization. A common assumption in previous work has been made that network parameters (e.g., probability of species colonization) are precisely known, which is unrealistic in real- world settings. We therefore address the robust river network design problem where the goal is to optimize river connectivity for fish movement by removing barriers. We assume that fish passability probabilities are known only imprecisely, but are within some interval bounds. We then develop a planning approach that computes the policies with either high robust ratio or low regret. Empirically, our approach scales well to large river networks. We also provide insights into the solutions generated by our robust approach, which has significantly higher robust ratio than the baseline solution with mean parameter estimates. △ Less",
      "url": "https://arxiv.org/abs/1612.00104"
    }
  ]
}