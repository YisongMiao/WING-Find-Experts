{
  "author": "Hadi Hosseini",
  "results": [
    {
      "title": "MEENA (PersianMMMU): Multimodal-Multilingual Educational Exams for N-level Assessment",
      "abstract": "Recent advancements in large vision-language models (VLMs) have primarily focused on English, with limited attention given to other languages. To address this gap, we introduce MEENA (also known as PersianMMMU), the first dataset designed to evaluate Persian VLMs across scientific, reasoning, and human-level understanding tasks. Our dataset comprises approximately 7,500 Persian and 3,000 English questions, covering a wide range of topics such as reasoning, mathematics, physics, diagrams, charts, and Persian art and literature. Key features of MEENA include: (1) diverse subject coverage spanning various educational levels, from primary to upper secondary school, (2) rich metadata, including difficulty levels and descriptive answers, (3) original Persian data that preserves cultural nuances, (4) a bilingual structure to assess cross-linguistic performance, and (5) a series of diverse experiments assessing various capabilities, including overall performance, the model's ability to attend to images, and its tendency to generate hallucinations. We hope this benchmark contributes to enhancing VLM capabilities beyond English. △ Less",
      "url": "https://arxiv.org/abs/2508.17290"
    },
    {
      "title": "Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities",
      "abstract": "In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. In addition to its incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that excels at multimodal understanding and it is now able to process up to 3 hours of video content. Its unique combination of long context, multimodal and reasoning capabilities can be combined to unlock new agentic workflows. Gemini 2.5 Flash provides excellent reasoning abilities at a fraction of the compute and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high performance at low latency and cost. Taken together, the Gemini 2.X model generation spans the full Pareto frontier of model capability vs cost, allowing users to explore the boundaries of what is possible with complex agentic problem solving. △ Less",
      "url": "https://arxiv.org/abs/2507.06261"
    },
    {
      "title": "Matching Markets Meet LLMs: Algorithmic Reasoning with Ranked Preferences",
      "abstract": "The rise of Large Language Models (LLMs) has driven progress in reasoning tasks -- from program synthesis to scientific hypothesis generation -- yet their ability to handle ranked preferences and structured algorithms in combinatorial domains remains underexplored. We study matching markets, a core framework behind applications like resource allocation and ride-sharing, which require reconciling individual ranked preferences to ensure stable outcomes. We evaluate several state-of-the-art models on a hierarchy of preference-based reasoning tasks -- ranging from stable-matching generation to instability detection, instability resolution, and fine-grained preference queries -- to systematically expose their logical and algorithmic limitations in handling ranked inputs. Surprisingly, even top-performing models with advanced reasoning struggle to resolve instability in large markets, often failing to identify blocking pairs or execute algorithms iteratively. We further show that parameter-efficient fine-tuning (LoRA) significantly improves performance in small markets, but fails to bring about a similar improvement on large instances, suggesting the need for more sophisticated strategies to improve LLMs' reasoning with larger-context inputs. △ Less",
      "url": "https://arxiv.org/abs/2506.04478"
    },
    {
      "title": "Two-Sided Manipulation Games in Stable Matching Markets",
      "abstract": "The Deferred Acceptance (DA) algorithm is an elegant procedure for finding a stable matching in two-sided matching markets. It ensures that no pair of agents prefers each other to their matched partners. In this work, we initiate the study of two-sided manipulations in matching markets as non-cooperative games. We introduce the accomplice manipulation game, where a man misreports to help a specific woman obtain a better partner, whenever possible. We provide a polynomial time algorithm for finding a pure strategy Nash equilibrium (NE) and show that our algorithm always yields a stable matching - although not every Nash equilibrium corresponds to a stable matching. Additionally, we show how our analytical techniques for the accomplice manipulation game can be applied to other manipulation games in matching markets, such as one-for-many and the standard self-manipulation games. We complement our theoretical findings with empirical evaluations of different properties of the resulting NE, such as the welfare of the agents. △ Less",
      "url": "https://arxiv.org/abs/2506.00554"
    },
    {
      "title": "Who Gets the Kidney? Human-AI Alignment, Indecision, and Moral Values",
      "abstract": "The rapid integration of Large Language Models (LLMs) in high-stakes decision-making -- such as allocating scarce resources like donor organs -- raises critical questions about their alignment with human moral values. We systematically evaluate the behavior of several prominent LLMs against human preferences in kidney allocation scenarios and show that LLMs: i) exhibit stark deviations from human values in prioritizing various attributes, and ii) in contrast to humans, LLMs rarely express indecision, opting for deterministic decisions even when alternative indecision mechanisms (e.g., coin flipping) are provided. Nonetheless, we show that low-rank supervised fine-tuning with few samples is often effective in improving both decision consistency and calibrating indecision modeling. These findings illustrate the necessity of explicit alignment strategies for LLMs in moral/ethical domains. △ Less",
      "url": "https://arxiv.org/abs/2506.00079"
    },
    {
      "title": "Bridging Theory and Perception in Fair Division: A Study on Comparative and Fair Share Notions",
      "abstract": "The allocation of resources among multiple agents is a fundamental problem in both economics and computer science. In these settings, fairness plays a crucial role in ensuring social acceptability and practical implementation of resource allocation algorithms. Traditional fair division solutions have given rise to a variety of approximate fairness notions, often as a response to the challenges posed by non-existence or computational intractability of exact solutions. However, the inherent incompatibility among these notions raises a critical question: which concept of fairness is most suitable for practical applications? In this paper, we examine two broad frameworks -- threshold-based and comparison-based fairness notions -- and evaluate their perceived fairness through a comprehensive human subject study. Our findings uncover novel insights into the interplay between perception of fairness, theoretical guarantees, the role of externalities and subjective valuations, and underlying cognitive processes, shedding light on the theory and practice of fair division. △ Less",
      "url": "https://arxiv.org/abs/2505.10433"
    },
    {
      "title": "AI Education in a Mirror: Challenges Faced by Academic and Industry Experts",
      "abstract": "As Artificial Intelligence (AI) technologies continue to evolve, the gap between academic AI education and real-world industry challenges remains an important area of investigation. This study provides preliminary insights into challenges AI professionals encounter in both academia and industry, based on semi-structured interviews with 14 AI experts - eight from industry and six from academia. We identify key challenges related to data quality and availability, model scalability, practical constraints, user behavior, and explainability. While both groups experience data and model adaptation difficulties, industry professionals more frequently highlight deployment constraints, resource limitations, and external dependencies, whereas academics emphasize theoretical adaptation and standardization issues. These exploratory findings suggest that AI curricula could better integrate real-world complexities, software engineering principles, and interdisciplinary learning, while recognizing the broader educational goals of building foundational and ethical reasoning skills. △ Less",
      "url": "https://arxiv.org/abs/2505.02856"
    },
    {
      "title": "The Algorithmic Landscape of Fair and Efficient Distribution of Delivery Orders in the Gig Economy",
      "abstract": "Distributing services, goods, and tasks in the gig economy heavily relies upon on-demand workers (aka agents), leading to new challenges varying from logistics optimization to the ethical treatment of gig workers. We focus on fair and efficient distribution of delivery tasks -- placed on the vertices of a graph -- among a fixed set of agents. We consider the fairness notion of minimax share (MMS), which aims to minimize the maximum (submodular) cost among agents and is particularly appealing in applications without monetary transfers. We propose a novel efficiency notion -- namely non-wastefulness -- that is desirable in a wide range of scenarios and, more importantly, does not suffer from computational barriers. Specifically, given a distribution of tasks, we can, in polynomial time, i) verify whether the distribution is non-wasteful and ii) turn it into an equivalent non-wasteful distribution. Moreover, we investigate several fixed-parameter tractable and polynomial-time algorithms and paint a complete picture of the (parameterized) complexity of finding fair and efficient distributions of tasks with respect to both the structure of the topology and natural restrictions of the input. Finally, we highlight how our findings shed light on computational aspects of other well-studied fairness notions, such as envy-freeness and its relaxations. △ Less",
      "url": "https://arxiv.org/abs/2503.16002"
    },
    {
      "title": "T2I-FineEval: Fine-Grained Compositional Metric for Text-to-Image Evaluation",
      "abstract": "Although recent text-to-image generative models have achieved impressive performance, they still often struggle with capturing the compositional complexities of prompts including attribute binding, and spatial relationships between different entities. This misalignment is not revealed by common evaluation metrics such as CLIPScore. Recent works have proposed evaluation metrics that utilize Visual Question Answering (VQA) by decomposing prompts into questions about the generated image for more robust compositional evaluation. Although these methods align better with human evaluations, they still fail to fully cover the compositionality within the image. To address this, we propose a novel metric that breaks down images into components, and texts into fine-grained questions about the generated image for evaluation. Our method outperforms previous state-of-the-art metrics, demonstrating its effectiveness in evaluating text-to-image generative models. Code is available at https://github.com/hadi-hosseini/ T2I-FineEval. △ Less",
      "url": "https://arxiv.org/abs/2503.11481"
    },
    {
      "title": "Fine-Grained Alignment and Noise Refinement for Compositional Text-to-Image Generation",
      "abstract": "Text-to-image generative models have made significant advancements in recent years; however, accurately capturing intricate details in textual prompts-such as entity missing, attribute binding errors, and incorrect relationships remains a formidable challenge. In response, we present an innovative, training-free method that directly addresses these challenges by incorporating tailored objectives to account for textual constraints. Unlike layout-based approaches that enforce rigid structures and limit diversity, our proposed approach offers a more flexible arrangement of the scene by imposing just the extracted constraints from the text, without any unnecessary additions. These constraints are formulated as losses-entity missing, entity mixing, attribute binding, and spatial relationships-integrated into a unified loss that is applied in the first generation stage. Furthermore, we introduce a feedback-driven system for fine-grained initial noise refinement. This system integrates a verifier that evaluates the generated image, identifies inconsistencies, and provides corrective feedback. Leveraging this feedback, our refinement method first targets the unmet constraints by refining the faulty attention maps caused by initial noise, through the optimization of selective losses associated with these constraints. Subsequently, our unified loss function is reapplied to proceed the second generation phase. Experimental results demonstrate that our method, relying solely on our proposed objective functions, significantly enhances compositionality, achieving a 24% improvement in human evaluation and a 25% gain in spatial relationships. Furthermore, our fine-grained noise refinement proves effective, boosting performance by up to 5%. Code is available at \\href{https://github.com/hadi-hosseini/noise-refinement}{https://github.com/hadi-hosseini/noise-refinement}. △ Less",
      "url": "https://arxiv.org/abs/2503.06506"
    },
    {
      "title": "CER: Confidence Enhanced Reasoning in LLMs",
      "abstract": "Ensuring the reliability of Large Language Models (LLMs) in complex reasoning tasks remains a formidable challenge, particularly in scenarios that demand precise mathematical calculations and knowledge-intensive open-domain generation. In this work, we introduce an uncertainty-aware framework designed to enhance the accuracy of LLM responses by systematically incorporating model confidence at critical decision points. We propose an approach that encourages multi-step reasoning in LLMs and quantify the confidence of intermediate answers such as numerical results in mathematical reasoning and proper nouns in open-domain generation. Then, the overall confidence of each reasoning chain is evaluated based on confidence of these critical intermediate steps. Finally, we aggregate the answer of generated response paths in a way that reflects the reliability of each generated content (as opposed to self-consistency in which each generated chain contributes equally to majority voting). We conducted extensive experiments in five datasets, three mathematical datasets and two open-domain datasets, using four LLMs. The results consistently validate the effectiveness of our novel confidence aggregation method, leading to an accuracy improvement of up to 7.4% and 5.8% over baseline approaches in math and open-domain generation tasks, respectively. Code is publicly available at https://github.com/ Aquasar11/CER. △ Less",
      "url": "https://arxiv.org/abs/2502.14634"
    },
    {
      "title": "Distributive Fairness in Large Language Models: Evaluating Alignment with Human Values",
      "abstract": "The growing interest in employing large language models (LLMs) for decision-making in social and economic contexts has raised questions about their potential to function as agents in these domains. A significant number of societal problems involve the distribution of resources, where fairness, along with economic efficiency, play a critical role in the desirability of outcomes. In this paper, we examine whether LLM responses adhere to fundamental fairness concepts such as equitability, envy-freeness, and Rawlsian maximin, and investigate their alignment with human preferences. We evaluate the performance of several LLMs, providing a comparative benchmark of their ability to reflect these measures. Our results demonstrate a lack of alignment between current LLM responses and human distributional preferences. Moreover, LLMs are unable to utilize money as a transferable resource to mitigate inequality. Nonetheless, we demonstrate a stark contrast when (some) LLMs are tasked with selecting from a predefined menu of options rather than generating one. In addition, we analyze the robustness of LLM responses to variations in semantic factors (e.g. intentions or personas) or non-semantic prompting changes (e.g. templates or orderings). Finally, we highlight potential strategies aimed at enhancing the alignment of LLM behavior with well-established fairness concepts. △ Less",
      "url": "https://arxiv.org/abs/2502.00313"
    },
    {
      "title": "Water Flow Detection Device Based on Sound Data Analysis and Machine Learning to Detect Water Leakage",
      "abstract": "In this paper, we introduce a novel mechanism that uses machine learning techniques to detect water leaks in pipes. The proposed simple and low-cost mechanism is designed that can be easily installed on building pipes with various sizes. The system works based on gathering and amplifying water flow signals using a mechanical sound amplifier. Then sounds are recorded and converted to digital signals in order to be analyzed. After feature extraction and selection, deep neural networks are used to discriminate between with and without leak pipes. The experimental results show that this device can detect at least 100 milliliters per minute (mL/min) of water flow in a pipe so that it can be used as a core of a water leakage detection system. △ Less",
      "url": "https://arxiv.org/abs/2501.11151"
    },
    {
      "title": "Equitable Allocations of Mixtures of Goods and Chores",
      "abstract": "Equitable allocation of indivisible items involves partitioning the items among agents such that everyone derives (almost) equal utility. We consider the approximate notion of \\textit{equitability up to one item} (EQ1) and focus on the settings containing mixtures of items (goods and chores), where an agent may derive positive, negative, or zero utility from an item. We first show that -- in stark contrast to the goods-only and chores-only settings -- an EQ1 allocation may not exist even for additive $\\{-1,1\\}$ bivalued instances, and its corresponding decision problem is computationally intractable. We focus on a natural domain of normalized valuations where the value of the entire set of items is constant for all agents. On the algorithmic side, we show that an EQ1 allocation can be computed efficiently for (i) $\\{-1, 0, 1\\}$ normalized valuations, (ii) objective but non-normalized valuations, (iii) two agents with type-normalized valuations. Previously, EQX allocations were known to exist only for 2 agents and objective valuations, while the case of subjective valuations remained computationally intractable even with two agents. We make progress by presenting an efficient algorithm that outputs an EQX allocation for $\\{-1,1\\}$ normalized subjective valuations for any number of agents. We complement our study by providing a comprehensive picture of achieving EQ1 allocations in conjunction with economic efficiency notions such as Pareto optimality and social welfare. △ Less",
      "url": "https://arxiv.org/abs/2501.06799"
    },
    {
      "title": "Strategyproof Matching of Roommates and Rooms",
      "abstract": "We initiate the study of matching roommates and rooms wherein the preferences of agents over other agents and rooms are complementary and represented by Leontief utilities. In this setting, 2n agents must be paired up and assigned to n rooms. Each agent has cardinal valuations over the rooms as well as compatibility values over all other agents. Under Leontief preferences, an agents utility for a matching is the minimum of the two values. We focus on the tradeoff between maximizing utilitarian social welfare and strategyproofness. Our main result shows that, in a stark contrast to the additive case, under binary Leontief utilities, there exist strategyproof mechanisms that maximize the social welfare. We further devise a strategyproof mechanism that implements such a welfare maximizing algorithm and is parameterized by the number of agents. Along the way, we highlight several possibility and impossibility results, and give upper bounds and lower bounds for welfare with or without strategyproofness. △ Less",
      "url": "https://arxiv.org/abs/2412.13887"
    },
    {
      "title": "Bandit Learning in Matching Markets: Utilitarian and Rawlsian Perspectives",
      "abstract": "Two-sided matching markets have demonstrated significant impact in many real-world applications, including school choice, medical residency placement, electric vehicle charging, ride sharing, and recommender systems. However, traditional models often assume that preferences are known, which is not always the case in modern markets, where preferences are unknown and must be learned. For example, a company may not know its preference over all job applicants a priori in online markets. Recent research has modeled matching markets as multi-armed bandit (MAB) problem and primarily focused on optimizing matching for one side of the market, while often resulting in a pessimal solution for the other side. In this paper, we adopt a welfarist approach for both sides of the market, focusing on two metrics: (1) Utilitarian welfare and (2) Rawlsian welfare, while maintaining market stability. For these metrics, we propose algorithms based on epoch Explore-Then-Commit (ETC) and analyze their regret bounds. Finally, we conduct simulated experiments to evaluate both welfare and market stability. △ Less",
      "url": "https://arxiv.org/abs/2412.00301"
    },
    {
      "title": "Surprisingly Popular Voting for Concentric Rank-Order Models",
      "abstract": "An important problem on social information sites is the recovery of ground truth from individual reports when the experts are in the minority. The wisdom of the crowd, i.e. the collective opinion of a group of individuals fails in such a scenario. However, the surprisingly popular (SP) algorithm~\\cite{prelec2017solution} can recover the ground truth even when the experts are in the minority, by asking the individuals to report additional prediction reports--their beliefs about the reports of others. Several recent works have extended the surprisingly popular algorithm to an equivalent voting rule (SP-voting) to recover the ground truth ranking over a set of $m$ alternatives. However, we are yet to fully understand when SP-voting can recover the ground truth ranking, and if so, how many samples (votes and predictions) it needs. We answer this question by proposing two rank-order models and analyzing the sample complexity of SP-voting under these models. In particular, we propose concentric mixtures of Mallows and Plackett-Luce models with $G (\\ge 2)$ groups. Our models generalize previously proposed concentric mixtures of Mallows models with $2$ groups, and we highlight the importance of $G > 2$ groups by identifying three distinct groups (expert, intermediate, and non-expert) from existing datasets. Next, we provide conditions on the parameters of the underlying models so that SP-voting can recover ground-truth rankings with high probability, and also derive sample complexities under the same. We complement the theoretical results by evaluating SP-voting on simulated and real datasets. △ Less",
      "url": "https://arxiv.org/abs/2411.08367"
    },
    {
      "title": "Putting Gale & Shapley to Work: Guaranteeing Stability Through Learning",
      "abstract": "Two-sided matching markets describe a large class of problems wherein participants from one side of the market must be matched to those from the other side according to their preferences. In many real-world applications (e.g. content matching or online labor markets), the knowledge about preferences may not be readily available and must be learned, i.e., one side of the market (aka agents) may not know their preferences over the other side (aka arms). Recent research on online settings has focused primarily on welfare optimization aspects (i.e. minimizing the overall regret) while paying little attention to the game-theoretic properties such as the stability of the final matching. In this paper, we exploit the structure of stable solutions to devise algorithms that improve the likelihood of finding stable solutions. We initiate the study of the sample complexity of finding a stable matching, and provide theoretical bounds on the number of samples needed to reach a stable matching with high probability. Finally, our empirical results demonstrate intriguing tradeoffs between stability and optimality of the proposed algorithms, further complementing our theoretical findings. △ Less",
      "url": "https://arxiv.org/abs/2410.04376"
    },
    {
      "title": "Feature-to-Image Data Augmentation: Improving Model Feature Extraction with Cluster-Guided Synthetic Samples",
      "abstract": "One of the growing trends in machine learning is the use of data generation techniques, since the performance of machine learning models is dependent on the quantity of the training dataset. However, in many real-world applications, particularly in medical and low-resource domains, collecting large datasets is challenging due to resource constraints, which leads to overfitting and poor generalization. This study introduces FICAug, a novel feature-to-image data augmentation framework designed to improve model generalization under limited data conditions by generating structured synthetic samples. FICAug first operates in the feature space, where original data are clustered using the k-means algorithm. Within pure-label clusters, synthetic data are generated through Gaussian sampling to increase diversity while maintaining label consistency. These synthetic features are then projected back into the image domain using a generative neural network, and a convolutional neural network is trained on the reconstructed images to learn enhanced representations. Experimental results demonstrate that FICAug significantly improves classification accuracy. In feature space, it achieved a cross-validation accuracy of 84.09%, while training a ResNet-18 model on the reconstructed images further boosted performance to 88.63%, illustrating the effectiveness of the proposed framework in extracting new and task-relevant features. △ Less",
      "url": "https://arxiv.org/abs/2409.17685"
    },
    {
      "title": "The Degree of Fairness in Efficient House Allocation",
      "abstract": "The classic house allocation problem is primarily concerned with finding a matching between a set of agents and a set of houses that guarantees some notion of economic efficiency (e.g. utilitarian welfare). While recent works have shifted focus on achieving fairness (e.g. minimizing the number of envious agents), they often come with notable costs on efficiency notions such as utilitarian or egalitarian welfare. We investigate the trade-offs between these welfare measures and several natural fairness measures that rely on the number of envious agents, the total (aggregate) envy of all agents, and maximum total envy of an agent. In particular, by focusing on envy-free allocations, we first show that, should one exist, finding an envy-free allocation with maximum utilitarian or egalitarian welfare is computationally tractable. We highlight a rather stark contrast between utilitarian and egalitarian welfare by showing that finding utilitarian welfare maximizing allocations that minimize the aforementioned fairness measures can be done in polynomial time while their egalitarian counterparts remain intractable (for the most part) even under binary valuations. We complement our theoretical findings by giving insights into the relationship between the different fairness measures and conducting empirical analysis. △ Less",
      "url": "https://arxiv.org/abs/2407.04664"
    },
    {
      "title": "The Surprising Effectiveness of SP Voting with Partial Preferences",
      "abstract": "We consider the problem of recovering the ground truth ordering (ranking, top-$k$, or others) over a large number of alternatives. The wisdom of crowd is a heuristic approach based on Condorcet's Jury theorem to address this problem through collective opinions. This approach fails to recover the ground truth when the majority of the crowd is misinformed. The surprisingly popular (SP) algorithm cite{prelec2017solution} is an alternative approach that is able to recover the ground truth even when experts are in minority. The SP algorithm requires the voters to predict other voters' report in the form of a full probability distribution over all rankings of alternatives. However, when the number of alternatives, $m$, is large, eliciting the prediction report or even the vote over $m$ alternatives might be too costly. In this paper, we design a scalable alternative of the SP algorithm which only requires eliciting partial preferences from the voters, and propose new variants of the SP algorithm. In particular, we propose two versions -- Aggregated-SP and Partial-SP -- that ask voters to report vote and prediction on a subset of size $k$ ($\\ll m$) in terms of top alternative, partial rank, or an approval set. Through a large-scale crowdsourcing experiment on MTurk, we show that both of our approaches outperform conventional preference aggregation algorithms for the recovery of ground truth rankings, when measured in terms of Kendall-Tau distance and Spearman's $ρ$. We further analyze the collected data and demonstrate that voters' behavior in the experiment, including the minority of the experts, and the SP phenomenon, can be correctly simulated by a concentric mixtures of Mallows model. Finally, we provide theoretical bounds on the sample complexity of SP algorithms with partial rankings to demonstrate the theoretical guarantees of the proposed methods. △ Less",
      "url": "https://arxiv.org/abs/2406.00870"
    },
    {
      "title": "Almost Envy-Freeness under Weakly Lexicographic Preferences",
      "abstract": "In fair division of indivisible items, domain restriction has played a key role in escaping from negative results and providing structural insights into the computational and axiomatic boundaries of fairness. One notable subdomain of additive preferences, the lexicographic domain, has yielded several positive results in dealing with goods, chores, and mixtures thereof. However, the majority of work within this domain primarily consider strict linear orders over items, which do not allow the modeling of more expressive preferences that contain indifferences (ties). We investigate the most prominent fairness notions of envy-freeness up to any (EFX) or some (EF1) item under weakly lexicographic preferences. For the goods-only setting, we develop an algorithm that can be customized to guarantee EF1, EFX, maximin share (MMS), or a combination thereof, along the efficiency notion of Pareto optimality (PO). From the conceptual perspective, we propose techniques such as preference graphs and potential envy that are independently of interest when dealing with ties. Finally, we demonstrate challenges in dealing with chores and highlight key algorithmic and axiomatic differences of finding EFX solutions with the goods-only setting. Nevertheless, we show that there is an algorithm that always returns an EF1 and PO allocation for the chores-only instances. △ Less",
      "url": "https://arxiv.org/abs/2404.19740"
    },
    {
      "title": "Effects of Li Doping on Superconducting Properties of Citrate Gel Prepared Y1-xLixBa2Cu3O7-d Compound",
      "abstract": "The Y1-xLixBa2Cu3O7-d polycrystalline bulk superconductors doped with Li substituting at the Y site at different concentrations (x = 0, 0.01, 0.02, 0.1) were prepared using the citrate-gel method to study the effects of doping on the superconducting temperature and critical current density. The question was whether Li addition characterized by a high Debye frequency would have any positive effects on Tc. The optimum citrate-gel and heat treatment conditions were identified as those yielding samples with a maximum grain size on the order of 50 micro.m (up to the optimum Li-doping level, x=0.01). Li substitution at the Y site was verified by structural, electrical, and magnetic measurements of the produced samples, whereas X-ray diffraction (XRD) analysis revealed the formation of a pure phase with no visible impurity phases. Moreover, AC magnetic susceptibility measurements showed no increase in the superconducting transition temperature Tc, consistent with the predicted results obtained by machine learning method, although it was theoretically expected to increase owing to the high Debye frequency of Li. This observation is consistent with magnetic coupling models for pairing mechanism in cuprates. Finally, because of the optimum conditions of the preparation procedure, nearly identical values of the critical current density (Jc) were recorded for samples with different Li doping levels (up to the optimum Li doping level). It was found that improved compound preparation conditions would have a critical and extensive effect on Jc enhancement, with nearly no Tc suppression. △ Less",
      "url": "https://arxiv.org/abs/2401.03471"
    },
    {
      "title": "The Fairness Fair: Bringing Human Perception into Collective Decision-Making",
      "abstract": "Fairness is one of the most desirable societal principles in collective decision-making. It has been extensively studied in the past decades for its axiomatic properties and has received substantial attention from the multiagent systems community in recent years for its theoretical and computational aspects in algorithmic decision-making. However, these studies are often not sufficiently rich to capture the intricacies of human perception of fairness in the ambivalent nature of the real-world problems. We argue that not only fair solutions should be deemed desirable by social planners (designers), but they should be governed by human and societal cognition, consider perceived outcomes based on human judgement, and be verifiable. We discuss how achieving this goal requires a broad transdisciplinary approach ranging from computing and AI to behavioral economics and human-AI interaction. In doing so, we identify shortcomings and long-term challenges of the current literature of fair division, describe recent efforts in addressing them, and more importantly, highlight a series of open research directions. △ Less",
      "url": "https://arxiv.org/abs/2312.14402"
    },
    {
      "title": "Tight Approximations for Graphical House Allocation",
      "abstract": "The Graphical House Allocation problem asks: how can $n$ houses (each with a fixed non-negative value) be assigned to the vertices of an undirected graph $G$, so as to minimize the \"aggregate local envy\", i.e., the sum of absolute differences along the edges of $G$? This problem generalizes the classical Minimum Linear Arrangement problem, as well as the well-known House Allocation Problem from Economics, the latter of which has notable practical applications in organ exchanges. Recent work has studied the computational aspects of Graphical House Allocation and observed that the problem is NP-hard and inapproximable even on particularly simple classes of graphs, such as vertex disjoint unions of paths. However, the dependence of any approximations on the structural properties of the underlying graph had not been studied. In this work, we give a complete characterization of the approximability of the Graphical House Allocation problem. We present algorithms to approximate the optimal envy on general graphs, trees, planar graphs, bounded-degree graphs, bounded-degree planar graphs, and bounded-degree trees. For each of these graph classes, we then prove matching lower bounds, showing that in each case, no significant improvement can be attained unless P = NP. We also present general approximation ratios as a function of structural parameters of the underlying graph, such as treewidth; these match the aforementioned tight upper bounds in general, and are significantly better approximations for many natural subclasses of graphs. Finally, we present constant factor approximation schemes for the special classes of complete binary trees and random graphs. △ Less",
      "url": "https://arxiv.org/abs/2307.12482"
    },
    {
      "title": "Distribution of Chores with Information Asymmetry",
      "abstract": "A well-regarded fairness notion when dividing indivisible chores is envy-freeness up to one item (EF1), which requires that pairwise envy can be eliminated by the removal of a single item. While an EF1 and Pareto optimal (PO) allocation of goods can always be found via well-known algorithms, even the existence of such solutions for chores remains open, to date. We take an epistemic approach utilizing information asymmetry by introducing dubious chores--items that inflict no cost on receiving agents but are perceived costly by others. On a technical level, dubious chores provide a more fine-grained approximation of envy-freeness than EF1. We show that finding allocations with minimal number of dubious chores is computationally hard. Nonetheless, we prove the existence of envy-free and fractional PO allocations for $n$ agents with only $2n-2$ dubious chores and strengthen it to $n-1$ dubious chores in four special classes of valuations. Our experimental analysis demonstrates that often only a few dubious chores are needed to achieve envy-freeness. △ Less",
      "url": "https://arxiv.org/abs/2305.02986"
    },
    {
      "title": "Fairly Allocating Goods and (Terrible) Chores",
      "abstract": "We study the fair allocation of mixtures of indivisible goods and chores under lexicographic preferences$\\unicode{x2014}$a subdomain of additive preferences. A prominent fairness notion for allocating indivisible items is envy-freeness up to any item (EFX). Yet, its existence and computation has remained a notable open problem. By identifying a class of instances with \"terrible chores\", we show that determining the existence of an EFX allocation is NP-complete. This result immediately implies the intractability of EFX under additive preferences. Nonetheless, we propose a natural subclass of lexicographic preferences for which an EFX and Pareto optimal (PO) allocation is guaranteed to exist and can be computed efficiently for any mixed instance. Focusing on two weaker fairness notions, we investigate finding EF1 and PO allocations for special instances with terrible chores, and show that MMS and PO allocations can be computed efficiently for any mixed instance with lexicographic preferences. △ Less",
      "url": "https://arxiv.org/abs/2305.01786"
    },
    {
      "title": "Fair Distribution of Delivery Orders",
      "abstract": "We initiate the study of fair distribution of delivery tasks among a set of agents wherein delivery jobs are placed along the vertices of a graph. Our goal is to fairly distribute delivery costs (modeled as a submodular function) among a fixed set of agents while satisfying some desirable notions of economic efficiency. We adopt well-established fairness concepts -- such as envy-freeness up to one item (EF1) and minimax share (MMS) -- to our setting and show that fairness is often incompatible with the efficiency notion of social optimality. We then characterize instances that admit fair and socially optimal solutions by exploiting graph structures. We further show that achieving fairness along with Pareto optimality is computationally intractable. We complement this by designing an XP algorithm (parameterized by the number of agents) for finding MMS and Pareto optimal solutions on every tree instance, and show that the same algorithm can be modified to find efficient solutions along with EF1, when such solutions exist. The latter crucially relies on an intriguing result that in our setting EF1 and Pareto optimality jointly imply MMS. We conclude by theoretically and experimentally analyzing the price of fairness. △ Less",
      "url": "https://arxiv.org/abs/2305.00040"
    },
    {
      "title": "Graphical House Allocation",
      "abstract": "The classical house allocation problem involves assigning $n$ houses (or items) to $n$ agents according to their preferences. A key criterion in such problems is satisfying some fairness constraints such as envy-freeness. We consider a generalization of this problem wherein the agents are placed along the vertices of a graph (corresponding to a social network), and each agent can only experience envy towards its neighbors. Our goal is to minimize the aggregate envy among the agents as a natural fairness objective, i.e., the sum of all pairwise envy values over all edges in a social graph. When agents have identical and evenly-spaced valuations, our problem reduces to the well-studied problem of linear arrangements. For identical valuations with possibly uneven spacing, we show a number of deep and surprising ways in which our setting is a departure from this classical problem. More broadly, we contribute several structural and computational results for various classes of graphs, including NP-hardness results for disjoint unions of paths, cycles, stars, or cliques, and fixed-parameter tractable (and, in some cases, polynomial-time) algorithms for paths, cycles, stars, cliques, and their disjoint unions. Additionally, a conceptual contribution of our work is the formulation of a structural property for disconnected graphs that we call separability which results in efficient parameterized algorithms for finding optimal allocations. △ Less",
      "url": "https://arxiv.org/abs/2301.01323"
    },
    {
      "title": "Efficient Relation-aware Neighborhood Aggregation in Graph Neural Networks via Tensor Decomposition",
      "abstract": "Numerous Graph Neural Networks (GNNs) have been developed to tackle the challenge of Knowledge Graph Embedding (KGE). However, many of these approaches overlook the crucial role of relation information and inadequately integrate it with entity information, resulting in diminished expressive power. In this paper, we propose a novel knowledge graph encoder that incorporates tensor decomposition within the aggregation function of Relational Graph Convolutional Network (R-GCN). Our model enhances the representation of neighboring entities by employing projection matrices of a low-rank tensor defined by relation types. This approach facilitates multi-task learning, thereby generating relation-aware representations. Furthermore, we introduce a low-rank estimation technique for the core tensor through CP decomposition, which effectively compresses and regularizes our model. We adopt a training strategy inspired by contrastive learning, which relieves the training limitation of the 1-N method inherent in handling vast graphs. We outperformed all our competitors on two common benchmark datasets, FB15k-237 and WN18RR, while using low-dimensional embeddings for entities and relations. △ Less",
      "url": "https://arxiv.org/abs/2212.05581"
    },
    {
      "title": "Epistemic vs. Counterfactual Fairness in Allocation of Resources",
      "abstract": "Resource allocation is fundamental to a variety of societal decision-making settings, ranging from the distribution of charitable donations to assigning limited public housing among interested families. A central challenge in this context is ensuring fair outcomes, which often requires balancing conflicting preferences of various stakeholders. While extensive research has been conducted on theoretical and algorithmic solutions within the fair division framework, much of this work neglects the subjective perception of fairness by individuals. This study focuses on the fairness notion of envy-freeness (EF), which ensures that no agent prefers the allocation of another agent according to their own preferences. While the existence of exact EF allocations may not always be feasible, various approximate relaxations, such as counterfactual and epistemic EF, have been proposed. Through a series of experiments with human participants, we compare perceptions of fairness between three widely studied counterfactual and epistemic relaxations of EF. Our findings indicate that allocations based on epistemic EF are perceived as fairer than those based on counterfactual relaxations. Additionally, we examine a variety of factors, including scale, balance of outcomes, and cognitive effort involved in evaluating fairness and their role in the complexity of reasoning across treatments. △ Less",
      "url": "https://arxiv.org/abs/2212.04574"
    },
    {
      "title": "Fast Online and Relational Tracking",
      "abstract": "To overcome challenges in multiple object tracking task, recent algorithms use interaction cues alongside motion and appearance features. These algorithms use graph neural networks or transformers to extract interaction features that lead to high computation costs. In this paper, a novel interaction cue based on geometric features is presented aiming to detect occlusion and re-identify lost targets with low computational cost. Moreover, in most algorithms, camera motion is considered negligible, which is a strong assumption that is not always true and leads to ID Switch or mismatching of targets. In this paper, a method for measuring camera motion and removing its effect is presented that efficiently reduces the camera motion effect on tracking. The proposed algorithm is evaluated on MOT17 and MOT20 datasets and it achieves the state-of-the-art performance of MOT17 and comparable results on MOT20. The code is also publicly available. △ Less",
      "url": "https://arxiv.org/abs/2208.03659"
    },
    {
      "title": "Introducing One Sided Margin Loss for Solving Classification Problems in Deep Networks",
      "abstract": "This paper introduces a new loss function, OSM (One-Sided Margin), to solve maximum-margin classification problems effectively. Unlike the hinge loss, in OSM the margin is explicitly determined with corresponding hyperparameters and then the classification problem is solved. In experiments, we observe that using OSM loss leads to faster training speeds and better accuracies than binary and categorical cross-entropy in several commonly used deep models for classification and optical character recognition problems. OSM has consistently shown better classification accuracies over cross-entropy and hinge losses for small to large neural networks. it has also led to a more efficient training procedure. We achieved state-of-the-art accuracies for small networks on several benchmark datasets of CIFAR10(98.82\\%), CIFAR100(91.56\\%), Flowers(98.04\\%), Stanford Cars(93.91\\%) with considerable improvements over other loss functions. Moreover, the accuracies are rather better than cross-entropy and hinge loss for large networks. Therefore, we strongly believe that OSM is a powerful alternative to hinge and cross-entropy losses to train deep neural networks on classification tasks. △ Less",
      "url": "https://arxiv.org/abs/2206.01002"
    },
    {
      "title": "Fairly Dividing Mixtures of Goods and Chores under Lexicographic Preferences",
      "abstract": "We study fair allocation of indivisible goods and chores among agents with \\emph{lexicographic} preferences -- a subclass of additive valuations. In sharp contrast to the goods-only setting, we show that an allocation satisfying \\emph{envy-freeness up to any item} (EFX) could fail to exist for a mixture of \\emph{objective} goods and chores. To our knowledge, this negative result provides the \\emph{first} counterexample for EFX over (any subdomain of) additive valuations. To complement this non-existence result, we identify a class of instances with (possibly subjective) mixed items where an EFX and Pareto optimal allocation always exists and can be efficiently computed. When the fairness requirement is relaxed to \\emph{maximin share} (MMS), we show positive existence and computation for \\emph{any} mixed instance. More broadly, our work examines the existence and computation of fair and efficient allocations both for mixed items as well as chores-only instances, and highlights the additional difficulty of these problems vis-{à}-vis their goods-only counterparts. △ Less",
      "url": "https://arxiv.org/abs/2203.07279"
    },
    {
      "title": "Class Fairness in Online Matching",
      "abstract": "In the classical version of online bipartite matching, there is a given set of offline vertices (aka agents) and another set of vertices (aka items) that arrive online. When each item arrives, its incident edges -- the agents who like the item -- are revealed and the algorithm must irrevocably match the item to such agents. We initiate the study of class fairness in this setting, where agents are partitioned into a set of classes and the matching is required to be fair with respect to the classes. We adopt popular fairness notions from the fair division literature such as envy-freeness (up to one item), proportionality, and maximin share fairness to our setting. Our class versions of these notions demand that all classes, regardless of their sizes, receive a fair treatment. We study deterministic and randomized algorithms for matching indivisible items (leading to integral matchings) and for matching divisible items (leading to fractional matchings). We design and analyze three novel algorithms. For matching indivisible items, we propose an adaptive-priority-based algorithm, MATCH-AND-SHIFT, prove that it achieves 1/2-approximation of both class envy-freeness up to one item and class maximin share fairness, and show that each guarantee is tight. For matching divisible items, we design a water-filling-based algorithm, EQUAL-FILLING, that achieves (1-1/e)-approximation of class envy-freeness and class proportionality; we prove (1-1/e) to be tight for class proportionality and establish a 3/4 upper bound on class envy-freeness. Finally, we build upon EQUAL-FILLING to design a randomized algorithm for matching indivisible items, EQAUL-FILLING-OCS, which achieves 0.593-approximation of class proportionality. The algorithm and its analysis crucially leverage the recently introduced technique of online correlated selection (OCS) [Fahrbach et al., 2020]. △ Less",
      "url": "https://arxiv.org/abs/2203.03751"
    },
    {
      "title": "Fair Stable Matching Meets Correlated Preferences",
      "abstract": "The stable matching problem sets the economic foundation of several practical applications ranging from school choice and medical residency to ridesharing and refugee placement. It is concerned with finding a matching between two disjoint sets of agents wherein no pair of agents prefer each other to their matched partners. The Deferred Acceptance (DA) algorithm is an elegant procedure that guarantees a stable matching for any input; however, its outcome may be unfair as it always favors one side by returning a matching that is optimal for one side (say men) and pessimal for the other side (say women). A desirable fairness notion is minimizing the sex-equality cost, i.e. the difference between the total rankings of both sides. Computing such stable matchings is a strongly NP-hard problem, which raises the question of what tractable algorithms to adopt in practice. We conduct a series of empirical evaluations on the properties of sex-equal stable matchings when preferences of agents on both sides are correlated. Our empirical results suggest that under correlated preferences, the DA algorithm returns stable matchings with low sex-equality cost, which further confirms its broad use in many practical applications. △ Less",
      "url": "https://arxiv.org/abs/2201.12484"
    },
    {
      "title": "Two for One $\\&$ One for All: Two-Sided Manipulation in Matching Markets",
      "abstract": "Strategic behavior in two-sided matching markets has been traditionally studied in a \"one-sided\" manipulation setting where the agent who misreports is also the intended beneficiary. Our work investigates \"two-sided\" manipulation of the deferred acceptance algorithm where the misreporting agent and the manipulator (or beneficiary) are on different sides. Specifically, we generalize the recently proposed accomplice manipulation model (where a man misreports on behalf of a woman) along two complementary dimensions: (a) the two for one model, with a pair of misreporting agents (man and woman) and a single beneficiary (the misreporting woman), and (b) the one for all model, with one misreporting agent (man) and a coalition of beneficiaries (all women). Our main contribution is to develop polynomial-time algorithms for finding an optimal manipulation in both settings. We obtain these results despite the fact that an optimal one for all strategy fails to be inconspicuous, while it is unclear whether an optimal two for one strategy satisfies the inconspicuousness property. We also study the conditions under which stability of the resulting matching is preserved. Experimentally, we show that two-sided manipulations are more frequently available and offer better quality matches than their one-sided counterparts. △ Less",
      "url": "https://arxiv.org/abs/2201.08774"
    },
    {
      "title": "Ordinal Maximin Share Approximation for Chores",
      "abstract": "We study the problem of fairly allocating a set of m indivisible chores (items with non-positive value) to n agents. We consider the desirable fairness notion of 1-out-of-d maximin share (MMS) -- the minimum value that an agent can guarantee by partitioning items into d bundles and receiving the least valued bundle -- and focus on ordinal approximation of MMS that aims at finding the largest d <= n for which 1-out-of-d MMS allocation exists. Our main contribution is a polynomial-time algorithm for 1-out-of-floor(2n/3) MMS allocation, and a proof of existence of 1-out-of-floor(3n/4) MMS allocation of chores. Furthermore, we show how to use recently-developed algorithms for bin-packing to approximate the latter bound up to a logarithmic factor in polynomial time. △ Less",
      "url": "https://arxiv.org/abs/2201.07424"
    },
    {
      "title": "Self-attention Presents Low-dimensional Knowledge Graph Embeddings for Link Prediction",
      "abstract": "A few models have tried to tackle the link prediction problem, also known as knowledge graph completion, by embedding knowledge graphs in comparably lower dimensions. However, the state-of-the-art results are attained at the cost of considerably increasing the dimensionality of embeddings which causes scalability issues in the case of huge knowledge bases. Transformers have been successfully used recently as powerful encoders for knowledge graphs, but available models still have scalability issues. To address this limitation, we introduce a Transformer-based model to gain expressive low-dimensional embeddings. We utilize a large number of self-attention heads as the key to applying query-dependent projections to capture mutual information between entities and relations. Empirical results on WN18RR and FB15k-237 as standard link prediction benchmarks demonstrate that our model has favorably comparable performance with the current state-of-the-art models. Notably, we yield our promising results with a significant reduction of 66.9% in the dimensionality of embeddings compared to the five best recent state-of-the-art competitors on average. △ Less",
      "url": "https://arxiv.org/abs/2112.10644"
    },
    {
      "title": "ParsiNorm: A Persian Toolkit for Speech Processing Normalization",
      "abstract": "In general, speech processing models consist of a language model along with an acoustic model. Regardless of the language model's complexity and variants, three critical pre-processing steps are needed in language models: cleaning, normalization, and tokenization. Among mentioned steps, the normalization step is so essential to format unification in pure textual applications. However, for embedded language models in speech processing modules, normalization is not limited to format unification. Moreover, it has to convert each readable symbol, number, etc., to how they are pronounced. To the best of our knowledge, there is no Persian normalization toolkits for embedded language models in speech processing modules, So in this paper, we propose an open-source normalization toolkit for text processing in speech applications. Briefly, we consider different readable Persian text like symbols (common currencies, #, @, URL, etc.), numbers (date, time, phone number, national code, etc.), and so on. Comparison with other available Persian textual normalization tools indicates the superiority of the proposed method in speech processing. Also, comparing the model's performance for one of the proposed functions (sentence separation) with other common natural language libraries such as HAZM and Parsivar indicates the proper performance of the proposed method. Besides, its evaluation of some Persian Wikipedia data confirms the proper performance of the proposed method. △ Less",
      "url": "https://arxiv.org/abs/2111.03470"
    },
    {
      "title": "CKMorph: A Comprehensive Morphological Analyzer for Central Kurdish",
      "abstract": "A morphological analyzer, which is a significant component of many natural language processing applications especially for morphologically rich languages, divides an input word into all its composing morphemes and identifies their morphological roles. In this paper, we introduce a comprehensive morphological analyzer for Central Kurdish (CK), a low-resourced language with a rich morphology. Building upon the limited existing literature, we first assembled and systematically categorized a comprehensive collection of the morphological and morphophonological rules of the language. Additionally, we collected and manually labeled a generative lexicon containing nearly 10,000 verb, noun and adjective stems, named entities, and other types of word stems. We used these rule sets and resources to implement CKMorph Analyzer based on finite-state transducers. In order to provide a benchmark for future research, we collected, manually labeled, and publicly shared test sets for evaluating accuracy and coverage of the analyzer. CKMorph was able to correctly analyze 95.9% of the accuracy test set, containing 1,000 CK words morphologically analyzed according to the context. Moreover, CKMorph gave at least one analysis for 95.5% of 4.22M CK tokens of the coverage test set. The demonstration of the application and resources including CK verb database and test sets are openly accessible at https://github.com/CKMorph. △ Less",
      "url": "https://arxiv.org/abs/2109.08615"
    },
    {
      "title": "Ordinal Maximin Share Approximation for Goods",
      "abstract": "In fair division of indivisible goods, $\\ell$-out-of-$d$ maximin share (MMS) is the value that an agent can guarantee by partitioning the goods into $d$ bundles and choosing the $\\ell$ least preferred bundles. Most existing works aim to guarantee to all agents a constant fraction of their 1-out-of-$n$ MMS. But this guarantee is sensitive to small perturbation in agents' cardinal valuations. We consider a more robust approximation notion, which depends only on the agents' \\emph{ordinal} rankings of bundles. We prove the existence of $\\ell$-out-of-$\\lfloor(\\ell+\\frac{1}{2})n\\rfloor$ MMS allocations of goods for any integer $\\ell\\geq 1$, and present a polynomial-time algorithm that finds a $1$-out-of-$\\lceil\\frac{3n}{2}\\rceil$ MMS allocation when $\\ell = 1$. We further develop an algorithm that provides a weaker ordinal approximation to MMS for any $\\ell > 1$. △ Less",
      "url": "https://arxiv.org/abs/2109.01925"
    },
    {
      "title": "Solving Viewing Graph Optimization for Simultaneous Position and Rotation Registration",
      "abstract": "A viewing graph is a set of unknown camera poses, as the vertices, and the observed relative motions, as the edges. Solving the viewing graph is an essential step in a Structure-from-Motion procedure, where a set of relative motions is obtained from a collection of 2D images. Almost all methods in the literature solve for the rotations separately, through rotation averaging process, and use them for solving the positions. Obtaining positions is the challenging part because the translation observations only tell the direction of the motions. It becomes more challenging when the set of edges comprises pairwise translation observations between either near and far cameras. In this paper an iterative method is proposed that overcomes these issues. Also a method is proposed which obtains the rotations and positions simultaneously. Experimental results show the-state-of-the-art performance of the proposed methods. △ Less",
      "url": "https://arxiv.org/abs/2108.12876"
    },
    {
      "title": "Optimal Triangulation Method is Not Really Optimal",
      "abstract": "Triangulation refers to the problem of finding a 3D point from its 2D projections on multiple camera images. For solving this problem, it is the common practice to use so-called optimal triangulation method, which we call the L2 method in this paper. But, the method can be optimal only if we assume no uncertainty in the camera parameters. Through extensive comparison on synthetic and real data, we observed that the L2 method is actually not the best choice when there is uncertainty in the camera parameters. Interestingly, it can be observed that the simple mid-point method outperforms other methods. Apart from its high performance, the mid-point method has a simple closed formed solution for multiple camera images while the L2 method is hard to be used for more than two camera images. Therefore, in contrast to the common practice, we argue that the simple mid-point method should be used in structure-from-motion applications where there is uncertainty in camera parameters. △ Less",
      "url": "https://arxiv.org/abs/2107.04618"
    },
    {
      "title": "Surprisingly Popular Voting Recovers Rankings, Surprisingly!",
      "abstract": "The wisdom of the crowd has long become the de facto approach for eliciting information from individuals or experts in order to predict the ground truth. However, classical democratic approaches for aggregating individual \\emph{votes} only work when the opinion of the majority of the crowd is relatively accurate. A clever recent approach, \\emph{surprisingly popular voting}, elicits additional information from the individuals, namely their \\emph{prediction} of other individuals' votes, and provably recovers the ground truth even when experts are in minority. This approach works well when the goal is to pick the correct option from a small list, but when the goal is to recover a true ranking of the alternatives, a direct application of the approach requires eliciting too much information. We explore practical techniques for extending the surprisingly popular algorithm to ranked voting by partial votes and predictions and designing robust aggregation rules. We experimentally demonstrate that even a little prediction information helps surprisingly popular voting outperform classical approaches. △ Less",
      "url": "https://arxiv.org/abs/2105.09386"
    },
    {
      "title": "Guaranteeing Maximin Shares: Some Agents Left Behind",
      "abstract": "The maximin share (MMS) guarantee is a desirable fairness notion for allocating indivisible goods. While MMS allocations do not always exist, several approximation techniques have been developed to ensure that all agents receive a fraction of their maximin share. We focus on an alternative approximation notion, based on the population of agents, that seeks to guarantee MMS for a fraction of agents. We show that no optimal approximation algorithm can satisfy more than a constant number of agents, and discuss the existence and computation of MMS for all but one agent and its relation to approximate MMS guarantees. We then prove the existence of allocations that guarantee MMS for $\\frac{2}{3}$ of agents, and devise a polynomial time algorithm that achieves this bound for up to nine agents. A key implication of our result is the existence of allocations that guarantee $\\text{MMS}^{\\lceil{3n/2}\\rceil}$, i.e., the value that agents receive by partitioning the goods into $\\lceil{\\frac{3}{2}n}\\rceil$ bundles, improving the best known guarantee of $\\text{MMS}^{2n-2}$. Finally, we provide empirical experiments using synthetic data. △ Less",
      "url": "https://arxiv.org/abs/2105.09383"
    },
    {
      "title": "Learning with partially separable data",
      "abstract": "There are partially separable data types that make classification tasks very hard. In other words, only parts of the data are informative meaning that looking at the rest of the data would not give any distinguishable hint for classification. In this situation, the typical assumption of having the whole labeled data as an informative unit set for classification does not work. Consequently, typical classification methods with the mentioned assumption fail in such a situation. In this study, we propose a framework for the classification of partially separable data types that are not classifiable using typical methods. An algorithm based on the framework is proposed that tries to detect separable subgroups of the data using an iterative clustering approach. Then the detected subgroups are used in the classification process. The proposed approach was tested on a real dataset for autism screening and showed its capability by distinguishing children with autism from normal ones, while the other methods failed to do so. △ Less",
      "url": "https://arxiv.org/abs/2103.06869"
    },
    {
      "title": "Simple online and real-time tracking with occlusion handling",
      "abstract": "Multiple object tracking is a challenging problem in computer vision due to difficulty in dealing with motion prediction, occlusion handling, and object re-identification. Many recent algorithms use motion and appearance cues to overcome these challenges. But using appearance cues increases the computation cost notably and therefore the speed of the algorithm decreases significantly which makes them inappropriate for online applications. In contrast, there are algorithms that only use motion cues to increase speed, especially for online applications. But these algorithms cannot handle occlusions and re-identify lost objects. In this paper, a novel online multiple object tracking algorithm is presented that only uses geometric cues of objects to tackle the occlusion and reidentification challenges simultaneously. As a result, it decreases the identity switch and fragmentation metrics. Experimental results show that the proposed algorithm could decrease identity switch by 40% and fragmentation by 28% compared to the state of the art online tracking algorithms. The code is also publicly available. △ Less",
      "url": "https://arxiv.org/abs/2103.04147"
    },
    {
      "title": "Jira: a Kurdish Speech Recognition System Designing and Building Speech Corpus and Pronunciation Lexicon",
      "abstract": "In this paper, we introduce the first large vocabulary speech recognition system (LVSR) for the Central Kurdish language, named Jira. The Kurdish language is an Indo-European language spoken by more than 30 million people in several countries, but due to the lack of speech and text resources, there is no speech recognition system for this language. To fill this gap, we introduce the first speech corpus and pronunciation lexicon for the Kurdish language. Regarding speech corpus, we designed a sentence collection in which the ratio of di-phones in the collection resembles the real data of the Central Kurdish language. The designed sentences are uttered by 576 speakers in a controlled environment with noise-free microphones (called AsoSoft Speech-Office) and in Telegram social network environment using mobile phones (denoted as AsoSoft Speech-Crowdsourcing), resulted in 43.68 hours of speech. Besides, a test set including 11 different document topics is designed and recorded in two corresponding speech conditions (i.e., Office and Crowdsourcing). Furthermore, a 60K pronunciation lexicon is prepared in this research in which we faced several challenges and proposed solutions for them. The Kurdish language has several dialects and sub-dialects that results in many lexical variations. Our methods for script standardization of lexical variations and automatic pronunciation of the lexicon tokens are presented in detail. To setup the recognition engine, we used the Kaldi toolkit. A statistical tri-gram language model that is extracted from the AsoSoft text corpus is used in the system. Several standard recipes including HMM-based models (i.e., mono, tri1, tr2, tri2, tri3), SGMM, and DNN methods are used to generate the acoustic model. These methods are trained with AsoSoft Speech-Office and AsoSoft Speech-Crowdsourcing and a combination of them. The best performance achieved by the SGMM acoustic model which results in 13.9% of the average word error rate (on different document topics) and 4.9% for the general topic. △ Less",
      "url": "https://arxiv.org/abs/2102.07412"
    },
    {
      "title": "Fair and Efficient Allocations under Lexicographic Preferences",
      "abstract": "Envy-freeness up to any good (EFX) provides a strong and intuitive guarantee of fairness in the allocation of indivisible goods. But whether such allocations always exist or whether they can be efficiently computed remains an important open question. We study the existence and computation of EFX in conjunction with various other economic properties under lexicographic preferences--a well-studied preference model in artificial intelligence and economics. In sharp contrast to the known results for additive valuations, we not only prove the existence of EFX and Pareto optimal allocations, but in fact provide an algorithmic characterization of these two properties. We also characterize the mechanisms that are, in addition, strategyproof, non-bossy, and neutral. When the efficiency notion is strengthened to rank-maximality, we obtain non-existence and computational hardness results, and show that tractability can be restored when EFX is relaxed to another well-studied fairness notion called maximin share guarantee (MMS). △ Less",
      "url": "https://arxiv.org/abs/2012.07680"
    }
  ]
}