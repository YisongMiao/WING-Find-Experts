{
  "author": "Laurent Simon",
  "results": [
    {
      "title": "PickleBall: Secure Deserialization of Pickle-based Machine Learning Models",
      "abstract": "Machine learning model repositories such as the Hugging Face Model Hub facilitate model exchanges. However, bad actors can deliver malware through compromised models. Existing defenses such as safer model formats, restrictive (but inflexible) loading policies, and model scanners have shortcomings: 44.9% of popular models on Hugging Face still use the insecure pickle format, 15% of these cannot be loaded by restrictive loading policies, and model scanners have both false positives and false negatives. Pickle remains the de facto standard for model exchange, and the ML community lacks a tool that offers transparent safe loading. We present PickleBall to help machine learning engineers load pickle-based models safely. PickleBall statically analyzes the source code of a given machine learning library and computes a custom policy that specifies a safe load-time behavior for benign models. PickleBall then dynamically enforces the policy during load time as a drop-in replacement for the pickle module. PickleBall generates policies that correctly load 79.8% of benign pickle-based models in our dataset, while rejecting all (100%) malicious examples in our dataset. In comparison, evaluated model scanners fail to identify known malicious models, and the state-of-art loader loads 22% fewer benign models than PickleBall. PickleBall removes the threat of arbitrary function invocation from malicious pickle-based models, raising the bar for attackers to depend on code reuse techniques. △ Less",
      "url": "https://arxiv.org/abs/2508.15987"
    },
    {
      "title": "Measurement of Parity-Violating Modes of the Dark Energy Spectroscopic Instrument (DESI) Year 1 Luminous Red Galaxies' 4-Point Correlation Function",
      "abstract": "Here we report the first measurement of the parity-violating (PV) 4-Point Correlation Function (4PCF) of the Dark Energy Spectroscopic Instrument's Year 1 Luminous Red Galaxy (DESI Y1 LRG) sample, motivated by the potential detection of the PV 4PCF in the Sloan Digital Sky Survey Baryon Oscillation Spectroscopic Survey (SDSS BOSS) galaxies. In our auto-correlation (\"auto\") analysis, we find a statistically significant excess of the PV signal compared to mocks without any PV, at 4-10$σ$ depending on details of the analysis. This could arise either from genuine PV or from an underestimation of the variance in the mocks; it is unlikely to arise, at the signal level, from a systematic. We then cross-correlate (\"cross\") the putative PV signal between different, independent patches of sky, and there find no detection of parity violation. The two measurements are in significant tension: while the cross has somewhat larger error bars than the auto, this is not sufficient to explain the discrepancy. We thus present the current work as an intriguing addition to the PV work on BOSS and as motivation for exploring further the relationship between the auto and cross PV 4PCF analyses. △ Less",
      "url": "https://arxiv.org/abs/2508.09133"
    },
    {
      "title": "Silicate clouds and a circumplanetary disk in the YSES-1 exoplanet system",
      "abstract": "Young exoplanets provide a critical link between understanding planet formation and atmospheric evolution. Direct imaging spectroscopy allows us to infer the properties of young, wide orbit, giant planets with high signal-to-noise. This allows us to compare this young population to exoplanets characterized with transmission spectroscopy, which has indirectly revealed the presence of clouds, photochemistry, and a diversity of atmospheric compositions. Direct detections have also been made for brown dwarfs, but direct studies of young giant planets in the mid-infrared were not possible prior to JWST. With two exoplanets around a solar type star, the YSES-1 system is an ideal laboratory for studying this early phase of exoplanet evolution. We report the first direct observations of silicate clouds in the atmosphere of the exoplanet YSES-1 c through its 9-11 micron absorption feature, and the first circumplanetary disk silicate emission around its sibling planet, YSES-1 b. The clouds of YSES-1 c are composed of either amorphous iron-enriched pyroxene or a combination of amorphous MgSiO3 and Mg2SiO4, with particle sizes of less than or equal to 0.1 micron at 1 millibar of pressure. We attribute the emission from the disk around YSES-1 b to be from submicron olivine dust grains, which may have formed through collisions of planet-forming bodies in the disk. △ Less",
      "url": "https://arxiv.org/abs/2507.18861"
    },
    {
      "title": "Apple Intelligence Foundation Language Models: Tech Report 2025",
      "abstract": "We introduce two multilingual, multimodal foundation language models that power Apple Intelligence features across Apple devices and services: i a 3B-parameter on-device model optimized for Apple silicon through architectural innovations such as KV-cache sharing and 2-bit quantization-aware training; and ii a scalable server model built on a novel Parallel-Track Mixture-of-Experts PT-MoE transformer that combines track parallelism, mixture-of-experts sparse computation, and interleaved global-local attention to deliver high quality with competitive cost on Apple's Private Cloud Compute platform. Both models are trained on large-scale multilingual and multimodal datasets sourced via responsible web crawling, licensed corpora, and high-quality synthetic data, then further refined with supervised fine-tuning and reinforcement learning on a new asynchronous platform. The resulting models support several additional languages while understanding images and executing tool calls. In public benchmarks and human evaluations, both the server model and the on-device model match or surpass comparably sized open baselines. A new Swift-centric Foundation Models framework exposes guided generation, constrained tool calling, and LoRA adapter fine-tuning, allowing developers to integrate these capabilities with a few lines of code. The latest advancements in Apple Intelligence models are grounded in our Responsible AI approach with safeguards like content filtering and locale-specific evaluation, as well as our commitment to protecting our users' privacy with innovations like Private Cloud Compute. △ Less",
      "url": "https://arxiv.org/abs/2507.13575"
    },
    {
      "title": "Benchmarking and Evaluation of AI Models in Biology: Outcomes and Recommendations from the CZI Virtual Cells Workshop",
      "abstract": "Artificial intelligence holds immense promise for transforming biology, yet a lack of standardized, cross domain, benchmarks undermines our ability to build robust, trustworthy models. Here, we present insights from a recent workshop that convened machine learning and computational biology experts across imaging, transcriptomics, proteomics, and genomics to tackle this gap. We identify major technical and systemic bottlenecks such as data heterogeneity and noise, reproducibility challenges, biases, and the fragmented ecosystem of publicly available resources and propose a set of recommendations for building benchmarking frameworks that can efficiently compare ML models of biological systems across tasks and data modalities. By promoting high quality data curation, standardized tooling, comprehensive evaluation metrics, and open, collaborative platforms, we aim to accelerate the development of robust benchmarks for AI driven Virtual Cells. These benchmarks are crucial for ensuring rigor, reproducibility, and biological relevance, and will ultimately advance the field toward integrated models that drive new discoveries, therapeutic insights, and a deeper understanding of cellular systems. △ Less",
      "url": "https://arxiv.org/abs/2507.10502"
    },
    {
      "title": "Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities",
      "abstract": "In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. In addition to its incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that excels at multimodal understanding and it is now able to process up to 3 hours of video content. Its unique combination of long context, multimodal and reasoning capabilities can be combined to unlock new agentic workflows. Gemini 2.5 Flash provides excellent reasoning abilities at a fraction of the compute and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high performance at low latency and cost. Taken together, the Gemini 2.X model generation spans the full Pareto frontier of model capability vs cost, allowing users to explore the boundaries of what is possible with complex agentic problem solving. △ Less",
      "url": "https://arxiv.org/abs/2507.06261"
    },
    {
      "title": "Multisubband Plasmons in InAs/GaSb Broken Gap Quantum Well",
      "abstract": "Multi-subband plasmon (MSP) modes in heavily doped InAs/GaSb broken-gap quantum wells grown via molecular beam epitaxy (MBE) are investigated. An $8$-band $\\vec{k} \\cdot \\vec{p}$ semiclassical model accurately predicts ellipsometric spectra, reflecting strong subband hybridization and non-parabolicity. In contrast, single-band plasmon models show qualitative discrepancies with experiment, even with adjusted effective masses. These findings highlight the potential of broken-gap wells for quantum technologies leveraging interband coupling and wavefunction hybridization. △ Less",
      "url": "https://arxiv.org/abs/2506.14491"
    },
    {
      "title": "Data Science: a Natural Ecosystem",
      "abstract": "This manuscript provides a holistic (data-centric) view of what we term essential data science, as a natural ecosystem with challenges and missions stemming from the data universe with its multiple combinations of the 5D complexities (data structure, domain, cardinality, causality, and ethics) with the phases of the data life cycle. Data agents perform tasks driven by specific goals. The data scientist is an abstract entity that comes from the logical organization of data agents with their actions. Data scientists face challenges that are defined according to the missions. We define specific discipline-induced data science, which in turn allows for the definition of pan-data science, a natural ecosystem that integrates specific disciplines with the essential data science. We semantically split the essential data science into computational, and foundational. We claim that there is a serious threat of divergence between computational and foundational data science. Especially, if no approach is taken to rate whether a data universe discovery should be useful or not. We suggest that rigorous approaches to measure the usefulness of data universe discoveries might mitigate such a divergence. △ Less",
      "url": "https://arxiv.org/abs/2506.11010"
    },
    {
      "title": "Machine Learning Models Have a Supply Chain Problem",
      "abstract": "Powerful machine learning (ML) models are now readily available online, which creates exciting possibilities for users who lack the deep technical expertise or substantial computing resources needed to develop them. On the other hand, this type of open ecosystem comes with many risks. In this paper, we argue that the current ecosystem for open ML models contains significant supply-chain risks, some of which have been exploited already in real attacks. These include an attacker replacing a model with something malicious (e.g., malware), or a model being trained using a vulnerable version of a framework or on restricted or poisoned data. We then explore how Sigstore, a solution designed to bring transparency to open-source software supply chains, can be used to bring transparency to open ML models, in terms of enabling model publishers to sign their models and prove properties about the datasets they use. △ Less",
      "url": "https://arxiv.org/abs/2505.22778"
    },
    {
      "title": "Enhanced magnetic activity in rapidly rotating binary stars",
      "abstract": "Stellar activity is fundamental to stellar evolution and the formation and habitability of exoplanets. The interaction between convective motions and rotation in cool stars results in a dynamo process that drives magnetic surface activity. In single stars, activity increases with rotation rate until it saturates for stars with rotation periods Prot < 3 - 10 d. However, the mechanism responsible for saturation remains unclear. Observations indicate that red giants in binary systems that are in spin-orbit resonance exhibit stronger chromospheric activity than single stars with similar rotation rates, suggesting that tidal flows can influence surface activity. Here, we investigate the chromospheric activity of main-sequence binary stars to understand the impact of tidal forces on saturation phenomena. For binaries with 0.5 < Prot/d < 1, mainly contact binaries that share a common thermal envelope, we find enhanced activity rather than saturation. This result supports theoretical predictions that a large-scale $α$ - $ω$ dynamo during common-envelope evolution can generate strong magnetic fields. We also observe supersaturation in chromospheric activity, a phenomenon tentatively noted previously in coronal activity, where activity levels fall below saturation and decrease with shorter rotation periods. Our findings emphasise the importance of studying stellar activity in stars with extreme properties compared to the Sun's. △ Less",
      "url": "https://arxiv.org/abs/2505.19967"
    },
    {
      "title": "An Explainable Diagnostic Framework for Neurodegenerative Dementias via Reinforcement-Optimized LLM Reasoning",
      "abstract": "The differential diagnosis of neurodegenerative dementias is a challenging clinical task, mainly because of the overlap in symptom presentation and the similarity of patterns observed in structural neuroimaging. To improve diagnostic efficiency and accuracy, deep learning-based methods such as Convolutional Neural Networks and Vision Transformers have been proposed for the automatic classification of brain MRIs. However, despite their strong predictive performance, these models find limited clinical utility due to their opaque decision making. In this work, we propose a framework that integrates two core components to enhance diagnostic transparency. First, we introduce a modular pipeline for converting 3D T1-weighted brain MRIs into textual radiology reports. Second, we explore the potential of modern Large Language Models (LLMs) to assist clinicians in the differential diagnosis between Frontotemporal dementia subtypes, Alzheimer's disease, and normal aging based on the generated reports. To bridge the gap between predictive accuracy and explainability, we employ reinforcement learning to incentivize diagnostic reasoning in LLMs. Without requiring supervised reasoning traces or distillation from larger models, our approach enables the emergence of structured diagnostic rationales grounded in neuroimaging findings. Unlike post-hoc explainability methods that retrospectively justify model decisions, our framework generates diagnostic rationales as part of the inference process-producing causally grounded explanations that inform and guide the model's decision-making process. In doing so, our framework matches the diagnostic performance of existing deep learning methods while offering rationales that support its diagnostic conclusions. △ Less",
      "url": "https://arxiv.org/abs/2505.19954"
    },
    {
      "title": "Empirical approaches to Frohlich excitonic polarons in polar semiconductors with application to 3D halide perovskites",
      "abstract": "Short abstract: The paper reviews the physics of Frohlich excitonic polarons from the viewpoint of empirical approaches with some original developments. Models for excitonic polarons in ionic semiconductors in the spirit of the Lee Low and Pines (LLP) model for free polarons were initiated by Toyozawa and Hermanson and extended by Pollman and Buttner (PB). The dominant electron-hole interaction with the lattice introduced by Frohlich is represented by a long-range effective interaction with a single longitudinal optical polar mode. The properties of the excitonic polarons are characterized by various physical quantities such as effective dielectric constants, effective masses, virtual phonon populations, carrier self-energies and binding energies, and effective electron-hole interactions mediated by the lattice. In 3D perovskites, the excitonic polarons deviate from the simplified picture of weakly interacting (almost free) polarons, with sizeable effects of electron-hole correlations on all the physical properties. △ Less",
      "url": "https://arxiv.org/abs/2505.07406"
    },
    {
      "title": "SWE-PolyBench: A multi-language benchmark for repository level evaluation of coding agents",
      "abstract": "Coding agents powered by large language models have shown impressive capabilities in software engineering tasks, but evaluating their performance across diverse programming languages and real-world scenarios remains challenging. We introduce SWE-PolyBench, a new multi-language benchmark for repository-level, execution-based evaluation of coding agents. SWE-PolyBench contains 2110 instances from 21 repositories and includes tasks in Java (165), JavaScript (1017), TypeScript (729) and Python (199), covering bug fixes, feature additions, and code refactoring. We provide a task and repository-stratified subsample (SWE-PolyBench500) and release an evaluation harness allowing for fully automated evaluation. To enable a more comprehensive comparison of coding agents, this work also presents a novel set of metrics rooted in syntax tree analysis. We evaluate leading open source coding agents on SWE-PolyBench, revealing their strengths and limitations across languages, task types, and complexity classes. Our experiments show that current agents exhibit uneven performances across languages and struggle with complex problems while showing higher performance on simpler tasks. SWE-PolyBench aims to drive progress in developing more versatile and robust AI coding assistants for real-world software engineering. Our datasets and code are available at: https://github.com/amazon-science/SWE-PolyBench △ Less",
      "url": "https://arxiv.org/abs/2504.08703"
    },
    {
      "title": "The X-ray Integral Field Unit at the end of the Athena reformulation phase",
      "abstract": "The Athena mission entered a redefinition phase in July 2022, driven by the imperative to reduce the mission cost at completion for the European Space Agency below an acceptable target, while maintaining the flagship nature of its science return. This notably called for a complete redesign of the X-ray Integral Field Unit (X-IFU) cryogenic architecture towards a simpler active cooling chain. Passive cooling via successive radiative panels at spacecraft level is now used to provide a 50 K thermal environment to an X-IFU owned cryostat. 4.5 K cooling is achieved via a single remote active cryocooler unit, while a multi-stage Adiabatic Demagnetization Refrigerator ensures heat lift down to the 50 mK required by the detectors. Amidst these changes, the core concept of the readout chain remains robust, employing Transition Edge Sensor microcalorimeters and a SQUID-based Time-Division Multiplexing scheme. Noteworthy is the introduction of a slower pixel. This enables an increase in the multiplexing factor (from 34 to 48) without compromising the instrument energy resolution, hence keeping significant system margins to the new 4 eV resolution requirement. This allows reducing the number of channels by more than a factor two, and thus the resource demands on the system, while keeping a 4' field of view (compared to 5' before). In this article, we will give an overview of this new architecture, before detailing its anticipated performances. Finally, we will present the new X-IFU schedule, with its short term focus on demonstration activities towards a mission adoption in early 2027. △ Less",
      "url": "https://arxiv.org/abs/2502.10866"
    },
    {
      "title": "Evaluation of Deep Audio Representations for Hearables",
      "abstract": "Effectively steering hearable devices requires understanding the acoustic environment around the user. In the computational analysis of sound scenes, foundation models have emerged as the state of the art to produce high-performance, robust, multi-purpose audio representations. We introduce and release Deep Evaluation of Audio Representations (DEAR), the first dataset and benchmark to evaluate the efficacy of foundation models in capturing essential acoustic properties for hearables. The dataset includes 1,158 audio tracks, each 30 seconds long, created by spatially mixing proprietary monologues with commercial, high-quality recordings of everyday acoustic scenes. Our benchmark encompasses eight tasks that assess the general context, speech sources, and technical acoustic properties of the audio scenes. Through our evaluation of four general-purpose audio representation models, we demonstrate that the BEATs model significantly surpasses its counterparts. This superiority underscores the advantage of models trained on diverse audio collections, confirming their applicability to a wide array of auditory tasks, including encoding the environment properties necessary for hearable steering. The DEAR dataset and associated code are available at https://dear-dataset.github.io. △ Less",
      "url": "https://arxiv.org/abs/2502.06664"
    },
    {
      "title": "Ultrashort Carbon Nanotubes with Luminescent Color Centers are Bright NIR-II Nano-Emitters",
      "abstract": "In the fields of bioimaging, photonics, and quantum science, it is equally crucial to combine high brightness with nanoscale size in short-wave infrared (SWIR) emitters. However, such nano-emitters are currently lacking. Here, we report that when functionalized with luminescent color centers, ultrashort carbon nanotubes with length much shorter than 100 nm, are surprisingly bright in the near-infrared second-biological window (NIR-II) of the SWIR domain. We discuss the origin of this exceptional brightness based on the uncontrollable presence of quenching defects in dispersed carbon nanotubes. We further investigate the nonlinear photoluminescence behavior of color centers functionalized carbon nanotubes in response to varying excitation conditions, spanning from ensemble measurements to single-nanotube experiments. We discuss how this behavior influences the determination of their photoluminescence quantum yields, which can reach values as high as 20% for ultrashort ones detected at the single nanotube level. Notably, the corresponding NIR-II brightness exceeds that of well-known visible emitters, including quantum dots. After rendering them biocompatible, we demonstrate point-spread function engineering and high-resolution, 3-dimensional single-particle tracking using these bright ultrashort carbon nanotubes allowing nanoscale imaging in the NIR-II window within thick brain tissue. △ Less",
      "url": "https://arxiv.org/abs/2501.08254"
    },
    {
      "title": "AnCoGen: Analysis, Control and Generation of Speech with a Masked Autoencoder",
      "abstract": "This article introduces AnCoGen, a novel method that leverages a masked autoencoder to unify the analysis, control, and generation of speech signals within a single model. AnCoGen can analyze speech by estimating key attributes, such as speaker identity, pitch, content, loudness, signal-to-noise ratio, and clarity index. In addition, it can generate speech from these attributes and allow precise control of the synthesized speech by modifying them. Extensive experiments demonstrated the effectiveness of AnCoGen across speech analysis-resynthesis, pitch estimation, pitch modification, and speech enhancement. △ Less",
      "url": "https://arxiv.org/abs/2501.05332"
    },
    {
      "title": "Experimental Demonstration of Logical Magic State Distillation",
      "abstract": "Realizing universal fault-tolerant quantum computation is a key goal in quantum information science. By encoding quantum information into logical qubits utilizing quantum error correcting codes, physical errors can be detected and corrected, enabling substantial reduction in logical error rates. However, the set of logical operations that can be easily implemented on such encoded qubits is often constrained, necessitating the use of special resource states known as 'magic states' to implement universal, classically hard circuits. A key method to prepare high-fidelity magic states is to perform 'distillation', creating them from multiple lower fidelity inputs. Here we present the experimental realization of magic state distillation with logical qubits on a neutral-atom quantum computer. Our approach makes use of a dynamically reconfigurable architecture to encode and perform quantum operations on many logical qubits in parallel. We demonstrate the distillation of magic states encoded in d=3 and d=5 color codes, observing improvements of the logical fidelity of the output magic states compared to the input logical magic states. These experiments demonstrate a key building block of universal fault-tolerant quantum computation, and represent an important step towards large-scale logical quantum processors. △ Less",
      "url": "https://arxiv.org/abs/2412.15165"
    },
    {
      "title": "Flexible and Efficient Semi-Empirical DFTB Parameters for Electronic Structure Prediction of 3D, 2D Iodide Perovskites and Heterostructures",
      "abstract": "Density Functional Tight-Binding (DFTB), an approximative approach derived from Density Functional Theory (DFT), has the potential to pave the way for simulations of large periodic or non-periodic systems. We have specifically tailored DFTB parameters to enhance the accuracy of electronic band gap calculations in both 3D and 2D lead-iodide perovskites, at a significantly reduced computational cost relative to state-of-the-art ab initio calculations. Our electronic DFTB parameters allow computing not only the band gap but also effective masses of perovskite materials with reasonable accuracy compared to existing experimental data and state-of-the-art DFT calculations. The electronic band structures of vacancy-ordered and, lead- and iodide- deficient perovskites are also explored. Additionally, we demonstrate the efficiency of DFTB in computing electronic band alignments in perovskite heterostructures. The DFTB-based approach is anticipated to be beneficial for studying large-scale systems such as heterostructures and nanocrystals. △ Less",
      "url": "https://arxiv.org/abs/2412.07016"
    },
    {
      "title": "Enhancing Computer Vision with Knowledge: a Rummikub Case Study",
      "abstract": "Artificial Neural Networks excel at identifying individual components in an image. However, out-of-the-box, they do not manage to correctly integrate and interpret these components as a whole. One way to alleviate this weakness is to expand the network with explicit knowledge and a separate reasoning component. In this paper, we evaluate an approach to this end, applied to the solving of the popular board game Rummikub. We demonstrate that, for this particular example, the added background knowledge is equally valuable as two-thirds of the data set, and allows to bring down the training time to half the original time. △ Less",
      "url": "https://arxiv.org/abs/2411.18172"
    },
    {
      "title": "Diachronic Document Dataset for Semantic Layout Analysis",
      "abstract": "We present a novel, open-access dataset designed for semantic layout analysis, built to support document recreation workflows through mapping with the Text Encoding Initiative (TEI) standard. This dataset includes 7,254 annotated pages spanning a large temporal range (1600-2024) of digitised and born-digital materials across diverse document types (magazines, papers from sciences and humanities, PhD theses, monographs, plays, administrative reports, etc.) sorted into modular subsets. By incorporating content from different periods and genres, it addresses varying layout complexities and historical changes in document structure. The modular design allows domain-specific configurations. We evaluate object detection models on this dataset, examining the impact of input size and subset-based training. Results show that a 1280-pixel input size for YOLO is optimal and that training on subsets generally benefits from incorporating them into a generic model rather than fine-tuning pre-trained weights. △ Less",
      "url": "https://arxiv.org/abs/2411.10068"
    },
    {
      "title": "The rush to the poles and the role of magnetic buoyancy in the solar dynamo",
      "abstract": "The butterfly diagram of the solar cycle exhibits a poleward migration of the diffuse magnetic field resulting from the decay of trailing sunspots. It is one component of what is sometimes referred to as the \"rush to the poles\". We investigate under which conditions the rush to the poles can be reproduced in flux-transport Babcock-Leighton dynamo models. We identify three main ways to reproduce it: a flux emergence probability that decreases rapidly with latitude; a threshold in subsurface toroidal field strength between slow and fast emergence; and an emergence rate based on magnetic buoyancy. We find that all three mechanisms lead to solar-like butterfly diagrams, but which present notable differences between them. The shape of the butterfly diagram is very sensitive to model parameters for the threshold prescription, while most models incorporating magnetic buoyancy converge to very similar butterfly diagrams, with butterfly wings widths of $\\lesssim\\pm 30^\\circ$, in very good agreement with observations. With turbulent diffusivities above $35~\\text{km}^2/\\text{s}$ but below about $40~\\text{km}^2/\\text{s}$, buoyancy models are strikingly solar-like. The threshold and magnetic buoyancy prescriptions make the models non-linear and as such can saturate the dynamo through latitudinal quenching. The period of the models involving buoyancy is independent of the source term amplitude, but emergence loss increases it by $\\simeq 60\\%$. For the rush to the poles to be visible, a mechanism suppressing (enhancing) emergences at high (low) latitudes must operate. It is not sufficient that the toroidal field be stored at low latitudes for emergences to be limited to low latitudes. From these models we infer that the Sun is not in the advection-dominated regime, but also not in the diffusion-dominated regime. The cycle period is set through a balance between advection, diffusion and flux emergence. △ Less",
      "url": "https://arxiv.org/abs/2411.05623"
    },
    {
      "title": "An Alternating Minimization Algorithm with Trajectory for Direct Exoplanet Detection -- The AMAT Algorithm",
      "abstract": "Effective image post-processing algorithms are vital for the successful direct imaging of exoplanets. Standard PSF subtraction methods use techniques based on a low-rank approximation to separate the rotating planet signal from the quasi-static speckles, and rely on signal-to-noise ratio maps to detect the planet. These steps do not interact or feed each other, leading to potential limitations in the accuracy and efficiency of exoplanet detection. We aim to develop a novel approach that iteratively finds the flux of the planet and the low-rank approximation of quasi-static signals, in an attempt to improve upon current PSF subtraction techniques. In this study, we extend the standard L2 norm minimization paradigm to an L1 norm minimization framework to better account for noise statistics in the high contrast images. Then, we propose a new method, referred to as Alternating Minimization Algorithm with Trajectory, that makes a more advanced use of estimating the low-rank approximation of the speckle field and the planet flux by alternating between them and utilizing both L1 and L2 norms. For the L1 norm minimization, we propose using L1 norm low-rank approximation, a low-rank approximation computed using an exact block-cyclic coordinate descent method, while we use randomized singular value decomposition for the L2 norm minimization. Additionally, we enhance the visibility of the planet signal using a likelihood ratio as a postprocessing step. Numerical experiments performed on a VLT/SPHERE-IRDIS dataset show the potential of AMAT to improve upon the existing approaches in terms of higher S/N, sensitivity limits, and ROC curves. Moreover, for a systematic comparison, we used datasets from the exoplanet data challenge to compare our algorithm to other algorithms in the challenge, and AMAT with likelihood ratio map performs better than most algorithms tested on the exoplanet data challenge. △ Less",
      "url": "https://arxiv.org/abs/2410.06310"
    },
    {
      "title": "Two waves of massive stars running away from the young cluster R136",
      "abstract": "Massive stars are predominantly born in stellar associations or clusters. Their radiation fields, stellar winds, and supernovae strongly impact their local environment. In the first few million years of a cluster's life, massive stars are dynamically ejected running away from the cluster at high speed. However, the production rate of dynamically ejected runaways is poorly constrained. Here we report on a sample of 55 massive runaway stars ejected from the young cluster R136 in the Large Magellanic Cloud. Astrometric analysis with Gaia reveals two channels of dynamically ejected runaways. The first channel ejects massive stars in all directions and is consistent with dynamical interactions during and after the birth of R136. The second channel launches stars in a preferred direction and may be related to a cluster interaction. We find that 23-33% of the most luminous stars initially born in R136 are runaways. Model predictions have significantly underestimated the dynamical escape fraction of massive stars. Consequently, their role in shaping and heating the interstellar and galactic medium, along with their role in driving galactic outflows, is far more important than previously thought. △ Less",
      "url": "https://arxiv.org/abs/2410.06255"
    },
    {
      "title": "The Blue Multi Unit Spectroscopic Explorer (BlueMUSE) on the VLT: science drivers and overview of instrument design",
      "abstract": "BlueMUSE is a blue-optimised, medium spectral resolution, panoramic integral field spectrograph under development for the Very Large Telescope (VLT). With an optimised transmission down to 350 nm, spectral resolution of R$\\sim$3500 on average across the wavelength range, and a large FoV (1 arcmin$^2$), BlueMUSE will open up a new range of galactic and extragalactic science cases facilitated by its specific capabilities. The BlueMUSE consortium includes 9 institutes located in 7 countries and is led by the Centre de Recherche Astrophysique de Lyon (CRAL). The BlueMUSE project development is currently in Phase A, with an expected first light at the VLT in 2031. We introduce here the Top Level Requirements (TLRs) derived from the main science cases, and then present an overview of the BlueMUSE system and its subsystems fulfilling these TLRs. We specifically emphasize the tradeoffs that are made and the key distinctions compared to the MUSE instrument, upon which the system architecture is built. △ Less",
      "url": "https://arxiv.org/abs/2406.13914"
    },
    {
      "title": "The PLATO Mission",
      "abstract": "PLATO (PLAnetary Transits and Oscillations of stars) is ESA's M3 mission designed to detect and characterise extrasolar planets and perform asteroseismic monitoring of a large number of stars. PLATO will detect small planets (down to <2 R_(Earth)) around bright stars (<11 mag), including terrestrial planets in the habitable zone of solar-like stars. With the complement of radial velocity observations from the ground, planets will be characterised for their radius, mass, and age with high accuracy (5 %, 10 %, 10 % for an Earth-Sun combination respectively). PLATO will provide us with a large-scale catalogue of well-characterised small planets up to intermediate orbital periods, relevant for a meaningful comparison to planet formation theories and to better understand planet evolution. It will make possible comparative exoplanetology to place our Solar System planets in a broader context. In parallel, PLATO will study (host) stars using asteroseismology, allowing us to determine the stellar properties with high accuracy, substantially enhancing our knowledge of stellar structure and evolution. The payload instrument consists of 26 cameras with 12cm aperture each. For at least four years, the mission will perform high-precision photometric measurements. Here we review the science objectives, present PLATO's target samples and fields, provide an overview of expected core science performance as well as a description of the instrument and the mission profile at the beginning of the serial production of the flight cameras. PLATO is scheduled for a launch date end 2026. This overview therefore provides a summary of the mission to the community in preparation of the upcoming operational phases. △ Less",
      "url": "https://arxiv.org/abs/2406.05447"
    },
    {
      "title": "The mean solar butterfly diagram and poloidal field generation rate at the surface of the Sun",
      "abstract": "The difference between individual solar cycles in the magnetic butterfly diagram can mostly be ascribed to the stochasticity of the emergence process. We aim to obtain the expectation value of the butterfly diagram from observations of four cycles. This allows us to further determine the generation rate of the surface radial magnetic field. We use data from Wilcox Solar Observatory to generate time-latitude diagrams spanning cycles 21 to 24 of the surface radial and toroidal magnetic fields, symmetrize them across the equator and cycle-average them. From the mean butterfly diagram and surface toroidal field we then infer the mean poloidal field generation rate at the surface of the Sun. The averaging procedure removes realization noise from individual cycles. The amount of emerging flux required to account for the evolution of the surface radial field is found to match that provided by the observed surface toroidal field and Joy's law. Cycle-averaging butterfly diagrams removes realization noise and artefacts due to imperfect scale separation, and corresponds to an ensemble average that can be interpreted in the mean-field framework. The result can then be directly compared to $αΩ$-type dynamo models. The Babcock-Leighton $α$-effect is consistent with observations, a result that can be appreciated only if the observational data is averaged in some way. △ Less",
      "url": "https://arxiv.org/abs/2405.17185"
    },
    {
      "title": "Euclid. IV. The NISP Calibration Unit",
      "abstract": "The near-infrared calibration unit (NI-CU) on board Euclid's Near-Infrared Spectrometer and Photometer (NISP) is the first astronomical calibration lamp based on light-emitting diodes (LEDs) to be operated in space. Euclid is a mission in ESA's Cosmic Vision 2015-2025 framework, to explore the dark universe and provide a next-level characterisation of the nature of gravitation, dark matter, and dark energy. Calibrating photometric and spectrometric measurements of galaxies to better than 1.5% accuracy in a survey homogeneously mapping ~14000 deg^2 of extragalactic sky requires a very detailed characterisation of near-infrared (NIR) detector properties, as well their constant monitoring in flight. To cover two of the main contributions - relative pixel-to-pixel sensitivity and non-linearity characteristics - as well as support other calibration activities, NI-CU was designed to provide spatially approximately homogeneous (<12% variations) and temporally stable illumination (0.1%-0.2% over 1200s) over the NISP detector plane, with minimal power consumption and energy dissipation. NI-CU is covers the spectral range ~[900,1900] nm - at cryo-operating temperature - at 5 fixed independent wavelengths to capture wavelength-dependent behaviour of the detectors, with fluence over a dynamic range of >=100 from ~15 ph s^-1 pixel^-1 to >1500 ph s^-1 pixel^-1. For this functionality, NI-CU is based on LEDs. We describe the rationale behind the decision and design process, describe the challenges in sourcing the right LEDs, as well as the qualification process and lessons learned. We also provide a description of the completed NI-CU, its capabilities and performance as well as its limits. NI-CU has been integrated into NISP and the Euclid satellite, and since Euclid's launch in July 2023 has started supporting survey operations. △ Less",
      "url": "https://arxiv.org/abs/2405.13494"
    },
    {
      "title": "Euclid. I. Overview of the Euclid mission",
      "abstract": "The current standard model of cosmology successfully describes a variety of measurements, but the nature of its main ingredients, dark matter and dark energy, remains unknown. Euclid is a medium-class mission in the Cosmic Vision 2015-2025 programme of the European Space Agency (ESA) that will provide high-resolution optical imaging, as well as near-infrared imaging and spectroscopy, over about 14,000 deg^2 of extragalactic sky. In addition to accurate weak lensing and clustering measurements that probe structure formation over half of the age of the Universe, its primary probes for cosmology, these exquisite data will enable a wide range of science. This paper provides a high-level overview of the mission, summarising the survey characteristics, the various data-processing steps, and data products. We also highlight the main science objectives and expected performance. △ Less",
      "url": "https://arxiv.org/abs/2405.13491"
    },
    {
      "title": "Drift Detection: Introducing Gaussian Split Detector",
      "abstract": "Recent research yielded a wide array of drift detectors. However, in order to achieve remarkable performance, the true class labels must be available during the drift detection phase. This paper targets at detecting drift when the ground truth is unknown during the detection phase. To that end, we introduce Gaussian Split Detector (GSD) a novel drift detector that works in batch mode. GSD is designed to work when the data follow a normal distribution and makes use of Gaussian mixture models to monitor changes in the decision boundary. The algorithm is designed to handle multi-dimension data streams and to work without the ground truth labels during the inference phase making it pertinent for real world use. In an extensive experimental study on real and synthetic datasets, we evaluate our detector against the state of the art. We show that our detector outperforms the state of the art in detecting real drift and in ignoring virtual drift which is key to avoid false alarms. △ Less",
      "url": "https://arxiv.org/abs/2405.08637"
    },
    {
      "title": "Life Cycle Assessment of the Athena X-ray Integral Field Unit",
      "abstract": "The X-ray Integral Field Unit (X-IFU) is the high-resolution X-ray spectrometer to fly on board the Athena Space Observatory of the European Space Agency (ESA). It is being developed by an international Consortium led by France, involving twelve ESA member states, plus the United States. It is a cryogenic instrument, involving state of the art technology, such as micro-calorimeters, to be read out by low noise electronics. As the instrument was undergoing its system requirement review (in 2022), a life cycle assessment (LCA) was performed to estimate the environmental impacts associated with the development of the sub-systems that were under the responsibility of the X-IFU Consortium. The assessment included the supply, manufacturing and testing of sub systems, as well as involved logistics and manpower. We find that the most significant environmental impacts arise from testing activities, which is related to energy consumption in clean rooms, office work, which is related to energy consumption in office buildings, and instrument manufacturing, which is related to the use of mineral and metal resources. Furthermore, business travels is another area of concern, despite the policy to reduced flying adopted by the Consortium. As the instrument is now being redesigned to fit within the new boundaries set by ESA, the LCA will be updated, with a focus on the hot spots identified in the first iteration. The new configuration, consolidated in 2023, is significantly different from the previously studied version and is marked by an increase of the perimeter of responsibility for the Consortium. This will need to be folded in the updated LCA, keeping the ambition to reduce the environmental footprint of X-IFU, while complying with its stringent requirements in terms of performance and risk management. △ Less",
      "url": "https://arxiv.org/abs/2404.15122"
    },
    {
      "title": "Gemini 1.5: Unlocking multimodal understanding across millions of tokens of context",
      "abstract": "In this report, we introduce the Gemini 1.5 family of models, representing the next generation of highly compute-efficient multimodal models capable of recalling and reasoning over fine-grained information from millions of tokens of context, including multiple long documents and hours of video and audio. The family includes two new models: (1) an updated Gemini 1.5 Pro, which exceeds the February version on the great majority of capabilities and benchmarks; (2) Gemini 1.5 Flash, a more lightweight variant designed for efficiency with minimal regression in quality. Gemini 1.5 models achieve near-perfect recall on long-context retrieval tasks across modalities, improve the state-of-the-art in long-document QA, long-video QA and long-context ASR, and match or surpass Gemini 1.0 Ultra's state-of-the-art performance across a broad set of benchmarks. Studying the limits of Gemini 1.5's long-context ability, we find continued improvement in next-token prediction and near-perfect retrieval (>99%) up to at least 10M tokens, a generational leap over existing models such as Claude 3.0 (200k) and GPT-4 Turbo (128k). Finally, we highlight real-world use cases, such as Gemini 1.5 collaborating with professionals on completing their tasks achieving 26 to 75% time savings across 10 different job categories, as well as surprising new capabilities of large language models at the frontier; when given a grammar manual for Kalamang, a language with fewer than 200 speakers worldwide, the model learns to translate English to Kalamang at a similar level to a person who learned from the same content. △ Less",
      "url": "https://arxiv.org/abs/2403.05530"
    },
    {
      "title": "Impact of Systematic Redshift Errors on the Cross-correlation of the Lyman-$α$ Forest with Quasars at Small Scales Using DESI Early Data",
      "abstract": "The Dark Energy Spectroscopic Instrument (DESI) will measure millions of quasar spectra by the end of its 5 year survey. Quasar redshift errors impact the shape of the Lyman-$α$ forest correlation functions, which can affect cosmological analyses and therefore cosmological interpretations. Using data from the DESI Early Data Release and the first two months of the main survey, we measure the systematic redshift error from an offset in the cross-correlation of the Lyman-$α$ forest with quasars. We find evidence for a redshift dependent bias causing redshifts to be underestimated with increasing redshift, stemming from improper modeling of the Lyman-$α$ optical depth in the templates used for redshift estimation. New templates were derived for the DESI Year 1 quasar sample at $z > 1.6$ and we found the redshift dependent bias, $Δr_\\parallel$, increased from $-1.94 \\pm 0.15$ $h^{-1}$ Mpc to $-0.08 \\pm 0.04$ $h^{-1}$ Mpc ($-205 \\pm 15~\\text{km s}^{-1}$ to $-9.0 \\pm 4.0~\\text{km s}^{-1}$). These new templates will be used to provide redshifts for the DESI Year 1 quasar sample. △ Less",
      "url": "https://arxiv.org/abs/2402.18009"
    },
    {
      "title": "Quantum walks, the discrete wave equation and Chebyshev polynomials",
      "abstract": "A quantum walk is the quantum analogue of a random walk. While it is relatively well understood how quantum walks can speed up random walk hitting times, it is a long-standing open question to what extent quantum walks can speed up the spreading or mixing rate of random walks on graphs. In this expository paper, inspired by a blog post by Terence Tao, we describe a particular perspective on this question that derives quantum walks from the discrete wave equation on graphs. This yields a description of the quantum walk dynamics as simply applying a Chebyshev polynomial to the random walk transition matrix. This perspective decouples the problem from its quantum origin, and highlights connections to earlier (non-quantum) work and the use of Chebyshev polynomials in random walk theory as in the Varopoulos-Carne bound. We illustrate the approach by proving a weak limit of the quantum walk dynamics on the lattice. This gives a different proof of the quadratically improved spreading behavior of quantum walks on lattices. △ Less",
      "url": "https://arxiv.org/abs/2402.07809"
    },
    {
      "title": "Multi-segmented non-isothermal compositional liquid gas well model for geothermal processes",
      "abstract": "We consider a non-isothermal compositional gas liquid model for the simulation of well operations in geothermal processes. The model accounts for phase transitions assumed to be at thermodynamical equilibrium and is based on an hydrodynamical Drift Flux Model (DFM) combined with a No Pressure Wave approximation of the momentum equation. The focus of this work is on the design of a robust discretization accounting for slanted and multibranch wells with the ability to simulate both transient behavior such as well opening as well as coupled simulations at the time scale of the reservoir. It is based on a staggered finite volume scheme in space combined with a fully implicit Euler time integration. The construction of consistent and stable numerical fluxes is a key feature for a robust numerical method. It is achieved by combining a monotone flux approximation for the phase superficial velocities with an upwind approximation of the phase molar fractions, density and enthalpy. In order to facilitate the coupling of the well and reservoir models, the Newton linearization accounts for the elimination of the hydrodynamical unknowns leading to Jacobian systems using the same primary unknowns than those of the reservoir model. The efficiency of our approach is investigated on both stand alone well test cases without and with cross flow, and on a fully coupled well-reservoir simulation. △ Less",
      "url": "https://arxiv.org/abs/2401.02406"
    },
    {
      "title": "Gemini: A Family of Highly Capable Multimodal Models",
      "abstract": "This report introduces a new family of multimodal models, Gemini, that exhibit remarkable capabilities across image, audio, video, and text understanding. The Gemini family consists of Ultra, Pro, and Nano sizes, suitable for applications ranging from complex reasoning tasks to on-device memory-constrained use-cases. Evaluation on a broad range of benchmarks shows that our most-capable Gemini Ultra model advances the state of the art in 30 of 32 of these benchmarks - notably being the first model to achieve human-expert performance on the well-studied exam benchmark MMLU, and improving the state of the art in every one of the 20 multimodal benchmarks we examined. We believe that the new capabilities of the Gemini family in cross-modal reasoning and language understanding will enable a wide variety of use cases. We discuss our approach toward post-training and deploying Gemini models responsibly to users through services including Gemini, Gemini Advanced, Google AI Studio, and Cloud Vertex AI. △ Less",
      "url": "https://arxiv.org/abs/2312.11805"
    },
    {
      "title": "GraL spectroscopic identification of multiply imaged quasars",
      "abstract": "Gravitational lensing is proven to be one of the most efficient tools for studying the Universe. The spectral confirmation of such sources requires extensive calibration. This paper discusses the spectral extraction technique for the case of multiple source spectra being very near each other. Using the masking technique, we first detect high Signal-to-Noise (S/N) peaks in the CCD spectral image corresponding to the location of the source spectra. This technique computes the cumulative signal using a weighted sum, yielding a reliable approximation for the total counts contributed by each source spectrum. We then proceed with the subtraction of the contaminating spectra. Applying this method, we confirm the nature of 11 lensed quasar candidates. △ Less",
      "url": "https://arxiv.org/abs/2312.08217"
    },
    {
      "title": "The JWST Early Release Science Program for Direct Observations of Exoplanetary Systems V: Do Self-Consistent Atmospheric Models Represent JWST Spectra? A Showcase With VHS 1256 b",
      "abstract": "The unprecedented medium-resolution (R~1500-3500) near- and mid-infrared (1-18um) spectrum provided by JWST for the young (140+/-20Myr) low-mass (12-20MJup) L-T transition (L7) companion VHS1256b gives access to a catalogue of molecular absorptions. In this study, we present a comprehensive analysis of this dataset utilizing a forward modelling approach, applying our Bayesian framework, ForMoSA. We explore five distinct atmospheric models to assess their performance in estimating key atmospheric parameters: Teff, log(g), [M/H], C/O, gamma, fsed, and R. Our findings reveal that each parameter's estimate is significantly influenced by factors such as the wavelength range considered and the model chosen for the fit. This is attributed to systematic errors in the models and their challenges in accurately replicating the complex atmospheric structure of VHS1256b, notably the complexity of its clouds and dust distribution. To propagate the impact of these systematic uncertainties on our atmospheric property estimates, we introduce innovative fitting methodologies based on independent fits performed on different spectral windows. We finally derived a Teff consistent with the spectral type of the target, considering its young age, which is confirmed by our estimate of log(g). Despite the exceptional data quality, attaining robust estimates for chemical abundances [M/H] and C/O, often employed as indicators of formation history, remains challenging. Nevertheless, the pioneering case of JWST's data for VHS1256b has paved the way for future acquisitions of substellar spectra that will be systematically analyzed to directly compare the properties of these objects and correct the systematics in the models. △ Less",
      "url": "https://arxiv.org/abs/2312.03852"
    },
    {
      "title": "2023 Astrophotonics Roadmap: pathways to realizing multi-functional integrated astrophotonic instruments",
      "abstract": "Photonics offer numerous functionalities that can be used to realize astrophotonic instruments. The most spectacular example to date is the ESO Gravity instrument at the Very Large Telescope in Chile. Integrated astrophotonic devices stand to offer critical advantages for instrument development, including extreme miniaturization, as well as integration, superior thermal and mechanical stabilization owing to the small footprint, and high replicability offering cost savings. Numerous astrophotonic technologies have been developed to address shortcomings of conventional instruments to date, including for example the development of photonic lanterns, complex aperiodic fiber Bragg gratings, complex beam combiners to enable long baseline interferometry, and laser frequency combs for high precision spectral calibration of spectrometers. Despite these successes, the facility implementation of photonic solutions in astronomical instrumentation is currently limited because of (1) low throughputs from coupling to fibers, coupling fibers to chips, propagation and bend losses, device losses, etc, (2) difficulties with scaling to large channel count devices needed for large bandwidths and high resolutions, and (3) efficient integration of photonics with detectors, to name a few. In this roadmap, we identify 24 areas that need further development. We outline the challenges and advances needed across those areas covering design tools, simulation capabilities, fabrication processes, the need for entirely new components, integration and hybridization and the characterization of devices. To realize these advances the astrophotonics community will have to work cooperatively with industrial partners who have more advanced manufacturing capabilities. With the advances described herein, multi-functional instruments will be realized leading to novel observing capabilities for both ground and space platforms. △ Less",
      "url": "https://arxiv.org/abs/2311.00615"
    },
    {
      "title": "Random Forest Kernel for High-Dimension Low Sample Size Classification",
      "abstract": "High dimension, low sample size (HDLSS) problems are numerous among real-world applications of machine learning. From medical images to text processing, traditional machine learning algorithms are usually unsuccessful in learning the best possible concept from such data. In a previous work, we proposed a dissimilarity-based approach for multi-view classification, the Random Forest Dissimilarity (RFD), that perfoms state-of-the-art results for such problems. In this work, we transpose the core principle of this approach to solving HDLSS classification problems, by using the RF similarity measure as a learned precomputed SVM kernel (RFSVM). We show that such a learned similarity measure is particularly suited and accurate for this classification context. Experiments conducted on 40 public HDLSS classification datasets, supported by rigorous statistical analyses, show that the RFSVM method outperforms existing methods for the majority of HDLSS problems and remains at the same time very competitive for low or non-HDLSS problems. △ Less",
      "url": "https://arxiv.org/abs/2310.14710"
    },
    {
      "title": "ZTD$_{JAVA}$: Mitigating Software Supply Chain Vulnerabilities via Zero-Trust Dependencies",
      "abstract": "Third-party libraries like Log4j accelerate software application development but introduce substantial risk. Vulnerabilities in these libraries have led to Software Supply Chain (SSC) attacks that compromised resources within the host system. These attacks benefit from current application permissions approaches: thirdparty libraries are implicitly trusted in the application runtime. An application runtime designed with Zero-Trust Architecture (ZTA) principles secure access to resources, continuous monitoring, and least-privilege enforcement could mitigate SSC attacks, as it would give zero implicit trust to these libraries. However, no individual security defense incorporates these principles at a low runtime cost. This paper proposes Zero-Trust Dependencies to mitigate SSC vulnerabilities: we apply the NIST ZTA to software applications. First, we assess the expected effectiveness and configuration cost of Zero-Trust Dependencies using a study of third-party software libraries and their vulnerabilities. Then, we present a system design, ZTD$_{SYS}$, that enables the application of Zero-Trust Dependencies to software applications and a prototype, ZTD$_{JAVA}$, for Java applications. Finally, with evaluations on recreated vulnerabilities and realistic applications, we show that ZTD$_{JAVA}$ can defend against prevalent vulnerability classes, introduces negligible cost, and is easy to configure and use. △ Less",
      "url": "https://arxiv.org/abs/2310.14117"
    },
    {
      "title": "The JWST Early Release Science Program for Direct Observations of Exoplanetary Systems III: Aperture Masking Interferometric Observations of the star HIP 65426 at 3.8 um",
      "abstract": "We present aperture masking interferometry (AMI) observations of the star HIP 65426 at $3.8\\,\\rm{μm}$ as a part of the JWST Direct Imaging Early Release Science (ERS) program obtained using the Near Infrared Imager and Slitless Spectrograph (NIRISS) instrument. This mode provides access to very small inner working angles (even separations slightly below the Michelson limit of $0.5λ/D$ for an interferometer), which are inaccessible with the classical inner working angles of the JWST coronagraphs. When combined with JWST's unprecedented infrared sensitivity, this mode has the potential to probe a new portion of parameter space across a wide array of astronomical observations. Using this mode, we are able to achieve a $5σ$ contrast of $Δm{\\sim}7.62{\\pm}0.13$ mag relative to the host star at separations ${\\gtrsim}0.07{\"}$, and the contrast deteriorates steeply at separations ${\\lesssim}0.07{\"}$. However, we detect no additional companions interior to the known companion HIP 65426 b (at separation ${\\sim}0.82{\"}$ or, $87^{+108}_{-31}\\,\\rm{au}$). Our observations thus rule out companions more massive than $10{-}12\\,\\rm{M_{Jup}}$ at separations ${\\sim}10{-}20\\,\\rm{au}$ from HIP 65426, a region out of reach of ground or space-based coronagraphic imaging. These observations confirm that the AMI mode on JWST is sensitive to planetary mass companions at close-in separations (${\\gtrsim}0.07{\"}$), even for thousands of more distant stars at $\\sim$100 pc, in addition to the stars in the nearby young moving groups as stated in previous works. This result will allow the planning and successful execution of future observations to probe the inner regions of nearby stellar systems, opening an essentially unexplored parameter space. △ Less",
      "url": "https://arxiv.org/abs/2310.11508"
    },
    {
      "title": "The JWST Early Release Science Program for Direct Observations of Exoplanetary Systems IV: NIRISS Aperture Masking Interferometry Performance and Lessons Learned",
      "abstract": "We present a performance analysis for the aperture masking interferometry (AMI) mode on board the James Webb Space Telescope Near Infrared Imager and Slitless Spectrograph (JWST/NIRISS). Thanks to self-calibrating observables, AMI accesses inner working angles down to and even within the classical diffraction limit. The scientific potential of this mode has recently been demonstrated by the Early Release Science (ERS) 1386 program with a deep search for close-in companions in the HIP 65426 exoplanetary system. As part of ERS 1386, we use the same data set to explore the random, static, and calibration errors of NIRISS AMI observables. We compare the observed noise properties and achievable contrast to theoretical predictions. We explore possible sources of calibration errors and show that differences in charge migration between the observations of HIP 65426 and point-spread function calibration stars can account for the achieved contrast curves. Lastly, we use self-calibration tests to demonstrate that with adequate calibration NIRISS F380M AMI can reach contrast levels of $\\sim9-10$ mag at $\\gtrsim λ/D$. These tests lead us to observation planning recommendations and strongly motivate future studies aimed at producing sophisticated calibration strategies taking these systematic effects into account. This will unlock the unprecedented capabilities of JWST/NIRISS AMI, with sensitivity to significantly colder, lower-mass exoplanets than lower-contrast ground-based AMI setups, at orbital separations inaccessible to JWST coronagraphy. △ Less",
      "url": "https://arxiv.org/abs/2310.11499"
    },
    {
      "title": "Open X-Embodiment: Robotic Learning Datasets and RT-X Models",
      "abstract": "Large, high-capacity models trained on diverse datasets have shown remarkable successes on efficiently tackling downstream applications. In domains from NLP to Computer Vision, this has led to a consolidation of pretrained models, with general pretrained backbones serving as a starting point for many applications. Can such a consolidation happen in robotics? Conventionally, robotic learning methods train a separate model for every application, every robot, and even every environment. Can we instead train generalist X-robot policy that can be adapted efficiently to new robots, tasks, and environments? In this paper, we provide datasets in standardized data formats and models to make it possible to explore this possibility in the context of robotic manipulation, alongside experimental results that provide an example of effective X-robot policies. We assemble a dataset from 22 different robots collected through a collaboration between 21 institutions, demonstrating 527 skills (160266 tasks). We show that a high-capacity model trained on this data, which we call RT-X, exhibits positive transfer and improves the capabilities of multiple robots by leveraging experience from other platforms. More details can be found on the project website https://robotics-transformer-x.github.io. △ Less",
      "url": "https://arxiv.org/abs/2310.08864"
    },
    {
      "title": "Spintronics for image recognition: performance benchmarking via ultrafast data-driven simulations",
      "abstract": "We present a demonstration of image classification using an echo-state network (ESN) relying on a single simulated spintronic nanostructure known as the vortex-based spin-torque oscillator (STVO) delayed in time. We employ an ultrafast data-driven simulation framework called the data-driven Thiele equation approach (DD-TEA) to simulate the STVO dynamics. This allows us to avoid the challenges associated with repeated experimental manipulation of such a nanostructured system. We showcase the versatility of our solution by successfully applying it to solve classification challenges with the MNIST, EMNIST-letters and Fashion MNIST datasets. Through our simulations, we determine that within an ESN with numerous learnable parameters the results obtained using the STVO dynamics as an activation function are comparable to the ones obtained with other conventional nonlinear activation functions like the reLU and the sigmoid. While achieving state-of-the-art accuracy levels on the MNIST dataset, our model's performance on EMNIST-letters and Fashion MNIST is lower due to the relative simplicity of the system architecture and the increased complexity of the tasks. We expect that the DD-TEA framework will enable the exploration of deeper architectures, ultimately leading to improved classification accuracy. △ Less",
      "url": "https://arxiv.org/abs/2308.05810"
    },
    {
      "title": "A Machine Learning Approach to Galactic Emission-Line Region Classification",
      "abstract": "Diagnostic diagrams of emission-line ratios have been used extensively to categorize extragalactic emission regions; however, these diagnostics are occasionally at odds with each other due to differing definitions. In this work, we study the applicability of supervised machine-learning techniques to systematically classify emission-line regions from the ratios of certain emission lines. Using the Million Mexican Model database, which contains information from grids of photoionization models using \\texttt{cloudy}, and from shock models, we develop training and test sets of emission line fluxes for three key diagnostic ratios. The sets are created for three classifications: classic \\hii{} regions, planetary nebulae, and supernova remnants. We train a neural network to classify a region as one of the three classes defined above given three key line ratios that are present both in the SITELLE and MUSE instruments' band-passes: [{\\sc O\\,iii}]$\\lambda5007$/H$β$, [{\\sc N\\,ii}]$\\lambda6583$/H$α$, ([{\\sc S\\,ii}]$\\lambda6717$+[{\\sc S\\,ii}]$\\lambda6731$)/H$α$. We also tested the impact of the addition of the [{\\sc O\\,ii}]$\\lambda3726,3729$/[{\\sc O\\,iii}]$\\lambda5007$ line ratio when available for the classification. A maximum luminosity limit is introduced to improve the classification of the planetary nebulae. Furthermore, the network is applied to SITELLE observations of a prominent field of M33. We discuss where the network succeeds and why it fails in certain cases. Our results provide a framework for the use of machine learning as a tool for the classification of extragalactic emission regions. Further work is needed to build more comprehensive training sets and adapt the method to additional observational constraints. △ Less",
      "url": "https://arxiv.org/abs/2306.11545"
    },
    {
      "title": "Unsupervised speech enhancement with deep dynamical generative speech and noise models",
      "abstract": "This work builds on a previous work on unsupervised speech enhancement using a dynamical variational autoencoder (DVAE) as the clean speech model and non-negative matrix factorization (NMF) as the noise model. We propose to replace the NMF noise model with a deep dynamical generative model (DDGM) depending either on the DVAE latent variables, or on the noisy observations, or on both. This DDGM can be trained in three configurations: noise-agnostic, noise-dependent and noise adaptation after noise-dependent training. Experimental results show that the proposed method achieves competitive performance compared to state-of-the-art unsupervised speech enhancement methods, while the noise-dependent training configuration yields a much more time-efficient inference process. △ Less",
      "url": "https://arxiv.org/abs/2306.07820"
    },
    {
      "title": "PaLM 2 Technical Report",
      "abstract": "We introduce PaLM 2, a new state-of-the-art language model that has better multilingual and reasoning capabilities and is more compute-efficient than its predecessor PaLM. PaLM 2 is a Transformer-based model trained using a mixture of objectives. Through extensive evaluations on English and multilingual language, and reasoning tasks, we demonstrate that PaLM 2 has significantly improved quality on downstream tasks across different model sizes, while simultaneously exhibiting faster and more efficient inference compared to PaLM. This improved efficiency enables broader deployment while also allowing the model to respond faster, for a more natural pace of interaction. PaLM 2 demonstrates robust reasoning capabilities exemplified by large improvements over PaLM on BIG-Bench and other reasoning tasks. PaLM 2 exhibits stable performance on a suite of responsible AI evaluations, and enables inference-time control over toxicity without additional overhead or impact on other capabilities. Overall, PaLM 2 achieves state-of-the-art performance across a diverse set of tasks and capabilities. When discussing the PaLM 2 family, it is important to distinguish between pre-trained models (of various sizes), fine-tuned variants of these models, and the user-facing products that use these models. In particular, user-facing products typically include additional pre- and post-processing steps. Additionally, the underlying models may evolve over time. Therefore, one should not expect the performance of user-facing products to exactly match the results reported in this report. △ Less",
      "url": "https://arxiv.org/abs/2305.10403"
    },
    {
      "title": "A multimodal dynamical variational autoencoder for audiovisual speech representation learning",
      "abstract": "In this paper, we present a multimodal and dynamical VAE (MDVAE) applied to unsupervised audio-visual speech representation learning. The latent space is structured to dissociate the latent dynamical factors that are shared between the modalities from those that are specific to each modality. A static latent variable is also introduced to encode the information that is constant over time within an audiovisual speech sequence. The model is trained in an unsupervised manner on an audiovisual emotional speech dataset, in two stages. In the first stage, a vector quantized VAE (VQ-VAE) is learned independently for each modality, without temporal modeling. The second stage consists in learning the MDVAE model on the intermediate representation of the VQ-VAEs before quantization. The disentanglement between static versus dynamical and modality-specific versus modality-common information occurs during this second training stage. Extensive experiments are conducted to investigate how audiovisual speech latent factors are encoded in the latent space of MDVAE. These experiments include manipulating audiovisual speech, audiovisual facial image denoising, and audiovisual speech emotion recognition. The results show that MDVAE effectively combines the audio and visual information in its latent space. They also show that the learned static representation of audiovisual speech can be used for emotion recognition with few labeled data, and with better accuracy compared with unimodal baselines and a state-of-the-art supervised model based on an audiovisual transformer architecture. △ Less",
      "url": "https://arxiv.org/abs/2305.03582"
    },
    {
      "title": "Parameter sensitivity analysis of a sea ice melt pond parametrisation and its emulation using neural networks",
      "abstract": "Accurate simulation of sea ice is critical for predictions of future Arctic sea ice loss, looming climate change impacts, and more. A key feature in Arctic sea ice is the formation of melt ponds. Each year melt ponds develop on the surface of the ice and primarily via affecting the albedo, they have an enormous effect on the energy budget and climate of the Arctic. As melt ponds are subgrid scale and their evolution occurs due to a number of competing, poorly understood factors, their representation in models is parametrised. Sobol sensitivity analysis, a form of variance based global sensitivity analysis is performed on an advanced melt pond parametrisation (MPP), in Icepack, a state-of-the-art thermodynamic column sea ice model. Results show that the model is very sensitive to changing its uncertain MPP parameter values, and that these have varying influences over model predictions both spatially and temporally. Such extreme sensitivity to parameters makes MPPs a potential source of prediction error in sea-ice model, given that the (often many) parameters in MPPs are usually poorly known. Machine learning (ML) techniques have shown great potential in learning and replacing subgrid scale processes in models. Given the complexity of melt pond physics and the need for accurate parameter values in MPPs, we propose an alternative data-driven MPPs that would prioritise the accuracy of albedo predictions. In particular, we constructed MPPs based either on linear regression or on nonlinear neural networks, and investigate if they could substitute the original physics-based MPP in Icepack. Our results shown that linear regression are insufficient as emulators, whilst neural networks can learn and emulate the MPP in Icepack very reliably. Icepack with the MPPs based on neural networks only slightly deviates from the original Icepack and overall offers the same long term model behaviour. △ Less",
      "url": "https://arxiv.org/abs/2304.05407"
    }
  ]
}