{
  "author": "Bo Han",
  "results": [
    {
      "title": "UI-TARS-2 Technical Report: Advancing GUI Agent with Multi-Turn Reinforcement Learning",
      "abstract": "The development of autonomous agents for graphical user interfaces (GUIs) presents major challenges in artificial intelligence. While recent advances in native agent models have shown promise by unifying perception, reasoning, action, and memory through end-to-end learning, open problems remain in data scalability, multi-turn reinforcement learning (RL), the limitations of GUI-only operation, and environment stability. In this technical report, we present UI-TARS-2, a native GUI-centered agent model that addresses these challenges through a systematic training methodology: a data flywheel for scalable data generation, a stabilized multi-turn RL framework, a hybrid GUI environment that integrates file systems and terminals, and a unified sandbox platform for large-scale rollouts. Empirical evaluation demonstrates that UI-TARS-2 achieves significant improvements over its predecessor UI-TARS-1.5. On GUI benchmarks, it reaches 88.2 on Online-Mind2Web, 47.5 on OSWorld, 50.6 on WindowsAgentArena, and 73.3 on AndroidWorld, outperforming strong baselines such as Claude and OpenAI agents. In game environments, it attains a mean normalized score of 59.8 across a 15-game suite-roughly 60% of human-level performance-and remains competitive with frontier proprietary models (e.g., OpenAI o3) on LMGame-Bench. Additionally, the model can generalize to long-horizon information-seeking tasks and software engineering benchmarks, highlighting its robustness across diverse agent tasks. Detailed analyses of training dynamics further provide insights into achieving stability and efficiency in large-scale agent RL. These results underscore UI-TARS-2's potential to advance the state of GUI agents and exhibit strong generalization to real-world interactive scenarios. △ Less",
      "url": "https://arxiv.org/abs/2509.02544"
    },
    {
      "title": "Characterization of SiPMs at 40 K for neutrino coherent detection based on pure CsI",
      "abstract": "Silicon photomultiplier (SiPM), as the core photoelectric sensor for coherent neutrino detection in low-temperature pure CsI, its working performance directly determines the measurement accuracy of the scintillator light yield. Our previous research has fully demonstrated the performance of pure CsI at liquid nitrogen temperature. More intriguingly, its performance is expected to be even better at 40 K. However, the performance characteristics of SiPM in the 40 K temperature range still remain to be explored. In this study, a self-developed adjustable temperature control system ranging from 30 K to 293 K was built to investigate the key performance parameters of SiPM at different temperatures, such as single photoelectron spectrum, gain, breakdown voltage, dark count rate, after-pulse, internal crosstalk, and single photoelectron resolution. Special emphasis was placed on examining the key performance parameters of SiPM in the 40 K temperature range to evaluate its feasibility for light yield measurement in this temperature range. The results show that this study obtained the parameter variation trends and optimal working conditions of 3 types of SiPM at different temperatures, thereby improving the sensitivity of the detector. This research provides important technical support for low-temperature detection in neutrino physics experiments. △ Less",
      "url": "https://arxiv.org/abs/2509.02041"
    },
    {
      "title": "LongCat-Flash Technical Report",
      "abstract": "We introduce LongCat-Flash, a 560-billion-parameter Mixture-of-Experts (MoE) language model designed for both computational efficiency and advanced agentic capabilities. Stemming from the need for scalable efficiency, LongCat-Flash adopts two novel designs: (a) Zero-computation Experts, which enables dynamic computational budget allocation and activates 18.6B-31.3B (27B on average) per token depending on contextual demands, optimizing resource usage. (b) Shortcut-connected MoE, which enlarges the computation-communication overlap window, demonstrating notable gains in inference efficiency and throughput compared to models of a comparable scale. We develop a comprehensive scaling framework for large models that combines hyperparameter transfer, model-growth initialization, a multi-pronged stability suite, and deterministic computation to achieve stable and reproducible training. Notably, leveraging the synergy among scalable architectural design and infrastructure efforts, we complete model training on more than 20 trillion tokens within 30 days, while achieving over 100 tokens per second (TPS) for inference at a cost of \\$0.70 per million output tokens. To cultivate LongCat-Flash towards agentic intelligence, we conduct a large-scale pre-training on optimized mixtures, followed by targeted mid- and post-training on reasoning, code, and instructions, with further augmentation from synthetic data and tool use tasks. Comprehensive evaluations demonstrate that, as a non-thinking foundation model, LongCat-Flash delivers highly competitive performance among other leading models, with exceptional strengths in agentic tasks. The model checkpoint of LongCat-Flash is open-sourced to foster community research. LongCat Chat: https://longcat.ai Hugging Face: https://huggingface.co/meituan-longcat GitHub: https://github.com/meituan-longcat △ Less",
      "url": "https://arxiv.org/abs/2509.01322"
    },
    {
      "title": "Helicity amplitude and branching fraction measurement of $χ_{cJ} \\rightarrow Λ\\barΛ $",
      "abstract": "Utilizing $2712.4 \\pm 14.3$ million $ψ(3686)$ events accumulated by the BESIII experiment, we perform a partial wave analysis of $ψ(3686)\\rightarrowγχ_{cJ}\\rightarrowγΛ\\barΛ$ decay ($J=0,1,2$). The ratio of the helicity amplitudes with same (++) and opposite (+-) helicity for $χ_{c2}\\rightarrowΛ\\barΛ$ decay is determined for the first time to be $R_{χ_{c2}}=0.575 \\pm 0.048 \\pm 0.018 $, with a relative phase angle $ΔΦ_{χ_{c2}} = 0.37 \\pm 0.15 \\pm 0.05 $~rad. The parameters of the angular distribution of $χ_{c2}$ are determined to be $α_{χ_{c2}} = -0.211 \\pm 0.100 \\pm 0.050 $ and $β_{χ_{c2}} = -0.039 \\pm 0.089 \\pm 0.033 $, based on the distribution $dN / d\\cosθ= 1 + α_{χ_{c2}} \\cos^2θ+ β_{χ_{c2}} \\cos^4θ$. The width of $χ_{c0}$ is determined to be $12.31 \\pm 0.26 \\pm 0.12 $~MeV. Additionally, the branching fractions for $χ_{cJ} \\rightarrow Λ\\barΛ$ are measured to be $(3.662 \\pm 0.048 \\pm 0.111) \\times 10^{-4}$, $(1.182 \\pm 0.026 \\pm 0.042) \\times 10^{-4}$, and $(1.704 \\pm 0.035 \\pm 0.057) \\times 10^{-4}$ for $χ_{c0}$, $χ_{c1}$ and $χ_{c2}$, respectively, where the first uncertainty is statistical and the second systematic. △ Less",
      "url": "https://arxiv.org/abs/2509.00289"
    },
    {
      "title": "Measurement of the branching fraction of $\\psip \\to ωηη$",
      "abstract": "Using a sample of (2.712 $\\pm$ 0.014)$\\times 10^{9}$ $\\psip$ events collected with the BESIII detector at the BEPCII collider in 2009, 2012, and 2021, the decay $\\psip \\to ωηη$ is observed for the first time. The branching fraction of the $ψ(3686)\\toωηη$ decay is measured to be (1.65 $\\pm$ 0.02 $\\pm$ 0.21)$\\times 10^{-5}$, where the first uncertainty is statistical and the second systematic. Clear structures associated with the well-established $ω(1420)$ and $f_{0}(1710)$ resonances are observed in the $ωη$ and $ηη$ invariant-mass spectra, respectively. △ Less",
      "url": "https://arxiv.org/abs/2508.19092"
    },
    {
      "title": "Building Self-Evolving Agents via Experience-Driven Lifelong Learning: A Framework and Benchmark",
      "abstract": "As AI advances toward general intelligence, the focus is shifting from systems optimized for static tasks to creating open-ended agents that learn continuously. In this paper, we introduce Experience-driven Lifelong Learning (ELL), a framework for building self-evolving agents capable of continuous growth through real-world interaction. The framework is built on four core principles: (1) Experience Exploration: Agents learn through continuous, self-motivated interaction with dynamic environments, navigating interdependent tasks and generating rich experiential trajectories. (2) Long-term Memory: Agents preserve and structure historical knowledge, including personal experiences, domain expertise, and commonsense reasoning, into a persistent memory system. (3) Skill Learning: Agents autonomously improve by abstracting recurring patterns from experience into reusable skills, which are actively refined and validated for application in new tasks. (4) Knowledge Internalization: Agents internalize explicit and discrete experiences into implicit and intuitive capabilities as \"second nature\". We also introduce StuLife, a benchmark dataset for ELL that simulates a student's holistic college journey, from enrollment to academic and personal development, across three core phases and ten detailed sub-scenarios. StuLife is designed around three key paradigm △ Less",
      "url": "https://arxiv.org/abs/2508.19005"
    },
    {
      "title": "Study of the $χ_{cJ}\\rightarrowΛ\\barΛη^\\prime$ decays",
      "abstract": "Using a data sample of $(2.712\\pm0.014)\\times10^{9}$ $ψ(3686)$ events collected with the BESIII detector at the BEPCII collider, we investigate the decays $χ_{cJ} \\rightarrow Λ\\barΛ η^\\prime$ for $J=0,~1,~2$ via the radiative transition $ψ(3686) \\rightarrow γχ_{cJ}$. The decays $χ_{c0,2}\\rightarrowΛ\\barΛη^\\prime$ are observed for the first time, with statistical significances of 6.7$\\,σ$ and 6.4$\\,σ$, respectively. Evidence for the decay $χ_{c1}\\rightarrowΛ\\barΛη^\\prime$ is found with a statistical significance of 3.3$\\,σ$. The corresponding branching fractions are measured to be $\\mathscr{B}(χ_{c0}\\rightarrowΛ\\barΛη^\\prime)=(7.56\\pm1.42\\pm0.90)\\times10^{-5}$, $\\mathscr{B}(χ_{c1}\\rightarrowΛ\\barΛη^\\prime)=(1.54\\pm0.51\\pm0.16)\\times10^{-5}$, and $\\mathscr{B}(χ_{c2}\\rightarrowΛ\\barΛη^\\prime)=(3.03\\pm0.61\\pm0.29)\\times10^{-5}$, where the first uncertainties are statistical and the second systematic. No significant excited $Λ$ baryon states or $Λ\\barΛ$ near-threshold enhancements are observed. △ Less",
      "url": "https://arxiv.org/abs/2508.18761"
    },
    {
      "title": "Search for $χ_{c1}\\to π^{+}π^{-}η_c$ via $ψ(3686)\\toγχ_{c1}$",
      "abstract": "Utilizing $(2712.4 \\pm 14.3) \\times 10^6$ $ψ(3686)$ events collected with the BESIII detector at the BEPCII collider, we search for the hadronic transition process $χ_{c1} \\to π^+π^-η_c$ following the decay $ψ(3686)\\to γχ_{c1}$. No significant signal is observed, and an upper limit of $\\mathcal{B}(χ_{c1}\\toπ^+π^-η_c)$ is determined to be $3.1 times 10^{-4}$~at 90\\% confidence level, which is one order of magnitude more stringent than the previous measurement. △ Less",
      "url": "https://arxiv.org/abs/2508.18601"
    },
    {
      "title": "Search for a bound state of $Λ_{c}\\barΣ_{c}$ near threshold",
      "abstract": "We search for a possible $Λ_{c} \\bar{Σ}_{c}$ bound state, denoted as $H_{c}^{\\pm}$, via the $ e^{+}e^{-} \\to π^{+} π^{-} Λ_{c}^{+}\\barΛ_{c}^{-}$ process for the first time. This analysis utilizes 207.8 and 159.3 pb$^{-1}$ of $e^{+}e^{-}$ annihilation data at the center-of-mass energies of 4918.02 and 4950.93 MeV, respectively, collected with the BESIII detector at the BEPCII collider. No statistically significant signal is observed. The upper limits of the product of Born cross section and branching fraction $σ(e^{+}e^{-} \\to π^{+} H_c^{-} + c.c.) \\times \\mathcal{B}(H_c^{-} \\rightarrow π^{-}Λ_{c}^{+}\\barΛ_{c}^{-})$ at a 90\\% confidence level are reported at each energy point and for various $H_{c}$ mass hypotheses (4715, 4720, 4725, 4730, and 4735 MeV/$c^{2}$) and widths (5, 10, or 20 MeV), with the upper limits ranging from 1.1 pb to 6.4 pb. △ Less",
      "url": "https://arxiv.org/abs/2508.18594"
    },
    {
      "title": "Visual-CoG: Stage-Aware Reinforcement Learning with Chain of Guidance for Text-to-Image Generation",
      "abstract": "Despite the promising progress of recent autoregressive models in text-to-image (T2I) generation, their ability to handle multi-attribute and ambiguous prompts remains limited. To address these limitations, existing works have applied chain-of-thought (CoT) to enable stage-aware visual synthesis and employed reinforcement learning (RL) to improve reasoning capabilities. However, most models provide reward signals only at the end of the generation stage. This monolithic final-only guidance makes it difficult to identify which stages contribute positively to the final outcome and may lead to suboptimal policies. To tackle this issue, we propose a Visual-Chain of Guidance (Visual-CoG) paradigm consisting of three stages: semantic reasoning, process refining, and outcome evaluation, with stage-aware rewards providing immediate guidance throughout the image generation pipeline. We further construct a visual cognition benchmark, VisCog-Bench, which comprises four subtasks to evaluate the effectiveness of semantic reasoning. Comprehensive evaluations on GenEval, T2I-CompBench, and the proposed VisCog-Bench show improvements of 15%, 5%, and 19%, respectively, demonstrating the superior performance of the proposed Visual-CoG. We will release all the resources soon. △ Less",
      "url": "https://arxiv.org/abs/2508.18032"
    },
    {
      "title": "Search for CP violation in e+e- -> psi(3770) -> DDbar via D -> KsPi0",
      "abstract": "Utilizing data sample of electron-positron collisions recorded with the BESIII detector at the center-of-mass energies of 3.773~GeV, corresponding to an integrated luminosity of 20.28~fb$^{-1}$, we report the first search for the CP forbidden process $e^+e^- \\to ψ(3773) \\to D^0\\bar{D}^0 \\to (K^0_Sπ^0)(K^0_Sπ^0)$. No significant signal is observed. We set the upper limit on the observed cross section to be 7.37~fb, and the upper limit on the joint branching fraction of the C-odd correlated neutral $D$ pair $\\mathcal{B}[(D^0\\bar{D}^0)_{\\text{C-odd}} \\to (K^0_Sπ^0)(K^0_Sπ^0)]$ to be $2.04 \\times 10^{-6}$ at the 90\\% confidence level. △ Less",
      "url": "https://arxiv.org/abs/2508.17819"
    },
    {
      "title": "See Beyond a Single View: Multi-Attribution Learning Leads to Better Conversion Rate Prediction",
      "abstract": "Conversion rate (CVR) prediction is a core component of online advertising systems, where the attribution mechanisms-rules for allocating conversion credit across user touchpoints-fundamentally determine label generation and model optimization. While many industrial platforms support diverse attribution mechanisms (e.g., First-Click, Last-Click, Linear, and Data-Driven Multi-Touch Attribution), conventional approaches restrict model training to labels from a single production-critical attribution mechanism, discarding complementary signals in alternative attribution perspectives. To address this limitation, we propose a novel Multi-Attribution Learning (MAL) framework for CVR prediction that integrates signals from multiple attribution perspectives to better capture the underlying patterns driving user conversions. Specifically, MAL is a joint learning framework consisting of two core components: the Attribution Knowledge Aggregator (AKA) and the Primary Target Predictor (PTP). AKA is implemented as a multi-task learner that integrates knowledge extracted from diverse attribution labels. PTP, in contrast, focuses on the task of generating well-calibrated conversion probabilities that align with the system-optimized attribution metric (e.g., CVR under the Last-Click attribution), ensuring direct compatibility with industrial deployment requirements. Additionally, we propose CAT, a novel training strategy that leverages the Cartesian product of all attribution label combinations to generate enriched supervision signals. This design substantially enhances the performance of the attribution knowledge aggregator. Empirical evaluations demonstrate the superiority of MAL over single-attribution learning baselines, achieving +0.51% GAUC improvement on offline metrics. Online experiments demonstrate that MAL achieved a +2.6% increase in ROI (Return on Investment). △ Less",
      "url": "https://arxiv.org/abs/2508.15217"
    },
    {
      "title": "Has GPT-5 Achieved Spatial Intelligence? An Empirical Study",
      "abstract": "Multi-modal models have achieved remarkable progress in recent years. Nevertheless, they continue to exhibit notable limitations in spatial understanding and reasoning, which are fundamental capabilities to achieving artificial general intelligence. With the recent release of GPT-5, allegedly the most powerful AI model to date, it is timely to examine where the leading models stand on the path toward spatial intelligence. First, we propose a comprehensive taxonomy of spatial tasks that unifies existing benchmarks and discuss the challenges in ensuring fair evaluation. We then evaluate state-of-the-art proprietary and open-source models on eight key benchmarks, at a cost exceeding one billion total tokens. Our empirical study reveals that (1) GPT-5 demonstrates unprecedented strength in spatial intelligence, yet (2) still falls short of human performance across a broad spectrum of tasks. Moreover, we (3) identify the more challenging spatial intelligence problems for multi-modal models, and (4) proprietary models do not exhibit a decisive advantage when facing the most difficult problems. In addition, we conduct a qualitative evaluation across a diverse set of scenarios that are intuitive for humans yet fail even the most advanced multi-modal models. △ Less",
      "url": "https://arxiv.org/abs/2508.13142"
    },
    {
      "title": "The Production and Decay Dynamics of the Charmed Baryon $Λ_c^+$ in $e^+e^-$ Annihilations near Threshold",
      "abstract": "The study of the charmed baryons is crucial for investigating the strong and weak interactions in the Standard Model and for gaining insights into the internal structure of baryons. In an $e^+e^-$ experiment the lightest charmed baryon, $Λ_c^+$, can be produced in pairs through the single photon annihilation process. This process can be described by two complex electromagnetic form factors. The presence of a non-zero relative phase between these form factors gives rise to a transverse polarization of the charmed baryon and provides additional constraints on the dynamic parameters in the decays. In this article, we present the first observation of the transverse polarization of $Λ_{c}^{+}$ in the reaction $e^+e^- \\to Λ_c^{+}\\barΛ_c^-$, based on $6.4~\\text{fb}^{-1}$ of $e^{+}e^{-}$ annihilation data collected at center-of-mass energies between 4600 MeV and 4951 MeV with the BESIII detector. The decay asymmetry parameters and strong phase shift in the decays $Λ_c^+ \\to pK_S^0$, $Λπ^+$, $Σ^0π^+$, $Σ^+π^0$ are also simultaneously extracted from the joint angular distributions. These results are vital for understanding CP violation and its role in the matter-antimatter asymmetry of the Universe. △ Less",
      "url": "https://arxiv.org/abs/2508.11400"
    },
    {
      "title": "Measurement of the Born cross section for $e^+e^- \\to p K^- K^- \\barΞ^+$ at $\\sqrt{s} =$ 3.5-4.9 GeV",
      "abstract": "Using $e^+ e^-$ collision data corresponding to a total integrated luminosity of 20 ${\\rm fb}^{-1}$ collected with the BESIII detector at the BEPCII collider, we present a measurement of the Born cross section for the process $e^+e^- \\to p K^-K^-\\barΞ^{+}$ at 39 center-of-mass energies between 3.5 and 4.9 GeV with a partial reconstruction technique. By performing a fit to the dressed cross section of $e^{+}e^{-}\\to p K^- K^-\\barΞ^{+}$ with a power law function for continuum production and one resonance at a time for the $ψ(3770)$, $ψ(4040)$, $ψ(4160)$, $ψ(4230)$, $ψ(4360)$, $ψ(4415)$ or $ψ(4660)$, respectively, the upper limits for the product of partial electronic width and branching fraction into the final state $p K^- K^- \\barΞ^+$ for these resonances are determined at the $90\\%$ confidence level. △ Less",
      "url": "https://arxiv.org/abs/2508.11276"
    },
    {
      "title": "Latent Fusion Jailbreak: Blending Harmful and Harmless Representations to Elicit Unsafe LLM Outputs",
      "abstract": "Large language models (LLMs) demonstrate impressive capabilities in various language tasks but are susceptible to jailbreak attacks that circumvent their safety alignments. This paper introduces Latent Fusion Jailbreak (LFJ), a representation-based attack that interpolates hidden states from harmful and benign query pairs to elicit prohibited responses. LFJ begins by selecting query pairs with high thematic and syntactic similarity, then performs gradient-guided interpolation at influential layers and tokens, followed by optimization to balance attack success, output fluency, and computational efficiency. Evaluations on models such as Vicuna and LLaMA-2 across benchmarks like AdvBench and MaliciousInstruct yield an average attack success rate (ASR) of 94.01%, outperforming existing methods. To mitigate LFJ, we propose an adversarial training defense that fine-tunes models on interpolated examples, reducing ASR by over 80% without degrading performance on benign inputs. Ablation studies validate the importance of query pair selection, hidden state interpolation components, and optimization strategies in LFJ's effectiveness. △ Less",
      "url": "https://arxiv.org/abs/2508.10029"
    },
    {
      "title": "Lightweight Auto-bidding based on Traffic Prediction in Live Advertising",
      "abstract": "Internet live streaming is widely used in online entertainment and e-commerce, where live advertising is an important marketing tool for anchors. An advertising campaign hopes to maximize the effect (such as conversions) under constraints (such as budget and cost-per-click). The mainstream control of campaigns is auto-bidding, where the performance depends on the decision of the bidding algorithm in each request. The most widely used auto-bidding algorithms include Proportional-Integral-Derivative (PID) control, linear programming (LP), reinforcement learning (RL), etc. Existing methods either do not consider the entire time traffic, or have too high computational complexity. In this paper, the live advertising has high requirements for real-time bidding (second-level control) and faces the difficulty of unknown future traffic. Therefore, we propose a lightweight bidding algorithm Binary Constrained Bidding (BiCB), which neatly combines the optimal bidding formula given by mathematical analysis and the statistical method of future traffic estimation, and obtains good approximation to the optimal result through a low complexity solution. In addition, we complement the form of upper and lower bound constraints for traditional auto-bidding modeling and give theoretical analysis of BiCB. Sufficient offline and online experiments prove BiCB's good performance and low engineering cost. △ Less",
      "url": "https://arxiv.org/abs/2508.06069"
    },
    {
      "title": "Deep Learning Based Dynamic Environment Reconstruction for Vehicular ISAC Scenarios",
      "abstract": "Integrated Sensing and Communication (ISAC) technology plays a critical role in future intelligent transportation systems, by enabling vehicles to perceive and reconstruct the surrounding environment through reuse of wireless signals, thereby reducing or even eliminating the need for additional sensors such as LiDAR or radar. However, existing ISAC based reconstruction methods often lack the ability to track dynamic scenes with sufficient accuracy and temporal consistency, limiting the real world applicability. To address this limitation, we propose a deep learning based framework for vehicular environment reconstruction by using ISAC channels. We first establish a joint channel environment dataset based on multi modal measurements from real world urban street scenarios. Then, a multistage deep learning network is developed to reconstruct the environment. Specifically, a scene decoder identifies the environmental context such as buildings, trees and so on; a cluster center decoder predicts coarse spatial layouts by localizing dominant scattering centers; a point cloud decoder recovers fine grained geometry and structure of surrounding environments. Experimental results demonstrate that the proposed method achieves high-quality dynamic environment reconstruction with a Chamfer Distance of 0.29 and F Score@1% of 0.87. In addition, complexity analysis demonstrates the efficiency and practical applicability of the method in real time scenarios. This work provides a pathway toward low cost environment reconstruction based on ISAC for future intelligent transportation. △ Less",
      "url": "https://arxiv.org/abs/2508.05226"
    },
    {
      "title": "Bidding-Aware Retrieval for Multi-Stage Consistency in Online Advertising",
      "abstract": "Online advertising systems typically use a cascaded architecture to manage massive requests and candidate volumes, where the ranking stages allocate traffic based on eCPM (predicted CTR $\\times$ Bid). With the increasing popularity of auto-bidding strategies, the inconsistency between the computationally sensitive retrieval stage and the ranking stages becomes more pronounced, as the former cannot access precise, real-time bids for the vast ad corpus. This discrepancy leads to sub-optimal platform revenue and advertiser outcomes. To tackle this problem, we propose Bidding-Aware Retrieval (BAR), a model-based retrieval framework that addresses multi-stage inconsistency by incorporating ad bid value into the retrieval scoring function. The core innovation is Bidding-Aware Modeling, incorporating bid signals through monotonicity-constrained learning and multi-task distillation to ensure economically coherent representations, while Asynchronous Near-Line Inference enables real-time updates to the embedding for market responsiveness. Furthermore, the Task-Attentive Refinement module selectively enhances feature interactions to disentangle user interest and commercial value signals. Extensive offline experiments and full-scale deployment across Alibaba's display advertising platform validated BAR's efficacy: 4.32% platform revenue increase with 22.2% impression lift for positively-operated advertisements. △ Less",
      "url": "https://arxiv.org/abs/2508.05206"
    },
    {
      "title": "Forgetting: A New Mechanism Towards Better Large Language Model Fine-tuning",
      "abstract": "Supervised fine-tuning (SFT) plays a critical role for pretrained large language models (LLMs), notably enhancing their capacity to acquire domain-specific knowledge while preserving or potentially augmenting their general-purpose capabilities. However, the efficacy of SFT hinges on data quality as well as data volume, otherwise it may result in limited performance gains or even degradation relative to the associated baselines. To mitigate such reliance, we suggest categorizing tokens within each corpus into two parts -- positive and negative tokens -- based on whether they are useful to improve model performance. Positive tokens can be trained in common ways, whereas negative tokens, which may lack essential semantics or be misleading, should be explicitly forgotten. Overall, the token categorization facilitate the model to learn less informative message, and the forgetting process shapes a knowledge boundary to guide the model on what information to learn more precisely. We conduct experiments on well-established benchmarks, finding that this forgetting mechanism not only improves overall model performance and also facilitate more diverse model responses. △ Less",
      "url": "https://arxiv.org/abs/2508.04329"
    },
    {
      "title": "Dichotomy of flat bands in the van der Waals ferromagnet Fe$_5$GeTe$_2$",
      "abstract": "Quantum materials with bands of narrow bandwidth near the Fermi level represent a promising platform for exploring a diverse range of fascinating physical phenomena, as the high density of states within the small energy window often enables the emergence of many-body physics. On one hand, flat bands can arise from strong Coulomb interactions that localize atomic orbitals. On the other hand, quantum destructive interference can quench the electronic kinetic energy. Although both have a narrow bandwidth, the two types of flat bands should exhibit very distinct spectral properties arising from their distinctive origins. So far, the two types of flat bands have only been realized in very different material settings and chemical environments, preventing a direct comparison. Here, we report the observation of the two types of flat bands within the same material system--an above-room-temperature van der Waals ferromagnet, Fe$_{5-x}$GeTe$_2$, distinguishable by a switchable iron site order. The contrasting nature of the flat bands is also identified by the remarkably distinctive temperature-evolution of the spectral features, indicating that one arises from electron correlations in the Fe(1) site-disordered phase, while the other geometrical frustration in the Fe(1) site-ordered phase. Our results therefore provide a direct juxtaposition of the distinct formation mechanism of flat bands in quantum materials, and an avenue for understanding the distinctive roles flat bands play in the presence of magnetism, topology, and lattice geometrical frustration, utilizing sublattice ordering as a key control parameter. △ Less",
      "url": "https://arxiv.org/abs/2508.03029"
    },
    {
      "title": "xDeepServe: Model-as-a-Service on Huawei CloudMatrix384",
      "abstract": "The rise of scaled-out LLMs and scaled-up SuperPods signals a new era in large-scale AI infrastructure. LLMs continue to scale out via MoE, as seen in recent models like DeepSeek, Kimi, and Qwen. In parallel, AI hardware is scaling up, with Huawei's CloudMatrix384 SuperPod offering hundreds of GB/s high-speed interconnects. Running large MoE models on SuperPod-scale hardware brings new challenges. It requires new execution models, scalable scheduling, efficient expert load balancing, and elimination of single points of failure. This paper presents xDeepServe, Huawei Cloud's LLM serving system designed for SuperPod-scale infrastructure. At its core is Transformerless, a disaggregated architecture that decomposes transformer models into modular units--attention, feedforward, and MoE--executed independently on NPUs connected via high-speed fabric. We implement this design in two forms: disaggregated prefill-decode and disaggregated MoE-attention. This fully disaggregated setup enables independent scaling of compute and memory without sacrificing performance. To support this architecture, we propose XCCL, a communication library that leverages CloudMatrix384's global shared memory to implement efficient point-to-point and all-to-all primitives. We also extend our serving engine FlowServe with system-level techniques, enabling scalable inference across hundreds of NPUs. △ Less",
      "url": "https://arxiv.org/abs/2508.02520"
    },
    {
      "title": "Measurement of Born Cross Sections and Effective Form Factors of $e^+e^-\\to Ω^{-}\\barΩ^{+}$ from$\\sqrt{s}$ = 3.7 to 4.7 GeV",
      "abstract": "Using $e^+e^-$ collision data corresponding to an integrated luminosity of 22.7 fb$^{-1}$, collected at center-of-mass energies between 3.7 and 4.7 GeV with the BESIII detector at the BEPCII storage ring, we measure the energy-dependent Born cross sections of $e^+e^-\\to Ω^{-}\\barΩ^+$ and the effective form factors of the $Ω^-$ baryon. The analysis employs a single baryon tagging method, and the results are consistent with theoretical predictions, providing critical constraints on the electromagnetic structure of the $Ω^-$ hyperon. No significant signal of charmonium or charmonium-like states decaying to $Ω^{-}\\barΩ^+$ is observed in the investigated energy range.This paper supersedes the withdrawn work arXiv:2505.03180v1. △ Less",
      "url": "https://arxiv.org/abs/2508.01359"
    },
    {
      "title": "Co-Reward: Self-supervised Reinforcement Learning for Large Language Model Reasoning via Contrastive Agreement",
      "abstract": "Although reinforcement learning with verifiable rewards (RLVR) shows promise in improving the reasoning ability of large language models (LLMs), the scaling up dilemma remains due to the reliance on human annotated labels especially for complex tasks. Recent alternatives that explore various self-reward signals exhibit the eliciting potential of LLM reasoning, but suffer from the non-negligible collapse issue. Inspired by the success of self-supervised learning, we propose \\textit{Co-Reward}, a novel RL framework that leverages contrastive agreement across semantically analogical questions as a reward basis. Specifically, we construct a similar question for each training sample (without labels) and synthesize their individual surrogate labels through a simple rollout voting, and then the reward is constructed by cross-referring the labels of each question pair to enforce the internal reasoning consistency across analogical inputs. Intuitively, such a self-supervised reward-shaping mechanism increases the difficulty of learning collapse into a trivial solution, and promotes stable reasoning elicitation and improvement through expanding the input sample variants. Empirically, Co-Reward achieves superior performance compared to other self-reward baselines on multiple reasoning benchmarks and LLM series, and reaches or even surpasses ground-truth (GT) labeled reward, with improvements of up to $+6.8\\%$ on MATH500 over GT reward on Llama-3.2-3B-Instruct. Our code is publicly available at https://github.com/tmlr-group/Co-Reward. △ Less",
      "url": "https://arxiv.org/abs/2508.00410"
    },
    {
      "title": "Precise Measurement of Chromo-Electric Dipole Moment of the Charm Quark",
      "abstract": "The combined symmetry of charge conjugation and parity ($C\\!P$) is tested in the hadronic transition $ψ(3686)\\toπ^+π^{-}J/ψ$, utilizing a dataset of 2.7 billion $ψ(3686)$ events collected by the BESIII detector at the BEPCII collider. The resulting asymmetry observable is $A_{cp} = (0.6\\pm1.8_{\\rm stat}\\pm0.1_{\\rm sys})\\times10^{-4}$ by combining the two channels $J/ψ\\to e^+e^-$ and $J/ψ\\toμ^+μ^-$ with unprecedented precision. Meanwhile, by considering the relationship between the chromo-electric dipole moment (CEDM) and the $A_{cp}$ observable derived from the quantum chromo-dynamics multipole expansion (QCDME) theory based on Chen-Kuang, as well as Cornell potential model, we yield the results of charm quark's CEDM with $d^{\\prime}_{c} = (2.6\\pm7.8_{\\rm stat}\\pm0.4_{\\rm sys}\\pm0.6_{\\rm theo})\\times10^{-16}$ $e\\cdot$cm, and $d^{\\prime}_{c} = (3.5\\pm10.5_{\\rm stat}\\pm0.6_{\\rm sys}\\pm0.5_{\\rm theo})\\times10^{-16}$ $e\\cdot$cm, respectively. These results correspond to an upper limit of $|d^{\\prime}_{c} |<2.1\\times10^{-15}\\ e\\cdot$cm at a 90\\% confidence level, an order of magnitude improvement in sensitivity compared to the previous direct bound using the same decay process. Our results provide insights into the dynamics of charmonium hadronic transitions, shedding light on their behavior in the context of $C\\!P$ violation. △ Less",
      "url": "https://arxiv.org/abs/2507.20618"
    },
    {
      "title": "Step-3 is Large yet Affordable: Model-system Co-design for Cost-effective Decoding",
      "abstract": "Large language models (LLMs) face low hardware efficiency during decoding, especially for long-context reasoning tasks. This paper introduces Step-3, a 321B-parameter VLM with hardware-aware model-system co-design optimized for minimizing decoding costs. Step-3 innovates in two key dimensions: (1) A novel Multi-Matrix Factorization Attention (MFA) mechanism that significantly reduces both KV cache size and computation while maintaining high attention expressiveness, and (2) Attention-FFN Disaggregation (AFD), a distributed inference system that decouples attention and Feed-Forward Network (FFN) layers into specialized subsystems. This co-design achieves unprecedented cost efficiency: Step-3 significantly reduces theoretical decoding costs compared with models like DeepSeek-V3 and Qwen3 MoE 235B, with the gains widening at longer context. Step-3 achieves low cost while activating 38B parameters per token (more than DeepSeek-V3 and Qwen3 MoE 235B), demonstrating that hardware-aligned attention arithmetic intensity, MoE sparsity, and AFD are critical to cost-effectiveness. We perform a head-to-head comparison with DeepSeek-V3 in its favorable scenarios. Our implementation on Hopper GPUs achieves a decoding throughput of up to 4,039 tokens per second per GPU under 50ms TPOT SLA (4K context, FP8, no MTP). It is higher than DeepSeek-V3's 2,324 in the same setup and sets a new Pareto frontier for LLM decoding. △ Less",
      "url": "https://arxiv.org/abs/2507.19427"
    },
    {
      "title": "MindSpeed RL: Distributed Dataflow for Scalable and Efficient RL Training on Ascend NPU Cluster",
      "abstract": "Reinforcement learning (RL) is a paradigm increasingly used to align large language models. Popular RL algorithms utilize multiple workers and can be modeled as a graph, where each node is the status of a worker and each edge represents dataflow between nodes. Owing to the heavy cross-node dependencies, the RL training system usually suffers from poor cluster scalability and low memory utilization. In this article, we introduce MindSpeed RL, an effective and efficient system for large-scale RL training. Unlike existing centralized methods, MindSpeed RL organizes the essential data dependencies in RL training, i.e., sample flow and resharding flow, from a distributed view. On the one hand, a distributed transfer dock strategy, which sets controllers and warehouses on the basis of the conventional replay buffer, is designed to release the dispatch overhead in the sample flow. A practical allgather--swap strategy is presented to eliminate redundant memory usage in resharding flow. In addition, MindSpeed RL further integrates numerous parallelization strategies and acceleration techniques for systematic optimization. Compared with existing state-of-the-art systems, comprehensive experiments on the RL training of popular Qwen2.5-Dense-7B/32B, Qwen3-MoE-30B, and DeepSeek-R1-MoE-671B show that MindSpeed RL increases the throughput by 1.42 ~ 3.97 times. Finally, we open--source MindSpeed RL and perform all the experiments on a super pod of Ascend with 384 neural processing units (NPUs) to demonstrate the powerful performance and reliability of Ascend. △ Less",
      "url": "https://arxiv.org/abs/2507.19017"
    },
    {
      "title": "SafeWork-R1: Coevolving Safety and Intelligence under the AI-45$^{\\circ}$ Law",
      "abstract": "We introduce SafeWork-R1, a cutting-edge multimodal reasoning model that demonstrates the coevolution of capabilities and safety. It is developed by our proposed SafeLadder framework, which incorporates large-scale, progressive, safety-oriented reinforcement learning post-training, supported by a suite of multi-principled verifiers. Unlike previous alignment methods such as RLHF that simply learn human preferences, SafeLadder enables SafeWork-R1 to develop intrinsic safety reasoning and self-reflection abilities, giving rise to safety `aha' moments. Notably, SafeWork-R1 achieves an average improvement of $46.54\\%$ over its base model Qwen2.5-VL-72B on safety-related benchmarks without compromising general capabilities, and delivers state-of-the-art safety performance compared to leading proprietary models such as GPT-4.1 and Claude Opus 4. To further bolster its reliability, we implement two distinct inference-time intervention methods and a deliberative search mechanism, enforcing step-level verification. Finally, we further develop SafeWork-R1-InternVL3-78B, SafeWork-R1-DeepSeek-70B, and SafeWork-R1-Qwen2.5VL-7B. All resulting models demonstrate that safety and capability can co-evolve synergistically, highlighting the generalizability of our framework in building robust, reliable, and trustworthy general-purpose AI. △ Less",
      "url": "https://arxiv.org/abs/2507.18576"
    },
    {
      "title": "NeuralDB: Scaling Knowledge Editing in LLMs to 100,000 Facts with Neural KV Database",
      "abstract": "Efficiently editing knowledge stored in large language models (LLMs) enables model updates without large-scale training. One possible solution is Locate-and-Edit (L\\&E), allowing simultaneous modifications of a massive number of facts. However, such editing may compromise the general abilities of LLMs and even result in forgetting edited facts when scaling up to thousands of edits. In this paper, we model existing linear L\\&E methods as querying a Key-Value (KV) database. From this perspective, we then propose NeuralDB, an editing framework that explicitly represents the edited facts as a neural KV database equipped with a non-linear gated retrieval module, % In particular, our gated module only operates when inference involves the edited facts, effectively preserving the general abilities of LLMs. Comprehensive experiments involving the editing of 10,000 facts were conducted on the ZsRE and CounterFacts datasets, using GPT2-XL, GPT-J (6B) and Llama-3 (8B). The results demonstrate that NeuralDB not only excels in editing efficacy, generalization, specificity, fluency, and consistency, but also preserves overall performance across six representative text understanding and generation tasks. Further experiments indicate that NeuralDB maintains its effectiveness even when scaled to 100,000 facts (\\textbf{50x} more than in prior work). △ Less",
      "url": "https://arxiv.org/abs/2507.18028"
    },
    {
      "title": "Step-Audio 2 Technical Report",
      "abstract": "This paper presents Step-Audio 2, an end-to-end multi-modal large language model designed for industry-strength audio understanding and speech conversation. By integrating a latent audio encoder and reasoning-centric reinforcement learning (RL), Step-Audio 2 achieves promising performance in automatic speech recognition (ASR) and audio understanding. To facilitate genuine end-to-end speech conversation, Step-Audio 2 incorporates the generation of discrete audio tokens into language modeling, significantly enhancing its responsiveness to paralinguistic information such as speaking styles and emotions. To effectively leverage the rich textual and acoustic knowledge in real-world data, Step-Audio 2 integrates retrieval-augmented generation (RAG) and is able to call external tools such as web search to mitigate hallucination and audio search to switch timbres. Trained on millions of hours of speech and audio data, Step-Audio 2 delivers intelligence and expressiveness across diverse conversational scenarios. Evaluation results demonstrate that Step-Audio 2 achieves state-of-the-art performance on various audio understanding and conversational benchmarks compared to other open-source and commercial solutions. Please visit https://github.com/stepfun-ai/Step-Audio2 for more information. △ Less",
      "url": "https://arxiv.org/abs/2507.16632"
    },
    {
      "title": "Voice-based AI Agents: Filling the Economic Gaps in Digital Health Delivery",
      "abstract": "The integration of voice-based AI agents in healthcare presents a transformative opportunity to bridge economic and accessibility gaps in digital health delivery. This paper explores the role of large language model (LLM)-powered voice assistants in enhancing preventive care and continuous patient monitoring, particularly in underserved populations. Drawing insights from the development and pilot study of Agent PULSE (Patient Understanding and Liaison Support Engine) -- a collaborative initiative between IBM Research, Cleveland Clinic Foundation, and Morehouse School of Medicine -- we present an economic model demonstrating how AI agents can provide cost-effective healthcare services where human intervention is economically unfeasible. Our pilot study with 33 inflammatory bowel disease patients revealed that 70\\% expressed acceptance of AI-driven monitoring, with 37\\% preferring it over traditional modalities. Technical challenges, including real-time conversational AI processing, integration with healthcare systems, and privacy compliance, are analyzed alongside policy considerations surrounding regulation, bias mitigation, and patient autonomy. Our findings suggest that AI-driven voice agents not only enhance healthcare scalability and efficiency but also improve patient engagement and accessibility. For healthcare executives, our cost-utility analysis demonstrates huge potential savings for routine monitoring tasks, while technologists can leverage our framework to prioritize improvements yielding the highest patient impact. By addressing current limitations and aligning AI development with ethical and regulatory frameworks, voice-based AI agents can serve as a critical entry point for equitable, sustainable digital healthcare solutions. △ Less",
      "url": "https://arxiv.org/abs/2507.16229"
    },
    {
      "title": "Precise Measurement of $^{216}$Po Half-life with Exact Parent-daughter Pairing in PandaX-4T",
      "abstract": "We report a precise measurement of $^{216}\\rm Po$ half-life using the PandaX-4T liquid xenon time projection chamber (TPC). $^{220}\\rm Rn $, emanating from a $^{228}\\rm Th $ calibration source, is injected to the detector and undergoes successive $α$ decays, first to $^{216}\\rm Po$ and then to $^{212}\\rm Pb$. PandaX-4T detector measures the 5-dimensional (5D) information of each decay, including time, energy, and 3-dimensional positions. Therefore, we can identify the $^{220}\\rm Rn $ and $^{216}\\rm Po$ decay events and pair them exactly to extract the lifetime of each $^{216}\\rm Po$. With a large data set and high-precision $^{220}\\rm $Rn-$^{216}\\rm $Po pairing technique, we measure the $^{216}\\rm Po$ half-life to be $143.7\\pm0.5$ ms, which is the most precise result to date and agrees with previously published values. The leading precision of this measurement demonstrates the power of 5D calorimeter and the potential of exact parent-daughter pairing in the xenon TPC. △ Less",
      "url": "https://arxiv.org/abs/2507.13241"
    },
    {
      "title": "Search for Light Dark Matter with 259-day data in PandaX-4T",
      "abstract": "We present a search for light dark matter particles through their interactions with atomic electrons and nucleons, utilizing PandaX-4T data with an effective exposure of 1.04 tonne$\\cdot$year for ionization-only data and 1.20 tonne$\\cdot$year for paired data. Our analysis focuses on the energy range (efficiency$>$0.01) of approximately 0.33 to 3 keV for nuclear recoils, and from 0.04 to 0.39 keV for electronic recoils. We establish the most stringent constraints on spin-independent dark matter-nucleon interactions within a mass range of 2.5 to 5.0 GeV/$c^2$, spin-dependent neutron-only interactions within 1.0 to 5.6 GeV/$c^2$, and spin-dependent proton-only interactions within 1.0 to 4.1 GeV/$c^2$. Additionally, our results improve the upper limits on the dark matter-electron scattering cross-section by a factor of 1.5 and 9.3 for heavy and light mediator scenarios respectively within 50 MeV/$c^2$ to 10 GeV/$c^2$, compared with previous best results. △ Less",
      "url": "https://arxiv.org/abs/2507.11930"
    },
    {
      "title": "Observation of the electromagnetic radiative decays of the \\boldmath{$Λ(1520)$} and \\boldmath{$Λ(1670)$} to \\boldmath{$γΣ^0$}",
      "abstract": "Using $(10087\\pm 44)\\times10^6$ $J/ψ$ events collected with the BESIII detector, we report the first observation of the electromagnetic radiative decays of the $Λ(1520)$ and $Λ(1670)$ to $γΣ^0$, with a statistical significance of $16.6σ$ and $23.5σ$, respectively. The ratio of the branching fractions $\\frac{\\mathcal{B}(Λ(1520)\\toγΛ)}{\\mathcal{B}(Λ(1520)\\toγΣ^0)}$ is determined to be $2.88\\pm0.27(\\text{stat.})\\pm0.21(\\text{syst.})$, which is in good agreement with flavor SU(3) symmetry. The branching fraction of $Λ(1520)\\toγΣ^0$ is measured to be $\\mathcal{B}(Λ(1520)\\toγΣ^0)=(2.95\\pm0.28(\\text{stat.})\\pm0.56(\\text{syst.}))\\times 10^{-3}$, corresponding to a partial width of $Γ(Λ(1520)\\toγΣ^0)=(47.2\\pm4.5(\\text{stat.})\\pm9.0(\\text{syst.}))$ keV, which is inconsistent with predictions from the relativized constituent quark model and the Algebraic model. Additionally, we observe a clear resonant structure in the $γΣ^0$ mass spectrum around 1.67 GeV/$c^2$, attributed to the $Λ(1670)$. The product branching fraction $\\mathcal{B}(J/ψ\\to\\barΛΛ(1670)+c.c.)\\times\\mathcal{B}(Λ(1670)\\toγΣ^0)$ is measured for the first time as $(5.39\\pm0.29(\\text{stat.})\\pm 0.44(\\text{syst.}))\\times 10^{-6}$. However, no corresponding structure is seen in the $γΛ$ mass spectrum, so an upper limit on the product branching fraction $\\mathcal{B}(J/ψ\\to\\barΛΛ(1670)+c.c.)\\times\\mathcal{B}(Λ(1670)\\toγΛ)$ is determined to be $5.97\\times10^{-7}$ at the 90\\% confidence level. △ Less",
      "url": "https://arxiv.org/abs/2507.11145"
    },
    {
      "title": "Search for the charged lepton flavor violating decay $ψ(3686)\\to e^{\\pm}μ^{\\mp}$",
      "abstract": "By analyzing $(2367.0\\pm11.1)\\times10^6$ $ψ(3686)$ events collected in $e^+e^-$ collisions at $\\sqrt{s}=3.686~\\rm GeV$ with the BESIII detector at the BEPCII collider, we report the first search for the charged lepton flavor violating decay $ψ(3686)\\to e^{\\pm}μ^{\\mp}$. No signal is found. An upper limit on the branching fraction $\\mathcal{B}(ψ(3686)\\to e^{\\pm}μ^{\\mp})$ is determined to be $1.4\\times10^{-8}$ at the 90\\% confidence level. △ Less",
      "url": "https://arxiv.org/abs/2507.10331"
    },
    {
      "title": "Search for the lepton number violating process $J/ψ\\to K^+K^+e^-e^- +c.c.$",
      "abstract": "Based on $(10087\\pm 44)\\times10^{6}$ $J/ψ$ events collected with the BESIII detector, we search for the lepton number violating decay $J/ψ\\to K^+K^+e^-e^- + c.c.$ for the first time. The upper limit on the branching fraction of this decay is set to be $2.1 \\times 10^{-9}$ at the 90$\\%$ confidence level with a frequentist method. This is the first search for $J/ψ$ decays with the lepton number change by two, offering valuable insights into the underlying physical processes. △ Less",
      "url": "https://arxiv.org/abs/2507.06872"
    },
    {
      "title": "Gemini 2.5: Pushing the Frontier with Advanced Reasoning, Multimodality, Long Context, and Next Generation Agentic Capabilities",
      "abstract": "In this report, we introduce the Gemini 2.X model family: Gemini 2.5 Pro and Gemini 2.5 Flash, as well as our earlier Gemini 2.0 Flash and Flash-Lite models. Gemini 2.5 Pro is our most capable model yet, achieving SoTA performance on frontier coding and reasoning benchmarks. In addition to its incredible coding and reasoning skills, Gemini 2.5 Pro is a thinking model that excels at multimodal understanding and it is now able to process up to 3 hours of video content. Its unique combination of long context, multimodal and reasoning capabilities can be combined to unlock new agentic workflows. Gemini 2.5 Flash provides excellent reasoning abilities at a fraction of the compute and latency requirements and Gemini 2.0 Flash and Flash-Lite provide high performance at low latency and cost. Taken together, the Gemini 2.X model generation spans the full Pareto frontier of model capability vs cost, allowing users to explore the boundaries of what is possible with complex agentic problem solving. △ Less",
      "url": "https://arxiv.org/abs/2507.06261"
    },
    {
      "title": "Introduction to the Chinese Space Station Survey Telescope (CSST)",
      "abstract": "The Chinese Space Station Survey Telescope (CSST) is a next-generation Stage-IV sky survey telescope, distinguished by its large field of view (FoV), high image quality, and multi-band observation capabilities. It can simultaneously conduct precise measurements of the Universe by performing multi-color photometric imaging and slitless spectroscopic surveys. The CSST is equipped with five scientific instruments, i.e. Multi-band Imaging and Slitless Spectroscopy Survey Camera (SC), Multi-Channel Imager (MCI), Integral Field Spectrograph (IFS), Cool Planet Imaging Coronagraph (CPI-C), and THz Spectrometer (TS). Using these instruments, CSST is expected to make significant contributions and discoveries across various astronomical fields, including cosmology, galaxy and active galactic nuclei (AGN), the Milky Way and nearby galaxies, stars, exoplanets, Solar System objects, astrometry, and transients and variable sources. This review aims to provide a comprehensive overview of the CSST instruments, observational capabilities, data products, and scientific potential. △ Less",
      "url": "https://arxiv.org/abs/2507.04618"
    },
    {
      "title": "When Data-Free Knowledge Distillation Meets Non-Transferable Teacher: Escaping Out-of-Distribution Trap is All You Need",
      "abstract": "Data-free knowledge distillation (DFKD) transfers knowledge from a teacher to a student without access the real in-distribution (ID) data. Its common solution is to use a generator to synthesize fake data and use them as a substitute for real ID data. However, existing works typically assume teachers are trustworthy, leaving the robustness and security of DFKD from untrusted teachers largely unexplored. In this work, we conduct the first investigation into distilling non-transferable learning (NTL) teachers using DFKD, where the transferability from an ID domain to an out-of-distribution (OOD) domain is prohibited. We find that NTL teachers fool DFKD through divert the generator's attention from the useful ID knowledge to the misleading OOD knowledge. This hinders ID knowledge transfer but prioritizes OOD knowledge transfer. To mitigate this issue, we propose Adversarial Trap Escaping (ATEsc) to benefit DFKD by identifying and filtering out OOD-like synthetic samples. Specifically, inspired by the evidence that NTL teachers show stronger adversarial robustness on OOD samples than ID samples, we split synthetic samples into two groups according to their robustness. The fragile group is treated as ID-like data and used for normal knowledge distillation, while the robust group is seen as OOD-like data and utilized for forgetting OOD knowledge. Extensive experiments demonstrate the effectiveness of ATEsc for improving DFKD against NTL teachers. Code is released at https://github.com/tmllab/2025_ICML_ATEsc. △ Less",
      "url": "https://arxiv.org/abs/2507.04119"
    },
    {
      "title": "Frontiers of Generative AI for Network Optimization: Theories, Limits, and Visions",
      "abstract": "While interest in the application of generative AI (GenAI) in network optimization has surged in recent years, its rapid progress has often overshadowed critical limitations intrinsic to generative models that remain insufficiently examined in existing literature. This survey provides a comprehensive review and critical analysis of GenAI in network optimization. We focus on the two dominant paradigms of GenAI including generative diffusion models (GDMs) and large pre-trained models (LPTMs), and organize our discussion around a categorization we introduce, dividing network optimization problems into two primary formulations: one-shot optimization and Markov decision process (MDP). We first trace key works, including foundational contributions from the AI community, and categorize current efforts in network optimization. We also review frontier applications of GDMs and LPTMs in other networking tasks, providing additional context. Furthermore, we present theoretical generalization bounds for GDMs in both one-shot and MDP settings, offering insights into the fundamental factors affecting model performance. Most importantly, we reflect on the overestimated perception of GenAI's general capabilities and caution against the all-in-one illusion it may convey. We highlight critical limitations, including difficulties in constraint satisfying, limited concept understanding, and the inherent probabilistic nature of outputs. We also propose key future directions, such as bridging the gap between generation and optimization. Although they are increasingly integrated in implementations, they differ fundamentally in both objectives and underlying mechanisms, necessitating a deeper understanding of their theoretical connections. Ultimately, this survey aims to provide a structured overview and a deeper insight into the strengths, limitations, and potential of GenAI in network optimization. △ Less",
      "url": "https://arxiv.org/abs/2507.01773"
    },
    {
      "title": "Benchmarking Generalizable Bimanual Manipulation: RoboTwin Dual-Arm Collaboration Challenge at CVPR 2025 MEIS Workshop",
      "abstract": "Embodied Artificial Intelligence (Embodied AI) is an emerging frontier in robotics, driven by the need for autonomous systems that can perceive, reason, and act in complex physical environments. While single-arm systems have shown strong task performance, collaborative dual-arm systems are essential for handling more intricate tasks involving rigid, deformable, and tactile-sensitive objects. To advance this goal, we launched the RoboTwin Dual-Arm Collaboration Challenge at the 2nd MEIS Workshop, CVPR 2025. Built on the RoboTwin Simulation platform (1.0 and 2.0) and the AgileX COBOT-Magic Robot platform, the competition consisted of three stages: Simulation Round 1, Simulation Round 2, and a final Real-World Round. Participants totally tackled 17 dual-arm manipulation tasks, covering rigid, deformable, and tactile-based scenarios. The challenge attracted 64 global teams and over 400 participants, producing top-performing solutions like SEM and AnchorDP3 and generating valuable insights into generalizable bimanual policy learning. This report outlines the competition setup, task design, evaluation methodology, key findings and future direction, aiming to support future research on robust and generalizable bimanual manipulation policies. The Challenge Webpage is available at https://robotwin-benchmark.github.io/cvpr-2025-challenge/. △ Less",
      "url": "https://arxiv.org/abs/2506.23351"
    },
    {
      "title": "Seamless Interaction: Dyadic Audiovisual Motion Modeling and Large-Scale Dataset",
      "abstract": "Human communication involves a complex interplay of verbal and nonverbal signals, essential for conveying meaning and achieving interpersonal goals. To develop socially intelligent AI technologies, it is crucial to develop models that can both comprehend and generate dyadic behavioral dynamics. To this end, we introduce the Seamless Interaction Dataset, a large-scale collection of over 4,000 hours of face-to-face interaction footage from over 4,000 participants in diverse contexts. This dataset enables the development of AI technologies that understand dyadic embodied dynamics, unlocking breakthroughs in virtual agents, telepresence experiences, and multimodal content analysis tools. We also develop a suite of models that utilize the dataset to generate dyadic motion gestures and facial expressions aligned with human speech. These models can take as input both the speech and visual behavior of their interlocutors. We present a variant with speech from an LLM model and integrations with 2D and 3D rendering methods, bringing us closer to interactive virtual agents. Additionally, we describe controllable variants of our motion models that can adapt emotional responses and expressivity levels, as well as generating more semantically-relevant gestures. Finally, we discuss methods for assessing the quality of these dyadic motion models, which are demonstrating the potential for more intuitive and responsive human-AI interactions. △ Less",
      "url": "https://arxiv.org/abs/2506.22554"
    },
    {
      "title": "Weak-to-Strong GraphRAG: Aligning Weak Retrievers with Large Language Models for Graph-based Retrieval Augmented Generation",
      "abstract": "Graph-based retrieval-augmented generation (RAG) enables large language models (LLMs) to ground responses with structured external knowledge from up-to-date knowledge graphs (KGs) and reduce hallucinations. However, LLMs often rely on a weak retriever in graph-based RAG: I) Due to the lack of ground truth, the retriever is often trained on weak supervision, which often introduces spurious signals to the LLMs. II) Due to the abstraction of graph data, the retrieved knowledge is often presented in unorganized forms. To mitigate the issue, we present Refined Graph-based RAG (ReG) to align weak retrievers to LLMs for graph-based RAG. Specifically, ReG incorporates LLM feedback to get rid of spurious signals and improve the quality of the supervision. Meanwhile, ReG introduces a structure-aware reorganization module to refactor the retrieval results into logically coherent evidence chains. Experiments on prominent benchmarks demonstrate that ReG significantly and consistently brings improvements across different LLM backbones by up to 10%. The improved supervision quality enables ReG to match the state-of-the-art performance with 5% training data and to transfer to out-of-distribution KGs. Notably, when adopted to reasoning-based LLMs, ReG reduces the reasoning token cost by up to 30% and improves the performance by up to 4%. △ Less",
      "url": "https://arxiv.org/abs/2506.22518"
    },
    {
      "title": "Unveiling Causal Reasoning in Large Language Models: Reality or Mirage?",
      "abstract": "Causal reasoning capability is critical in advancing large language models (LLMs) toward strong artificial intelligence. While versatile LLMs appear to have demonstrated capabilities in understanding contextual causality and providing responses that obey the laws of causality, it remains unclear whether they perform genuine causal reasoning akin to humans. However, current evidence indicates the contrary. Specifically, LLMs are only capable of performing shallow (level-1) causal reasoning, primarily attributed to the causal knowledge embedded in their parameters, but they lack the capacity for genuine human-like (level-2) causal reasoning. To support this hypothesis, methodologically, we delve into the autoregression mechanism of transformer-based LLMs, revealing that it is not inherently causal. Empirically, we introduce a new causal Q&A benchmark called CausalProbe-2024, whose corpora are fresh and nearly unseen for the studied LLMs. The LLMs exhibit a significant performance drop on CausalProbe-2024 compared to earlier benchmarks, indicating the fact that they primarily engage in level-1 causal reasoning. To bridge the gap towards level-2 causal reasoning, we draw inspiration from the fact that human reasoning is usually facilitated by general knowledge and intended goals. We propose G^2-Reasoner, a method that incorporates general knowledge and goal-oriented prompts into LLMs' causal reasoning processes. Experiments demonstrate that G^2-Reasoner significantly enhances LLMs' causal reasoning capability, particularly in fresh and counterfactual contexts. This work sheds light on a new path for LLMs to advance towards genuine causal reasoning, going beyond level-1 and making strides towards level-2. △ Less",
      "url": "https://arxiv.org/abs/2506.21215"
    },
    {
      "title": "Precise Measurement of the $Λ$ Electric Dipole Moment through the Entangled Strange Baryon-Antibaryon System",
      "abstract": "The dominance of matter over antimatter in the universe has consistently driven the pursuit of new physics beyond the Standard Model that violates charge-parity symmetry. Unlike the well-constrained electrons and neutrons, strange baryons (hyperons) remain a largely unexplored territory, in which interactions between hyperons and particles from new physics could induce a non-trivial electric dipole moment (EDM). However, direct measurements of hyperon EDMs through spin precession are highly challenging due to their short lifetimes. In this paper, we present a novel method to extract the EDM of the lightest hyperon, $Λ$, using the entangled $Λ$$\\overlineΛ$ system. Our result is consistent with zero, achieving a three-order-of-magnitude improvement over the previous upper limit established in the 1980s with comparable statistics, providing stringent constraints on potential new physics. △ Less",
      "url": "https://arxiv.org/abs/2506.19180"
    },
    {
      "title": "jina-embeddings-v4: Universal Embeddings for Multimodal Multilingual Retrieval",
      "abstract": "We introduce jina-embeddings-v4, a 3.8 billion parameter multimodal embedding model that unifies text and image representations through a novel architecture supporting both single-vector and multi-vector embeddings in the late interaction style. The model incorporates task-specific Low-Rank Adaptation (LoRA) adapters to optimize performance across diverse retrieval scenarios, including query-document retrieval, semantic text similarity, and code search. Comprehensive evaluations demonstrate that jina-embeddings-v4 achieves state-of-the-art performance on both single-modal and cross-modal retrieval tasks, with particular strength in processing visually rich content such as tables, charts, diagrams, and mixed-media formats. To facilitate evaluation of this capability, we also introduce Jina-VDR, a novel benchmark specifically designed for visually rich image retrieval. △ Less",
      "url": "https://arxiv.org/abs/2506.18902"
    },
    {
      "title": "What You Think Is What You Get: Bridge User Intent and Transfer Function Design through Multimodal Large Language Models",
      "abstract": "Direct volume rendering (DVR) is a fundamental technique for visualizing volumetric data, with transfer functions (TFs) playing a crucial role in extracting meaningful structures. However, designing effective TFs remains unintuitive due to the semantic gap between user intent and TF parameter space. Researchers have developed numerous TF optimization methods to bridge this gap. However, existing methods still face two challenges: large exploration space and weak generalizability. To address these issues, we propose What You Think is What You Get (WYTWYG) framework, which leveraging Multi-model Large Language Models (MLLMs) to guide the TF optimization based on user intent. Specifically, we first introduce a novel TF optimization approach comprising two core components: (1) an evolution-based explorer for effective exploration of the TF space, and (2) a volume rendering quality evaluator based on MLLMs to provide generalizable visual guidance. We further propose a TF interactive design system based on this approach. We demonstrate the general applicability of our framework through three case studies, and validate the effectiveness of each component through extensive experiments. Our code is available at: https://github.com/wyysteelhead/TFevolve. △ Less",
      "url": "https://arxiv.org/abs/2506.18407"
    },
    {
      "title": "Measurements of the absolute branching fractions of the doubly Cabibbo-suppressed decays $D^+\\to K^+π^0$, $D^+\\to K^+η$ and $D^+\\to K^+η^{\\prime}$",
      "abstract": "Using $20.3\\,\\rm fb^{-1}$ of $e^+e^-$ collision data collected at a center-of-mass energy of 3.773\\,GeV with the BESIII detector, we present improved measurements of the absolute branching fractions of the doubly Cabibbo-suppressed decays $D^+\\to K^+π^0$, $D^+\\to K^+η$ and $ D^+ \\to K^+ η^{\\prime}$ with the double-tag method. The statistical significance of each signal decay exceeds $10σ$. The branching fractions are determined to be ${\\mathcal B}(D^+\\to K^+ π^0) = (1.45 \\pm 0.06 \\pm 0.06)\\times 10^{-4}$, ${\\mathcal B}(D^+\\to K^+ η) = (1.17 \\pm 0.10 \\pm 0.03)\\times 10^{-4}$ and ${\\mathcal B}(D^+\\to K^+ η^{\\prime}) = (1.88 \\pm 0.15 \\pm 0.06)\\times 10^{-4}$, where the first uncertainties are statistical and the second systematic. These results are consistent with the world average values but with significantly improved precision. △ Less",
      "url": "https://arxiv.org/abs/2506.15533"
    },
    {
      "title": "NTIRE 2025 Image Shadow Removal Challenge Report",
      "abstract": "This work examines the findings of the NTIRE 2025 Shadow Removal Challenge. A total of 306 participants have registered, with 17 teams successfully submitting their solutions during the final evaluation phase. Following the last two editions, this challenge had two evaluation tracks: one focusing on reconstruction fidelity and the other on visual perception through a user study. Both tracks were evaluated with images from the WSRD+ dataset, simulating interactions between self- and cast-shadows with a large number of diverse objects, textures, and materials. △ Less",
      "url": "https://arxiv.org/abs/2506.15524"
    },
    {
      "title": "Resolving Phonons in Superconductor Bi2Sr2CaCu2O8+δ at Sub-Unit-Cell Resolution",
      "abstract": "The role of phonons in cuprates remains controversial, with their complex lattice structure complicating the investigation. Here, we identify phonon modes originating from charge reservoir and superconducting layers of Bi2Sr2CaCu2O8+δ using sub-unit-cell resolved electron energy loss spectroscopy in a scanning transmission electron microscope. We reveal several phonon modes exhibiting layer-specific localization: ~78 meV in-plane modes localized in the CuO2 planes, ~42 meV hybrid (in-plane and out-of-plane) modes in the CuO2 planes, and ~38 meV hybrid modes in the BiO layers. We also observe a periodic modulation of phonon frequencies induced by the superstructure, spatially correlating with the superconducting gap variation. Our findings offer new insights into the electron-phonon coupling in cuprates, fostering deeper exploration of the microscopic linkage between lattice dynamics and superconductivity. △ Less",
      "url": "https://arxiv.org/abs/2506.14127"
    }
  ]
}