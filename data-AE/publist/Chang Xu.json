{
  "author": "Chang Xu",
  "results": [
    {
      "title": "Amplitude analysis of $B^0 \\rightarrow η_c(1S) K^+ π^- $ decays",
      "abstract": "An amplitude analysis of the $B^0 \\rightarrow η_{c}(1S) K^+ π^- $ decays with $η_{c}(1S) \\to p \\bar{p}$ is performed using a sample corresponding to an integrated luminosity of 9$\\text{fb}^{-1}$ of $pp$ collision data collected by the LHCb detector at centre-of-mass energies of $\\sqrt{s}$ = 7, 8 and 13TeV. The data are described with a model including only intermediate contributions from known $K^{0\\star}$ resonances. Evidence for an exotic resonance in the $η_{c}(1S) π^{-} $ system, reported in a previous analysis of this decay channel, is not confirmed. The inclusive branching fraction of the $B^0 \\rightarrow η_{c}(1S) K^+ π^- $ decays is measured to be \\begin{align*} \\mathcal{B}(B^0 \\rightarrow η_{c}(1S) K^+ π^- ) = (5.82 \\pm 0.20 \\pm 0.23 \\pm 0.55) \\times 10^{-4}, \\end{align*} where the first uncertainty is statistical, the second systematic, and the third arises from the limited knowledge of external branching fractions. △ Less",
      "url": "https://arxiv.org/abs/2509.03133"
    },
    {
      "title": "Observation of $e^+e^-\\toηΥ(2S)$ and search for $e^+e^-\\toηΥ(1S),~γX_b$ at $\\sqrt{s}$ near 10.75 GeV",
      "abstract": "We present an analysis of the processes $e^{+}e^{-}\\toηΥ(1S)$, $ηΥ(2S)$, and $γX_b$ with $X_b\\toπ^+π^-χ_{bJ},~χ_{bJ}\\toγΥ(1S)$ $(J=1,~2)$ reconstructed from $γγπ^+π^-\\ell^+\\ell^-~(\\ell=e,~μ)$ final states in $19.6~{\\rm fb^{-1}}$ of Belle II data collected at four energy points near the peak of the $Υ(10753)$ resonance. Here, $X_b$ is a hypothetical bottomonium-sector partner of the $X(3872)$. A signal of $e^{+}e^{-}\\toηΥ(2S)$ is observed with a significance greater than $6.0σ$. The central value of the Born cross section at 10.653 GeV is measured to be higher than that at 10.745 GeV, and we find evidence for a possible new state near $B^{*}\\bar B^{*}$ threshold, with a significance of $3.2σ$. No significant signal is observed for $e^{+}e^{-}\\toηΥ(1S)$ or $γX_b$. Upper limits on the Born cross sections for the processes $e^{+}e^{-}\\toηΥ(1S)$ and $e^{+}e^{-}\\toγX_b$ with $X_b\\toπ^+π^-χ_{bJ}$ are determined. △ Less",
      "url": "https://arxiv.org/abs/2509.01917"
    },
    {
      "title": "Oyster-I: Beyond Refusal -- Constructive Safety Alignment for Responsible Language Models",
      "abstract": "Large language models (LLMs) typically deploy safety mechanisms to prevent harmful content generation. Most current approaches focus narrowly on risks posed by malicious actors, often framing risks as adversarial events and relying on defensive refusals. However, in real-world settings, risks also come from non-malicious users seeking help while under psychological distress (e.g., self-harm intentions). In such cases, the model's response can strongly influence the user's next actions. Simple refusals may lead them to repeat, escalate, or move to unsafe platforms, creating worse outcomes. We introduce Constructive Safety Alignment (CSA), a human-centric paradigm that protects against malicious misuse while actively guiding vulnerable users toward safe and helpful results. Implemented in Oyster-I (Oy1), CSA combines game-theoretic anticipation of user reactions, fine-grained risk boundary discovery, and interpretable reasoning control, turning safety into a trust-building process. Oy1 achieves state-of-the-art safety among open models while retaining high general capabilities. On our Constructive Benchmark, it shows strong constructive engagement, close to GPT-5, and unmatched robustness on the Strata-Sword jailbreak dataset, nearing GPT-o1 levels. By shifting from refusal-first to guidance-first safety, CSA redefines the model-user relationship, aiming for systems that are not just safe, but meaningfully helpful. We release Oy1, code, and the benchmark to support responsible, user-centered AI. △ Less",
      "url": "https://arxiv.org/abs/2509.01909"
    },
    {
      "title": "Strata-Sword: A Hierarchical Safety Evaluation towards LLMs based on Reasoning Complexity of Jailbreak Instructions",
      "abstract": "Large language models (LLMs) have gained widespread recognition for their superior comprehension and have been deployed across numerous domains. Building on Chain-of-Thought (CoT) ideology, Large Reasoning models (LRMs) further exhibit strong reasoning skills, enabling them to infer user intent more accurately and respond appropriately. However, both LLMs and LRMs face the potential safety risks under jailbreak attacks, which raise concerns about their safety capabilities. Current safety evaluation methods often focus on the content dimensions, or simply aggregate different attack methods, lacking consideration of the complexity. In fact, instructions of different complexity can reflect the different safety capabilities of the model: simple instructions can reflect the basic values of the model, while complex instructions can reflect the model's ability to deal with deeper safety risks. Therefore, a comprehensive benchmark needs to be established to evaluate the safety performance of the model in the face of instructions of varying complexity, which can provide a better understanding of the safety boundaries of the LLMs. Thus, this paper first quantifies \"Reasoning Complexity\" as an evaluable safety dimension and categorizes 15 jailbreak attack methods into three different levels according to the reasoning complexity, establishing a hierarchical Chinese-English jailbreak safety benchmark for systematically evaluating the safety performance of LLMs. Meanwhile, to fully utilize unique language characteristics, we first propose some Chinese jailbreak attack methods, including the Chinese Character Disassembly attack, Lantern Riddle attack, and Acrostic Poem attack. A series of experiments indicate that current LLMs and LRMs show different safety boundaries under different reasoning complexity, which provides a new perspective to develop safer LLMs and LRMs. △ Less",
      "url": "https://arxiv.org/abs/2509.01444"
    },
    {
      "title": "IndusGCC: A Data Benchmark and Evaluation Framework for GUI-Based General Computer Control in Industrial Automation",
      "abstract": "As Industry 4.0 progresses, flexible manufacturing has become a cornerstone of modern industrial systems, with equipment automation playing a pivotal role. However, existing control software for industrial equipment, typically reliant on graphical user interfaces (GUIs) that require human interactions such as mouse clicks or screen touches, poses significant barriers to the adoption of code-based equipment automation. Recently, Large Language Model-based General Computer Control (LLM-GCC) has emerged as a promising approach to automate GUI-based operations. However, industrial settings pose unique challenges, including visually diverse, domain-specific interfaces and mission-critical tasks demanding high precision. This paper introduces IndusGCC, the first dataset and benchmark tailored to LLM-GCC in industrial environments, encompassing 448 real-world tasks across seven domains, from robotic arm control to production line configuration. IndusGCC features multimodal human interaction data with the equipment software, providing robust supervision for GUI-level code generation. Additionally, we propose a novel evaluation framework with functional and structural metrics to assess LLM-generated control scripts. Experimental results on mainstream LLMs demonstrate both the potential of LLM-GCC and the challenges it faces, establishing a strong foundation for future research toward fully automated factories. Our data and code are publicly available at: \\href{https://github.com/Golden-Arc/IndustrialLLM}{https://github.com/Golden-Arc/IndustrialLLM. △ Less",
      "url": "https://arxiv.org/abs/2509.01199"
    },
    {
      "title": "Helicity amplitude and branching fraction measurement of $χ_{cJ} \\rightarrow Λ\\barΛ $",
      "abstract": "Utilizing $2712.4 \\pm 14.3$ million $ψ(3686)$ events accumulated by the BESIII experiment, we perform a partial wave analysis of $ψ(3686)\\rightarrowγχ_{cJ}\\rightarrowγΛ\\barΛ$ decay ($J=0,1,2$). The ratio of the helicity amplitudes with same (++) and opposite (+-) helicity for $χ_{c2}\\rightarrowΛ\\barΛ$ decay is determined for the first time to be $R_{χ_{c2}}=0.575 \\pm 0.048 \\pm 0.018 $, with a relative phase angle $ΔΦ_{χ_{c2}} = 0.37 \\pm 0.15 \\pm 0.05 $~rad. The parameters of the angular distribution of $χ_{c2}$ are determined to be $α_{χ_{c2}} = -0.211 \\pm 0.100 \\pm 0.050 $ and $β_{χ_{c2}} = -0.039 \\pm 0.089 \\pm 0.033 $, based on the distribution $dN / d\\cosθ= 1 + α_{χ_{c2}} \\cos^2θ+ β_{χ_{c2}} \\cos^4θ$. The width of $χ_{c0}$ is determined to be $12.31 \\pm 0.26 \\pm 0.12 $~MeV. Additionally, the branching fractions for $χ_{cJ} \\rightarrow Λ\\barΛ$ are measured to be $(3.662 \\pm 0.048 \\pm 0.111) \\times 10^{-4}$, $(1.182 \\pm 0.026 \\pm 0.042) \\times 10^{-4}$, and $(1.704 \\pm 0.035 \\pm 0.057) \\times 10^{-4}$ for $χ_{c0}$, $χ_{c1}$ and $χ_{c2}$, respectively, where the first uncertainty is statistical and the second systematic. △ Less",
      "url": "https://arxiv.org/abs/2509.00289"
    },
    {
      "title": "A Central Differential Flux with High-Order Dissipation for Robust Simulations of Transcritical Flows",
      "abstract": "The simulation of transcritical flows remains challenging due to strong thermodynamic nonlinearities that induce spurious pressure oscillations in conventional schemes.While primitive-variable formulations offer improved robustness under such conditions, they are always limited by energy conservation errors and the absence of systematic high-order treatments for numerical fluxes. In this paper, we introduce the Central Differential flux with High-Order Dissipation (CDHD), a novel numerical flux solver designed for primitive-variable discretization. This method combines a central flux for advection with a minimal, upwind-biased dissipation term to stabilize the simulation while maintaining formal accuracy. The dissipation term effectively suppresses oscillations and improves stability in transcritical flows. Compared to traditional primitive-variable approaches, CDHD reduces the energy conservation error in two order of magnitude. When incorporated into a hybrid framework with a conservative shock-capturing scheme, the method robustly handles both smooth transcritical phenomena and shock waves. Numerical tests validate the accuracy, stability, and energy-preserving capabilities of CDHD, demonstrating its potential as a reliable tool for complex real-gas flow simulations. △ Less",
      "url": "https://arxiv.org/abs/2508.21599"
    },
    {
      "title": "Upper Limits on the Isotropic Gravitational-Wave Background from the first part of LIGO, Virgo, and KAGRA's fourth Observing Run",
      "abstract": "We present results from the search for an isotropic gravitational-wave background using Advanced LIGO and Advanced Virgo data from O1 through O4a, the first part of the fourth observing run. This background is the accumulated signal from unresolved sources throughout cosmic history and encodes information about the merger history of compact binaries throughout the Universe, as well as exotic physics and potentially primordial processes from the early cosmos. Our cross-correlation analysis reveals no statistically significant background signal, enabling us to constrain several theoretical scenarios. For compact binary coalescences which approximately follow a 2/3 power-law spectrum, we constrain the fractional energy density to $Ω_{\\rm GW}(25{\\rm Hz})\\leq 2.0\\times 10^{-9}$ (95% cred.), a factor of 1.7 improvement over previous results. Scale-invariant backgrounds are constrained to $Ω_{\\rm GW}(25{\\rm Hz})\\leq 2.8\\times 10^{-9}$, representing a 2.1x sensitivity gain. We also place new limits on gravity theories predicting non-standard polarization modes and confirm that terrestrial magnetic noise sources remain below detection threshold. Combining these spectral limits with population models for GWTC-4, the latest gravitational-wave event catalog, we find our constraints remain above predicted merger backgrounds but are approaching detectability. The joint analysis combining the background limits shown here with the GWTC-4 catalog enables improved inference of the binary black hole merger rate evolution across cosmic time. Employing GWTC-4 inference results and standard modeling choices, we estimate that the total background arising from compact binary coalescences is $Ω_{\\rm CBC}(25{\\rm Hz})={0.9^{+1.1}_{-0.5}\\times 10^{-9}}$ at 90% confidence, where the largest contribution is due to binary black holes only, $Ω_{\\rm BBH}(25{\\rm Hz})=0.8^{+1.1}_{-0.5}\\times 10^{-9}$. △ Less",
      "url": "https://arxiv.org/abs/2508.20721"
    },
    {
      "title": "Joint Contact Planning for Navigation and Communication in GNSS-Libration Point Systems",
      "abstract": "Deploying satellites at Earth-Moon Libration Points (LPs) addresses the inherent deep-space coverage gaps of low-altitude GNSS constellations. Integrating LP satellites with GNSS into a joint constellation enables a more robust and comprehensive Positioning, Navigation, and Timing (PNT) system, while also extending navigation and communication services to spacecraft operating in cislunar space (i.e., users). However, the long propagation delays between LP satellites, users, and GNSS satellites result in significantly different link durations compared to those within the GNSS constellation. Scheduling inter-satellite links (ISLs) is a core task of Contact Plan Design (CPD). Existing CPD approaches focus exclusively on GNSS constellations, assuming uniform link durations, and thus cannot accommodate the heterogeneous link timescales present in a joint GNSS-LP system. To overcome this limitation, we introduce a Joint CPD (J-CPD) scheme tailored to handle ISLs with differing duration units across integrated constellations. The key contributions of J-CPD are: (i):introduction of LongSlots (Earth-Moon scale links) and ShortSlots (GNSS-scale links); (ii):a hierarchical and crossed CPD process for scheduling LongSlots and ShortSlots ISLs; (iii):an energy-driven link scheduling algorithm adapted to the CPD process. Simulations on a joint BeiDou-LP constellation demonstrate that J-CPD surpasses the baseline FCP method in both delay and ranging coverage, while maintaining high user satisfaction and enabling tunable trade-offs through adjustable potential-energy parameters. To our knowledge, this is the first CPD framework to jointly optimize navigation and communication in GNSS-LP systems, representing a key step toward unified and resilient deep-space PNT architectures. △ Less",
      "url": "https://arxiv.org/abs/2508.20479"
    },
    {
      "title": "Inclusive $B$-meson flavour-tagging algorithm at LHCb",
      "abstract": "A new algorithm is developed to identify the flavour of neutral $B$ mesons at production in $pp$ collisions by utilising all tracks from the hadronisation process. The algorithm is calibrated separately for $B^0$ and $B^{0}_{s}$ mesons using $B^{0}\\to J/ψK^{+}π^-$ and $B^{0}_{s}\\to D_{s}^{-}π^+$ decays from $pp$ collision data collected by the LHCb experiment at a centre-of-mass energy of 13\\,TeV. This new algorithm improves the tagging power by 35\\% for $B^{0}$ mesons and 20\\% for $B^{0}_{s}$ mesons when compared to the combined performance of the existing LHCb flavour-tagging algorithms. △ Less",
      "url": "https://arxiv.org/abs/2508.20180"
    },
    {
      "title": "VibeVoice Technical Report",
      "abstract": "This report presents VibeVoice, a novel model designed to synthesize long-form speech with multiple speakers by employing next-token diffusion, which is a unified method for modeling continuous data by autoregressively generating latent vectors via diffusion. To enable this, we introduce a novel continuous speech tokenizer that, when compared to the popular Encodec model, improves data compression by 80 times while maintaining comparable performance. The tokenizer effectively preserves audio fidelity while significantly boosting computational efficiency for processing long sequences. Thus, VibeVoice can synthesize long-form speech for up to 90 minutes (in a 64K context window length) with a maximum of 4 speakers, capturing the authentic conversational ``vibe'' and surpassing open-source and proprietary dialogue models. △ Less",
      "url": "https://arxiv.org/abs/2508.19205"
    },
    {
      "title": "Measurement of the branching fraction of $\\psip \\to ωηη$",
      "abstract": "Using a sample of (2.712 $\\pm$ 0.014)$\\times 10^{9}$ $\\psip$ events collected with the BESIII detector at the BEPCII collider in 2009, 2012, and 2021, the decay $\\psip \\to ωηη$ is observed for the first time. The branching fraction of the $ψ(3686)\\toωηη$ decay is measured to be (1.65 $\\pm$ 0.02 $\\pm$ 0.21)$\\times 10^{-5}$, where the first uncertainty is statistical and the second systematic. Clear structures associated with the well-established $ω(1420)$ and $f_{0}(1710)$ resonances are observed in the $ωη$ and $ηη$ invariant-mass spectra, respectively. △ Less",
      "url": "https://arxiv.org/abs/2508.19092"
    },
    {
      "title": "Study of the $χ_{cJ}\\rightarrowΛ\\barΛη^\\prime$ decays",
      "abstract": "Using a data sample of $(2.712\\pm0.014)\\times10^{9}$ $ψ(3686)$ events collected with the BESIII detector at the BEPCII collider, we investigate the decays $χ_{cJ} \\rightarrow Λ\\barΛ η^\\prime$ for $J=0,~1,~2$ via the radiative transition $ψ(3686) \\rightarrow γχ_{cJ}$. The decays $χ_{c0,2}\\rightarrowΛ\\barΛη^\\prime$ are observed for the first time, with statistical significances of 6.7$\\,σ$ and 6.4$\\,σ$, respectively. Evidence for the decay $χ_{c1}\\rightarrowΛ\\barΛη^\\prime$ is found with a statistical significance of 3.3$\\,σ$. The corresponding branching fractions are measured to be $\\mathscr{B}(χ_{c0}\\rightarrowΛ\\barΛη^\\prime)=(7.56\\pm1.42\\pm0.90)\\times10^{-5}$, $\\mathscr{B}(χ_{c1}\\rightarrowΛ\\barΛη^\\prime)=(1.54\\pm0.51\\pm0.16)\\times10^{-5}$, and $\\mathscr{B}(χ_{c2}\\rightarrowΛ\\barΛη^\\prime)=(3.03\\pm0.61\\pm0.29)\\times10^{-5}$, where the first uncertainties are statistical and the second systematic. No significant excited $Λ$ baryon states or $Λ\\barΛ$ near-threshold enhancements are observed. △ Less",
      "url": "https://arxiv.org/abs/2508.18761"
    },
    {
      "title": "Beyond Quality: Unlocking Diversity in Ad Headline Generation with Large Language Models",
      "abstract": "The generation of ad headlines plays a vital role in modern advertising, where both quality and diversity are essential to engage a broad range of audience segments. Current approaches primarily optimize language models for headline quality or click-through rates (CTR), often overlooking the need for diversity and resulting in homogeneous outputs. To address this limitation, we propose DIVER, a novel framework based on large language models (LLMs) that are jointly optimized for both diversity and quality. We first design a semantic- and stylistic-aware data generation pipeline that automatically produces high-quality training pairs with ad content and multiple diverse headlines. To achieve the goal of generating high-quality and diversified ad headlines within a single forward pass, we propose a multi-stage multi-objective optimization framework with supervised fine-tuning (SFT) and reinforcement learning (RL). Experiments on real-world industrial datasets demonstrate that DIVER effectively balances quality and diversity. Deployed on a large-scale content-sharing platform serving hundreds of millions of users, our framework improves advertiser value (ADVV) and CTR by 4.0% and 1.4%. △ Less",
      "url": "https://arxiv.org/abs/2508.18739"
    },
    {
      "title": "Search for $χ_{c1}\\to π^{+}π^{-}η_c$ via $ψ(3686)\\toγχ_{c1}$",
      "abstract": "Utilizing $(2712.4 \\pm 14.3) \\times 10^6$ $ψ(3686)$ events collected with the BESIII detector at the BEPCII collider, we search for the hadronic transition process $χ_{c1} \\to π^+π^-η_c$ following the decay $ψ(3686)\\to γχ_{c1}$. No significant signal is observed, and an upper limit of $\\mathcal{B}(χ_{c1}\\toπ^+π^-η_c)$ is determined to be $3.1 times 10^{-4}$~at 90\\% confidence level, which is one order of magnitude more stringent than the previous measurement. △ Less",
      "url": "https://arxiv.org/abs/2508.18601"
    },
    {
      "title": "Search for a bound state of $Λ_{c}\\barΣ_{c}$ near threshold",
      "abstract": "We search for a possible $Λ_{c} \\bar{Σ}_{c}$ bound state, denoted as $H_{c}^{\\pm}$, via the $ e^{+}e^{-} \\to π^{+} π^{-} Λ_{c}^{+}\\barΛ_{c}^{-}$ process for the first time. This analysis utilizes 207.8 and 159.3 pb$^{-1}$ of $e^{+}e^{-}$ annihilation data at the center-of-mass energies of 4918.02 and 4950.93 MeV, respectively, collected with the BESIII detector at the BEPCII collider. No statistically significant signal is observed. The upper limits of the product of Born cross section and branching fraction $σ(e^{+}e^{-} \\to π^{+} H_c^{-} + c.c.) \\times \\mathcal{B}(H_c^{-} \\rightarrow π^{-}Λ_{c}^{+}\\barΛ_{c}^{-})$ at a 90\\% confidence level are reported at each energy point and for various $H_{c}$ mass hypotheses (4715, 4720, 4725, 4730, and 4735 MeV/$c^{2}$) and widths (5, 10, or 20 MeV), with the upper limits ranging from 1.1 pb to 6.4 pb. △ Less",
      "url": "https://arxiv.org/abs/2508.18594"
    },
    {
      "title": "GWTC-4.0: Methods for Identifying and Characterizing Gravitational-wave Transients",
      "abstract": "The Gravitational-Wave Transient Catalog (GWTC) is a collection of candidate gravitational-wave transient signals identified and characterized by the LIGO-Virgo-KAGRA Collaboration. Producing the contents of the GWTC from detector data requires complex analysis methods. These comprise techniques to model the signal; identify the transients in the data; evaluate the quality of the data and mitigate possible instrumental issues; infer the parameters of each transient; compare the data with the waveform models for compact binary coalescences; and handle the large amount of results associated with all these different analyses. In this paper, we describe the methods employed to produce the catalog's fourth release, GWTC-4.0, focusing on the analysis of the first part of the fourth observing run of Advanced LIGO, Advanced Virgo and KAGRA. △ Less",
      "url": "https://arxiv.org/abs/2508.18081"
    },
    {
      "title": "GWTC-4.0: An Introduction to Version 4.0 of the Gravitational-Wave Transient Catalog",
      "abstract": "The Gravitational-Wave Transient Catalog (GWTC) is a collection of short-duration (transient) gravitational wave signals identified by the LIGO-Virgo-KAGRA Collaboration in gravitational-wave data produced by the eponymous detectors. The catalog provides information about the identified candidates, such as the arrival time and amplitude of the signal and properties of the signal's source as inferred from the observational data. GWTC is the data release of this dataset and version 4.0 extends the catalog to include observations made during the first part of the fourth LIGO-Virgo-KAGRA observing run up until 2024 January 31. This paper marks an introduction to a collection of articles related to this version of the catalog, GWTC-4.0. The collection of articles accompanying the catalog provides documentation of the methods used to analyze the data, summaries of the catalog of events, observational measurements drawn from the population, and detailed discussions of selected candidates △ Less",
      "url": "https://arxiv.org/abs/2508.18080"
    },
    {
      "title": "Measurement of branching fractions and $CP$ asymmetries in $\\mathitΛ_b^0(\\mathitΞ_b^0)\\!\\to pK_{\\mathrm S}^0h^-$ decays",
      "abstract": "A study of $\\mathitΛ_b^0$ and $\\mathitΞ_b^0$ baryon decays to the final states $pK_{\\mathrm S}^0π^-$ and $pK_{\\mathrm S}^0K^-$ is performed using $pp$ collision data collected by the LHCb experiment, corresponding to an integrated luminosity of $9\\,\\mathrm{fb}^{-1}$. The decays $\\mathitΛ_b^0\\!\\to pK_{\\mathrm S}^0K^-$ and $\\mathitΞ_b^0\\!\\to pK_{\\mathrm S}^0K^-$ are observed for the first time, with significances reaching eight standard deviations. The branching fractions and integrated $CP$ asymmetries are measured for the $\\mathitΛ_b^0\\!\\to pK_{\\mathrm S}^0π^-$, $\\mathitΛ_b^0\\!\\to pK_{\\mathrm S}^0K^-$, and $\\mathitΞ_b^0\\!\\to pK_{\\mathrm S}^0K^-$ decays. For the decay $\\mathitΛ_b^0\\!\\to pK_{\\mathrm S}^0π^-$, the $CP$ asymmetries are measured in different regions of the Dalitz plot. No evidence of $CP$ violation is observed. △ Less",
      "url": "https://arxiv.org/abs/2508.17836"
    },
    {
      "title": "Search for CP violation in e+e- -> psi(3770) -> DDbar via D -> KsPi0",
      "abstract": "Utilizing data sample of electron-positron collisions recorded with the BESIII detector at the center-of-mass energies of 3.773~GeV, corresponding to an integrated luminosity of 20.28~fb$^{-1}$, we report the first search for the CP forbidden process $e^+e^- \\to ψ(3773) \\to D^0\\bar{D}^0 \\to (K^0_Sπ^0)(K^0_Sπ^0)$. No significant signal is observed. We set the upper limit on the observed cross section to be 7.37~fb, and the upper limit on the joint branching fraction of the C-odd correlated neutral $D$ pair $\\mathcal{B}[(D^0\\bar{D}^0)_{\\text{C-odd}} \\to (K^0_Sπ^0)(K^0_Sπ^0)]$ to be $2.04 \\times 10^{-6}$ at the 90\\% confidence level. △ Less",
      "url": "https://arxiv.org/abs/2508.17819"
    },
    {
      "title": "Femtojoule-per-operation photonic computer for the subset sum problem",
      "abstract": "Energy-efficient computing is becoming increasingly important in the information era. However, electronic computers with von Neumann architecture can hardly meet the challenge due to the inevitable energy-intensive data movement, especially when tackling computationally hard problems or complicated tasks. Here, we experimentally demonstrate an energy-efficient photonic computer that solves intractable subset sum problem (SSP) by making use of the extremely low energy level of photons (~10^(-19) J) and a time-of-flight storage technique. We show that the energy consumption of the photonic computer maintains no larger than 10^(-15) J per operation at a reasonably large problem size N=33, and it consumes 10^(8) times less energy than the most energy-efficient supercomputer for a medium-scale problem. In addition, when the photonic computer is applied to deal with real-life problems that involves iterative computation of the SSP, the photonic advantage in energy consumption is further enhanced and massive energy can be saved. Our results indicate the superior competitiveness of the photonic computer in the energy costs of complex computation, opening a possible path to green computing. △ Less",
      "url": "https://arxiv.org/abs/2508.17274"
    },
    {
      "title": "First observation of the charmless baryonic decay $B^+\\to\\barΛp\\bar{p}p$",
      "abstract": "A search for the charmless baryonic decay $B^+\\to \\barΛ p\\bar{p}p$ is performed using proton-proton collision data recorded by the LHCb experiment, corresponding to an integrated luminosity of 5.4~$\\text{fb}^{-1}$. The branching fraction for this decay is measured for the first time relative to that of the topologically similar decay $B^+\\to J/ψK^+$, with $J/ψ\\to \\barΛ p K^-$. The branching fraction is measured to be \\mbox{$\\mathcal{B}(B^+\\to \\barΛ p\\bar{p}p) = (2.08 \\pm 0.34 \\pm 0.12 \\pm 0.26) \\times 10^{-7}$}, where the first uncertainty is statistical, the second is systematic, and the third arises from the uncertainty in the normalization channel branching fraction. The $CP$ asymmetry is measured to be $\\mathcal{A}_{CP}=(5.4\\pm 15.6\\pm 2.4)\\%$, where the uncertainties are statistical and systematic. The background-subtracted invariant-mass distributions of $\\barΛp$ and $\\bar{p}$ pairs exhibit pronounced enhancements at both kinematic thresholds, in contrast to a uniform phase-space distribution. △ Less",
      "url": "https://arxiv.org/abs/2508.16259"
    },
    {
      "title": "Search for $e^+ e^- \\to γχ_{bJ}$ ($J$ = 0, 1, 2) near $\\sqrt{s} = 10.746$ GeV at Belle II",
      "abstract": "We search for the $e^+ e^- \\to γχ_{bJ}$ ($J$ = 0, 1, 2) processes at center-of-mass energies $\\sqrt{s}$ = 10.653, 10.701, 10.746, and 10.804 GeV. These data were collected with the Belle II detector at the SuperKEKB collider and correspond to 3.5, 1.6, 9.8, and 4.7 fb$^{-1}$ of integrated luminosity, respectively. We set upper limits at the 90\\% confidence level on the Born cross sections for $e^+ e^- \\to γχ_{bJ}$ at each center-of-mass energy $\\sqrt{s}$ near 10.746 GeV. The upper limits at 90\\% confidence level on the Born cross sections for $e^+ e^- \\to γχ_{b1}$ are significantly smaller than the corresponding measured values for $e^+e^-\\toωχ_{b1}$ and $e^+e^-\\toπ^+π^-Υ(2S)$ at $\\sqrt{s}$ = 10.746 GeV. △ Less",
      "url": "https://arxiv.org/abs/2508.16036"
    },
    {
      "title": "D3FNet: A Differential Attention Fusion Network for Fine-Grained Road Structure Extraction in Remote Perception Systems",
      "abstract": "Extracting narrow roads from high-resolution remote sensing imagery remains a significant challenge due to their limited width, fragmented topology, and frequent occlusions. To address these issues, we propose D3FNet, a Dilated Dual-Stream Differential Attention Fusion Network designed for fine-grained road structure segmentation in remote perception systems. Built upon the encoder-decoder backbone of D-LinkNet, D3FNet introduces three key innovations:(1) a Differential Attention Dilation Extraction (DADE) module that enhances subtle road features while suppressing background noise at the bottleneck; (2) a Dual-stream Decoding Fusion Mechanism (DDFM) that integrates original and attention-modulated features to balance spatial precision with semantic context; and (3) a multi-scale dilation strategy (rates 1, 3, 5, 9) that mitigates gridding artifacts and improves continuity in narrow road prediction. Unlike conventional models that overfit to generic road widths, D3FNet specifically targets fine-grained, occluded, and low-contrast road segments. Extensive experiments on the DeepGlobe and CHN6-CUG benchmarks show that D3FNet achieves superior IoU and recall on challenging road regions, outperforming state-of-the-art baselines. Ablation studies further verify the complementary synergy of attention-guided encoding and dual-path decoding. These results confirm D3FNet as a robust solution for fine-grained narrow road extraction in complex remote and cooperative perception scenarios. △ Less",
      "url": "https://arxiv.org/abs/2508.15537"
    },
    {
      "title": "NVIDIA Nemotron Nano 2: An Accurate and Efficient Hybrid Mamba-Transformer Reasoning Model",
      "abstract": "We introduce Nemotron-Nano-9B-v2, a hybrid Mamba-Transformer language model designed to increase throughput for reasoning workloads while achieving state-of-the-art accuracy compared to similarly-sized models. Nemotron-Nano-9B-v2 builds on the Nemotron-H architecture, in which the majority of the self-attention layers in the common Transformer architecture are replaced with Mamba-2 layers, to achieve improved inference speed when generating the long thinking traces needed for reasoning. We create Nemotron-Nano-9B-v2 by first pre-training a 12-billion-parameter model (Nemotron-Nano-12B-v2-Base) on 20 trillion tokens using an FP8 training recipe. After aligning Nemotron-Nano-12B-v2-Base, we employ the Minitron strategy to compress and distill the model with the goal of enabling inference on up to 128k tokens on a single NVIDIA A10G GPU (22GiB of memory, bfloat16 precision). Compared to existing similarly-sized models (e.g., Qwen3-8B), we show that Nemotron-Nano-9B-v2 achieves on-par or better accuracy on reasoning benchmarks while achieving up to 6x higher inference throughput in reasoning settings like 8k input and 16k output tokens. We are releasing Nemotron-Nano-9B-v2, Nemotron-Nano12B-v2-Base, and Nemotron-Nano-9B-v2-Base checkpoints along with the majority of our pre- and post-training datasets on Hugging Face. △ Less",
      "url": "https://arxiv.org/abs/2508.14444"
    },
    {
      "title": "First observation of $CP$ violation and measurement of polarization in $B^+\\toρ(770)^0 K^*(892)^+$ decays",
      "abstract": "An amplitude analysis of the $B^+\\to(π^+π^-)(K^0_{\\mathrm{S}}π^+)$ decay is performed in the mass regions $0.30 < m_{π^+π^-} < 1.10\\,\\mathrm{GeV}/c^2$ and $0.75 < m_{K^0_{\\mathrm{S}}π^+} < 1.20\\,\\mathrm{GeV}/c^2$, using $pp$ collision data recorded with the LHCb detector corresponding to an integrated luminosity of $9\\,\\mathrm{fb}^{-1}$. The polarization fractions and $CP$ asymmetries for $B^+\\toρ(770)^0K^*(892)^+$ decays are measured. Violation of the $CP$ symmetry in the decay $B^+\\toρ(770)^0K^*(892)^+$ is observed for the first time, with a significance exceeding nine standard deviations. The $CP$ asymmetry is measured to be ${\\cal A}_{CP} = 0.507 \\pm 0.062\\ \\text{(stat)} \\pm 0.017\\ \\text{(syst)}$ and the $CP$-averaged longitudinal polarization fraction of $f_L = 0.720 \\pm 0.028\\ \\text{(stat)} \\pm 0.009\\ \\text{(syst)}$. The measurements help to shed light on the polarization puzzle of $B$ mesons decaying to two vector mesons. △ Less",
      "url": "https://arxiv.org/abs/2508.13563"
    },
    {
      "title": "EventTSF: Event-Aware Non-Stationary Time Series Forecasting",
      "abstract": "Time series forecasting plays a vital role in critical domains like energy and transportation, where non-stationary dynamics are deeply intertwined with events in other modalities such as texts. However, incorporating natural language-based external events to improve non-stationary forecasting remains largely unexplored, as most approaches still rely on a single modality, resulting in limited contextual knowledge and model underperformance. Enabling fine-grained multimodal interactions between temporal and textual data is challenged by three fundamental issues: (1) the difficulty of fine-grained synchronization between time-varying discrete textual events and continuous time series; (2) the inherent temporal uncertainty introduced by textual semantics; and (3) the misalignment between textual event embeddings and multi-resolution temporal patterns. In this work, we address these challenges by introducing event-aware non-stationary time series forecasting (EventTSF), an autoregressive generation framework that integrates historical time series with textual events to make subsequent forecasts. Specifically, EventTSF uses autoregressive diffusion with flow matching at each step to capture nuanced temporal-event interactions. To handle event-induced uncertainty, flow matching timesteps are adaptively controlled according to event semantic signals. The underlying denoiser employs a multimodal U-shaped diffusion transformer that efficiently fuses temporal and textual modalities across different resolutions. Extensive experiments on 8 synthetic and real-world datasets show that EventTSF outperforms 12 baselines across diverse event-aware non-stationary time series forecasting scenarios, achieving substantial improvements of 10.7% higher forecasting accuracy and $1.13\\times$ faster training efficiency. △ Less",
      "url": "https://arxiv.org/abs/2508.13434"
    },
    {
      "title": "The Production and Decay Dynamics of the Charmed Baryon $Λ_c^+$ in $e^+e^-$ Annihilations near Threshold",
      "abstract": "The study of the charmed baryons is crucial for investigating the strong and weak interactions in the Standard Model and for gaining insights into the internal structure of baryons. In an $e^+e^-$ experiment the lightest charmed baryon, $Λ_c^+$, can be produced in pairs through the single photon annihilation process. This process can be described by two complex electromagnetic form factors. The presence of a non-zero relative phase between these form factors gives rise to a transverse polarization of the charmed baryon and provides additional constraints on the dynamic parameters in the decays. In this article, we present the first observation of the transverse polarization of $Λ_{c}^{+}$ in the reaction $e^+e^- \\to Λ_c^{+}\\barΛ_c^-$, based on $6.4~\\text{fb}^{-1}$ of $e^{+}e^{-}$ annihilation data collected at center-of-mass energies between 4600 MeV and 4951 MeV with the BESIII detector. The decay asymmetry parameters and strong phase shift in the decays $Λ_c^+ \\to pK_S^0$, $Λπ^+$, $Σ^0π^+$, $Σ^+π^0$ are also simultaneously extracted from the joint angular distributions. These results are vital for understanding CP violation and its role in the matter-antimatter asymmetry of the Universe. △ Less",
      "url": "https://arxiv.org/abs/2508.11400"
    },
    {
      "title": "Measurement of the Born cross section for $e^+e^- \\to p K^- K^- \\barΞ^+$ at $\\sqrt{s} =$ 3.5-4.9 GeV",
      "abstract": "Using $e^+ e^-$ collision data corresponding to a total integrated luminosity of 20 ${\\rm fb}^{-1}$ collected with the BESIII detector at the BEPCII collider, we present a measurement of the Born cross section for the process $e^+e^- \\to p K^-K^-\\barΞ^{+}$ at 39 center-of-mass energies between 3.5 and 4.9 GeV with a partial reconstruction technique. By performing a fit to the dressed cross section of $e^{+}e^{-}\\to p K^- K^-\\barΞ^{+}$ with a power law function for continuum production and one resonance at a time for the $ψ(3770)$, $ψ(4040)$, $ψ(4160)$, $ψ(4230)$, $ψ(4360)$, $ψ(4415)$ or $ψ(4660)$, respectively, the upper limits for the product of partial electronic width and branching fraction into the final state $p K^- K^- \\barΞ^+$ for these resonances are determined at the $90\\%$ confidence level. △ Less",
      "url": "https://arxiv.org/abs/2508.11276"
    },
    {
      "title": "Not There Yet: Evaluating Vision Language Models in Simulating the Visual Perception of People with Low Vision",
      "abstract": "Advances in vision language models (VLMs) have enabled the simulation of general human behavior through their reasoning and problem solving capabilities. However, prior research has not investigated such simulation capabilities in the accessibility domain. In this paper, we evaluate the extent to which VLMs can simulate the vision perception of low vision individuals when interpreting images. We first compile a benchmark dataset through a survey study with 40 low vision participants, collecting their brief and detailed vision information and both open-ended and multiple-choice image perception and recognition responses to up to 25 images. Using these responses, we construct prompts for VLMs (GPT-4o) to create simulated agents of each participant, varying the included information on vision information and example image responses. We evaluate the agreement between VLM-generated responses and participants' original answers. Our results indicate that VLMs tend to infer beyond the specified vision ability when given minimal prompts, resulting in low agreement (0.59). The agreement between the agent' and participants' responses remains low when only either the vision information (0.59) or example image responses (0.59) are provided, whereas a combination of both significantly increase the agreement (0.70, p < 0.0001). Notably, a single example combining both open-ended and multiple-choice responses, offers significant performance improvements over either alone (p < 0.0001), while additional examples provided minimal benefits (p > 0.05). △ Less",
      "url": "https://arxiv.org/abs/2508.10972"
    },
    {
      "title": "A Suspended 4H-Silicon Carbide Membrane Platform for Defect Integration into Quantum Devices",
      "abstract": "4H-silicon carbide is a promising platform for solid-state quantum technology due to its commercial availability as a wide bandgap semiconductor and ability to host numerous spin-active color centers. Integrating color centers into suspended nanodevices enhances defect control and readout--key advances needed to fully harness their potential. However, challenges in developing robust fabrication processes for 4H-SiC thin films--due to the material's chemical and mechanical stability--limit their implementation in quantum applications. Here, we report on a new fabrication approach that first synthesizes suspended thin films from a monolithic platform, then patterns devices. With this technique, we fabricate and characterize structures tailored for defect integration, demonstrating 1D photonic crystal cavities, with and without waveguide interfaces, and lithium niobate on 4H-SiC acoustic cavities. This approach allows for greater fabrication flexibility--supporting high temperature annealing and heterogeneous material platform compatibility--providing a versatile platform for scalable fabrication of 4H-SiC devices for quantum technologies. △ Less",
      "url": "https://arxiv.org/abs/2508.10814"
    },
    {
      "title": "CARES: Collaborative Agentic Reasoning for Error Detection in Surgery",
      "abstract": "Robotic-assisted surgery (RAS) introduces complex challenges that current surgical error detection methods struggle to address effectively due to limited training data and methodological constraints. Therefore, we construct MERP (Multi-class Error in Robotic Prostatectomy), a comprehensive dataset for error detection in robotic prostatectomy with frame-level annotations featuring six clinically aligned error categories. In addition, we propose CARES (Collaborative Agentic Reasoning for Error Detection in Surgery), a novel zero-shot clinically-informed and risk-stratified agentic reasoning architecture for multi-class surgical error detection. CARES implements adaptive generation of medically informed, error-specific Chain-of-Thought (CoT) prompts across multiple expertise levels. The framework employs risk-aware routing to assign error task to expertise-matched reasoning pathways based on complexity and clinical impact. Subsequently, each pathway decomposes surgical error analysis into three specialized agents with temporal, spatial, and procedural analysis. Each agent analyzes using dynamically selected prompts tailored to the assigned expertise level and error type, generating detailed and transparent reasoning traces. By incorporating clinically informed reasoning from established surgical assessment guidelines, CARES enables zero-shot surgical error detection without prior training. Evaluation demonstrates superior performance with 54.3 mF1 on RARP and 52.0 mF1 on MERP datasets, outperforming existing zero-shot approaches by up to 14% while remaining competitive with trained models. Ablation studies demonstrate the effectiveness of our method. The dataset and code will be publicly available. △ Less",
      "url": "https://arxiv.org/abs/2508.08764"
    },
    {
      "title": "WeChat-YATT: A Scalable, Simple, Efficient, and Production Ready Training Library",
      "abstract": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a prominent paradigm for training large language models and multimodal systems. Despite the notable advances enabled by existing RLHF training frameworks, significant challenges remain to scale to complex multimodal workflows and adapt to dynamic workloads. In particular, current systems often encounter limitations related to controller scalability when managing large models, as well as inefficiencies in orchestrating intricate RLHF pipelines, especially in scenarios that require dynamic sampling and resource allocation. In this paper, we introduce WeChat-YATT Yet Another Transformer Trainer in WeChat, a simple, scalable, and balanced RLHF training framework specifically designed to address these challenges. WeChat-YATT features a parallel controller programming model that enables flexible and efficient orchestration of complex RLHF workflows, effectively mitigating bottlenecks associated with centralized controller architectures and facilitating scalability in large-scale data scenarios. In addition, we propose a dynamic placement schema that adaptively partitions computational resources and schedules workloads, thereby significantly reducing hardware idle time and improving GPU utilization under variable training conditions. We evaluate WeChat-YATT across diverse experimental scenarios, demonstrating its substantial throughput improvements over state-of-the-art RLHF training frameworks. Furthermore, WeChat-YATT has been successfully deployed to train models that support WeChat product features for a large-scale user base, underscoring its effectiveness and robustness in real-world applications. We have made WeChat-YATT publicly available at https://www.github.com/tencent/WeChat-YATT. △ Less",
      "url": "https://arxiv.org/abs/2508.07970"
    },
    {
      "title": "Pinching-Antenna System Design with LoS Blockage: Does In-Waveguide Attenuation Matter?",
      "abstract": "In the literature of pinching-antenna systems, in-waveguide attenuation is often neglected to simplify system design and enable more tractable analysis. However, its effect on overall system performance has received limited attention in the existing literature. While a recent study has shown that, in line-of-sight (LoS)-dominated environments, the data rate loss incurred by omitting in-waveguide attenuation is negligible when the communication area is not excessively large, its effect under more general conditions remains unclear. This work extends the analysis to more realistic scenarios involving arbitrary levels of LoS blockage. We begin by examining a single-user case and derive an explicit expression for the average data rate loss caused by neglecting in-waveguide attenuation. The results demonstrate that, even for large service areas, the rate loss remains negligible under typical LoS blockage conditions. We then consider a more general multi-user scenario, where multiple pinching antennas, each deployed on a separate waveguide, jointly serve multiple users. The objective is to maximize the average sum rate by jointly optimize antenna positions and transmit beamformers to maximize the average sum rate under probabilistic LoS blockage. To solve the resulting stochastic and nonconvex optimization problem, we propose a dynamic sample average approximation (SAA) algorithm. At each iteration, this method replaces the expected objective with an empirical average computed from dynamically regenerated random channel realizations, ensuring that the optimization accurately reflects the current antenna configuration. Extensive simulation results are provided to the proposed algorithm and demonstrate the substantial performance gains of pinching-antenna systems, particularly in environments with significant LoS blockage. △ Less",
      "url": "https://arxiv.org/abs/2508.07131"
    },
    {
      "title": "MDK12-Bench: A Comprehensive Evaluation of Multimodal Large Language Models on Multidisciplinary Exams",
      "abstract": "Multimodal large language models (MLLMs), which integrate language and visual cues for problem-solving, are crucial for advancing artificial general intelligence (AGI). However, current benchmarks for measuring the intelligence of MLLMs suffer from limited scale, narrow coverage, and unstructured knowledge, offering only static and undifferentiated evaluations. To bridge this gap, we introduce MDK12-Bench, a large-scale multidisciplinary benchmark built from real-world K-12 exams spanning six disciplines with 141K instances and 6,225 knowledge points organized in a six-layer taxonomy. Covering five question formats with difficulty and year annotations, it enables comprehensive evaluation to capture the extent to which MLLMs perform over four dimensions: 1) difficulty levels, 2) temporal (cross-year) shifts, 3) contextual shifts, and 4) knowledge-driven reasoning. We propose a novel dynamic evaluation framework that introduces unfamiliar visual, textual, and question form shifts to challenge model generalization while improving benchmark objectivity and longevity by mitigating data contamination. We further evaluate knowledge-point reference-augmented generation (KP-RAG) to examine the role of knowledge in problem-solving. Key findings reveal limitations in current MLLMs in multiple aspects and provide guidance for enhancing model robustness, interpretability, and AI-assisted education. △ Less",
      "url": "https://arxiv.org/abs/2508.06851"
    },
    {
      "title": "Real-time scattering and freeze-out dynamics in Rydberg-atom lattice gauge theory",
      "abstract": "Understanding the non-equilibrium dynamics of gauge theories remains a fundamental challenge in high-energy physics. Indeed, most large scale experiments on gauge theories intrinsically rely on very far-from equilibrium dynamics, from heavy-ion to lepton and hadron collisions, which is in general extremely challenging to treat ab initio. Quantum simulation holds intriguing potential in tackling this problem and pioneering experiments have observed different characteristic features of gauge theories, such as string breaking and false vacuum decay. Here, using a programmable Rydberg atom array, we observe real-time scattering and freeze-out dynamics in a (1+1)-dimensional U(1) lattice gauge theory. Through spatiotemporal Hamiltonian engineering, we demonstrate dynamical confinement-deconfinement transitions, revealing string fragmentation and symmetry restoration during quenches. We track scattering processes with single-site resolution across a range of parameter regimes. Utilizing a double quench protocol, we observe dynamical freeze-out: upon quenching the Hamiltonian after scattering, despite the injection of an extensive energy, the system evolution -- in terms of both low-order correlations and entanglement -- freezes, effectively stabilizing a highly correlated equilibrium state -- a situation that reminisces that of collisions between heavy ions. Our work establishes a high-resolution approach for probing non-perturbative gauge dynamics, opening alternative pathways toward studying far-from-equilibrium phenomena in high-energy physics. △ Less",
      "url": "https://arxiv.org/abs/2508.06639"
    },
    {
      "title": "Deuteron identification via time of flight with LHCb",
      "abstract": "It is shown that the timing capabilities of the LHCb detector operated during the LHC Run 2 can be used to identify light ion particles with momenta of a few GeV/$c$. This is achieved by estimating the particle time of flight through a newly developed technique. A dedicated reconstruction procedure and a neural-network-based estimator of the particle speed have been developed to enable deuteron identification by suppressing the abundant background from lighter particles. The performance of the identification procedure is demonstrated in a sample of proton-helium collisions at $\\sqrt{s_{\\text{NN}}}=110$ GeV, where the production of deuteron and triton particles is observed. This novel approach opens the way to study deuteron and antideuteron production for different collision systems at different energy scales, exploiting the rich dataset collected by the LHCb experiment. △ Less",
      "url": "https://arxiv.org/abs/2508.06305"
    },
    {
      "title": "One Size Does Not Fit All: A Distribution-Aware Sparsification for More Precise Model Merging",
      "abstract": "Model merging has emerged as a compelling data-free paradigm for multi-task learning, enabling the fusion of multiple fine-tuned models into a single, powerful entity. A key technique in merging methods is sparsification, which prunes redundant parameters from task vectors to mitigate interference. However, prevailing approaches employ a ``one-size-fits-all'' strategy, applying a uniform sparsity ratio that overlooks the inherent structural and statistical heterogeneity of model parameters. This often leads to a suboptimal trade-off, where critical parameters are inadvertently pruned while less useful ones are retained. To address this limitation, we introduce \\textbf{TADrop} (\\textbf{T}ensor-wise \\textbf{A}daptive \\textbf{Drop}), an adaptive sparsification strategy that respects this heterogeneity. Instead of a global ratio, TADrop assigns a tailored sparsity level to each parameter tensor based on its distributional properties. The core intuition is that tensors with denser, more redundant distributions can be pruned aggressively, while sparser, more critical ones are preserved. As a simple and plug-and-play module, we validate TADrop by integrating it with foundational, classic, and SOTA merging methods. Extensive experiments across diverse tasks (vision, language, and multimodal) and models (ViT, BEiT) demonstrate that TADrop consistently and significantly boosts their performance. For instance, when enhancing a leading merging method, it achieves an average performance gain of 2.0\\% across 8 ViT-B/32 tasks. TADrop provides a more effective way to mitigate parameter interference by tailoring sparsification to the model's structure, offering a new baseline for high-performance model merging. △ Less",
      "url": "https://arxiv.org/abs/2508.06163"
    },
    {
      "title": "SLIP: Soft Label Mechanism and Key-Extraction-Guided CoT-based Defense Against Instruction Backdoor in APIs",
      "abstract": "With the development of customized large language model (LLM) agents, a new threat of black-box backdoor attacks has emerged, where malicious instructions are injected into hidden system prompts. These attacks easily bypass existing defenses that rely on white-box access, posing a serious security challenge. To address this, we propose SLIP, a Soft Label mechanism and key-extraction-guided CoT-based defense against Instruction backdoors in APIs. SLIP is designed based on two key insights. First, to counteract the model's oversensitivity to triggers, we propose a Key-extraction-guided Chain-of-Thought (KCoT). Instead of only considering the single trigger or the input sentence, KCoT prompts the agent to extract task-relevant key phrases. Second, to guide the LLM toward correct answers, our proposed Soft Label Mechanism (SLM) prompts the agent to quantify the semantic correlation between key phrases and candidate answers. Crucially, to mitigate the influence of residual triggers or misleading content in phrases extracted by KCoT, which typically causes anomalous scores, SLM excludes anomalous scores deviating significantly from the mean and subsequently averages the remaining scores to derive a more reliable semantic representation. Extensive experiments on classification and question-answer (QA) tasks demonstrate that SLIP is highly effective, reducing the average attack success rate (ASR) from 90.2% to 25.13% while maintaining high accuracy on clean data and outperforming state-of-the-art defenses. Our code are available in https://github.com/CAU-ISS-Lab/Backdoor-Attack-Defense-LLMs/tree/main/SLIP. △ Less",
      "url": "https://arxiv.org/abs/2508.06153"
    },
    {
      "title": "Multi-Modal Neural Radio Radiance Field for Localized Statistical Channel Modelling",
      "abstract": "This paper presents MM-LSCM, a self-supervised multi-modal neural radio radiance field framework for localized statistical channel modeling (LSCM) for next-generation network optimization. Traditional LSCM methods rely solely on RSRP data, limiting their ability to model environmental structures that affect signal propagation. To address this, we propose a dual-branch neural architecture that integrates RSRP data and LiDAR point cloud information, enhancing spatial awareness and predictive accuracy. MM-LSCM leverages volume-rendering-based multi-modal synthesis to align radio propagation with environmental obstacles and employs a self-supervised training approach, eliminating the need for costly labeled data. Experimental results demonstrate that MM-LSCM significantly outperforms conventional methods in channel reconstruction accuracy and robustness to noise, making it a promising solution for real-world wireless network optimization. △ Less",
      "url": "https://arxiv.org/abs/2508.06054"
    },
    {
      "title": "A Metric for MLLM Alignment in Large-scale Recommendation",
      "abstract": "Multimodal recommendation has emerged as a critical technique in modern recommender systems, leveraging content representations from advanced multimodal large language models (MLLMs). To ensure these representations are well-adapted, alignment with the recommender system is essential. However, evaluating the alignment of MLLMs for recommendation presents significant challenges due to three key issues: (1) static benchmarks are inaccurate because of the dynamism in real-world applications, (2) evaluations with online system, while accurate, are prohibitively expensive at scale, and (3) conventional metrics fail to provide actionable insights when learned representations underperform. To address these challenges, we propose the Leakage Impact Score (LIS), a novel metric for multimodal recommendation. Rather than directly assessing MLLMs, LIS efficiently measures the upper bound of preference data. We also share practical insights on deploying MLLMs with LIS in real-world scenarios. Online A/B tests on both Content Feed and Display Ads of Xiaohongshu's Explore Feed production demonstrate the effectiveness of our proposed method, showing significant improvements in user spent time and advertiser value. △ Less",
      "url": "https://arxiv.org/abs/2508.04963"
    },
    {
      "title": "Experimental Study of Bremsstrahlung Gamma Ray Emission and Short-Range Correlations in $^{124}$Sn+$^{124}$Sn Collisions at 25 MeV/u",
      "abstract": "Short-range correlation (SRC) in nuclei refers to the nucleons forming temporally correlated pairs in close proximity, giving rise to the high momentum of the nucleons beyond the Fermi surface. It has been reported that bremsstrahlung $γ$ production from neutron-proton process in heavy-ion reactions provides a potential probe of the existence of SRC in nuclei. In this paper, we present in detail the precision measurement of bremsstrahlung $γ$-rays in $\\rm ^{124}Sn$+$\\rm ^{124}Sn$ reactions at 25 MeV/u using the Compact Spectrometer for Heavy IoN Experiment (CSHINE). The experimental setup, detector calibration, trigger scheme and data analysis procedures as well as the model comparison are presented in detail. Background contributions are subtracted using two methods to ensure robustness. By comparing the experimental $γ$ spectrum with the Isospin-dependent Boltzmann-Uehling-Uhlenbeck simulations, the high momentum tail (HMT) fraction of $R_{\\rm HMT}=(20 \\pm 3)\\%$ is derived in $^{124}$Sn nuclei. This work presents the detailed experimental measurement and analysis framework for the precise determination of the HMT fraction via bremsstrahlung $γ$-ray emission, demonstrating a new paradigm to study nucleon SRCs in nuclei using low-energy heavy-ion collisions. △ Less",
      "url": "https://arxiv.org/abs/2508.04550"
    },
    {
      "title": "Probing the Gaps in ChatGPT Live Video Chat for Real-World Assistance for People who are Blind or Visually Impaired",
      "abstract": "Recent advancements in large multimodal models have provided blind or visually impaired (BVI) individuals with new capabilities to interpret and engage with the real world through interactive systems that utilize live video feeds. However, the potential benefits and challenges of such capabilities to support diverse real-world assistive tasks remain unclear. In this paper, we present findings from an exploratory study with eight BVI participants. Participants used ChatGPT's Advanced Voice with Video, a state-of-the-art live video AI released in late 2024, in various real-world scenarios, from locating objects to recognizing visual landmarks, across unfamiliar indoor and outdoor environments. Our findings indicate that current live video AI effectively provides guidance and answers for static visual scenes but falls short in delivering essential live descriptions required in dynamic situations. Despite inaccuracies in spatial and distance information, participants leveraged the provided visual information to supplement their mobility strategies. Although the system was perceived as human-like due to high-quality voice interactions, assumptions about users' visual abilities, hallucinations, generic responses, and a tendency towards sycophancy led to confusion, distrust, and potential risks for BVI users. Based on the results, we discuss implications for assistive video AI agents, including incorporating additional sensing capabilities for real-world use, determining appropriate intervention timing beyond turn-taking interactions, and addressing ecological and safety concerns. △ Less",
      "url": "https://arxiv.org/abs/2508.03651"
    },
    {
      "title": "The ALMA-QUARKS Survey: III. Clump-to-core fragmentation and search for high-mass starless cores",
      "abstract": "The Querying Underlying mechanisms of massive star formation with ALMA-Resolved gas Kinematics and Structures (QUARKS) survey observed 139 infrared-bright (IR-bright) massive protoclusters at 1.3 mm wavelength with ALMA. This study investigates clump-to-core fragmentation and searches for candidate high-mass starless cores within IR-bright clumps using combined ALMA 12-m (C-2) and Atacama Compact Array (ACA) 7-m data, providing $\\sim$ 1 arcsec ($\\sim\\rm0.02~pc$ at 3.7 kpc) resolution and $\\sim\\rm0.6\\,mJy\\,beam^{-1}$ continuum sensitivity ($\\sim 0.3~M_{\\odot}$ at 30 K). We identified 1562 compact cores from 1.3 mm continuum emission using getsf. Observed linear core separations ($λ_{\\rm obs}$) are significantly less than the thermal Jeans length ($λ_{\\rm J}$), with the $λ_{\\rm obs}/λ_{\\rm J}$ ratios peaking at $\\sim0.2$. This indicates that thermal Jeans fragmentation has taken place within the IR-bright protocluster clumps studied here. The observed low ratio of $λ_{\\rm obs}/λ_{\\rm J}\\ll 1$ could be the result of evolving core separation or hierarchical fragmentation. Based on associated signatures of star formation (e.g., outflows and ionized gas), we classified cores into three categories: 127 starless, 971 warm, and 464 evolved cores. Two starless cores have mass exceeding 16$\\,M_{\\odot}$, and represent high-mass candidates. The scarcity of such candidates suggests that competitive accretion-type models could be more applicable than turbulent core accretion-type models in high-mass star formation within these IR-bright protocluster clumps. △ Less",
      "url": "https://arxiv.org/abs/2508.03229"
    },
    {
      "title": "FedVLA: Federated Vision-Language-Action Learning with Dual Gating Mixture-of-Experts for Robotic Manipulation",
      "abstract": "Vision-language-action (VLA) models have significantly advanced robotic manipulation by enabling robots to interpret language instructions for task execution. However, training these models often relies on large-scale user-specific data, raising concerns about privacy and security, which in turn limits their broader adoption. To address this, we propose FedVLA, the first federated VLA learning framework, enabling distributed model training that preserves data privacy without compromising performance. Our framework integrates task-aware representation learning, adaptive expert selection, and expert-driven federated aggregation, enabling efficient and privacy-preserving training of VLA models. Specifically, we introduce an Instruction Oriented Scene-Parsing mechanism, which decomposes and enhances object-level features based on task instructions, improving contextual understanding. To effectively learn diverse task patterns, we design a Dual Gating Mixture-of-Experts (DGMoE) mechanism, where not only input tokens but also self-aware experts adaptively decide their activation. Finally, we propose an Expert-Driven Aggregation strategy at the federated server, where model aggregation is guided by activated experts, ensuring effective cross-client knowledge transfer.Extensive simulations and real-world robotic experiments demonstrate the effectiveness of our proposals. Notably, DGMoE significantly improves computational efficiency compared to its vanilla counterpart, while FedVLA achieves task success rates comparable to centralized training, effectively preserving data privacy. △ Less",
      "url": "https://arxiv.org/abs/2508.02190"
    },
    {
      "title": "Measurement of transverse $Λ$ and $\\barΛ$ hyperon polarization in $p$Pb collisions at $\\sqrt{s_{NN}} = 5.02$ TeV",
      "abstract": "The transverse polarization of $Λ$ and $\\barΛ$ hyperons is measured in $p$Pb collisions collected by the LHCb experiment at a nucleon-nucleon center-of-mass energy of $5.02 $ TeV. The polarization is averaged over hyperon transverse momentum in the range $0.15 < p_{T} < 6.00 $ GeV/$c$, and Feynman-$x$ in the ranges $0.005 < x_{F} < 0.040$ (forward region) and $-0.10 < x_{F} < -0.01$ (backward region) defined relative to the proton beam direction. The transverse polarization is found to be compatible with zero for both $Λ$ and $\\barΛ$ hyperons. The results are also measured as a function of $p_{T}$ and $x_{F}$ with no significant dependence on these variables observed. The results are compared with previous experimental measurements at different center-of-mass energies and collision environments. △ Less",
      "url": "https://arxiv.org/abs/2508.02009"
    },
    {
      "title": "Measurement of Born Cross Sections and Effective Form Factors of $e^+e^-\\to Ω^{-}\\barΩ^{+}$ from$\\sqrt{s}$ = 3.7 to 4.7 GeV",
      "abstract": "Using $e^+e^-$ collision data corresponding to an integrated luminosity of 22.7 fb$^{-1}$, collected at center-of-mass energies between 3.7 and 4.7 GeV with the BESIII detector at the BEPCII storage ring, we measure the energy-dependent Born cross sections of $e^+e^-\\to Ω^{-}\\barΩ^+$ and the effective form factors of the $Ω^-$ baryon. The analysis employs a single baryon tagging method, and the results are consistent with theoretical predictions, providing critical constraints on the electromagnetic structure of the $Ω^-$ hyperon. No significant signal of charmonium or charmonium-like states decaying to $Ω^{-}\\barΩ^+$ is observed in the investigated energy range.This paper supersedes the withdrawn work arXiv:2505.03180v1. △ Less",
      "url": "https://arxiv.org/abs/2508.01359"
    },
    {
      "title": "Unlocking New Paths for Science with Extreme-Mass-Ratio Inspirals: Machine Learning-Enhanced MCMC for Accurate Parameter Inversion",
      "abstract": "The detection of gravitational waves from extreme-mass-ratio inspirals (EMRIs) in space-borne antennas like Taiji and LISA promises deep insights into strong-field gravity and black hole physics. However, the complex, highly degenerate, and non-convex likelihood landscapes characteristic of EMRI parameter spaces pose severe challenges for conventional Markov chain Monte Carlo (MCMC) methods. Under realistic instrumental noise and broad priors, these methods demand impractical computational costs but are prone to becoming trapped in local maxima, leading to biased and unreliable parameter estimates. To address this, we introduce Flow-Matching Markov Chain Monte Carlo (FM-MCMC), a novel Bayesian framework that integrates continuous normalizing flows (CNFs) with parallel tempering MCMC (PTMCMC). By generating high-likelihood regions via CNFs and refining them through PTMCMC, FM-MCMC enables robust exploration of the nontrivial parameter spaces, while achieving orders-of-magnitude improvement in computational efficiency and, more importantly, ensuring statistically reliable and unbiased inference. By enabling real-time, unbiased parameter inference, FM-MCMC could unlock the full scientific potential of EMRI observations, and would serve as a scalable pipeline for precision gravitational-wave astronomy. △ Less",
      "url": "https://arxiv.org/abs/2508.00348"
    },
    {
      "title": "Stable-Sim2Real: Exploring Simulation of Real-Captured 3D Data with Two-Stage Depth Diffusion",
      "abstract": "3D data simulation aims to bridge the gap between simulated and real-captured 3D data, which is a fundamental problem for real-world 3D visual tasks. Most 3D data simulation methods inject predefined physical priors but struggle to capture the full complexity of real data. An optimal approach involves learning an implicit mapping from synthetic to realistic data in a data-driven manner, but progress in this solution has met stagnation in recent studies. This work explores a new solution path of data-driven 3D simulation, called Stable-Sim2Real, based on a novel two-stage depth diffusion model. The initial stage finetunes Stable-Diffusion to generate the residual between the real and synthetic paired depth, producing a stable but coarse depth, where some local regions may deviate from realistic patterns. To enhance this, both the synthetic and initial output depth are fed into a second-stage diffusion, where diffusion loss is adjusted to prioritize these distinct areas identified by a 3D discriminator. We provide a new benchmark scheme to evaluate 3D data simulation methods. Extensive experiments show that training the network with the 3D simulated data derived from our method significantly enhances performance in real-world 3D visual tasks. Moreover, the evaluation demonstrates the high similarity between our 3D simulated data and real-captured patterns. Project page: https://mutianxu.github.io/stable-sim2real/. △ Less",
      "url": "https://arxiv.org/abs/2507.23483"
    },
    {
      "title": "Dynamical freezing and enhanced magnetometry in an interacting spin ensemble",
      "abstract": "Understanding and controlling non-equilibrium dynamics in quantum many-body systems is a fundamental challenge in modern physics, with profound implications for advancing quantum technologies. Typically, periodically driven systems in the absence of conservation laws thermalize to a featureless \"infinite-temperature\" state, erasing all memory of their initial conditions. However, this paradigm can break down through mechanisms such as integrability, many-body localization, quantum many-body scars, and Hilbert space fragmentation. Here, we report the experimental observation of dynamical freezing, a distinct mechanism of thermalization breakdown in driven systems, and demonstrate its application in quantum sensing using an ensemble of approximately $10^4$ interacting nitrogen-vacancy spins in diamond. By precisely controlling the driving frequency and detuning, we observe emergent long-lived spin magnetization and coherent oscillatory micromotions, persisting over timescales exceeding the interaction-limited coherence time ($T_2$) by more than an order of magnitude. Leveraging these unconventional dynamics, we develop a dynamical-freezing-enhanced ac magnetometry that extends optimal sensing times far beyond $T_2$, outperforming conventional dynamical decoupling magnetometry with a 4.3 dB sensitivity enhancement. Our results not only provide clear experimental observation of dynamical freezing -- a peculiar mechanism defying thermalization through emergent conservation laws -- but also establish a robust control method generally applicable to diverse physical platforms, with broad implications in quantum metrology and beyond. △ Less",
      "url": "https://arxiv.org/abs/2507.22982"
    }
  ]
}