{
  "author": "Eleanor Rieffel",
  "results": [
    {
      "title": "Fusion for High-Dimensional Linear Optical Quantum Computing with Improved Success Probability",
      "abstract": "Type-II fusion is a probabilistic entangling measurement that is essential to measurement-based linear optical quantum computing and can be used for quantum teleportation more broadly. However, it remains under-explored for high-dimensional qudits. Our main result gives a Type-II fusion protocol with proven success probability approximately $2/d^2$ for qudits of arbitrary dimension $d$. This generalizes a previous method which only applied to even-dimensional qudits. We believe this protocol to be the most efficient known protocol for Type-II fusion, with the $d=5$ case beating the previous record by a factor of approximately $723$. We discuss the construction of the required $(d-2)$-qudit ancillary state using a silicon spin qudit ancilla coupled to a microwave cavity through time-bin multiplexing. We then introduce a general framework of extra-dimensional corrections, a natural technique in linear optics that can be used to non-deterministically correct non-maximally-entangled projections into Bell measurements. We use this method to analyze and improve several different circuits for high-dimensional Type-II fusion and compare their benefits and drawbacks. △ Less",
      "url": "https://arxiv.org/abs/2505.16816"
    },
    {
      "title": "How to Build a Quantum Supercomputer: Scaling from Hundreds to Millions of Qubits",
      "abstract": "In the span of four decades, quantum computation has evolved from an intellectual curiosity to a potentially realizable technology. Today, small-scale demonstrations have become possible for quantum algorithmic primitives on hundreds of physical qubits and proof-of-principle error-correction on a single logical qubit. Nevertheless, despite significant progress and excitement, the path toward a full-stack scalable technology is largely unknown. There are significant outstanding quantum hardware, fabrication, software architecture, and algorithmic challenges that are either unresolved or overlooked. These issues could seriously undermine the arrival of utility-scale quantum computers for the foreseeable future. Here, we provide a comprehensive review of these scaling challenges. We show how the road to scaling could be paved by adopting existing semiconductor technology to build much higher-quality qubits, employing system engineering approaches, and performing distributed quantum computation within heterogeneous high-performance computing infrastructures. These opportunities for research and development could unlock certain promising applications, in particular, efficient quantum simulation/learning of quantum data generated by natural or engineered quantum systems. To estimate the true cost of such promises, we provide a detailed resource and sensitivity analysis for classically hard quantum chemistry calculations on surface-code error-corrected quantum computers given current, target, and desired hardware specifications based on superconducting qubits, accounting for a realistic distribution of errors. Furthermore, we argue that, to tackle industry-scale classical optimization and machine learning problems in a cost-effective manner, heterogeneous quantum-probabilistic computing with custom-designed accelerators should be considered as a complementary path toward scalability. △ Less",
      "url": "https://arxiv.org/abs/2411.10406"
    },
    {
      "title": "Optimized Quantum Simulation Algorithms for Scalar Quantum Field Theories",
      "abstract": "We provide practical simulation methods for scalar field theories on a quantum computer that yield improved asymptotics as well as concrete gate estimates for the simulation and physical qubit estimates using the surface code. We achieve these improvements through two optimizations. First, we consider a different approach for estimating the elements of the S-matrix. This approach is appropriate in general for 1+1D and for certain low-energy elastic collisions in higher dimensions. Second, we implement our approach using a series of different fault-tolerant simulation algorithms for Hamiltonians formulated both in the field occupation basis and field amplitude basis. Our algorithms are based on either second-order Trotterization or qubitization. The cost of Trotterization in occupation basis scales as $\\widetilde{O}(λN^7 |Ω|^3/(M^{5/2} ε^{3/2})$ where $λ$ is the coupling strength, $N$ is the occupation cutoff $|Ω|$ is the volume of the spatial lattice, $M$ is the mass of the particles and $ε$ is the uncertainty in the energy calculation used for the $S$-matrix determination. Qubitization in the field basis scales as $\\widetilde{O}(|Ω|^2 (k^2 Λ+kM^2)/ε)$ where $k$ is the cutoff in the field and $Λ$ is a scaled coupling constant. We find in both cases that the bounds suggest physically meaningful simulations can be performed using on the order of $4\\times 10^6$ physical qubits and $10^{12}$ $T$-gates which corresponds to roughly one day on a superconducting quantum computer with surface code and a cycle time of 100 ns, placing simulation of scalar field theory within striking distance of the gate counts for the best available chemistry simulation results. △ Less",
      "url": "https://arxiv.org/abs/2407.13819"
    },
    {
      "title": "Imposing Constraints on Driver Hamiltonians and Mixing Operators: From Theory to Practical Implementation",
      "abstract": "Constructing Driver Hamiltonians and Mixing Operators such that they satisfy constraints is an important ansatz construction for quantum algorithms. We give general algebraic expressions for finding Hamiltonian terms and analogously unitary primitives, that satisfy constraint embeddings and use these to give complexity characterizations of the related problems. Finding operators that enforce classical constraints is proven to be NP-Complete in the general case; algorithmic procedures with worse-case polynomial runtime to find any operators with a constant locality bound, applicable for many constraints. We then give algorithmic procedures to turn these algebraic primitives into Hamiltonian drivers and unitary mixers that can be used for Constrained Quantum Annealing (CQA) and Quantum Alternating Operator Ansatz (QAOA) constructions by tackling practical problems related to finding an appropriate set of reduced generators and defining corresponding drivers and mixers accordingly. We then apply these concepts to the construction of ansaetze for 1-in-3 SAT instances. We consider the ordinary x-mixer QAOA, a novel QAOA approach based on the maximally disjoint subset, and a QAOA approach based on the disjoint subset as well as higher order constraint satisfaction terms. We empirically benchmark these approaches on instances sized between 12 and 22, showing the best relative performance for the tailored ansaetze and that exponential curve fits on the results are consistent with a quadratic speedup by utilizing alternative ansaetze to the x-mixer. We provide very general algorithmic prescriptions for finding driver or mixing terms that satisfy embedded constraints that can be utilized to probe quantum speedups for constraints problems with linear, quadratic, or even higher order polynomial constraints. △ Less",
      "url": "https://arxiv.org/abs/2407.01975"
    },
    {
      "title": "Assessing and Advancing the Potential of Quantum Computing: A NASA Case Study",
      "abstract": "Quantum computing is one of the most enticing computational paradigms with the potential to revolutionize diverse areas of future-generation computational systems. While quantum computing hardware has advanced rapidly, from tiny laboratory experiments to quantum chips that can outperform even the largest supercomputers on specialized computational tasks, these noisy-intermediate scale quantum (NISQ) processors are still too small and non-robust to be directly useful for any real-world applications. In this paper, we describe NASA's work in assessing and advancing the potential of quantum computing. We discuss advances in algorithms, both near- and longer-term, and the results of our explorations on current hardware as well as with simulations, including illustrating the benefits of algorithm-hardware co-design in the NISQ era. This work also includes physics-inspired classical algorithms that can be used at application scale today. We discuss innovative tools supporting the assessment and advancement of quantum computing and describe improved methods for simulating quantum systems of various types on high-performance computing systems that incorporate realistic error models. We provide an overview of recent methods for benchmarking, evaluating, and characterizing quantum hardware for error mitigation, as well as insights into fundamental quantum physics that can be harnessed for computational purposes. △ Less",
      "url": "https://arxiv.org/abs/2406.15601"
    },
    {
      "title": "General protocols for the efficient distillation of indistinguishable photons",
      "abstract": "We introduce state-of-the-art protocols to distill indistinguishable photons, reducing distinguishability error rates by a factor of $n$, while using a modest amount of resources scaling only linearly in $n$. Our resource requirements are both significantly lower and have fewer hardware requirements than previous works, making large-scale distillation experimentally feasible for the first time. This efficient reduction of distinguishability error rates has direct applications to fault-tolerant linear optical quantum computation, potentially leading to improved thresholds for photon loss errors and allowing smaller code distances, thus reducing overall resource costs. Our protocols are based on Fourier transforms on finite abelian groups, special cases of which include the discrete Fourier transform and Hadamard matrices. This general perspective allows us to unify previous results on distillation protocols and introduce a large family of efficient schemes. We utilize the rich mathematical structure of Fourier transforms, including symmetries and related suppression laws, to quantify the performance of these distillation protocols both analytically and numerically. Finally, our work resolves an open question concerning suppression laws for the $n$-photon discrete Fourier transform: the suppression laws are exactly characterized by the well-known Zero Transmission Law if and only if $n$ is a prime power. △ Less",
      "url": "https://arxiv.org/abs/2404.14217"
    },
    {
      "title": "Classical and Quantum Distributed Algorithms for the Survivable Network Design Problem",
      "abstract": "We investigate distributed classical and quantum approaches for the survivable network design problem (SNDP), sometimes called the generalized Steiner problem. These problems generalize many complex graph problems of interest, such as the traveling salesperson problem, the Steiner tree problem, and the k-connected network problem. To our knowledge, no classical or quantum algorithms for the SNDP have been formulated in the distributed settings we consider. We describe algorithms that are heuristics for the general problem but give concrete approximation bounds under specific parameterizations of the SNDP, which in particular hold for the three aforementioned problems that SNDP generalizes. We use a classical, centralized algorithmic framework first studied in (Goemans & Bertsimas 1993) and provide a distributed implementation thereof. Notably, we obtain asymptotic quantum speedups by leveraging quantum shortest path computations in this framework, generalizing recent work of (Kerger et al. 2023). These results raise the question of whether there is a separation between the classical and quantum models for application-scale instances of the problems considered. △ Less",
      "url": "https://arxiv.org/abs/2404.10748"
    },
    {
      "title": "Syncopated Dynamical Decoupling for Suppressing Crosstalk in Quantum Circuits",
      "abstract": "Theoretically understanding and experimentally characterizing and modifying the underlying Hamiltonian of a quantum system is of utmost importance in achieving high-fidelity quantum gates for quantum computing. In this work, we explore the use of dynamical decoupling (DD) in characterizing undesired two-qubit couplings as well as the underlying single-qubit decoherence, and in suppressing them. We develop a syncopated dynamical decoupling technique which protects against decoherence and selectively targets unwanted two-qubit interactions, overcoming both significant hurdles to achieving precise quantum control and realizing quantum computing on many hardware prototypes. On a transmon-qubit-based superconducting quantum device, we identify separate white and $1/f$ noise components underlying the single-qubit decoherence and a static ZZ coupling between pairs of qubits. We suppress these errors using syncopated dynamical decoupling in two-qubit benchmarking experiments and significantly boost performance in a realistic algorithmic quantum circuit. △ Less",
      "url": "https://arxiv.org/abs/2403.07836"
    },
    {
      "title": "Dynamical Logical Qubits in the Bacon-Shor Code",
      "abstract": "The Bacon-Shor code is a quantum error correcting subsystem code composed of weight 2 check operators that admits a single logical qubit, and has distance $d$ on a $d \\times d$ square lattice. We show that when viewed as a Floquet code, by choosing an appropriate measurement schedule of the check operators, it can additionally host several dynamical logical qubits. Specifically, we identify a period 4 measurement schedule of the check operators that preserves logical information between the instantaneous stabilizer groups. Such a schedule measures not only the usual stabilizers of the Bacon-Shor code, but also additional stabilizers that protect the dynamical logical qubits against errors. We show that the code distance of these Floquet-Bacon-Shor codes scales as $Θ(d/\\sqrt{k})$ on an $n = d \\times d$ lattice with $k$ dynamical logical qubits, along with the logical qubit of the parent subsystem code. Unlike the usual Bacon-Shor code, the Floquet-Bacon-Shor code family introduced here can therefore saturate the subsystem bound $kd = O(n)$. Moreover, several errors are shown to be self-corrected purely by the measurement schedule itself. This work provides insights into the design space for dynamical codes and expands the known approaches for constructing Floquet codes. △ Less",
      "url": "https://arxiv.org/abs/2403.03291"
    },
    {
      "title": "Advancing Quantum Networking: Some Tools and Protocols for Ideal and Noisy Photonic Systems",
      "abstract": "Quantum networking at many scales will be critical to future quantum technologies and experiments on quantum systems. Photonic links enable quantum networking. They will connect co-located quantum processors to enable large-scale quantum computers, provide links between distant quantum computers to support distributed, delegated, and blind quantum computing, and will link distant nodes in space enabling new tests of fundamental physics. Here, we discuss recent work advancing photonic tools and protocols that support quantum networking. We provide analytical results and numerics for the effect of distinguishability errors on key photonic circuits; we considered a variety of error models and developed new metrics for benchmarking the quality of generated photonic states. We review a distillation protocol by one of the authors that mitigates distinguishability errors. We also review recent results by a subset of the authors on the efficient simulation of photonic circuits via approximation by coherent states. We study some interactions between the theory of universal sets, unitary t-designs, and photonics: while many of the results we state in this direction may be known to experts, we aim to bring them to the attention of the broader quantum information science community and to phrase them in ways that are more familiar to this community. We prove, translating a result from representation theory, that there are no non-universal infinite closed $2$-designs in $U(V)$ when $\\dim V \\geq 2$. As a consequence, we observe that linear optical unitaries form a $1$-design but not a 2-design. Finally, we apply a result of Oszmaniec and Zimborás to prove that augmenting the linear optical unitaries with any nontrivial SNAP gate is sufficient to achieve universality. △ Less",
      "url": "https://arxiv.org/abs/2403.02515"
    },
    {
      "title": "Benchmarking the Operation of Quantum Heuristics and Ising Machines: Scoring Parameter Setting Strategies on Optimization Applications",
      "abstract": "We discuss guidelines for evaluating the performance of parameterized stochastic solvers for optimization problems, with particular attention to systems that employ novel hardware, such as digital quantum processors running variational algorithms, analog processors performing quantum annealing, or coherent Ising Machines. We illustrate through an example a benchmarking procedure grounded in the statistical analysis of the expectation of a given performance metric measured in a test environment. In particular, we discuss the necessity and cost of setting parameters that affect the algorithm's performance. The optimal value of these parameters could vary significantly between instances of the same target problem. We present an open-source software package that facilitates the design, evaluation, and visualization of practical parameter tuning strategies for complex use of the heterogeneous components of the solver. We examine in detail an example using parallel tempering and a simulator of a photonic Coherent Ising Machine computing and display the scoring of an illustrative baseline family of parameter-setting strategies that feature an exploration-exploitation trade-off. △ Less",
      "url": "https://arxiv.org/abs/2402.10255"
    },
    {
      "title": "Design and execution of quantum circuits using tens of superconducting qubits and thousands of gates for dense Ising optimization problems",
      "abstract": "We develop a hardware-efficient ansatz for variational optimization, derived from existing ansatze in the literature, that parametrizes subsets of all interactions in the Cost Hamiltonian in each layer. We treat gate orderings as a variational parameter and observe that doing so can provide significant performance boosts in experiments. We carried out experimental runs of a compilation-optimized implementation of fully-connected Sherrington-Kirkpatrick Hamiltonians on a 50-qubit linear-chain subsystem of Rigetti Aspen-M-3 transmon processor. Our results indicate that, for the best circuit designs tested, the average performance at optimized angles and gate orderings increases with circuit depth (using more parameters), despite the presence of a high level of noise. We report performance significantly better than using a random guess oracle for circuits involving up to approx 5000 two-qubit and approx 5000 one-qubit native gates. We additionally discuss various takeaways of our results toward more effective utilization of current and future quantum processors for optimization. △ Less",
      "url": "https://arxiv.org/abs/2308.12423"
    },
    {
      "title": "Generating Hard Ising Instances With Planted Solutions Using Post-Quantum Cryptographic Protocols",
      "abstract": "In this paper we present a novel method to generate hard instances with planted solutions based on the public-private McEliece post-quantum cryptographic protocol. Unlike other planting methods rooted in the infinite-size statistical analysis, our cryptographic protocol generates instances which are all hard (in cryptographic terms), with the hardness tuned by the size of the private key, and with a guaranteed unique ground state. More importantly, because of the private-public key protocol, planted solutions cannot be easily recovered by a direct inspection of the planted instances without the knowledge of the private key used to generate them, therefore making our protocol suitable to test and evaluate quantum devices without the risk of \"backdoors\" being exploited. △ Less",
      "url": "https://arxiv.org/abs/2308.09704"
    },
    {
      "title": "Scrambling and operator entanglement in local non-Hermitian quantum systems",
      "abstract": "The breakdown of Lieb-Robinson bounds in local, non-Hermitian quantum systems opens up the possibility for a rich landscape of quantum many-body phenomenology. We elucidate this by studying information scrambling and quantum chaos in non-Hermitian variants of paradigmatic local quantum spin-chain models. We utilize a mixture of exact diagonalization and tensor network techniques for our numerical results and focus on three dynamical quantities: (i) out-of-time-ordered correlators (OTOCs), (ii) operator entanglement of the dynamics, and (iii) entanglement growth following a quench from product initial states. We show that while OTOCs fail to capture information scrambling in a simple, local, non-Hermitian transverse-field Ising model, the closely related operator entanglement is a robust measure of dynamical properties of interest. Moreover, we show that the short-time growth of operator entanglement can generically detect ``entanglement phase transitions'' in these systems while its long-time average is shown to be a reliable indicator of quantum chaos and entanglement phases. This allows us to extend operator entanglement based diagnostics from previous works on closed and open quantum systems, to the new arena of monitored quantum dynamics. Finally, we remark on the efficacy of these dynamical quantities in detecting integrability/chaos in the presence of continuous monitoring. △ Less",
      "url": "https://arxiv.org/abs/2305.12054"
    },
    {
      "title": "Mind the $\\tilde{\\mathcal{O}}$: Asymptotically Better, but Still Impractical, Quantum Distributed Algorithms",
      "abstract": "The CONGEST and CONGEST-CLIQUE models have been carefully studied to represent situations where the communication bandwidth between processors in a network is severely limited. Messages of only $O(log(n))$ bits of information each may be sent between processors in each round. The quantum versions of these models allow the processors instead to communicate and compute with quantum bits under the same bandwidth limitations. This leads to the following natural research question: What problems can be solved more efficiently in these quantum models than in the classical ones? Building on existing work, we contribute to this question in two ways. Firstly, we present two algorithms in the Quantum CONGEST-CLIQUE model of distributed computation that succeed with high probability; one for producing an approximately optimal Steiner Tree, and one for producing an exact directed minimum spanning tree, each of which uses $\\tilde{O}(n^{1/4})$ rounds of communication and $\\tilde{O}(n^{9/4})$ messages, where $n$ is the number of nodes in the network. The algorithms thus achieve a lower asymptotic round and message complexity than any known algorithms in the classical CONGEST-CLIQUE model. At a high level, we achieve these results by combining classical algorithmic frameworks with quantum subroutines. An existing framework for using distributed version of Grover's search algorithm to accelerate triangle finding lies at the core of the asymptotic speedup. Secondly, we carefully characterize the constants and logarithmic factors involved in our algorithms as well as related algorithms, otherwise commonly obscured by $\\tilde{O}$ notation. The analysis shows that some improvements are needed to render both our and existing related quantum and classical algorithms practical, as their asymptotic speedups only help for very large values of $n$. △ Less",
      "url": "https://arxiv.org/abs/2304.02825"
    },
    {
      "title": "Anomaly Detection in Aeronautics Data with Quantum-compatible Discrete Deep Generative Model",
      "abstract": "Deep generative learning cannot only be used for generating new data with statistical characteristics derived from input data but also for anomaly detection, by separating nominal and anomalous instances based on their reconstruction quality. In this paper, we explore the performance of three unsupervised deep generative models -- variational autoencoders (VAEs) with Gaussian, Bernoulli, and Boltzmann priors -- in detecting anomalies in flight-operations data of commercial flights consisting of multivariate time series. We devised two VAE models with discrete latent variables (DVAEs), one with a factorized Bernoulli prior and one with a restricted Boltzmann machine (RBM) as prior, because of the demand for discrete-variable models in machine-learning applications and because the integration of quantum devices based on two-level quantum systems requires such models. The DVAE with RBM prior, using a relatively simple -- and classically or quantum-mechanically enhanceable -- sampling technique for the evolution of the RBM's negative phase, performed better than the Bernoulli DVAE and on par with the Gaussian model, which has a continuous latent space. Our studies demonstrate the competitiveness of a discrete deep generative model with its Gaussian counterpart on anomaly-detection tasks. Moreover, the DVAE model with RBM prior can be easily integrated with quantum sampling by outsourcing its generative process to measurements of quantum states obtained from a quantum annealer or gate-model device. △ Less",
      "url": "https://arxiv.org/abs/2303.12302"
    },
    {
      "title": "Quantum-Enhanced Greedy Combinatorial Optimization Solver",
      "abstract": "Combinatorial optimization is a broadly attractive area for potential quantum advantage, but no quantum algorithm has yet made the leap. Noise in quantum hardware remains a challenge, and more sophisticated quantum-classical algorithms are required to bolster their performance. Here, we introduce an iterative quantum heuristic optimization algorithm to solve combinatorial optimization problems. The quantum algorithm reduces to a classical greedy algorithm in the presence of strong noise. We implement the quantum algorithm on a programmable superconducting quantum system using up to 72 qubits for solving paradigmatic Sherrington-Kirkpatrick Ising spin glass problems. We find the quantum algorithm systematically outperforms its classical greedy counterpart, signaling a quantum enhancement. Moreover, we observe an absolute performance comparable with a state-of-the-art semidefinite programming method. Classical simulations of the algorithm illustrate that a key challenge to reaching quantum advantage remains improving the quantum device characteristics. △ Less",
      "url": "https://arxiv.org/abs/2303.05509"
    },
    {
      "title": "A Parameter Setting Heuristic for the Quantum Alternating Operator Ansatz",
      "abstract": "Parameterized quantum circuits are widely studied approaches for tackling optimization problems. A prominent example is the Quantum Alternating Operator Ansatz (QAOA), an approach that builds off the structure of the Quantum Approximate Optimization Algorithm. Finding high-quality parameters efficiently for QAOA remains a major challenge in practice. In this work, we introduce a classical strategy for parameter setting, suitable for common cases in which the number of distinct cost values grows only polynomially with the problem size. The crux of our strategy is that we replace the cost function expectation value step of QAOA with a parameterized model that can be efficiently evaluated classically. This model is based on empirical observations that QAOA states have large overlaps with states where variable configurations with the same cost have the same amplitude, which we define as Perfect Homogeneity. We thus define a Classical Homogeneous Proxy for QAOA in which Perfect Homogeneity holds exactly, and which yields information describing both states and expectation values. We classically determine high-quality parameters for this proxy, and use these parameters in QAOA, an approach we label the Homogeneous Heuristic for Parameter Setting. We numerically examine this heuristic for MaxCut on random graphs. For up to $3$ QAOA levels we are easily able to find parameters that match approximation ratios returned by previous globally optimized approaches. For levels up to $20$ we obtain parameters with approximation ratios monotonically increasing with depth, while a strategy that uses parameter transfer instead fails to converge with comparable classical resources. These results suggest that our heuristic may find good parameters in regimes that are intractable with noisy intermediate-scale quantum devices. Finally, we outline how our heuristic may be applied to wider classes of problems. △ Less",
      "url": "https://arxiv.org/abs/2211.09270"
    },
    {
      "title": "A \"thoughtful\" Local Friendliness no-go theorem: a prospective experiment with new assumptions to suit",
      "abstract": "A recent paper by two of us and co-workers, based on an extended Wigner's friend scenario, demonstrated that certain empirical correlations predicted by quantum theory (QT) violate inequalities derived from a set of metaphysical assumptions we called \"Local Friendliness\" (LF). These assumptions are strictly weaker than those used for deriving Bell inequalities. Crucial to the theorem was the premise that a quantum system with reversible evolution could be an observer (colloquially, a \"friend\"). However, that paper was noncommittal on what would constitute an observer for the purpose of an experiment. Here, we present a new LF no-go theorem which takes seriously the idea that a system's having *thoughts* is a sufficient condition for it to be an observer. Our new derivation of the LF inequalities uses four metaphysical assumptions, three of which are thought-related, including one that is explicitly called \"Friendliness\". These four assumptions, in conjunction, allow one to derive LF inequalities for experiments involving the type of system that \"Friendliness\" refers to. In addition to these four metaphysical assumptions, this new no-go theorem requires two assumptions about what is *technologically* feasible: Human-Level Artificial Intelligence, and Universal Quantum Computing which is fast and large scale. The latter is often motivated by the belief that QT is universal, but this is *not* an assumption of the theorem. The intent of the new theorem is to give a clear goal for future experimentalists, and a clear motivation for trying to achieve that goal. We review various approaches to QT in light of our theorem. The popular stance that \"quantum theory needs no interpretation\" does not question any of our assumptions and so is ruled out. Finally, we quantitatively discuss how difficult the experiment we envisage would be, and briefly discuss milestones on the paths towards it. △ Less",
      "url": "https://arxiv.org/abs/2209.08491"
    },
    {
      "title": "Towards solving the Fermi-Hubbard model via tailored quantum annealers",
      "abstract": "The Fermi-Hubbard model (FHM) on a two dimensional square lattice has long been an important testbed and target for simulating fermionic Hamiltonians on quantum hardware. We present an alternative for quantum simulation of FHMs based on an adiabatic protocol that could be an attractive target for next generations of quantum annealers. Our results rely on a recently introduced low-weight encoding that allows the FHM to be expressed in terms of Pauli operators with locality of at most three. We theoretically and numerically determine promising quantum annealing setups for both interacting 2D spinless and spinful systems, that enable to reach near the ground state solution with high fidelity for systems as big as $6\\times 6$ (spinless) and $4\\times 3$ (spinful). Moreover, we demonstrate the scaling properties of the minimal gap and analyze robustness of the protocol against control noise. Additionally, we identify and discuss basic experimental requirements to construct near term annealing hardware tailored to simulate these problems. Finally, we perform a detailed resource estimation for the introduced adiabatic protocol, and discuss pros and cons of this approach relative to gate-based approaches for near-term platforms. △ Less",
      "url": "https://arxiv.org/abs/2207.14374"
    },
    {
      "title": "Advantage of pausing: parameter setting for quantum annealers",
      "abstract": "Prior work showed the efficacy of pausing midanneal: such a pause improved the probability of success by orders of magnitude in a class of native problem instances and improved the time to solution in a class of embedded problem instances. A physics-based picture provides qualitative suggestions for where pausing midanneal is effective, for the interplay between annealing schedule parameters and other annealing properties and parameters such as embedding size and strength of the ferromagnetic coupling $|J_F|$, and for the conditions under which pausing can improve the time to solution. Here, through demonstrations on an updated annealing architecture that has higher connectivity than previous annealers, and on multiple embedded problem classes, we are able to confirm various aspects of this picture. We demonstrate the robustness of the optimal pause parameters across platforms and problem classes, explore how to set $|J_F|$ to optimize performance in different scenarios, and provide empirical evidence that short pauses trump longer overall annealing times in time to solution. We also identify the number of different coefficients in a problem as a predictor of problem hardness, and explore its interplay with the optimal $|J_F|$ and embedding size. Based on these results we are able to present qualitative guidelines for parameter setting in quantum annealers. △ Less",
      "url": "https://arxiv.org/abs/2205.12936"
    },
    {
      "title": "Logical shadow tomography: Efficient estimation of error-mitigated observables",
      "abstract": "We introduce a technique to estimate error-mitigated expectation values on noisy quantum computers. Our technique performs shadow tomography on a logical state to produce a memory-efficient classical reconstruction of the noisy density matrix. Using efficient classical post-processing, one can mitigate errors by projecting a general nonlinear function of the noisy density matrix into the codespace. The subspace expansion and virtual distillation can be viewed as special cases of the new framekwork. We show our method is favorable in the quantum and classical resources overhead. Relative to subspace expansion which requires $O\\left(2^{N} \\right)$ samples to estimate a logical Pauli observable with $[[N, k]]$ error correction code, our technique requires only $O\\left(4^{k} \\right)$ samples. Relative to virtual distillation, our technique can compute powers of the density matrix without additional copies of quantum states or quantum memory. We present numerical evidence using logical states encoded with up to sixty physical qubits and show fast convergence to error-free expectation values with only $10^5$ samples under 1% depolarizing noise. △ Less",
      "url": "https://arxiv.org/abs/2203.07263"
    },
    {
      "title": "Dual Map Framework for Noise Characterization of Quantum Computers",
      "abstract": "In order to understand the capabilities and limitations of quantum computers, it is necessary to develop methods that efficiently characterize and benchmark error channels present on these devices. In this paper, we present a method that faithfully reconstructs a marginal (local) approximation of the effective noise (MATEN) channel, that acts as a single layer at the end of the circuit. We first introduce a dual map framework that allows us to analytically derive expectation values of observables with respect to noisy circuits. These findings are supported by numerical simulations of the quantum approximate optimization algorithm (QAOA) that also justify the MATEN, even in the presence of non-local errors that occur during a circuit. Finally, we demonstrate the performance of the method on Rigetti's Aspen-9 quantum computer for QAOA circuits up to six qubits, successfully predicting the observed measurements on a majority of the qubits. △ Less",
      "url": "https://arxiv.org/abs/2112.04414"
    },
    {
      "title": "Inter-generational comparison of quantum annealers in solving hard scheduling problems",
      "abstract": "We compare the performance of four quantum annealers, the D-Wave Two, 2X, 2000Q, and Advantage in solving an identical ensemble of a parametrized family of scheduling problems. These problems are NP-complete and, in fact, equivalent to vertex coloring problems. They are also practically motivated and closely connected to planning problems from artificial intelligence. We examine factors contributing to the performance differences while separating the contributions from hardware upgrades, support for shorter anneal times, and possible optimization of ferromagnetic couplings. While shorter anneal times can improve the time to solution (TTS) at any given problem size, the scaling of TTS with respect to the problem size worsens for shorter anneal times. In contrast, optimizing the ferromagnetic coupling improves both the absolute TTS and the scaling. There is a statistically significant improvement in performance between D-Wave Two and 2X and from all older generation annealers to Advantage, even when operated under identical anneal time and ferromagnetic couplings. However, the performance improvement from 2X to 2000Q requires the anneal time and ferromagnetic couplings to be optimized. Overall, owing to these inter-generational hardware improvements and optimizations, the scaling exponent reduces from $1.01 \\pm 0.01$ on Two to $0.173 \\pm 0.009$ on Advantage. △ Less",
      "url": "https://arxiv.org/abs/2112.00727"
    },
    {
      "title": "HybridQ: A Hybrid Simulator for Quantum Circuits",
      "abstract": "Developing state-of-the-art classical simulators of quantum circuits is of utmost importance to test and evaluate early quantum technology and understand the true potential of full-blown error-corrected quantum computers. In the past few years, multiple theoretical and numerical advances have continuously pushed the boundary of what is classically simulable, hence the development of a plethora of tools which are often limited to a specific purpose or designed for a particular hardware (e.g. CPUs vs. GPUs). Moreover, such tools are typically developed using tailored languages and syntax, which makes it hard to compare results from, and create hybrid approaches using, different simulation techniques. To support unified and optimized use of these techniques across platforms, we developed HybridQ, a highly extensible platform designed to provide a common framework to integrate multiple state-of-the-art techniques to run on a variety of hardware. The philosophy behind its development has been driven by three main pillars: \"Easy to Use\", \"Easy to Extend\", and \"Use the Best Available Technology\". The powerful tools of HybridQ allow users to manipulate, develop, and extend noiseless and noisy circuits for different hardware architectures. HybridQ supports large-scale high-performance computing (HPC) simulations, automatically balancing workload among different processor nodes and enabling the use of multiple backends to maximize parallel efficiency. Everything is then glued together by a simple and expressive language that allows seamless switching from one technique to another as well as from one hardware to the next, without the need to write lengthy translations, thus greatly simplifying the development of new hybrid algorithms and techniques. △ Less",
      "url": "https://arxiv.org/abs/2111.06868"
    },
    {
      "title": "Mixer-Phaser Ansätze for Quantum Optimization with Hard Constraints",
      "abstract": "We introduce multiple parametrized circuit ansätze and present the results of a numerical study comparing their performance with a standard Quantum Alternating Operator Ansatz approach. The ansätze are inspired by mixing and phase separation in the QAOA, and also motivated by compilation considerations with the aim of running on near-term superconducting quantum processors. The methods are tested on random instances of a weighted quadratic binary constrained optimization problem that is fully connected for which the space of feasible solutions has constant Hamming weight. For the parameter setting strategies and evaluation metric used, the average performance achieved by the QAOA is effectively matched by the one obtained by a \"mixer-phaser\" ansatz that can be compiled in less than half-depth of standard QAOA on most superconducting qubit processors. △ Less",
      "url": "https://arxiv.org/abs/2107.06651"
    },
    {
      "title": "Analytical Framework for Quantum Alternating Operator Ansätze",
      "abstract": "We develop a framework for analyzing layered quantum algorithms such as quantum alternating operator ansätze. Our framework relates quantum cost gradient operators, derived from the cost and mixing Hamiltonians, to classical cost difference functions that reflect cost function neighborhood structure. By considering QAOA circuits from the Heisenberg picture, we derive exact general expressions for expectation values as series expansions in the algorithm parameters, cost gradient operators, and cost difference functions. This enables novel interpretability and insight into QAOA behavior in various parameter regimes. For single-level QAOA1 we show the leading-order changes in the output probabilities and cost expectation value explicitly in terms of classical cost differences, for arbitrary cost functions. This demonstrates that, for sufficiently small positive parameters, probability flows from lower to higher cost states on average. By selecting signs of the parameters, we can control the direction of flow. We use these results to derive a classical random algorithm emulating QAOA1 in the small-parameter regime, i.e., that produces bitstring samples with the same probabilities as QAOA1 up to small error. For deeper QAOAp circuits we apply our framework to derive analogous and additional results in several settings. In particular we show QAOA always beats random guessing. We describe how our framework incorporates cost Hamiltonian locality for specific problem classes, including causal cone approaches, and applies to QAOA performance analysis with arbitrary parameters. We illuminate our results with a number of examples including applications to QUBO problems, MaxCut, and variants of MaxSat. We illustrate the application to QAOA circuits using mixing unitaries beyond the transverse-field mixer through two examples of constrained optimization, Max Independent Set and Graph Coloring. △ Less",
      "url": "https://arxiv.org/abs/2105.06996"
    },
    {
      "title": "Practical Verification of Quantum Properties in Quantum Approximate Optimization Runs",
      "abstract": "In order to assess whether quantum resources can provide an advantage over classical computation, it is necessary to characterize and benchmark the non-classical properties of quantum algorithms in a practical manner. In this paper, we show that using measurements in no more than 3 out of the possible $3^N$ bases, one can not only reconstruct the single-qubit reduced density matrices and measure the ability to create coherent superpositions, but also possibly verify entanglement across all $N$ qubits participating in the algorithm. We introduce a family of generalized Bell-type observables for which we establish an upper bound to the expectation values in fully separable states by proving a generalization of the Cauchy-Schwarz inequality, which may serve of independent interest. We demonstrate that a subset of such observables can serve as entanglement witnesses for QAOA-MaxCut states, and further argue that they are especially well tailored for this purpose by defining and computing an entanglement potency metric on witnesses. A subset of these observables also certify, in a weaker sense, the entanglement in GHZ states, which share the $\\mathbb{Z}_2$ symmetry of QAOA-MaxCut. The construction of such witnesses follows directly from the cost Hamiltonian to be optimized, and not through the standard technique of using the projector of the state being certified. It may thus provide insights to construct similar witnesses for other variational algorithms prevalent in the NISQ era. We demonstrate our ideas with proof-of-concept experiments on the Rigetti Aspen-9 chip for ansatze containing up to 24 qubits. △ Less",
      "url": "https://arxiv.org/abs/2105.01639"
    },
    {
      "title": "Simulations of state-of-the-art fermionic neural network wave functions with diffusion Monte Carlo",
      "abstract": "Recently developed neural network-based \\emph{ab-initio} solutions (Pfau et. al arxiv:1909.02487v2) for finding ground states of fermionic systems can generate state-of-the-art results on a broad class of systems. In this work, we improve the results for this Ansatz with Diffusion Monte Carlo. Additionally, we introduce several modifications to the network (Fermi Net) and optimization method (Kronecker Factored Approximate Curvature) that reduce the number of required resources while maintaining or improving the modelling performance. In terms of the model, we remove redundant computations and alter the way data is handled in the permutation equivariant function. The Diffusion Monte Carlo results exceed or match state-of-the-art performance for all systems investigated: atomic systems Be-Ne, and the carbon cation C$^+$. △ Less",
      "url": "https://arxiv.org/abs/2103.12570"
    },
    {
      "title": "Perils of Embedding for Quantum Sampling",
      "abstract": "Given quantum hardware that enables sampling from a family of natively implemented Hamiltonians, how well can one use that hardware to sample from a Hamiltonian outside that family? A common approach is to minor embed the desired Hamiltonian in a native Hamiltonian. In Phys. Rev. Research 2, 023020 (2020) it was shown that minor embedding can be detrimental for classical thermal sampling. Here, we generalize these results by considering quantum thermal sampling in the transverse-field Ising model, i.e. sampling a Hamiltonian with non-zero off diagonal terms. To study these systems numerically we introduce a modification to standard cluster update quantum Monte-Carlo (QMC) techniques, which allows us to much more efficiently obtain thermal samples of an embedded Hamiltonian, enabling us to simulate systems of much larger sizes and larger transverse-field strengths than would otherwise be possible. Our numerics focus on models that can be implemented on current quantum devices using planar two-dimensional lattices, which exhibit finite-temperature quantum phase transitions. Our results include: i) An estimate on the probability to sample the logical subspace directly as a function of transverse-field, temperature, and total system size, which agrees with QMC simulations. ii) We show that typically measured observables (diagonal energy and magnetization) are biased by the embedding process, in the regime of intermediate transverse field strength, meaning that the extracted values are not the same as in the native model. iii) By considering individual embedding realizations akin to 'realizations of disorder', we provide numerical evidence suggesting that as the embedding size is increased, the critical point shifts to increasingly large values of the transverse-field. △ Less",
      "url": "https://arxiv.org/abs/2103.07036"
    },
    {
      "title": "Quantum-accelerated constraint programming",
      "abstract": "Constraint programming (CP) is a paradigm used to model and solve constraint satisfaction and combinatorial optimization problems. In CP, problems are modeled with constraints that describe acceptable solutions and solved with backtracking tree search augmented with logical inference. In this paper, we show how quantum algorithms can accelerate CP, at both the levels of inference and search. Leveraging existing quantum algorithms, we introduce a quantum-accelerated filtering algorithm for the $\\texttt{alldifferent}$ global constraint and discuss its applicability to a broader family of global constraints with similar structure. We propose frameworks for the integration of quantum filtering algorithms within both classical and quantum backtracking search schemes, including a novel hybrid classical-quantum backtracking search method. This work suggests that CP is a promising candidate application for early fault-tolerant quantum computers and beyond. △ Less",
      "url": "https://arxiv.org/abs/2103.04502"
    },
    {
      "title": "Quantum algorithms with local particle number conservation: noise effects and error correction",
      "abstract": "Quantum circuits with local particle number conservation (LPNC) restrict the quantum computation to a subspace of the Hilbert space of the qubit register. In a noiseless or fault-tolerant quantum computation, such quantities are preserved. In the presence of noise, however, the evolution's symmetry could be broken and non-valid states could be sampled at the end of the computation. On the other hand, the restriction to a subspace in the ideal case suggest the possibility of more resource efficient error mitigation techniques for circuits preserving symmetries that are not possible for general circuits. Here, we analyze the probability of staying in such symmetry-preserved subspaces under noise, providing an exact formula for local depolarizing noise. We apply our findings to benchmark, under depolarizing noise, the symmetry robustness of XY-QAOA, which has local particle number conserving symmetries, and is a special case of the Quantum Alternating Operator Ansatz. We also analyze the influence of the choice of encoding the problem on the symmetry robustness of the algorithm and discuss a simple adaption of the bit flip code to correct for symmetry-breaking errors with reduced resources. △ Less",
      "url": "https://arxiv.org/abs/2011.06873"
    },
    {
      "title": "Character randomized benchmarking for non-multiplicity-free groups with applications to subspace, leakage, and matchgate randomized benchmarking",
      "abstract": "Randomized benchmarking (RB) is a powerful method for determining the error rate of experimental quantum gates. Traditional RB, however, is restricted to gatesets, such as the Clifford group, that form a unitary 2-design. The recently introduced character RB can benchmark more general gates using techniques from representation theory; up to now, however, this method has only been applied to \"multiplicity-free\" groups, a mathematical restriction on these groups. In this paper, we extend the original character RB derivation to explicitly treat non-multiplicity-free groups, and derive several applications. First, we derive a rigorous version of the recently introduced subspace RB, which seeks to characterize a set of one- and two-qubit gates that are symmetric under SWAP. Second, we develop a new leakage RB protocol that applies to more general groups of gates. Finally, we derive a scalable RB protocol for the matchgate group, a group that like the Clifford group is non-universal but becomes universal with the addition of one additional gate. This example provides one of the few examples of a scalable non-Clifford RB protocol. In all three cases, compared to existing theories, our method requires similar resources, but either provides a more accurate estimate of gate fidelity, or applies to a more general group of gates. In conclusion, we discuss the potential, and challenges, of using non-multiplicity-free character RB to develop new classes of scalable RB protocols and methods of characterizing specific gates. △ Less",
      "url": "https://arxiv.org/abs/2011.00007"
    },
    {
      "title": "Ferromagnetically shifting the power of pausing",
      "abstract": "We study the interplay between quantum annealing parameters in embedded problems, providing both deeper insights into the physics of these devices and pragmatic recommendations to improve performance on optimization problems. We choose as our test case the class of degree-bounded minimum spanning tree problems. Through runs on a D-Wave quantum annealer, we demonstrate that pausing in a specific time window in the anneal provides improvement in the probability of success and in the time-to-solution for these problems. The time window is consistent across problem instances, and its location is within the region suggested by prior theory and seen in previous results on native problems. An approach to enable gauge transformations for problems with the qubit coupling strength $J$ in an asymmetric range is presented and shown to significantly improve performance. We also confirm that the optimal pause location exhibits a shift with the magnitude of the ferromagnetic coupling, $|J_F|$, between physical qubits representing the same logical one. We extend the theoretical picture for pausing and thermalization in quantum annealing to the embedded case. This picture, along with perturbation theory analysis, and exact numerical results on small problems, confirms that the effective pause region moves earlier in the anneal as $|J_F|$ increases. It also suggests why pausing, while still providing significant benefit, has a less pronounced effect on embedded problems. △ Less",
      "url": "https://arxiv.org/abs/2006.08526"
    },
    {
      "title": "High-Dimensional Similarity Search with Quantum-Assisted Variational Autoencoder",
      "abstract": "Recent progress in quantum algorithms and hardware indicates the potential importance of quantum computing in the near future. However, finding suitable application areas remains an active area of research. Quantum machine learning is touted as a potential approach to demonstrate quantum advantage within both the gate-model and the adiabatic schemes. For instance, the Quantum-assisted Variational Autoencoder has been proposed as a quantum enhancement to the discrete VAE. We extend on previous work and study the real-world applicability of a QVAE by presenting a proof-of-concept for similarity search in large-scale high-dimensional datasets. While exact and fast similarity search algorithms are available for low dimensional datasets, scaling to high-dimensional data is non-trivial. We show how to construct a space-efficient search index based on the latent space representation of a QVAE. Our experiments show a correlation between the Hamming distance in the embedded space and the Euclidean distance in the original space on the Moderate Resolution Imaging Spectroradiometer (MODIS) dataset. Further, we find real-world speedups compared to linear search and demonstrate memory-efficient scaling to half a billion data points. △ Less",
      "url": "https://arxiv.org/abs/2006.07680"
    },
    {
      "title": "Augmented fidelities for single qubit gates",
      "abstract": "An average gate fidelity is a standard performance metric to quantify deviation between an ideal unitary gate transformation and its realistic experimental implementation. The average is taken with respect to states uniformly distributed over the full Hilbert space. We analytically (single-qubit) and numerically (two-qubit) show how this average changes if the uniform distribution condition is relaxed, replaced by parametrized distributions - polar cap and von Mises-Fisher distributions - and how the resulting fidelities can differentiate certain noise models. In particular, we demonstrate that Pauli channels with different noise rates along the three axes can be faithfully distinguished using these augmented fidelities. △ Less",
      "url": "https://arxiv.org/abs/2006.03086"
    },
    {
      "title": "Planning for Compilation of a Quantum Algorithm for Graph Coloring",
      "abstract": "The problem of compiling general quantum algorithms for implementation on near-term quantum processors has been introduced to the AI community. Previous work demonstrated that temporal planning is an attractive approach for part of this compilationtask, specifically, the routing of circuits that implement the Quantum Alternating Operator Ansatz (QAOA) applied to the MaxCut problem on a quantum processor architecture. In this paper, we extend the earlier work to route circuits that implement QAOA for Graph Coloring problems. QAOA for coloring requires execution of more, and more complex, operations on the chip, which makes routing a more challenging problem. We evaluate the approach on state-of-the-art hardware architectures from leading quantum computing companies. Additionally, we apply a planning approach to qubit initialization. Our empirical evaluation shows that temporal planning compares well to reasonable analytic upper bounds, and that solving qubit initialization with a classical planner generally helps temporal planners in finding shorter-makespan compilations for QAOA for Graph Coloring. These advances suggest that temporal planning can be an effective approach for more complex quantum computing algorithms and architectures. △ Less",
      "url": "https://arxiv.org/abs/2002.10917"
    },
    {
      "title": "Supplementary information for \"Quantum supremacy using a programmable superconducting processor\"",
      "abstract": "This is an updated version of supplementary information to accompany \"Quantum supremacy using a programmable superconducting processor\", an article published in the October 24, 2019 issue of Nature. The main article is freely available at https://www.nature.com/articles/s41586-019-1666-5. Summary of changes since arXiv:1910.11333v1 (submitted 23 Oct 2019): added URL for qFlex source code; added Erratum section; added Figure S41 comparing statistical and total uncertainty for log and linear XEB; new References [1,65]; miscellaneous updates for clarity and style consistency; miscellaneous typographical and formatting corrections. △ Less",
      "url": "https://arxiv.org/abs/1910.11333"
    },
    {
      "title": "Perils of Embedding for Sampling Problems",
      "abstract": "Advances in techniques for thermal sampling in classical and quantum systems would deepen understanding of the underlying physics. Unfortunately, one often has to rely solely on inexact numerical simulation, due to the intractability of computing the partition function in many systems of interest. Emerging hardware, such as quantum annealers, provide novel tools for such investigations, but it is well known that studying general, non-native systems on such devices requires graph minor embedding, at the expense of introducing additional variables. The effect of embedding for sampling is more pronounced than for optimization; for optimization one is just concerned with the ground state physics, whereas for sampling one needs to consider states at all energies. We argue that as the system size or the embedding size grows, the chance of a sample being in the subspace of interest - the logical subspace - can be exponentially suppressed. Though the severity of this scaling can be lessened through favorable parameter choices, certain physical constraints (such as a fixed temperature and range of couplings) provide hard limits on what is currently feasible. Furthermore, we show that up to some practical and reasonable assumptions, any type of post-processing to project samples back into the logical subspace will bias the resulting statistics. We introduce a new such technique, based on resampling, that substantially outperforms majority vote, which is shown to fail quite dramatically at preserving distribution properties. △ Less",
      "url": "https://arxiv.org/abs/1909.12184"
    },
    {
      "title": "Optimizing quantum heuristics with meta-learning",
      "abstract": "Variational quantum algorithms, a class of quantum heuristics, are promising candidates for the demonstration of useful quantum computation. Finding the best way to amplify the performance of these methods on hardware is an important task. Here, we evaluate the optimization of quantum heuristics with an existing class of techniques called `meta-learners'. We compare the performance of a meta-learner to Bayesian optimization, evolutionary strategies, L-BFGS-B and Nelder-Mead approaches, for two quantum heuristics (quantum alternating operator ansatz and variational quantum eigensolver), on three problems, in three simulation environments. We show that the meta-learner comes near to the global optima more frequently than all other optimizers we tested in a noisy parameter setting environment. We also find that the meta-learner is generally more resistant to noise, for example seeing a smaller reduction in performance in Noisy and Sampling environments and performs better on average by a `gain' metric than its closest comparable competitor L-BFGS-B. These results are an important indication that meta-learning and associated machine learning methods will be integral to the useful application of noisy near-term quantum computers. △ Less",
      "url": "https://arxiv.org/abs/1908.03185"
    },
    {
      "title": "Generalized swap networks for near-term quantum computing",
      "abstract": "The practical use of many types of near-term quantum computers requires accounting for their limited connectivity. One way of overcoming limited connectivity is to insert swaps in the circuit so that logical operations can be performed on physically adjacent qubits, which we refer to as solving the `routing via matchings' problem. We address the routing problem for families of quantum circuits defined by a hypergraph wherein each hyperedge corresponds to a potential gate. Our main result is that any unordered set of $k$-qubit gates on distinct $k$-qubit subsets of $n$ logical qubits can be ordered and parallelized in $O(n^{k-1})$ depth using a linear arrangement of $n$ physical qubits; the construction is completely general and achieves optimal scaling in the case where gates acting on all $\\binom{n}{k}$ sets of $k$ qubits are desired. We highlight two classes of problems for which our method is particularly useful. First, it applies to sets of mutually commuting gates, as in the (diagonal) phase separators of Quantum Alternating Operator Ansatz (Quantum Approximate Optimization Algorithm) circuits. For example, a single level of a QAOA circuit for Maximum Cut can be implemented in linear depth, and a single level for $3$-SAT in quadratic depth. Second, it applies to sets of gates that do not commute but for which compilation efficiency is the dominant criterion in their ordering. In particular, it can be adapted to Trotterized time-evolution of fermionic Hamiltonians under the Jordan-Wigner transformation, and also to non-standard mixers in QAOA. Using our method, a single Trotter step of the electronic structure Hamiltonian in an arbitrary basis of $n$ orbitals can be done in $O(n^3)$ depth while a Trotter step of the unitary coupled cluster singles and doubles method can be implemented in $O(n^2 η)$ depth, where $η$ is the number of electrons. △ Less",
      "url": "https://arxiv.org/abs/1905.05118"
    },
    {
      "title": "From Ansätze to Z-gates: a NASA View of Quantum Computing",
      "abstract": "For the last few years, the NASA Quantum Artificial Intelligence Laboratory (QuAIL) has been performing research to assess the potential impact of quantum computers on challenging computational problems relevant to future NASA missions. A key aspect of this research is devising methods to most effectively utilize emerging quantum computing hardware. Research questions include what experiments on early quantum hardware would give the most insight into the potential impact of quantum computing, the design of algorithms to explore on such hardware, and the development of tools to minimize the quantum resource requirements. We survey work relevant to these questions, with a particular emphasis on our recent work in quantum algorithms and applications, in elucidating mechanisms of quantum mechanics and their uses for quantum computational purposes, and in simulation, compilation, and physics-inspired classical algorithms. To our early application thrusts in planning and scheduling, fault diagnosis, and machine learning, we add thrusts related to robustness of communication networks and the simulation of many-body systems for material science and chemistry. We provide a brief update on quantum annealing work, but concentrate on gate-model quantum computing research advances within the last couple of years. △ Less",
      "url": "https://arxiv.org/abs/1905.02860"
    },
    {
      "title": "Establishing the Quantum Supremacy Frontier with a 281 Pflop/s Simulation",
      "abstract": "Noisy Intermediate-Scale Quantum (NISQ) computers are entering an era in which they can perform computational tasks beyond the capabilities of the most powerful classical computers, thereby achieving \"Quantum Supremacy\", a major milestone in quantum computing. NISQ Supremacy requires comparison with a state-of-the-art classical simulator. We report HPC simulations of hard random quantum circuits (RQC), which have been recently used as a benchmark for the first experimental demonstration of Quantum Supremacy, sustaining an average performance of 281 Pflop/s (true single precision) on Summit, currently the fastest supercomputer in the World. These simulations were carried out using qFlex, a tensor-network-based classical high-performance simulator of RQCs. Our results show an advantage of many orders of magnitude in energy consumption of NISQ devices over classical supercomputers. In addition, we propose a standard benchmark for NISQ computers based on qFlex. △ Less",
      "url": "https://arxiv.org/abs/1905.00444"
    },
    {
      "title": "Quantum-assisted associative adversarial network: Applying quantum annealing in deep learning",
      "abstract": "We present an algorithm for learning a latent variable generative model via generative adversarial learning where the canonical uniform noise input is replaced by samples from a graphical model. This graphical model is learned by a Boltzmann machine which learns low-dimensional feature representation of data extracted by the discriminator. A quantum annealer, the D-Wave 2000Q, is used to sample from this model. This algorithm joins a growing family of algorithms that use a quantum annealing subroutine in deep learning, and provides a framework to test the advantages of quantum-assisted learning in GANs. Fully connected, symmetric bipartite and Chimera graph topologies are compared on a reduced stochastically binarized MNIST dataset, for both classical and quantum annealing sampling methods. The quantum-assisted associative adversarial network successfully learns a generative model of the MNIST dataset for all topologies, and is also applied to the LSUN dataset bedrooms class for the Chimera topology. Evaluated using the Fréchet inception distance and inception score, the quantum and classical versions of the algorithm are found to have equivalent performance for learning an implicit generative model of the MNIST dataset. △ Less",
      "url": "https://arxiv.org/abs/1904.10573"
    },
    {
      "title": "$XY$-mixers: analytical and numerical results for QAOA",
      "abstract": "The Quantum Alternating Operator Ansatz (QAOA) is a promising gate-model meta-heuristic for combinatorial optimization. Applying the algorithm to problems with constraints presents an implementation challenge for near-term quantum resources. This work explores strategies for enforcing hard constraints by using $XY$-Hamiltonians as mixing operators (mixers). Despite the complexity of simulating the $XY$ model, we demonstrate that for problems represented through one-hot-encoding, certain classes of the mixer Hamiltonian can be implemented without Trotter error in depth $O(κ)$ where $κ$ is the number of assignable colors. We also specify general strategies for implementing QAOA circuits on all-to-all connected hardware graphs and linearly connected hardware graphs inspired by fermionic simulation techniques. Performance is validated on graph coloring problems that are known to be challenging for a given classical algorithm. The general strategy of using $XY$-mixers is borne out numerically, demonstrating a significant improvement over the general $X$-mixer, and moreover the generalized $W$-state yields better performance than easier-to-generate classical initial states when $XY$ mixers are used. △ Less",
      "url": "https://arxiv.org/abs/1904.09314"
    },
    {
      "title": "A flexible high-performance simulator for verifying and benchmarking quantum circuits implemented on real hardware",
      "abstract": "Here we present qFlex, a flexible tensor network based quantum circuit simulator. qFlex can compute both exact amplitudes, essential for the verification of the quantum hardware, as well as low fidelity amplitudes, in order to mimic sampling from Noisy Intermediate-Scale Quantum (NISQ) devices. In this work, we focus on random quantum circuits (RQCs) in the range of sizes expected for supremacy experiments. Fidelity $f$ simulations are performed at a cost that is $1/f$ lower than perfect fidelity ones. We also present a technique to eliminate the overhead introduced by rejection sampling in most tensor network approaches. We benchmark the simulation of square lattices and Google's Bristlecone QPU. Our analysis is supported by extensive simulations on NASA HPC clusters Pleiades and Electra. For our most computationally demanding simulation, the two clusters combined reached a peak of 20 PFLOPS (single precision), i.e., $64\\%$ of their maximum achievable performance, which represents the largest numerical computation in terms of sustained FLOPs and number of nodes utilized ever run on NASA HPC clusters. Finally, we introduce a novel multithreaded, cache-efficient tensor index permutation algorithm of general application. △ Less",
      "url": "https://arxiv.org/abs/1811.09599"
    },
    {
      "title": "Power of Pausing: Advancing Understanding of Thermalization in Experimental Quantum Annealers",
      "abstract": "We investigate alternative annealing schedules on the current generation of quantum annealing hardware (the D-Wave 2000Q), which includes the use of forward and reverse annealing with an intermediate pause. This work provides new insights into the inner workings of these devices (and quantum devices in general), particular into how thermal effects govern the system dynamics. We show that a pause mid-way through the anneal can cause a dramatic change in the output distribution, and we provide evidence suggesting thermalization is indeed occurring during such a pause. We demonstrate that upon pausing the system in a narrow region shortly after the minimum gap, the probability of successfully finding the ground state of the problem Hamiltonian can be increased over an order of magnitude. We relate this effect to relaxation (i.e. thermalization) after diabatic and thermal excitations that occur in the region near to the minimum gap. For a set of large-scale problems of up to 500 qubits, we demonstrate that the distribution returned from the annealer very closely matches a (classical) Boltzmann distribution of the problem Hamiltonian, albeit one with a temperature at least 1.5 times higher than the (effective) temperature of the annealer. Moreover, we show that larger problems are more likely to thermalize to a classical Boltzmann distribution. △ Less",
      "url": "https://arxiv.org/abs/1810.05881"
    },
    {
      "title": "Experimental investigation of performance differences between Coherent Ising Machines and a quantum annealer",
      "abstract": "Physical annealing systems provide heuristic approaches to solving NP-hard Ising optimization problems. Here, we study the performance of two types of annealing machines--a commercially available quantum annealer built by D-Wave Systems, and measurement-feedback coherent Ising machines (CIMs) based on optical parametric oscillator networks--on two classes of problems, the Sherrington-Kirkpatrick (SK) model and MAX-CUT. The D-Wave quantum annealer outperforms the CIMs on MAX-CUT on regular graphs of degree 3. On denser problems, however, we observe an exponential penalty for the quantum annealer ($\\exp(-α_\\textrm{DW} N^2)$) relative to CIMs ($\\exp(-α_\\textrm{CIM} N)$) for fixed anneal times, on both the SK model and on 50%-edge-density MAX-CUT, where the coefficients $α_\\textrm{CIM}$ and $α_\\textrm{DW}$ are problem-class-dependent. On instances with over $50$ vertices, a several-orders-of-magnitude time-to-solution difference exists between CIMs and the D-Wave annealer. An optimal-annealing-time analysis is also consistent with a significant projected performance difference. The difference in performance between the sparsely connected D-Wave machine and the measurement-feedback facilitated all-to-all connectivity of the CIMs provides strong experimental support for efforts to increase the connectivity of quantum annealers. △ Less",
      "url": "https://arxiv.org/abs/1805.05217"
    },
    {
      "title": "Comparing and Integrating Constraint Programming and Temporal Planning for Quantum Circuit Compilation",
      "abstract": "Recently, the makespan-minimization problem of compiling a general class of quantum algorithms into near-term quantum processors has been introduced to the AI community. The research demonstrated that temporal planning is a strong approach for a class of quantum circuit compilation (QCC) problems. In this paper, we explore the use of constraint programming (CP) as an alternative and complementary approach to temporal planning. We extend previous work by introducing two new problem variations that incorporate important characteristics identified by the quantum computing community. We apply temporal planning and CP to the baseline and extended QCC problems as both stand-alone and hybrid approaches. Our hybrid methods use solutions found by temporal planning to warm start CP, leveraging the ability of the former to find satisficing solutions to problems with a high degree of task optionality, an area that CP typically struggles with. The CP model, benefiting from inferred bounds on planning horizon length and task counts provided by the warm start, is then used to find higher quality solutions. Our empirical evaluation indicates that while stand-alone CP is only competitive for the smallest problems, CP in our hybridization with temporal planning out-performs stand-alone temporal planning in the majority of problem classes. △ Less",
      "url": "https://arxiv.org/abs/1803.06775"
    },
    {
      "title": "Quantum Annealing Applied to De-Conflicting Optimal Trajectories for Air Traffic Management",
      "abstract": "We present the mapping of a class of simplified air traffic management (ATM) problems (strategic conflict resolution) to quadratic unconstrained boolean optimization (QUBO) problems. The mapping is performed through an original representation of the conflict-resolution problem in terms of a conflict graph, where nodes of the graph represent flights and edges represent a potential conflict between flights. The representation allows a natural decomposition of a real world instance related to wind-optimal trajectories over the Atlantic ocean into smaller subproblems, that can be discretized and are amenable to be programmed in quantum annealers. In the study, we tested the new programming techniques and we benchmark the hardness of the instances using both classical solvers and the D-Wave 2X and D-Wave 2000Q quantum chip. The preliminary results show that for reasonable modeling choices the most challenging subproblems which are programmable in the current devices are solved to optimality with 99% of probability within a second of annealing time. △ Less",
      "url": "https://arxiv.org/abs/1711.04889"
    }
  ]
}